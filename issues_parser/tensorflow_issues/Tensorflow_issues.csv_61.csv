Issue Number,Issue Title,Issue Body
12280,Output quantized graphs,"Feature:

It'd be nice if it was easy to output as a u8 instead of a f32 between 0-1. I think a simple modification to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/quantization/quantize_graph.py should do it --quantized_output would be symmetrical"
12278,Integrating both Tensorflow an Caffe in C++,"

------------------------

### System information
- I  wrote custom code
- Ubuntu 14.04:
- TensorFlow installed from source
- TF 1.2.1:
-  C++ API 
-  No Bazel
- CUDA 7.5


### Describe the problem
Hello!

I am trying to write a code in C++ using the Eclipse IDE without Bazel. To get my project to do this I had to perform some hacks to generate the Tensorflow C++ APIs. Tensorflow C++ seems to be working fine. I can load a model and perform predictions. I have another code that uses a Caffe library to do something else. These both seem to work fine separately.

I want to integrate BOTH Tensorflow and Caffe in my code. When I include both libraries I get issues that seem to imply that both TF C++ and Caffe(which uses glog) redefine certain things. This leads to my code not working.
 
Is there a way to make TF use glog? Am I doing something wrong? It seems that this is a bug....

Thank you for any help


### Source code / logs
In file included from /usr/local/include/tf/tensorflow/core/platform/logging.h:25:0,
                 from /usr/local/include/tf/tensorflow/core/lib/core/status.h:25,
                 from /usr/local/include/tf/tensorflow/core/framework/variant.h:28,
                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,
                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,
                 from ../ME_T42.cpp:297:
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:87:0: warning: ""LOG"" redefined [enabled by default]
 #define LOG(severity) _TF_LOG_##severity
 ^
In file included from /bin/caffe-master/distribute/include/caffe/common.hpp:6:0,
                 from /bin/caffe-master/distribute/include/caffe/blob.hpp:8,
                 from /bin/caffe-master/distribute/include/caffe/caffe.hpp:7,
                 from /home/yevgeniy/Desktop/tovarish/ExtraLibraries/GOTURN-master/src/network/regressor.h:4,
                 from ../ME_T42.cpp:24:
/usr/include/glog/logging.h:483:0: note: this is the location of the previous definition
 #define LOG(severity) COMPACT_GOOGLE_LOG_ ## severity.stream()
 ^
In file included from /usr/local/include/tf/tensorflow/core/platform/logging.h:25:0,
                 from /usr/local/include/tf/tensorflow/core/lib/core/status.h:25,
                 from /usr/local/include/tf/tensorflow/core/framework/variant.h:28,
                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,
                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,
                 from ../ME_T42.cpp:297:
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:117:0: warning: ""VLOG_IS_ON"" redefined [enabled by default]
 #define VLOG_IS_ON(lvl) _VLOG_IS_ON(lvl, __FILE__)
 ^
In file included from /usr/include/glog/logging.h:490:0,
                 from /bin/caffe-master/distribute/include/caffe/common.hpp:6,
                 from /bin/caffe-master/distribute/include/caffe/blob.hpp:8,
                 from /bin/caffe-master/distribute/include/caffe/caffe.hpp:7,
                 from /home/yevgeniy/Desktop/tovarish/ExtraLibraries/GOTURN-master/src/network/regressor.h:4,
                 from ../ME_T42.cpp:24:
/usr/include/glog/vlog_is_on.h:82:0: note: this is the location of the previous definition
 #define VLOG_IS_ON(verboselevel)                                \
 ^
In file included from /usr/local/include/tf/tensorflow/core/platform/logging.h:25:0,
                 from /usr/local/include/tf/tensorflow/core/lib/core/status.h:25,
                 from /usr/local/include/tf/tensorflow/core/framework/variant.h:28,
                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,
                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,
                 from ../ME_T42.cpp:297:
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:119:0: warning: ""VLOG"" redefined [enabled by default]
 #define VLOG(lvl)                                   \
 ^
In file included from /bin/caffe-master/distribute/include/caffe/common.hpp:6:0,
                 from /bin/caffe-master/distribute/include/caffe/blob.hpp:8,
                 from /bin/caffe-master/distribute/include/caffe/caffe.hpp:7,
                 from /home/yevgeniy/Desktop/tovarish/ExtraLibraries/GOTURN-master/src/network/regressor.h:4,
                 from ../ME_T42.cpp:24:
/usr/include/glog/logging.h:1068:0: note: this is the location of the previous definition
 #define VLOG(verboselevel) LOG_IF(INFO, VLOG_IS_ON(verboselevel))
 ^
In file included from /usr/local/include/tf/tensorflow/core/platform/logging.h:25:0,
                 from /usr/local/include/tf/tensorflow/core/lib/core/status.h:25,
                 from /usr/local/include/tf/tensorflow/core/framework/variant.h:28,
                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,
                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,
                 from ../ME_T42.cpp:297:
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:127:0: warning: ""CHECK"" redefined [enabled by default]
 #define CHECK(condition)              \
 ^
In file included from /bin/caffe-master/distribute/include/caffe/common.hpp:6:0,
                 from /bin/caffe-master/distribute/include/caffe/blob.hpp:8,
                 from /bin/caffe-master/distribute/include/caffe/caffe.hpp:7,
                 from /home/yevgeniy/Desktop/tovarish/ExtraLibraries/GOTURN-master/src/network/regressor.h:4,
                 from ../ME_T42.cpp:24:
/usr/include/glog/logging.h:561:0: note: this is the location of the previous definition
 #define CHECK(condition)  \
 ^
In file included from /usr/local/include/tf/tensorflow/core/platform/logging.h:25:0,
                 from /usr/local/include/tf/tensorflow/core/lib/core/status.h:25,
                 from /usr/local/include/tf/tensorflow/core/framework/variant.h:28,
                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,
                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,
                 from ../ME_T42.cpp:297:
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:268:0: warning: ""CHECK_OP_LOG"" redefined [enabled by default]
 #define CHECK_OP_LOG(name, op, val1, val2)                            \
 ^
In file included from /bin/caffe-master/distribute/include/caffe/common.hpp:6:0,
                 from /bin/caffe-master/distribute/include/caffe/blob.hpp:8,
                 from /bin/caffe-master/distribute/include/caffe/caffe.hpp:7,
                 from /home/yevgeniy/Desktop/tovarish/ExtraLibraries/GOTURN-master/src/network/regressor.h:4,
                 from ../ME_T42.cpp:24:
/usr/include/glog/logging.h:719:0: note: this is the location of the previous definition
 #define CHECK_OP_LOG(name, op, val1, val2, log)                         \
 ^
In file included from /usr/local/include/tf/tensorflow/core/platform/logging.h:25:0,
                 from /usr/local/include/tf/tensorflow/core/lib/core/status.h:25,
                 from /usr/local/include/tf/tensorflow/core/framework/variant.h:28,
                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,
                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,
                 from ../ME_T42.cpp:297:
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:276:0: warning: ""CHECK_OP"" redefined [enabled by default]
 #define CHECK_OP(name, op, val1, val2) CHECK_OP_LOG(name, op, val1, val2)
 ^
In file included from /bin/caffe-master/distribute/include/caffe/common.hpp:6:0,
                 from /bin/caffe-master/distribute/include/caffe/blob.hpp:8,
                 from /bin/caffe-master/distribute/include/caffe/caffe.hpp:7,
                 from /home/yevgeniy/Desktop/tovarish/ExtraLibraries/GOTURN-master/src/network/regressor.h:4,
                 from ../ME_T42.cpp:24:
/usr/include/glog/logging.h:740:0: note: this is the location of the previous definition
 #define CHECK_OP(name, op, val1, val2) \
 ^
In file included from /usr/local/include/tf/tensorflow/core/platform/logging.h:25:0,
                 from /usr/local/include/tf/tensorflow/core/lib/core/status.h:25,
                 from /usr/local/include/tf/tensorflow/core/framework/variant.h:28,
                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,
                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,
                 from ../ME_T42.cpp:297:
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:279:0: warning: ""CHECK_EQ"" redefined [enabled by default]
 #define CHECK_EQ(val1, val2) CHECK_OP(Check_EQ, ==, val1, val2)
 ^
In file included from /bin/caffe-master/distribute/include/caffe/common.hpp:6:0,
                 from /bin/caffe-master/distribute/include/caffe/blob.hpp:8,
                 from /bin/caffe-master/distribute/include/caffe/caffe.hpp:7,
                 from /home/yevgeniy/Desktop/tovarish/ExtraLibraries/GOTURN-master/src/network/regressor.h:4,
                 from ../ME_T42.cpp:24:
/usr/include/glog/logging.h:765:0: note: this is the location of the previous definition
 #define CHECK_EQ(val1, val2) CHECK_OP(_EQ, ==, val1, val2)
 ^
In file included from /usr/local/include/tf/tensorflow/core/platform/logging.h:25:0,
                 from /usr/local/include/tf/tensorflow/core/lib/core/status.h:25,
                 from /usr/local/include/tf/tensorflow/core/framework/variant.h:28,
                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,
                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,
                 from ../ME_T42.cpp:297:
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:280:0: warning: ""CHECK_NE"" redefined [enabled by default]
 #define CHECK_NE(val1, val2) CHECK_OP(Check_NE, !=, val1, val2)
 ^
In file included from /bin/caffe-master/distribute/include/caffe/common.hpp:6:0,
                 from /bin/caffe-master/distribute/include/caffe/blob.hpp:8,
                 from /bin/caffe-master/distribute/include/caffe/caffe.hpp:7,
                 from /home/yevgeniy/Desktop/tovarish/ExtraLibraries/GOTURN-master/src/network/regressor.h:4,
                 from ../ME_T42.cpp:24:
/usr/include/glog/logging.h:766:0: note: this is the location of the previous definition
 #define CHECK_NE(val1, val2) CHECK_OP(_NE, !=, val1, val2)
 ^
In file included from /usr/local/include/tf/tensorflow/core/platform/logging.h:25:0,
                 from /usr/local/include/tf/tensorflow/core/lib/core/status.h:25,
                 from /usr/local/include/tf/tensorflow/core/framework/variant.h:28,
                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,
                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,
                 from ../ME_T42.cpp:297:
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:281:0: warning: ""CHECK_LE"" redefined [enabled by default]
 #define CHECK_LE(val1, val2) CHECK_OP(Check_LE, <=, val1, val2)
 ^
In file included from /bin/caffe-master/distribute/include/caffe/common.hpp:6:0,
                 from /bin/caffe-master/distribute/include/caffe/blob.hpp:8,
                 from /bin/caffe-master/distribute/include/caffe/caffe.hpp:7,
                 from /home/yevgeniy/Desktop/tovarish/ExtraLibraries/GOTURN-master/src/network/regressor.h:4,
                 from ../ME_T42.cpp:24:
/usr/include/glog/logging.h:767:0: note: this is the location of the previous definition
 #define CHECK_LE(val1, val2) CHECK_OP(_LE, <=, val1, val2)
 ^
In file included from /usr/local/include/tf/tensorflow/core/platform/logging.h:25:0,
                 from /usr/local/include/tf/tensorflow/core/lib/core/status.h:25,
                 from /usr/local/include/tf/tensorflow/core/framework/variant.h:28,
                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,
                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,
                 from ../ME_T42.cpp:297:
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:282:0: warning: ""CHECK_LT"" redefined [enabled by default]
 #define CHECK_LT(val1, val2) CHECK_OP(Check_LT, <, val1, val2)
 ^
In file included from /bin/caffe-master/distribute/include/caffe/common.hpp:6:0,
                 from /bin/caffe-master/distribute/include/caffe/blob.hpp:8,
                 from /bin/caffe-master/distribute/include/caffe/caffe.hpp:7,
                 from /home/yevgeniy/Desktop/tovarish/ExtraLibraries/GOTURN-master/src/network/regressor.h:4,
                 from ../ME_T42.cpp:24:
/usr/include/glog/logging.h:768:0: note: this is the location of the previous definition
 #define CHECK_LT(val1, val2) CHECK_OP(_LT, < , val1, val2)
 ^
In file included from /usr/local/include/tf/tensorflow/core/platform/logging.h:25:0,
                 from /usr/local/include/tf/tensorflow/core/lib/core/status.h:25,
                 from /usr/local/include/tf/tensorflow/core/framework/variant.h:28,
                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,
                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,
                 from ../ME_T42.cpp:297:
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:283:0: warning: ""CHECK_GE"" redefined [enabled by default]
 #define CHECK_GE(val1, val2) CHECK_OP(Check_GE, >=, val1, val2)
 ^
In file included from /bin/caffe-master/distribute/include/caffe/common.hpp:6:0,
                 from /bin/caffe-master/distribute/include/caffe/blob.hpp:8,
                 from /bin/caffe-master/distribute/include/caffe/caffe.hpp:7,
                 from /home/yevgeniy/Desktop/tovarish/ExtraLibraries/GOTURN-master/src/network/regressor.h:4,
                 from ../ME_T42.cpp:24:
/usr/include/glog/logging.h:769:0: note: this is the location of the previous definition
 #define CHECK_GE(val1, val2) CHECK_OP(_GE, >=, val1, val2)
 ^
In file included from /usr/local/include/tf/tensorflow/core/platform/logging.h:25:0,
                 from /usr/local/include/tf/tensorflow/core/lib/core/status.h:25,
                 from /usr/local/include/tf/tensorflow/core/framework/variant.h:28,
                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,
                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,
                 from ../ME_T42.cpp:297:
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:284:0: warning: ""CHECK_GT"" redefined [enabled by default]
 #define CHECK_GT(val1, val2) CHECK_OP(Check_GT, >, val1, val2)
 ^
In file included from /bin/caffe-master/distribute/include/caffe/common.hpp:6:0,
                 from /bin/caffe-master/distribute/include/caffe/blob.hpp:8,
                 from /bin/caffe-master/distribute/include/caffe/caffe.hpp:7,
                 from /home/yevgeniy/Desktop/tovarish/ExtraLibraries/GOTURN-master/src/network/regressor.h:4,
                 from ../ME_T42.cpp:24:
/usr/include/glog/logging.h:770:0: note: this is the location of the previous definition
 #define CHECK_GT(val1, val2) CHECK_OP(_GT, > , val1, val2)
 ^
In file included from /usr/local/include/tf/tensorflow/core/platform/logging.h:25:0,
                 from /usr/local/include/tf/tensorflow/core/lib/core/status.h:25,
                 from /usr/local/include/tf/tensorflow/core/framework/variant.h:28,
                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,
                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,
                 from ../ME_T42.cpp:297:
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:285:0: warning: ""CHECK_NOTNULL"" redefined [enabled by default]
 #define CHECK_NOTNULL(val)                                 \
 ^
In file included from /bin/caffe-master/distribute/include/caffe/common.hpp:6:0,
                 from /bin/caffe-master/distribute/include/caffe/blob.hpp:8,
                 from /bin/caffe-master/distribute/include/caffe/caffe.hpp:7,
                 from /home/yevgeniy/Desktop/tovarish/ExtraLibraries/GOTURN-master/src/network/regressor.h:4,
                 from ../ME_T42.cpp:24:
/usr/include/glog/logging.h:775:0: note: this is the location of the previous definition
 #define CHECK_NOTNULL(val) \
 ^
In file included from ../ME_T42.cpp:62:0:
/usr/local/include/tf/tensorflow/core/lib/core/status.h:37:7: error: expected identifier before ‘int’
 class Status {
       ^
In file included from /usr/local/include/tf/tensorflow/core/framework/variant.h:28:0,
                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,
                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,
                 from ../ME_T42.cpp:297:
/usr/local/include/tf/tensorflow/core/lib/core/status.h:37:14: error: expected unqualified-id before ‘{’ token
 class Status {
              ^
../ME_T42.cpp:2862:1: error: expected ‘}’ at end of input
 }
 ^
../ME_T42.cpp:197:12: warning: ‘c’ defined but not used [-Wunused-variable]
 static int c=0;
            ^
make: *** [ME_T42.o] Error 1
"
12277,Hash mismatch for cub project while building tf_label_image_example project in Release mode on Windows 64 bit build.,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64 Bit
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: Output copied below
- **Python version**: 3.5.3 (Anaconda)
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**:-
- **Exact command to reproduce**: Building tf_label_image_example project in Release mode

== cat /etc/issue ===============================================
MINGW64_NT-10.0 DESKTOP-SL66NSK 2.8.0(0.309/5/3) 2017-05-19 13:17 x86_64 Msys

== are we in docker =============================================
No

== compiler =====================================================
bash: c++: command not found

== uname -a =====================================================
MINGW64_NT-10.0 DESKTOP-SL66NSK 2.8.0(0.309/5/3) 2017-05-19 13:17 x86_64 Msys

== check pips ===================================================
numpy (1.12.1)
numpydoc (0.6.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensorflow'

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
bash: nvidia-smi: command not found

== cuda libs  ===================================================


### Describe the problem
I downloaded the latest source code from github and used the windows instructions:
https://www.tensorflow.org/install/install_windows
I used Anaconda installation instructions and followed this with installation from source.

8>      C:/CPA/Tensorflow/tensorflow/tensorflow/contrib/cmake/build/downloads/1.6.4.zip
8>    does not match expected value
8>      expected: '966d0c4f41e2bdc81aebf9ccfbf0baffaac5a74f00b826b06f4dee79b2bb8cee'
8>        actual: '4198e9c447a1e2a963b9e0e4d861df48baa47fb02e5e4fc507d1834afc99185a'
8>  -- Hash mismatch, removing...
8>  -- Retry after 60 seconds (attempt #5) ...
8>  -- Using src='https://github.com/NVlabs/cub/archive/1.6.4.zip'
MSB6006: ""cmd.exe"" exited with code 1.




### Source code / logs
1>------ Build started: Project: zlib, Configuration: Release x64 ------
2>------ Build started: Project: gif, Configuration: Release x64 ------
3>------ Build started: Project: farmhash, Configuration: Release x64 ------
4>------ Build started: Project: highwayhash, Configuration: Release x64 ------
5>------ Build started: Project: jpeg, Configuration: Release x64 ------
6>------ Build started: Project: lmdb, Configuration: Release x64 ------
7>------ Build started: Project: fft2d, Configuration: Release x64 ------
8>------ Build started: Project: cub, Configuration: Release x64 ------
4>  Performing update step for 'highwayhash'
8>  Building Custom Rule C:/CPA/Tensorflow/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
1>  Performing update step for 'zlib'
9>------ Build started: Project: lmdb_create_destination_dir, Configuration: Release x64 ------
10>------ Build started: Project: farmhash_create_destination_dir, Configuration: Release x64 ------
11>------ Build started: Project: gif_create_destination_dir, Configuration: Release x64 ------
12>------ Build started: Project: jpeg_create_destination_dir, Configuration: Release x64 ------
13>------ Build started: Project: re2, Configuration: Release x64 ------
14>------ Build started: Project: highwayhash_create_destination_dir, Configuration: Release x64 ------
15>------ Build started: Project: png, Configuration: Release x64 ------
8>  CMake does not need to re-run because C:/CPA/Tensorflow/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
8>  Creating directories for 'cub'
8>  Performing download step (download, verify and extract) for 'cub'
8>  -- Downloading...
8>     dst='C:/CPA/Tensorflow/tensorflow/tensorflow/contrib/cmake/build/downloads/1.6.4.zip'
8>     timeout='none'
8>  -- Using src='https://github.com/NVlabs/cub/archive/1.6.4.zip'
16>------ Build started: Project: protobuf, Configuration: Release x64 ------
17>------ Build started: Project: zlib_create_destination_dir, Configuration: Release x64 ------
13>  Performing update step for 're2'
18>------ Build started: Project: png_create_destination_dir, Configuration: Release x64 ------
19>------ Build started: Project: farmhash_copy_headers_to_destination, Configuration: Release x64 ------
20>------ Build started: Project: highwayhash_copy_headers_to_destination, Configuration: Release x64 ------
21>------ Build started: Project: gif_copy_headers_to_destination, Configuration: Release x64 ------
22>------ Build started: Project: jpeg_copy_headers_to_destination, Configuration: Release x64 ------
16>  Performing update step for 'protobuf'
23>------ Build started: Project: zlib_copy_headers_to_destination, Configuration: Release x64 ------
24>------ Build started: Project: grpc, Configuration: Release x64 ------
25>------ Build started: Project: jsoncpp, Configuration: Release x64 ------
8>  -- [download 0% complete]
8>  -- [download 1% complete]
26>------ Build started: Project: eigen, Configuration: Release x64 ------
27>------ Build started: Project: gemmlowp, Configuration: Release x64 ------
8>  -- [download 2% complete]
28>------ Build started: Project: png_copy_headers_to_destination, Configuration: Release x64 ------
29>------ Build started: Project: lmdb_copy_headers_to_destination, Configuration: Release x64 ------
8>  -- [download 3% complete]
24>  Performing update step for 'grpc'
25>  Performing update step for 'jsoncpp'
8>  -- [download 4% complete]
8>  -- [download 5% complete]
8>  -- [download 6% complete]
8>  -- [download 7% complete]
8>  -- [download 8% complete]
8>  -- [download 9% complete]
8>  -- [download 10% complete]
8>  -- [download 12% complete]
8>  -- [download 13% complete]
8>  -- [download 14% complete]
8>  -- [download 15% complete]
8>  -- [download 16% complete]
8>  -- [download 17% complete]
8>  -- [download 18% complete]
8>  -- [download 19% complete]
8>  -- [download 20% complete]
8>  -- [download 21% complete]
8>  -- [download 22% complete]
8>  -- [download 23% complete]
8>  -- [download 25% complete]
8>  -- [download 26% complete]
8>  -- [download 27% complete]
8>  -- [download 28% complete]
8>  -- [download 29% complete]
8>  -- [download 30% complete]
8>  -- [download 31% complete]
8>  -- [download 32% complete]
8>  -- [download 33% complete]
8>  -- [download 34% complete]
8>  -- [download 35% complete]
8>  -- [download 36% complete]
8>  -- [download 37% complete]
30>------ Build started: Project: create_cc_ops_header_dir, Configuration: Release x64 ------
8>  -- [download 39% complete]
8>  -- [download 40% complete]
8>  -- [download 41% complete]
8>  -- [download 42% complete]
8>  -- [download 43% complete]
8>  -- [download 44% complete]
8>  -- [download 45% complete]
8>  -- [download 46% complete]
8>  -- [download 47% complete]
8>  -- [download 48% complete]
8>  -- [download 50% complete]
8>  -- [download 51% complete]
8>  -- [download 52% complete]
8>  -- [download 54% complete]
8>  -- [download 56% complete]
8>  -- [download 57% complete]
8>  -- [download 58% complete]
8>  -- [download 59% complete]
8>  -- [download 60% complete]
8>  -- [download 61% complete]
8>  -- [download 62% complete]
8>  -- [download 63% complete]
8>  -- [download 65% complete]
8>  -- [download 66% complete]
8>  -- [download 67% complete]
8>  -- [download 68% complete]
8>  -- [download 69% complete]
8>  -- [download 70% complete]
8>  -- [download 71% complete]
8>  -- [download 72% complete]
8>  -- [download 73% complete]
8>  -- [download 75% complete]
8>  -- [download 78% complete]
8>  -- [download 80% complete]
8>  -- [download 83% complete]
8>  -- [download 86% complete]
8>  -- [download 88% complete]
8>  -- [download 89% complete]
8>  -- [download 90% complete]
8>  -- [download 91% complete]
8>  -- [download 92% complete]
8>  -- [download 93% complete]
8>  -- [download 94% complete]
8>  -- [download 95% complete]
8>  -- [download 96% complete]
8>  -- [download 97% complete]
8>  -- [download 98% complete]
8>  -- [download 99% complete]
8>  -- [download 100% complete]
8>  -- verifying file...
8>         file='C:/CPA/Tensorflow/tensorflow/tensorflow/contrib/cmake/build/downloads/1.6.4.zip'
8>  -- SHA256 hash of
8>      C:/CPA/Tensorflow/tensorflow/tensorflow/contrib/cmake/build/downloads/1.6.4.zip
8>    does not match expected value
8>      expected: '966d0c4f41e2bdc81aebf9ccfbf0baffaac5a74f00b826b06f4dee79b2bb8cee'
8>        actual: '4198e9c447a1e2a963b9e0e4d861df48baa47fb02e5e4fc507d1834afc99185a'
8>  -- Hash mismatch, removing...
8>  -- Retrying...
8>  -- Using src='https://github.com/NVlabs/cub/archive/1.6.4.zip'
8>  -- [download 0% complete]
8>  -- [download 1% complete]
8>  -- [download 2% complete]
8>  -- [download 3% complete]
8>  -- [download 4% complete]
8>  -- [download 5% complete]
8>  -- [download 6% complete]
8>  -- [download 7% complete]
8>  -- [download 8% complete]
8>  -- [download 9% complete]
8>  -- [download 10% complete]
8>  -- [download 11% complete]
8>  -- [download 12% complete]
8>  -- [download 13% complete]
8>  -- [download 14% complete]
8>  -- [download 15% complete]
8>  -- [download 16% complete]
8>  -- [download 17% complete]
8>  -- [download 18% complete]
8>  -- [download 19% complete]
8>  -- [download 21% complete]
8>  -- [download 23% complete]
8>  -- [download 24% complete]
8>  -- [download 25% complete]
8>  -- [download 26% complete]
8>  -- [download 27% complete]
8>  -- [download 29% complete]
8>  -- [download 30% complete]
8>  -- [download 31% complete]
8>  -- [download 32% complete]
8>  -- [download 33% complete]
8>  -- [download 34% complete]
8>  -- [download 35% complete]
8>  -- [download 36% complete]
8>  -- [download 37% complete]
8>  -- [download 38% complete]
8>  -- [download 39% complete]
8>  -- [download 40% complete]
8>  -- [download 41% complete]
8>  -- [download 42% complete]
8>  -- [download 43% complete]
8>  -- [download 44% complete]
8>  -- [download 45% complete]
8>  -- [download 46% complete]
8>  -- [download 47% complete]
8>  -- [download 49% complete]
8>  -- [download 50% complete]
8>  -- [download 51% complete]
8>  -- [download 52% complete]
8>  -- [download 53% complete]
8>  -- [download 54% complete]
8>  -- [download 55% complete]
8>  -- [download 56% complete]
8>  -- [download 57% complete]
8>  -- [download 58% complete]
8>  -- [download 59% complete]
8>  -- [download 60% complete]
8>  -- [download 61% complete]
8>  -- [download 62% complete]
8>  -- [download 64% complete]
8>  -- [download 65% complete]
8>  -- [download 66% complete]
8>  -- [download 67% complete]
8>  -- [download 68% complete]
8>  -- [download 70% complete]
8>  -- [download 71% complete]
8>  -- [download 72% complete]
8>  -- [download 73% complete]
8>  -- [download 74% complete]
8>  -- [download 75% complete]
8>  -- [download 76% complete]
8>  -- [download 79% complete]
8>  -- [download 81% complete]
8>  -- [download 84% complete]
8>  -- [download 87% complete]
8>  -- [download 89% complete]
8>  -- [download 90% complete]
8>  -- [download 91% complete]
8>  -- [download 92% complete]
8>  -- [download 93% complete]
8>  -- [download 94% complete]
8>  -- [download 95% complete]
8>  -- [download 96% complete]
8>  -- [download 97% complete]
8>  -- [download 98% complete]
8>  -- [download 99% complete]
8>  -- [download 100% complete]
8>  -- verifying file...
8>         file='C:/CPA/Tensorflow/tensorflow/tensorflow/contrib/cmake/build/downloads/1.6.4.zip'
8>  -- SHA256 hash of
8>      C:/CPA/Tensorflow/tensorflow/tensorflow/contrib/cmake/build/downloads/1.6.4.zip
8>    does not match expected value
8>      expected: '966d0c4f41e2bdc81aebf9ccfbf0baffaac5a74f00b826b06f4dee79b2bb8cee'
8>        actual: '4198e9c447a1e2a963b9e0e4d861df48baa47fb02e5e4fc507d1834afc99185a'
8>  -- Hash mismatch, removing...
8>  -- Retry after 5 seconds (attempt #2) ...
8>  -- Using src='https://github.com/NVlabs/cub/archive/1.6.4.zip'
8>  -- [download 0% complete]
8>  -- [download 1% complete]
8>  -- [download 2% complete]
8>  -- [download 3% complete]
8>  -- [download 4% complete]
8>  -- [download 5% complete]
8>  -- [download 6% complete]
8>  -- [download 7% complete]
8>  -- [download 9% complete]
8>  -- [download 10% complete]
8>  -- [download 11% complete]
8>  -- [download 12% complete]
8>  -- [download 13% complete]
8>  -- [download 14% complete]
8>  -- [download 15% complete]
8>  -- [download 16% complete]
8>  -- [download 17% complete]
8>  -- [download 18% complete]
8>  -- [download 19% complete]
8>  -- [download 20% complete]
8>  -- [download 21% complete]
8>  -- [download 22% complete]
8>  -- [download 23% complete]
8>  -- [download 24% complete]
8>  -- [download 25% complete]
8>  -- [download 26% complete]
8>  -- [download 27% complete]
8>  -- [download 28% complete]
8>  -- [download 29% complete]
8>  -- [download 30% complete]
8>  -- [download 31% complete]
8>  -- [download 32% complete]
8>  -- [download 33% complete]
8>  -- [download 34% complete]
8>  -- [download 35% complete]
8>  -- [download 36% complete]
8>  -- [download 37% complete]
8>  -- [download 38% complete]
8>  -- [download 39% complete]
8>  -- [download 40% complete]
8>  -- [download 41% complete]
8>  -- [download 42% complete]
8>  -- [download 43% complete]
8>  -- [download 44% complete]
8>  -- [download 45% complete]
8>  -- [download 46% complete]
8>  -- [download 47% complete]
8>  -- [download 48% complete]
8>  -- [download 49% complete]
8>  -- [download 50% complete]
8>  -- [download 51% complete]
8>  -- [download 52% complete]
8>  -- [download 53% complete]
8>  -- [download 54% complete]
8>  -- [download 55% complete]
8>  -- [download 56% complete]
8>  -- [download 57% complete]
8>  -- [download 58% complete]
8>  -- [download 59% complete]
8>  -- [download 60% complete]
8>  -- [download 61% complete]
8>  -- [download 62% complete]
8>  -- [download 63% complete]
8>  -- [download 64% complete]
8>  -- [download 65% complete]
8>  -- [download 66% complete]
8>  -- [download 67% complete]
8>  -- [download 69% complete]
8>  -- [download 70% complete]
8>  -- [download 71% complete]
8>  -- [download 72% complete]
8>  -- [download 74% complete]
8>  -- [download 75% complete]
8>  -- [download 77% complete]
8>  -- [download 80% complete]
8>  -- [download 83% complete]
8>  -- [download 85% complete]
8>  -- [download 88% complete]
8>  -- [download 89% complete]
8>  -- [download 90% complete]
8>  -- [download 91% complete]
8>  -- [download 92% complete]
8>  -- [download 93% complete]
8>  -- [download 94% complete]
8>  -- [download 95% complete]
8>  -- [download 96% complete]
8>  -- [download 97% complete]
8>  -- [download 98% complete]
8>  -- [download 99% complete]
8>  -- [download 100% complete]
8>  -- verifying file...
8>         file='C:/CPA/Tensorflow/tensorflow/tensorflow/contrib/cmake/build/downloads/1.6.4.zip'
8>  -- SHA256 hash of
8>      C:/CPA/Tensorflow/tensorflow/tensorflow/contrib/cmake/build/downloads/1.6.4.zip
8>    does not match expected value
8>      expected: '966d0c4f41e2bdc81aebf9ccfbf0baffaac5a74f00b826b06f4dee79b2bb8cee'
8>        actual: '4198e9c447a1e2a963b9e0e4d861df48baa47fb02e5e4fc507d1834afc99185a'
8>  -- Hash mismatch, removing...
8>  -- Retry after 5 seconds (attempt #3) ...
8>  -- Using src='https://github.com/NVlabs/cub/archive/1.6.4.zip'
8>  -- [download 0% complete]
8>  -- [download 1% complete]
8>  -- [download 2% complete]
8>  -- [download 3% complete]
8>  -- [download 4% complete]
8>  -- [download 5% complete]
8>  -- [download 6% complete]
8>  -- [download 7% complete]
8>  -- [download 8% complete]
8>  -- [download 9% complete]
8>  -- [download 10% complete]
8>  -- [download 11% complete]
8>  -- [download 12% complete]
8>  -- [download 13% complete]
8>  -- [download 14% complete]
8>  -- [download 15% complete]
8>  -- [download 16% complete]
8>  -- [download 17% complete]
8>  -- [download 18% complete]
8>  -- [download 19% complete]
8>  -- [download 20% complete]
8>  -- [download 21% complete]
8>  -- [download 22% complete]
8>  -- [download 23% complete]
8>  -- [download 24% complete]
8>  -- [download 25% complete]
8>  -- [download 26% complete]
8>  -- [download 27% complete]
8>  -- [download 28% complete]
8>  -- [download 29% complete]
8>  -- [download 30% complete]
8>  -- [download 31% complete]
8>  -- [download 32% complete]
8>  -- [download 33% complete]
8>  -- [download 34% complete]
8>  -- [download 35% complete]
8>  -- [download 36% complete]
8>  -- [download 37% complete]
8>  -- [download 38% complete]
8>  -- [download 39% complete]
8>  -- [download 40% complete]
8>  -- [download 41% complete]
8>  -- [download 42% complete]
8>  -- [download 43% complete]
8>  -- [download 44% complete]
8>  -- [download 45% complete]
8>  -- [download 46% complete]
8>  -- [download 47% complete]
8>  -- [download 48% complete]
8>  -- [download 49% complete]
8>  -- [download 50% complete]
8>  -- [download 53% complete]
8>  -- [download 55% complete]
8>  -- [download 58% complete]
8>  -- [download 60% complete]
8>  -- [download 63% complete]
8>  -- [download 66% complete]
8>  -- [download 68% complete]
8>  -- [download 71% complete]
8>  -- [download 74% complete]
8>  -- [download 75% complete]
8>  -- [download 76% complete]
8>  -- [download 77% complete]
8>  -- [download 78% complete]
8>  -- [download 79% complete]
8>  -- [download 80% complete]
8>  -- [download 81% complete]
8>  -- [download 82% complete]
8>  -- [download 83% complete]
8>  -- [download 84% complete]
8>  -- [download 85% complete]
8>  -- [download 86% complete]
8>  -- [download 87% complete]
8>  -- [download 88% complete]
8>  -- [download 89% complete]
8>  -- [download 90% complete]
8>  -- [download 91% complete]
8>  -- [download 92% complete]
8>  -- [download 93% complete]
8>  -- [download 94% complete]
8>  -- [download 95% complete]
8>  -- [download 96% complete]
8>  -- [download 97% complete]
8>  -- [download 98% complete]
8>  -- [download 99% complete]
8>  -- [download 100% complete]
8>  -- verifying file...
8>         file='C:/CPA/Tensorflow/tensorflow/tensorflow/contrib/cmake/build/downloads/1.6.4.zip'
8>  -- SHA256 hash of
8>      C:/CPA/Tensorflow/tensorflow/tensorflow/contrib/cmake/build/downloads/1.6.4.zip
8>    does not match expected value
8>      expected: '966d0c4f41e2bdc81aebf9ccfbf0baffaac5a74f00b826b06f4dee79b2bb8cee'
8>        actual: '4198e9c447a1e2a963b9e0e4d861df48baa47fb02e5e4fc507d1834afc99185a'
8>  -- Hash mismatch, removing...
8>  -- Retry after 15 seconds (attempt #4) ...
8>  -- Using src='https://github.com/NVlabs/cub/archive/1.6.4.zip'
8>  -- [download 0% complete]
8>  -- [download 1% complete]
8>  -- [download 2% complete]
8>  -- [download 4% complete]
8>  -- [download 5% complete]
8>  -- [download 6% complete]
8>  -- [download 7% complete]
8>  -- [download 8% complete]
8>  -- [download 9% complete]
8>  -- [download 10% complete]
8>  -- [download 11% complete]
8>  -- [download 12% complete]
8>  -- [download 14% complete]
8>  -- [download 15% complete]
8>  -- [download 16% complete]
8>  -- [download 17% complete]
8>  -- [download 18% complete]
8>  -- [download 19% complete]
8>  -- [download 21% complete]
8>  -- [download 22% complete]
8>  -- [download 23% complete]
8>  -- [download 24% complete]
8>  -- [download 25% complete]
8>  -- [download 26% complete]
8>  -- [download 27% complete]
8>  -- [download 28% complete]
8>  -- [download 29% complete]
8>  -- [download 30% complete]
8>  -- [download 31% complete]
8>  -- [download 32% complete]
8>  -- [download 33% complete]
8>  -- [download 34% complete]
8>  -- [download 35% complete]
8>  -- [download 36% complete]
8>  -- [download 37% complete]
8>  -- [download 38% complete]
8>  -- [download 39% complete]
8>  -- [download 40% complete]
8>  -- [download 41% complete]
8>  -- [download 42% complete]
8>  -- [download 43% complete]
8>  -- [download 44% complete]
8>  -- [download 45% complete]
8>  -- [download 46% complete]
8>  -- [download 47% complete]
8>  -- [download 48% complete]
8>  -- [download 49% complete]
8>  -- [download 50% complete]
8>  -- [download 51% complete]
8>  -- [download 52% complete]
8>  -- [download 53% complete]
8>  -- [download 54% complete]
8>  -- [download 55% complete]
8>  -- [download 56% complete]
8>  -- [download 57% complete]
8>  -- [download 58% complete]
8>  -- [download 59% complete]
8>  -- [download 60% complete]
8>  -- [download 61% complete]
8>  -- [download 63% complete]
8>  -- [download 64% complete]
8>  -- [download 65% complete]
8>  -- [download 66% complete]
8>  -- [download 67% complete]
8>  -- [download 68% complete]
8>  -- [download 69% complete]
8>  -- [download 70% complete]
8>  -- [download 71% complete]
8>  -- [download 72% complete]
8>  -- [download 73% complete]
8>  -- [download 74% complete]
8>  -- [download 75% complete]
8>  -- [download 76% complete]
8>  -- [download 77% complete]
8>  -- [download 78% complete]
8>  -- [download 79% complete]
8>  -- [download 80% complete]
8>  -- [download 81% complete]
8>  -- [download 82% complete]
8>  -- [download 83% complete]
8>  -- [download 84% complete]
8>  -- [download 85% complete]
8>  -- [download 86% complete]
8>  -- [download 87% complete]
8>  -- [download 88% complete]
8>  -- [download 89% complete]
8>  -- [download 90% complete]
8>  -- [download 91% complete]
8>  -- [download 92% complete]
8>  -- [download 93% complete]
8>  -- [download 94% complete]
8>  -- [download 95% complete]
8>  -- [download 96% complete]
8>  -- [download 97% complete]
8>  -- [download 98% complete]
8>  -- [download 99% complete]
8>  -- [download 100% complete]
8>  -- verifying file...
8>         file='C:/CPA/Tensorflow/tensorflow/tensorflow/contrib/cmake/build/downloads/1.6.4.zip'
8>  -- SHA256 hash of
8>      C:/CPA/Tensorflow/tensorflow/tensorflow/contrib/cmake/build/downloads/1.6.4.zip
8>    does not match expected value
8>      expected: '966d0c4f41e2bdc81aebf9ccfbf0baffaac5a74f00b826b06f4dee79b2bb8cee'
8>        actual: '4198e9c447a1e2a963b9e0e4d861df48baa47fb02e5e4fc507d1834afc99185a'
8>  -- Hash mismatch, removing...
8>  -- Retry after 60 seconds (attempt #5) ...
8>  -- Using src='https://github.com/NVlabs/cub/archive/1.6.4.zip'
8>  -- [download 0% complete]
8>  -- [download 1% complete]
8>  -- [download 2% complete]
8>  -- [download 3% complete]
8>  -- [download 4% complete]
8>  -- [download 5% complete]
8>  -- [download 6% complete]
8>  -- [download 7% complete]
8>  -- [download 9% complete]
8>  -- [download 10% complete]
8>  -- [download 11% complete]
8>  -- [download 12% complete]
8>  -- [download 13% complete]
8>  -- [download 14% complete]
8>  -- [download 15% complete]
8>  -- [download 16% complete]
8>  -- [download 17% complete]
8>  -- [download 18% complete]
8>  -- [download 19% complete]
8>  -- [download 20% complete]
8>  -- [download 21% complete]
8>  -- [download 22% complete]
8>  -- [download 23% complete]
8>  -- [download 24% complete]
8>  -- [download 25% complete]
8>  -- [download 26% complete]
8>  -- [download 27% complete]
8>  -- [download 28% complete]
8>  -- [download 29% complete]
8>  -- [download 30% complete]
8>  -- [download 31% complete]
8>  -- [download 32% complete]
8>  -- [download 33% complete]
8>  -- [download 34% complete]
8>  -- [download 35% complete]
8>  -- [download 36% complete]
8>  -- [download 37% complete]
8>  -- [download 38% complete]
8>  -- [download 39% complete]
8>  -- [download 40% complete]
8>  -- [download 41% complete]
8>  -- [download 42% complete]
8>  -- [download 43% complete]
8>  -- [download 44% complete]
8>  -- [download 45% complete]
8>  -- [download 46% complete]
8>  -- [download 47% complete]
8>  -- [download 48% complete]
8>  -- [download 49% complete]
8>  -- [download 50% complete]
8>  -- [download 51% complete]
8>  -- [download 52% complete]
8>  -- [download 53% complete]
8>  -- [download 55% complete]
8>  -- [download 56% complete]
8>  -- [download 57% complete]
8>  -- [download 59% complete]
8>  -- [download 60% complete]
8>  -- [download 63% complete]
8>  -- [download 66% complete]
8>  -- [download 68% complete]
8>  -- [download 71% complete]
8>  -- [download 74% complete]
8>  -- [download 75% complete]
8>  -- [download 76% complete]
8>  -- [download 77% complete]
8>  -- [download 78% complete]
8>  -- [download 79% complete]
8>  -- [download 80% complete]
8>  -- [download 81% complete]
8>  -- [download 82% complete]
8>  -- [download 83% complete]
8>  -- [download 84% complete]
8>  -- [download 85% complete]
8>  -- [download 86% complete]
8>  -- [download 87% complete]
8>  -- [download 88% complete]
8>  -- [download 89% complete]
8>  -- [download 90% complete]
8>  -- [download 91% complete]
8>  -- [download 92% complete]
8>  -- [download 93% complete]
8>  -- [download 94% complete]
8>  -- [download 95% complete]
8>  -- [download 96% complete]
8>  -- [download 97% complete]
8>  -- [download 98% complete]
8>  -- [download 99% complete]
8>  -- [download 100% complete]
8>  -- verifying file...
8>         file='C:/CPA/Tensorflow/tensorflow/tensorflow/contrib/cmake/build/downloads/1.6.4.zip'
8>  -- SHA256 hash of
8>      C:/CPA/Tensorflow/tensorflow/tensorflow/contrib/cmake/build/downloads/1.6.4.zip
8>    does not match expected value
8>      expected: '966d0c4f41e2bdc81aebf9ccfbf0baffaac5a74f00b826b06f4dee79b2bb8cee'
8>        actual: '4198e9c447a1e2a963b9e0e4d861df48baa47fb02e5e4fc507d1834afc99185a'
8>  -- Hash mismatch, removing...
8>  CMake Error at cub-stamp/download-cub.cmake:157 (message):
8>    Each download failed!
8>
8>
8>
8>
8>
8>C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(171,5): error MSB6006: ""cmd.exe"" exited with code 1.

"
12270,tensorboard 0.1.1: No such file or directory: '/usr/local/lib/python3.4/dist-packages/tensorflow/tensorboard/TAG',"Just upgrade to TF 1.3 and tensorboard 0.1.1 from Pypi. Ubuntu 14.04 64 bit and python 3.4. 

Running tensorboard gives me the following error: 
```
weiliu@roundvalley:~/projects/seismic$ tensorboard --logdir ./summay_2d
Traceback (most recent call last):
  File ""/usr/local/bin/tensorboard"", line 11, in <module>
    sys.exit(main())
  File ""/home/weiliu/.local/lib/python3.4/site-packages/tensorboard/main.py"", line 219, in main
    tb = create_tb_app(plugins)
  File ""/home/weiliu/.local/lib/python3.4/site-packages/tensorboard/main.py"", line 122, in create_tb_app
    plugins=plugins)
  File ""/home/weiliu/.local/lib/python3.4/site-packages/tensorboard/backend/application.py"", line 83, in standard_tensorboard_wsgi
    return TensorBoardWSGIApp(logdir, plugins, multiplexer, reload_interval)
  File ""/home/weiliu/.local/lib/python3.4/site-packages/tensorboard/backend/application.py"", line 121, in __init__
    self.tag = get_tensorboard_tag()
  File ""/home/weiliu/.local/lib/python3.4/site-packages/tensorboard/backend/application.py"", line 372, in get_tensorboard_tag
    tag = tf.resource_loader.load_resource('tensorboard/TAG').strip()
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/resource_loader.py"", line 50, in load_resource
    with open(path, 'rb') as f:
FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.4/dist-packages/tensorflow/tensorboard/TAG'
Error in sys.excepthook:
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/apport_python_hook.py"", line 63, in apport_excepthook
    from apport.fileutils import likely_packaged, get_recent_crashes
  File ""/usr/lib/python3/dist-packages/apport/__init__.py"", line 5, in <module>
    from apport.report import Report
  File ""/usr/lib/python3/dist-packages/apport/report.py"", line 30, in <module>
    import apport.fileutils
  File ""/usr/lib/python3/dist-packages/apport/fileutils.py"", line 23, in <module>
    from apport.packaging_impl import impl as packaging
  File ""/usr/lib/python3/dist-packages/apport/packaging_impl.py"", line 20, in <module>
    import apt
  File ""/usr/lib/python3/dist-packages/apt/__init__.py"", line 34, in <module>
    apt_pkg.init_config()
SystemError: E:Opening configuration file /etc/apt/apt.conf - ifstream::ifstream (13: Permission denied)

Original exception was:
Traceback (most recent call last):
  File ""/usr/local/bin/tensorboard"", line 11, in <module>
    sys.exit(main())
  File ""/home/weiliu/.local/lib/python3.4/site-packages/tensorboard/main.py"", line 219, in main
    tb = create_tb_app(plugins)
  File ""/home/weiliu/.local/lib/python3.4/site-packages/tensorboard/main.py"", line 122, in create_tb_app
    plugins=plugins)
  File ""/home/weiliu/.local/lib/python3.4/site-packages/tensorboard/backend/application.py"", line 83, in standard_tensorboard_wsgi
    return TensorBoardWSGIApp(logdir, plugins, multiplexer, reload_interval)
  File ""/home/weiliu/.local/lib/python3.4/site-packages/tensorboard/backend/application.py"", line 121, in __init__
    self.tag = get_tensorboard_tag()
  File ""/home/weiliu/.local/lib/python3.4/site-packages/tensorboard/backend/application.py"", line 372, in get_tensorboard_tag
    tag = tf.resource_loader.load_resource('tensorboard/TAG').strip()
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/resource_loader.py"", line 50, in load_resource
    with open(path, 'rb') as f:
FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.4/dist-packages/tensorflow/tensorboard/TAG'
```

I checked `/usr/local/lib/python3.4/dist-packages/tensorflow/` and didn't find `tensorboard` folder there. 

Here is related `pip3 list` output: 
```
tensorflow-gpu (1.3.0rc2)
tensorflow-tensorboard (0.1.1)
``` 

EDIT: I realize this is more like a issue of Tensorboard than Tensorflow. Please feel free to move it to correct place if that helps solve it. "
12268,add args to control padding in the definition of pretrained model in slim,"in some situation, the outputs of ""same"" and ""valid"" padding are same for original model, but when do fine-tuning with a new model, they are not,  adding an arg to control padding is meaningful."
12266,compile using `bazel test`  failed on mac,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac 10.11.6 
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.5.3-homebrew
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**:

### Describe the problem

It seems that tensorflow cannot work with clang of Mac.  And even though I install gcc 4.8 with brew, and set `export CC=gcc_4_8_install_dir`,  compile still failed. 

```bash
~ ❯❯❯ clang --version
Apple LLVM version 8.0.0 (clang-800.0.42.1)
Target: x86_64-apple-darwin15.6.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin
```

### Source code / logs

```python
~/W/g/tensorflow ❯❯❯ bazel test -c opt //tensorflow/contrib/learn:estimators_test
WARNING: /Users/facai/Workshop/github/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /Users/facai/Workshop/github/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
INFO: Found 1 test target...
ERROR: /private/var/tmp/_bazel_facai/c1230027f58dd63b64621179de2d0b21/external/boringssl/BUILD:116:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1).
error: unknown warning option '-Wunused-but-set-parameter'; did you mean '-Wunused-parameter'? [-Werror,-Wunknown-warning-option]
error: unknown warning option '-Wno-free-nonheap-object'; did you mean '-Wno-sequence-point'? [-Werror,-Wunknown-warning-option]
Target //tensorflow/contrib/learn:estimators_test failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1.079s, Critical Path: 0.07s

Executed 0 out of 1 test: 1 fails to build.
```
"
12265,Android demo - SDD-Mobilenet model download failed,"**System information**

OS Platform and Distribution:Windows 10
TensorFlow installed from:Source
TensorFlow version:master branch
Python version: 2.7.12
Bazel version:NA
CUDA/cuDNN version:NA
GPU model and memory:NA

**Describe the problem**
Hi,
I'm trying to compile the android demo example project just released the August 11, 2017.
I'm building it with Android Studio and gradlle.
The build fails downloading the SSD-Mobilenet model with this error : 
```
Execution failed for task ':downloadFile'.
> org.apache.http.client.ClientProtocolException: Forbidden
```

I can download the other models, but not this one.
How can I fix it ?
Thanks"
12264,Simple EditDistance constructor is missing in C++,"### System information
- **Windows 10**
- **TensorFlow installed from source**
- **TensorFlow version 1.3-rc2**
- **Python version 3.5.3**:
- **Bazel N/A**
- **CUDA/cuDNN version N/A**
- **GPU model and memory N/A**
- **N/A**

### Describe the problem
There is no simplier EditDistance::EditDistance() constructor there which only accept sparse::SparseTensor arguments. The EditDistance constructor wants a lot of arguments

`EditDistance::EditDistance(const ::tensorflow::Scope& scope,
                           ::tensorflow::Input hypothesis_indices,
                           ::tensorflow::Input hypothesis_values,
                           ::tensorflow::Input hypothesis_shape,
                           ::tensorflow::Input truth_indices,
                           ::tensorflow::Input truth_values,
                           ::tensorflow::Input truth_shape, const
                           EditDistance::Attrs& attrs)`

instead of

`EditDistance::EditDistance(const ::tensorflow::Scope& scope,
                           const sparse::SparseTensor& hypothesis,
                            const sparse::SparseTensor& truth,
                           EditDistance::Attrs& attrs)`

Internally all hypothesis and truth parameters are put into SparseTensor object. Why SparseTensor parameters are not used in the constructor?

### Source code / logs
N/A
"
12263,How do I build a whole compute graph  and Variables/Weights  arguments then training it in C++?,
12262,Where is gen_logging_ops source code?,"Hi,

I have a question about this import from tensorflow/tensorflow/python/summary/summary.py line 53:
`from tensorflow.python.ops import gen_logging_ops as _gen_logging_ops`

When I go to the directory  tensorflow/python/ops, there is no such file called gen_logging_ops.

I am wondering how this worked out? BTW, I am tracking the r1.3 version.

Any suggestion is highly appreciated!


Best wishes,
Binhang 

"
12261,"Using Bazel to bulid a tensorflow C++ source in Windows, but stuck at this problem. ","OS Platform and Distribution : Windows 10
Bazel version :0.5.3

I download the source on https://github.com/tensorflow/tensorflow, and I want to build the example from it. But when I use command ""bazel build ..."", it prompt error.

(tensorflow) D:\FsCode\tensorflow>bazel build tensorflow/examples/label_image/...
ERROR: D:/fscode/tensorflow/tensorflow/core/BUILD:1528:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by D:/fscode/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: D:/fscode/tensorflow/tensorflow/core/BUILD:1528:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by D:/fscode/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: D:/fscode/tensorflow/tensorflow/core/BUILD:1528:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by D:/fscode/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: Analysis of target '//tensorflow/examples/label_image:label_image' failed; build aborted.
INFO: Elapsed time: 1.569s

I search the problem on issues. Somebody suggest to run ./configure. But in Windows system, this can not be run.
"
12260,"Hexagon build fails, readme.md needs to be revisited.","OS: Ubuntu 16.04 64bits
Android Version: 7.1 (Nougat)
NDK Version: android-ndk-r12b

Hexagon build readme should be revisited after recent code changes.


I used same command as given in the readme.md
`
tensorflow/tensorflow/contrib/makefile/build_all_android.sh -x /home/kzos/TFHEXLIBS -t hexagon_graph_execution -s /home/kzos/experiment/tensorflow/tensorflow/contrib/makefile/sub_makefiles/hexagon_graph_execution/Makefile.in
`

I am getting below error:
'
tensorflow/contrib/makefile/Makefile:46: *** ""hexagon is only supported on Android"".  Stop.
`


"
12259,tensorflow compile error,"When I run :bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=opt --config=cuda //tensorflow/...
ERROR: /home/wangmeng/tools/tensorflow/tensorflow/compiler/xla/tools/BUILD:109:1: Linking of rule '//tensorflow/compiler/xla/tools:replay_computation_hlo_evaluator' failed (Exit 1)
bazel-out/local_linux-opt/bin/tensorflow/compiler/plugin/executor/libplugin_lib.lo(device.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#1}::_FUN(tensorflow::OpKernelConstruction*)':
device.cc:(.text._ZN10tensorflowUlPNS_20OpKernelConstructionEE_4_FUNES1_+0x1e): undefined reference to `tensorflow::XlaDeviceLaunchOp::XlaDeviceLaunchOp(tensorflow::OpKernelConstruction*)'
collect2: error: ld returned 1 exit status
INFO: Elapsed time: 61.551s, Critical Path: 36.75s
FAILED: Build did NOT complete successfully

tensorflow r1.3,bazel 0.5.3
"
12258,variable_scope.name is not displayed correctly,"![snip20170814_1](https://user-images.githubusercontent.com/25893395/29259398-45c4299a-80f5-11e7-854e-6cdcfd76f5d7.png)

As shown, scope2's name should be new_scope_1
"
12256,No module named 'tensorflow',"I downloaded the Anaconda 4.1.1 For Windows with Python 3.5.2 version.

Create a conda environment named tensorflow by invoking the following command:
C:> conda create -n tensorflow

Activate the conda environment by issuing the following command:
C:> activate tensorflow
(tensorflow)C:> 

I install the CPU-only version of TensorFlow

I installed it successfully.
but then....

c>python
python version 3.5.2 .... (anaconda).............

import tensorflow as tf
[it appears this Error : ]
Traceback :
File """", line 1, in 
ModuleNotFoundError: No module named 'tensorflow'"
12255,Fail to tune the number of CPU for training,"### System information

== uname -a =====================================================
Linux aws-prophet-tf01 4.4.53-1.el7.centos.x86_64 #1 SMP Sun Mar 12 12:38:41 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.1)
protobuf (3.3.0)
tensorflow (1.2.1)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.2.1
tf.GIT_VERSION = v1.2.0-5-g435cdfc
tf.COMPILER_VERSION = v1.2.0-5-g435cdfc
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================

### Describe the problem

We are using TensorFlow to benchmark and train an simple neural network model. We found that the usage of CPU was around 200% while we have more idle CPUs. And we have manually set the configuration of session like this which will not work.

```
config = tf.ConfigProto(device_count={""CPU"": 4}, # limit to num_cpu_core CPU usage  
                inter_op_parallelism_threads = 1,   
                intra_op_parallelism_threads = 4,  
                log_device_placement=True)  
with tf.Session(config = config) as sess:  
```

Here is my script to benchmark and using `tf.embedding_lookup_sparse` for the space dataset in https://github.com/tobegit3hub/tensorflow_template_application/blob/master/sparse_classifier.py .

### Source code / logs

The usage of CPU looked like this.

<img width=""762"" alt=""screen shot 2017-08-11 at 5 15 29 pm"" src=""https://user-images.githubusercontent.com/2715000/29255695-4dd7deb6-80d6-11e7-8d7b-2c9f7ef3d687.png"">

Although we had set the configuration of session. It seemed that all op are placed in `cpu:0`. The log looks like this.

```
2017-08-14 01:44:00.058798: I tensorflow/core/common_runtime/simple_placer.cc:847] ParseExample/Parse[54/1040]
parse_keys_0: (Const)/job:localhost/replica:0/task:0/cpu:0
ParseExample/ParseExample/names: (Const): /job:localhost/replica:0/task:0/cpu:0
2017-08-14 01:44:00.058818: I tensorflow/core/common_runtime/simple_placer.cc:847] ParseExample/ParseExample/names: (Const)/job:localhost/replica:0/task:0/cpu:0
ParseExample/Const: (Const): /job:localhost/replica:0/task:0/cpu:0                                            2017-08-14 01:44:00.058849: I tensorflow/core/common_runtime/simple_placer.cc:847] ParseExample/Const: (Const)/job:localhost/replica:0/task:0/cpu:0
shuffle_batch/n: (Const): /job:localhost/replica:0/task:0/cpu:0
2017-08-14 01:44:00.058868: I tensorflow/core/common_runtime/simple_placer.cc:847] shuffle_batch/n: (Const)/job:localhost/replica:0/task:0/cpu:0
shuffle_batch/fraction_over_100_of_1024_full/tags: (Const): /job:localhost/replica:0/task:0/cpu:0
2017-08-14 01:44:00.058886: I tensorflow/core/common_runtime/simple_placer.cc:847] shuffle_batch/fraction_over_100_of_1024_full/tags: (Const)/job:localhost/replica:0/task:0/cpu:0
shuffle_batch/mul/y: (Const): /job:localhost/replica:0/task:0/cpu:0
2017-08-14 01:44:00.058901: I tensorflow/core/common_runtime/simple_placer.cc:847] shuffle_batch/mul/y: (Const)/job:localhost/replica:0/task:0/cpu:0
```"
12254,Principle of setting 'hash_bucket_size' parameter ?,"Question 1: 
In [`wide_n_deep_tutorial.py`,](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py) there is a hyper-parameter named `hash_bucket_size` for both `tf.feature_column.categorical_column_with_hash_bucket` and `tf.feature_column.crossed_column` methods,  and the value is `hash_bucket_size=1000`.

But why `1000`? How to set this parameter ?



Question 2:
The second question about `crossed_columns `, that is,
`crossed_columns = [
    tf.feature_column.crossed_column(
        [""education"", ""occupation""], hash_bucket_size=1000),
    tf.feature_column.crossed_column(
        [age_buckets, ""education"", ""occupation""], hash_bucket_size=1000),
    tf.feature_column.crossed_column(
        [""native_country"", ""occupation""], hash_bucket_size=1000)
]
` in [`wide_n_deep_tutorial.py`,](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py)

Why choose `[""education"", ""occupation""]`, `[age_buckets, ""education"", ""occupation""]` and `[""native_country"", ""occupation""]` as crossed_columns , are there any rule of thumb ?

"
12253,not installing ,"Collecting tensorflow
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow
"
12252,Cannot use mean_absolute_error,"I made a very simple neural network on tf and I wanted to use mean absolute error loss function, however I got this error right after I created the optimizer:

No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [...] and loss Tensor(...)

This is what I did:
```
cost = tf.metrics.mean_absolute_error(pred, y)[0]
optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(cost)
```

I tried another loss function and it worked. In fact, I read that it's difficult to provide a gradient when the absolute value is involved, but I did exactly the same in Keras and it works. In fact, I also did the following (which is basically the mean absolute error) and it works as well!

```
cost = tf.reduce_mean(tf.abs(tf.subtract(pred, y)))
optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(cost)
```

Why the function doesn't work?

JM.
"
12251,"an error occurred which is ""No algorithm without scratch worked "" when using tensorflow to train data","### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.1-cp36-cp36m-linux_x86_64.whl
- **TensorFlow version (use command below)**:'1.2.1'
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:cuda-8.0/cudnn-5.1
- **GPU model and memory**:nvidia Tesla K80 /11G


### Describe the problem
when I use tensorflow to train my model, an error always occur when my my data size is larger than [250,250,250]. 

### Source code / logs
### this  is my code :

```
import tensorflow as tf
import numpy as np
n=250
x_train=np.arange(n*n*n).reshape([1,n,n,n,1])
y_train=np.zeros(n*n*n)
tf_x = tf.placeholder(tf.float32,[1,None,None,None,1])
tf_y = tf.placeholder(tf.int64,[None])
output=tf.layers.conv3d(tf_x,
                        filters=2,
                        kernel_size=(2,2,2),
                        strides=(1,1,1), 
                        padding=""same"",
                        activation=tf.nn.relu)

logits = tf.reshape(output, [-1, 2])
cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=tf_y)
loss = tf.reduce_mean(cross_entropy)
train_step = tf.train.AdamOptimizer(1e-6).minimize(loss)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(train_step,feed_dict={tf_x:x_train,tf_y:y_train})
    print(sess.run(loss,feed_dict={tf_x:x_train,tf_y:y_train}))
```

### This is the error I met:

```
`NotFoundError                             Traceback (most recent call last)
/data/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1138     try:
-> 1139       return fn(*args)
   1140     except errors.OpError as e:

/data/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1120                                  feed_dict, fetch_list, target_list,
-> 1121                                  status, run_metadata)
   1122 

/data/conda/lib/python3.6/contextlib.py in __exit__(self, type, value, traceback)
     87             try:
---> 88                 next(self.gen)
     89             except StopIteration:

/data/conda/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py in raise_exception_on_not_ok_status()
    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),
--> 466           pywrap_tensorflow.TF_GetCode(status))
    467   finally:

NotFoundError: No algorithm without scratch worked!
	 [[Node: gradients_2/conv3d_3/convolution_grad/Conv3DBackpropInputV2 = Conv3DBackpropInputV2[T=DT_FLOAT, data_format=""NDHWC"", padding=""SAME"", strides=[1, 1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/gpu:0""](gradients_2/conv3d_3/convolution_grad/Shape, conv3d_2/kernel/read, gradients_2/conv3d_3/BiasAdd_grad/tuple/control_dependency)]]

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
<ipython-input-10-78d05b6135a4> in <module>()
     19 with tf.Session() as sess:
     20     sess.run(tf.global_variables_initializer())
---> 21     sess.run(train_step,feed_dict={tf_x:x_train,tf_y:y_train})
     22     print(sess.run(loss,feed_dict={tf_x:x_train,tf_y:y_train}))
     23 #     print(sess.run(loss,feed_dict={tf_x:x_train,tf_y:y_train}))

/data/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    787     try:
    788       result = self._run(None, fetches, feed_dict, options_ptr,
--> 789                          run_metadata_ptr)
    790       if run_metadata:
    791         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/data/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
    995     if final_fetches or final_targets:
    996       results = self._do_run(handle, final_targets, final_fetches,
--> 997                              feed_dict_string, options, run_metadata)
    998     else:
    999       results = []

/data/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1130     if handle is None:
   1131       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
-> 1132                            target_list, options, run_metadata)
   1133     else:
   1134       return self._do_call(_prun_fn, self._session, handle, feed_dict,

/data/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1150         except KeyError:
   1151           pass
-> 1152       raise type(e)(node_def, op, message)
   1153 
   1154   def _extend_graph(self):

NotFoundError: No algorithm without scratch worked!
	 [[Node: gradients_2/conv3d_3/convolution_grad/Conv3DBackpropInputV2 = Conv3DBackpropInputV2[T=DT_FLOAT, data_format=""NDHWC"", padding=""SAME"", strides=[1, 1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/gpu:0""](gradients_2/conv3d_3/convolution_grad/Shape, conv3d_2/kernel/read, gradients_2/conv3d_3/BiasAdd_grad/tuple/control_dependency)]]

Caused by op 'gradients_2/conv3d_3/convolution_grad/Conv3DBackpropInputV2', defined at:
  File ""/data/conda/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/data/conda/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/data/conda/lib/python3.6/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/data/conda/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/data/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/data/conda/lib/python3.6/site-packages/zmq/eventloop/ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""/data/conda/lib/python3.6/site-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/data/conda/lib/python3.6/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/data/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/data/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/data/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/data/conda/lib/python3.6/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/data/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/data/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""/data/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/data/conda/lib/python3.6/site-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/data/conda/lib/python3.6/site-packages/ipykernel/zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/data/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2698, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/data/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2802, in run_ast_nodes
    if self.run_code(code, result):
  File ""/data/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-10-78d05b6135a4>"", line 18, in <module>
    train_step = tf.train.AdamOptimizer(1e-6).minimize(loss)
  File ""/data/conda/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 315, in minimize
    grad_loss=grad_loss)
  File ""/data/conda/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 386, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/data/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 540, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/data/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 346, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/data/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 540, in <lambda>
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/data/conda/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py"", line 80, in _Conv3DGrad
    data_format=data_format),
  File ""/data/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 666, in conv3d_backprop_input_v2
    name=name)
  File ""/data/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/data/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/data/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

...which was originally created as op 'conv3d_3/convolution', defined at:
  File ""/data/conda/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
[elided 18 identical lines from previous traceback]
  File ""/data/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-10-78d05b6135a4>"", line 13, in <module>
    activation=tf.nn.relu)
  File ""/data/conda/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py"", line 728, in conv3d
    return layer.apply(inputs)
  File ""/data/conda/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 492, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/data/conda/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 441, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/data/conda/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py"", line 158, in call
    data_format=utils.convert_data_format(self.data_format, self.rank + 2))
  File ""/data/conda/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 670, in convolution
    op=op)
  File ""/data/conda/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 338, in with_space_to_batch
    return op(input, num_spatial_dims, padding)
  File ""/data/conda/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 662, in op
    name=name)
  File ""/data/conda/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 146, in _non_atrous_convolution
    name=name)
  File ""/data/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 526, in conv3d
    data_format=data_format, name=name)
  File ""/data/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)

NotFoundError (see above for traceback): No algorithm without scratch worked!
	 [[Node: gradients_2/conv3d_3/convolution_grad/Conv3DBackpropInputV2 = Conv3DBackpropInputV2[T=DT_FLOAT, data_format=""NDHWC"", padding=""SAME"", strides=[1, 1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/gpu:0""](gradients_2/conv3d_3/convolution_grad/Shape, conv3d_2/kernel/read, gradients_2/conv3d_3/BiasAdd_grad/tuple/control_dependency)]]`
```
"
12250,"TypeError: Cannot interpret feed_dict key as Tensor: The name 'DecodeJpeg/contents:0' refers to a Tensor which does not exist. The operation, 'DecodeJpeg/contents', does not exist in the graph.","TypeError: Cannot interpret feed_dict key as Tensor: The name 'DecodeJpeg/contents:0' refers to a Tensor which does not exist. The operation, 'DecodeJpeg/contents', does not exist in the graph.

I am trying to run retrain_test.py on image but not getting output instead getting error.

Traceback (most recent call last):
  File ""retraining-example.py"", line 88, in <module>
    run_inference_on_image()
  File ""retraining-example.py"", line 71, in run_inference_on_image
    {'DecodeJpeg/contents:0': image_data})
  File ""/home/omer/installed/acaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 778, in run
    run_metadata_ptr)
  File ""/home/omer/installed/acaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 933, in _run
    + e.args[0])
TypeError: Cannot interpret feed_dict key as Tensor: The name 'DecodeJpeg/contents:0' refers to a Tensor which does not exist. The operation, 'DecodeJpeg/contents', does not exist in the graph."
12249,tf.estimator.Estimator breaks when using python 3.5 type annotations,"Minimal example:
```
import tensorflow as tf

def model_fn(features: dict, labels: tf.Tensor, mode: str):
    pass

estimator = tf.estimator.Estimator(model_fn)
```

results in 
```
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 173, in __init__
    _verify_model_fn_args(model_fn, params)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 742, in _verify_model_fn_args
    args = set(_model_fn_args(model_fn))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 737, in _model_fn_args
    return tuple(tf_inspect.getargspec(fn).args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_inspect.py"", line 45, in getargspec
    if d.decorator_argspec is not None), _inspect.getargspec(target))
  File ""/usr/lib/python3.5/inspect.py"", line 1045, in getargspec
    raise ValueError(""Function has keyword-only arguments or annotations""
ValueError: Function has keyword-only arguments or annotations, use getfullargspec() API which can support them
```

### System information
- **OS Platform and Distribution**: Linux Mint 17.2
- **TensorFlow version**: v1.2.0-0-g12f033d 1.2.0
- **Python version**: 3.5

"
12245,Error importing tensorflow-gpu,"
![capture](https://user-images.githubusercontent.com/20387587/29246512-8065cd0a-801a-11e7-8916-0e10817a1dce.PNG)

I installed Cuda Toolkit, cuDNN 5.1 on windows 8.1
I also installed tensorflow-gpu using pip3
When I tried importing tensorflow in python, I got the above error
Can you please help me out?


"
12242,Broken download links of the Windows GPU,"Hi,

`Windows GPU` download link are broken."
12237,CUDA_ERROR_OUT_OF_MEMORY,"```
$ python run_atari.py
2017-08-12 08:25:13.924739: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.924772: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.924778: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.924781: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.924785: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.924993: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.925079: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.925109: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.925135: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.925154: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.925180: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.925191: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.925209: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.925249: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.925290: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.930447: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.930465: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.930468: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.930471: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.930473: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.932529: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.932550: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.932555: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.932560: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.932564: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.938520: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.938540: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.938546: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.938550: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.938554: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.943387: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.943402: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.943406: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.943409: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.943412: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.969437: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.969456: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.969460: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.969462: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:13.969465: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-08-12 08:25:14.793472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties:
name: Quadro P400
major: 6 minor: 1 memoryClockRate (GHz) 1.2525
pciBusID 0000:01:00.0
Total memory: 1.94GiB
Free memory: 1.71GiB
2017-08-12 08:25:14.793509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0
2017-08-12 08:25:14.793515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y
2017-08-12 08:25:14.793530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro P400, pci bus id: 0000:01:00.0)
2017-08-12 08:25:14.793544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties:
name: Quadro P400
major: 6 minor: 1 memoryClockRate (GHz) 1.2525
pciBusID 0000:01:00.0
Total memory: 1.94GiB
Free memory: 1.71GiB
2017-08-12 08:25:14.793565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0
2017-08-12 08:25:14.793569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y
2017-08-12 08:25:14.793574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro P400, pci bus id: 0000:01:00.0)
2017-08-12 08:25:14.794371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties:
name: Quadro P400
major: 6 minor: 1 memoryClockRate (GHz) 1.2525
pciBusID 0000:01:00.0
Total memory: 1.94GiB
Free memory: 1.71GiB
2017-08-12 08:25:14.794393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0
2017-08-12 08:25:14.794400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y
2017-08-12 08:25:14.794414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro P400, pci bus id: 0000:01:00.0)
2017-08-12 08:25:14.797353: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 1.52G (1631518720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.798216: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 199.94M (209649664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.798345: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 1.37G (1468366848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.802545: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 1.23G (1321530112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.803248: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 1.11G (1189377024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.803964: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 1020.85M (1070439424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.804914: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 918.77M (963395584 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.806389: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 826.89M (867056128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
[2017-08-12 08:25:14,806] Making new env: PongNoFrameskip-v4
2017-08-12 08:25:14.807060: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 744.20M (780350464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
[2017-08-12 08:25:14,807] Making new env: PongNoFrameskip-v4
2017-08-12 08:25:14.811390: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 669.78M (702315520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.812196: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 602.80M (632083968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.813940: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 542.52M (568875776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.814633: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 488.27M (511988224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.814680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties:
name: Quadro P400
major: 6 minor: 1 memoryClockRate (GHz) 1.2525
pciBusID 0000:01:00.0
Total memory: 1.94GiB
Free memory: 5.94MiB
2017-08-12 08:25:14.814703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0
2017-08-12 08:25:14.814712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y
2017-08-12 08:25:14.814728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro P400, pci bus id: 0000:01:00.0)
2017-08-12 08:25:14.818246: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 439.44M (460789504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.818910: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 7.94M (8323072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.819004: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 395.50M (414710528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.819713: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 7.14M (7490816 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.820048: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 355.95M (373239552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.820746: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 6.43M (6741760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.822546: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 320.35M (335915776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.823119: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 5.79M (6067712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.823313: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 288.32M (302324224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.823724: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 5.21M (5460992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.823876: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 259.49M (272091904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.828059: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 4.69M (4914944 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.828254: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 233.54M (244882944 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.830112: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 4.22M (4423680 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.830394: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 210.18M (220394752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.831054: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 189.17M (198355456 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.834707: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 170.25M (178520064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.835248: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 153.22M (160668160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.835705: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 137.90M (144601344 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
[2017-08-12 08:25:14,835] Making new env: PongNoFrameskip-v4
2017-08-12 08:25:14.837829: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 124.11M (130141440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.838592: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 111.70M (117127424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.840518: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 100.53M (105414912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.841699: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 90.48M (94873600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.845234: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 81.43M (85386240 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.845857: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 73.29M (76847616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.846513: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 65.96M (69163008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.847151: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 59.36M (62246912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.849240: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 53.43M (56022272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.850945: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 48.08M (50420224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.851940: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 43.28M (45378304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.855851: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 38.95M (40840704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.856374: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 35.05M (36756736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.857234: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 31.55M (33081088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.857762: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 28.39M (29773056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.858341: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 25.55M (26795776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.858817: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 23.00M (24116224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.859718: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 20.70M (21704704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.860385: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 18.63M (19534336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:14.860857: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 16.77M (17581056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
[2017-08-12 08:25:14,863] Making new env: PongNoFrameskip-v4
2017-08-12 08:25:15.081855: E tensorflow/core/common_runtime/direct_session.cc:138] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Traceback (most recent call last):
  File ""run_atari.py"", line 55, in <module>
    main()
  File ""run_atari.py"", line 52, in main
    train('PongNoFrameskip-v4', num_timesteps=40e6, seed=0, num_cpu=8)
  File ""run_atari.py"", line 24, in train
    sess = U.single_threaded_session()
  File ""/nohome/jaan/abhishek/code/baselines/baselines/common/tf_util.py"", line 233, in single_threaded_session
    return make_session(1)
  File ""/nohome/jaan/abhishek/code/baselines/baselines/common/tf_util.py"", line 228, in make_session
    return tf.Session(config=tf_config)
  File ""/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1292, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 562, in __init__
    self._session = tf_session.TF_NewDeprecatedSession(opts, status)
  File ""/nohome/jaan/abhishek/anaconda3/lib/python3.6/contextlib.py"", line 89, in __exit__
    next(self.gen)
  File ""/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2017-08-12 08:25:15.099150: E tensorflow/core/common_runtime/direct_session.cc:138] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Traceback (most recent call last):
  File ""run_atari.py"", line 55, in <module>
    main()
  File ""run_atari.py"", line 52, in main
    train('PongNoFrameskip-v4', num_timesteps=40e6, seed=0, num_cpu=8)
  File ""run_atari.py"", line 24, in train
    sess = U.single_threaded_session()
  File ""/nohome/jaan/abhishek/code/baselines/baselines/common/tf_util.py"", line 233, in single_threaded_session
    return make_session(1)
  File ""/nohome/jaan/abhishek/code/baselines/baselines/common/tf_util.py"", line 228, in make_session
    return tf.Session(config=tf_config)
  File ""/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1292, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 562, in __init__
    self._session = tf_session.TF_NewDeprecatedSession(opts, status)
  File ""/nohome/jaan/abhishek/anaconda3/lib/python3.6/contextlib.py"", line 89, in __exit__
    next(self.gen)
  File ""/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2017-08-12 08:25:15.121723: E tensorflow/core/common_runtime/direct_session.cc:138] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 2086797312
Traceback (most recent call last):
  File ""run_atari.py"", line 55, in <module>
    main()
  File ""run_atari.py"", line 52, in main
    train('PongNoFrameskip-v4', num_timesteps=40e6, seed=0, num_cpu=8)
  File ""run_atari.py"", line 24, in train
    sess = U.single_threaded_session()
  File ""/nohome/jaan/abhishek/code/baselines/baselines/common/tf_util.py"", line 233, in single_threaded_session
    return make_session(1)
  File ""/nohome/jaan/abhishek/code/baselines/baselines/common/tf_util.py"", line 228, in make_session
    return tf.Session(config=tf_config)
  File ""/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1292, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 562, in __init__
    self._session = tf_session.TF_NewDeprecatedSession(opts, status)
  File ""/nohome/jaan/abhishek/anaconda3/lib/python3.6/contextlib.py"", line 89, in __exit__
    next(self.gen)
  File ""/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2017-08-12 08:25:15.138143: E tensorflow/core/common_runtime/direct_session.cc:138] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Traceback (most recent call last):
  File ""run_atari.py"", line 55, in <module>
    main()
  File ""run_atari.py"", line 52, in main
    train('PongNoFrameskip-v4', num_timesteps=40e6, seed=0, num_cpu=8)
  File ""run_atari.py"", line 24, in train
    sess = U.single_threaded_session()
  File ""/nohome/jaan/abhishek/code/baselines/baselines/common/tf_util.py"", line 233, in single_threaded_session
    return make_session(1)
  File ""/nohome/jaan/abhishek/code/baselines/baselines/common/tf_util.py"", line 228, in make_session
    return tf.Session(config=tf_config)
  File ""/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1292, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 562, in __init__
    self._session = tf_session.TF_NewDeprecatedSession(opts, status)
  File ""/nohome/jaan/abhishek/anaconda3/lib/python3.6/contextlib.py"", line 89, in __exit__
    next(self.gen)
  File ""/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2017-08-12 08:25:16,309] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2017-08-12 08:25:16,310] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2017-08-12 08:25:16,344] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2017-08-12 08:25:16,344] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2017-08-12 08:25:16,373] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2017-08-12 08:25:16,373] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2017-08-12 08:25:16,385] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2017-08-12 08:25:16,385] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
2017-08-12 08:25:16.507902: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 1.51G (1617277696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-08-12 08:25:16.508376: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 1.51G (1617277696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Job 1, 'python run_atari.py' has stopped
```

It fails, even when the memory is there!
```
Total memory: 1.94GiB
Free memory: 1.71GiB
2017-08-12 08:25:14.794393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0
2017-08-12 08:25:14.794400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y
2017-08-12 08:25:14.794414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro P400, pci bus id: 0000:01:00.0)
2017-08-12 08:25:14.797353: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 1.52G (1631518720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
```


```
$ nvidia-smi
Sat Aug 12 08:34:26 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.82                 Driver Version: 375.82                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro P400         Off  | 0000:01:00.0      On |                  N/A |
| 41%   50C    P8    12W /  N/A |    116MiB /  1990MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K40c          Off  | 0000:03:00.0     Off |                    0 |
| 26%   49C    P8    21W / 235W |      0MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla K40c          Off  | 0000:04:00.0     Off |                    0 |
| 28%   54C    P8    30W / 235W |      0MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1835    G   /usr/lib/xorg/Xorg                              60MiB |
|    0      1946    G   gnome-shell                                     54MiB |
+-----------------------------------------------------------------------------+
```"
12236,"""/gpu:0/stream"" not showing up on timeline","I am generating timeline with TensorFlow r1.3.
I can only see `/job:localhost/replica:0/task:0/cpu:0` and `/job:localhost/replica:0/task:0/gpu:0`.
But I can't find `/gpu:0/stream`

Any idea?

![image](https://user-images.githubusercontent.com/425637/29240963-bb7047dc-7f24-11e7-9cd3-2422c97487b0.png)

My code to regenerate the timeline.
```
import os
import tensorflow as tf
from tensorflow.python.client import timeline

dim = 6000
n = 5

with tf.device('/gpu:0'):
    X, Y, Z, _X, _Y = [], [], [], [], []
    for i in range(n):
        X.append(tf.random_uniform([dim, dim], 0, 10, name='X' + str(i)))
        Y.append(tf.random_uniform([dim, dim], 0, 10, name='Y' + str(i)))
        _X.append(tf.placeholder(dtype=tf.float32, shape=[dim, dim]))
        _Y.append(tf.placeholder(dtype=tf.float32, shape=[dim, dim]))
        Z.append(tf.matmul(_X[i], _Y[i]))
    W = tf.ones([dim, dim])
    for i in range(n):
        W = tf.matmul(W, Z[i])

sess = tf.Session(config=tf.ConfigProto(
                      graph_options=tf.GraphOptions(build_cost_model=1),
                      gpu_options=tf.GPUOptions(allow_growth=True)))
sess.run(tf.global_variables_initializer())
X_, Y_ = sess.run([X, Y])

run_metadata = tf.RunMetadata()
run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)

W_ = sess.run(W,
              {_i: i_ for _i, i_ in zip(_X + _Y, X_ + Y_)},
              options=run_options,
              run_metadata=run_metadata)

tf.profiler.profile(
    tf.get_default_graph(),
    run_meta=run_metadata,
    options=(tf.profiler.ProfileOptionBuilder(
        tf.profiler.ProfileOptionBuilder.time_and_memory()
    ).build()))

tl = timeline.Timeline(run_metadata.step_stats)
with open('/tmp/timeline.json', 'w') as f:
    f.write(tl.generate_chrome_trace_format(show_dataflow=True,
                                            show_memory=True))
```"
12235,Feature: add safe mode for tf.cast,"tf.cast allows to cast float value to int without any warnning. It seems quite unsafe and data will be truncated.

```python
In [34]: tf.__version__
Out[34]: '1.2.1'

In [35]: sess.run(tf.cast(tf.constant(100000, tf.float32), tf.int16
))
Out[35]: -31072
```

Perhaps  it will be better, like [astype](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.astype.html) in numpy, to add a `casting` to support both safe and unsafe casting mode.

I can work on it if possible.
"
12234,Lack of documentation in tf.decode_raw,"Hi, 
i need to load arrays stored in tfrecords files, so i need to convert the raw string/byte data to number, but i don't understand the output i get using `tf.decode_raw`. Unfortunately function documentation doesn't help me.

Here an example of the strange (for me) behavior of this function

```
import numpy
import tensorflow as tf
sess = tf.Session()


array = numpy.array([0.1, 0.2, 0.4, 0.8, 0.9, 1.1])
print(array.tobytes())
print(numpy.fromstring(array.tobytes()))

tensoraw = tf.constant(array.tobytes())

print(sess.run(tensoraw))
print(sess.run(tf.decode_raw(tensoraw, tf.float32)))

rawArray = sess.run(tensoraw)
decodedArray = sess.run(tf.decode_raw(tensoraw, tf.float32))
print(numpy.fromstring(rawArray))
print(numpy.fromstring(decodedArray))
```
with this output
```
b'\x9a\x99\x99\x99\x99\x99\xb9?\x9a\x99\x99\x99\x99\x99\xc9?\x9a\x99\x99\x99\x99\x99\xd9?\x9a\x99\x99\x99\x99\x99\xe9?\xcd\xcc\xcc\xcc\xcc\xcc\xec?\x9a\x99\x99\x99\x99\x99\xf1?'

[ 0.1  0.2  0.4  0.8  0.9  1.1]

b'\x9a\x99\x99\x99\x99\x99\xb9?\x9a\x99\x99\x99\x99\x99\xc9?\x9a\x99\x99\x99\x99\x99\xd9?\x9a\x99\x99\x99\x99\x99\xe9?\xcd\xcc\xcc\xcc\xcc\xcc\xec?\x9a\x99\x99\x99\x99\x99\xf1?'

[ -1.58818684e-23   1.44999993e+00  -1.58818684e-23   1.57499993e+00
  -1.58818684e-23   1.69999993e+00  -1.58818684e-23   1.82499993e+00
  -1.07374184e+08   1.84999990e+00  -1.58818684e-23   1.88749993e+00]

[ 0.1  0.2  0.4  0.8  0.9  1.1]

[ 0.1  0.2  0.4  0.8  0.9  1.1]
```

The strangest thing for me is that numpy.fromstring applied both to the raw tensor and the decoded tensor (evaluated) gives the same output.
Thank you."
12233,add new op for tensorflow in windows os,"Hello,
how can create new op in window os (a example or manual).

the same link in  linux ( https://www.tensorflow.org/extend/adding_an_op#build_the_op_library)

Thanks.

"
12232,Default Python lib paths are not matched with the Python exe user inputted when 'configure',"Here is the output of 'configure':
```
Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3
Found possible Python library paths:
/usr/local/lib/python2.7/dist-packages
/usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is /usr/local/lib/python2.7/dist-packages
...
```
In above,  user typed `Python3` for the location but the library paths found are with `Python2`. 

### 1. Env
  1.1 OS: Ubuntu 16.04 LTS (64 bit) with CUDA 8
  1.2 Bazel Build label: 0.5.3
  1.3 Python:
        - Python 2.7 and Python 3.5 are installed
        - Default `python` cmd in this OS is `python2.7`
  1.4 Tensorflow `git rev-parse HEAD`:  863d7e7f0202cf5b513f2e3e691e7686562c8d74
### 2. Desc
  Using `python3 configure.py` to start configuration instead of `./configure` would save the trouble directly, but may not preferable to others.
  
"
12231,Tensorflow inception speed issue,"hello this is Rohan Naik,
i developed an app which interact with tensorflow running on server and shows parking spaces available for your car.
i am using inception model which was retrained and modified to process live video and tell the users where parking slots are available.
The issue is it takes a 10 seconds to process 8 thread which run the image recognition function. Is there any way i can make it run faster.
Is ""tensorflow slim"" model can process the images faster?
[code.txt](https://github.com/tensorflow/tensorflow/files/1219473/code.txt)

"
12227,"Invalid argument: NodeDef mentions attr 'Tshape' not in Op<name=Reshape; signature=tensor:T, shape:int32 -> output:T; attr=T:type>;","I got this kind of error message once my Android app launced.

Error message

    08-12 10:28:32.307 25515-25533/? E/native: executor.cc:334 Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'Tshape' not in Op<name=Reshape; signature=tensor:T, shape:int32 -> output:T; attr=T:type>; NodeDef: pool_3/_reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](pool_3, pool_3/_reshape/shape)
                                               	 [[Node: pool_3/_reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](pool_3, pool_3/_reshape/shape)]]
    08-12 10:28:32.352 25515-25533/? I/native: tensorflow_jni.cc:299 End computing. Ran in 786ms (786ms avg over 1 runs)
    08-12 10:28:32.357 25515-25533/? A/native: tensorflow_jni.cc:304 Error during inference: Invalid argument: NodeDef mentions attr 'Tshape' not in Op<name=Reshape; signature=tensor:T, shape:int32 -> output:T; attr=T:type>; NodeDef: pool_3/_reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](pool_3, pool_3/_reshape/shape)
                                               	 [[Node: pool_3/_reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](pool_3, pool_3/_reshape/shape)]]
    08-12 10:28:32.357 25515-25533/? A/libc: Fatal signal 6 (SIGABRT), code -6 in tid 25533 (InferenceThread)


These two line of codes that possibility produced the error message

1. https://github.com/datomnurdin/AndroidTensorFlowBirdExample/blob/master/app/src/main/jni/include/tensorflow/core/common_runtime/executor.cc

2. https://github.com/datomnurdin/AndroidTensorFlowBirdExample/blob/master/app/src/main/jni/tensorflow_jni.cc

Please advice. Thank you."
12225,request: Make tensorflow checkpoints portable,"### System information
Not applicable, this is a general feature request.

### Describe the problem
Feature request: make checkpoint files portable.
Checkpoint files are not portable, because they use absolute paths. If I move the directory containing a trained graph, and then try to restore from a checkpoint, I get ""unable to match files"" errors because tensorflow does not know to look in the checkpoint directory for checkpoint files."
12224,MonitoredSession has no `close`,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Minimum custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: mac os 10.12.6
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 2.7.13
- **Bazel version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**:


### Describe the problem
Bug: MonitoredSession has no `close` but it should have according to https://www.tensorflow.org/api_docs/python/tf/train/MonitoredSession#close

I think in some cases, self._sess is not initialized.

### Source code / logs
minimum code to reproduce:
with tf.train.MonitoredTrainingSession() as sess:
  sess.close()

compared to working code using simple session:
with tf.Session() as sess:
  sess.close()"
12222,Feature: Add dependency version,"Tensor examples : 
tensorflow/tensorflow/examples/adding_an_op/

do not come with version of TensorFlow.

So, as tensorFlow API is still changing a lot, breaks appear when:
    using older-examples  with new TF
    new examples with older TF.

Please provide versionning for example scripts:

1) Example of versioning in the comments of the example script:
(to ensure we can reproduce it) : 
'''
Script MNIST
TensorFlow  1.0.1
Numpy   1.9.1


'''

2) Please provide different version of exampleof scripts:
   Major current version t
   Major version t-1
   Major version t-2

Question:  Can you include example scripts as part of regression tests ?




------------------------

"
12221,problem in imprting the tensorflow in phython,"Hi,
I installed tensorflow cpu version but I got the following error. actually, I am using python 3.5.2 
so please anyone can help to solve this problem! 

PMicrosoft Windows [Version 6.1.7601]
Copyright (c) 2009 Microsoft Corporation.  All rights reserved.

C:\Windows\system32>python
Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\Install\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Install\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Install\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Install\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Install\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Install\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Install\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Install\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Install\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Install\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Install\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Install\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Install\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Install\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Install\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>
"
12220,Anyone working with tensorflow in visual studio ? I am facing some issues integrating tensorflow in vs2017. I can run the code of tensorflow in cmd but couldn't run it on vs. I hope someone comes up with a solution. Thanks in advance.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12215,CTC loss with dynamic length,"This is a very specific questions, I'm afraid nobody on stackoverflow will ever answer this. I will copy the text from there, the original question can be found there: 
https://stackoverflow.com/questions/45568266/tensorflow-ctc-loss-ctc-merge-repeated-parameter

And I'm not 100% sure if this behaviour is wanted or a bug (I think its the former one but I'm really not sure).


I'm using Tensorflow 1.0 and its CTC loss [1]. When training, I sometimes get the ""No valid path found."" warning (which harms learning). It is not due to a high learning rate as sometimes reported by other Tensorflow users.

After analyzing it a bit, I found the pattern that causes this warning:
 - feeding an input sequence into the ctc_loss with length seqLen
 - feeding a label with labelLen characters
 - label has numRepeatedChars repeated chars in it, where I count ""ab"" as 0, ""aa"" as 1, ""aaa"" as 2 and so on 
 - warning occurs, when: seqLen - labelLen < numRepeatedChars


Three examples:

 - Ex.1: label=""abb"", len(label)=3, len(inputSequence)=3 => (3-3=0)<1 is true --> warning
 - Ex.2: label=""abb"", len(label)=3, len(inputSequence)=4 => (4-3=1)<1 is false --> no warning
 - Ex.3: label=""bbb"", len(label)=3, len(inputSequence)=4 => (4-3=1)<2 is true --> warning

When I now set the ctc_loss parameter ctc_merge_repeated=False, then the warning disappears.

Three questions:

 - Q1: why is there a warning when repeated chars occur? I thought, as long as the input sequence is not shorter than the target labelling, there is no problem. And when repeated chars are merged in the label, then it gets even shorter, therefore the condition that the input sequence is not shorter still holds.
 - Q2: why does the ctc_loss in its default settings produce this warning? Repeated chars are common in the domains CTCs are used such as handwritten text recognition (HTR)
 - Q3: what settings should I use when doing HTR? Of course labels can have repeated chars. Therefore ctc_merge_repeated=False would make sense. Any suggestions?

Python program to reproduce warning:

```
import tensorflow as tf
import numpy as np

def createGraph():
    tinputs=tf.placeholder(tf.float32, [100, 1, 65]) # max 100 time steps, 1 batch element, 64+1 classes
    tlabels=tf.SparseTensor(tf.placeholder(tf.int64, shape=[None,2]) , tf.placeholder(tf.int32,[None]), tf.placeholder(tf.int64,[2])) # labels
    tseqLen=tf.placeholder(tf.int32, [None]) # list of sequence length in batch
    tloss=tf.reduce_mean(tf.nn.ctc_loss(labels=tlabels, inputs=tinputs, sequence_length=tseqLen, ctc_merge_repeated=True)) # ctc loss
    return (tinputs, tlabels, tseqLen, tloss)

def getNextBatch(nc): # next batch with given number of chars in label
    indices=[[0,i] for i in range(nc)]
    values=[i%65 for i in range(nc)]
    values[0]=0
    values[1]=0 # TODO: (un)comment this to trigger warning
    shape=[1, nc]
    labels=tf.SparseTensorValue(indices, values, shape)
    seqLen=[nc]
    inputs=np.random.rand(100, 1, 65)
    return (labels, inputs, seqLen) 


(tinputs, tlabels, tseqLen, tloss)=createGraph()

sess=tf.Session()
sess.run(tf.global_variables_initializer())

nc=3 # number of chars in label
print('next batch with 1 element has label len='+str(nc))
(labels, inputs, seqLen)=getNextBatch(nc)
res=sess.run([tloss], { tlabels: labels, tinputs:inputs, tseqLen:seqLen } )
```

This is the C++ Tensorflow code [2] where the warning comes from:
```
// It is possible that no valid path is found if the activations for the
// targets are zero.
if (log_p_z_x == kLogZero) {
    LOG(WARNING) << ""No valid path found."";
    dy_b = y;
    return;
}
```

[1] https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/ctc_loss
[2] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/ctc/ctc_loss_calculator.cc


-----

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: costum
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.0.0-rc2-15-g47bba63-dirty, 1.0.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: [problem also on CPU]
- **GPU model and memory**: [problem also on CPU]
- **Exact command to reproduce**: see code above
"
12214,tensorflow in python ,"Hi,
I am trying to use tensorflow in ipython, Linux and I got the following error
==================================================================
h@h:~$ source activate tensorflow  
(tensorflow) hx@hx:~$ python 
Python 2.7.13 |Continuum Analytics, Inc.| (default, Dec 20 2016, 23:09:15) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/hx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/hx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 63, in <module>
    from tensorflow.python.framework.framework_lib import *
  File ""/home/hx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/framework_lib.py"", line 100, in <module>
    from tensorflow.python.framework.subscribe import subscribe
  File ""/home/hx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/subscribe.py"", line 26, in <module>
    from tensorflow.python.ops import variables
  File ""/home/hx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 26, in <module>
    from tensorflow.python.ops import control_flow_ops
  File ""/home/hx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 70, in <module>
    from tensorflow.python.ops import tensor_array_ops
  File ""/home/hx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 33, in <module>
    from tensorflow.python.util import tf_should_use
  File ""/home/hx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py"", line 28, in <module>
    from backports import weakref  # pylint: disable=g-bad-import-order
ImportError: cannot import name weakref
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/hx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/hx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
ImportError: cannot import name pywrap_tensorflow
>>> 
====================================================================
and not in tensorflow like this:
h@h:~$ python
Python 2.7.13 |Anaconda custom (64-bit)| (default, Dec 20 2016, 23:09:15) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: No module named tensorflow
>>> 
=====================================================================
Thanks!


"
12213,Including scatter_nd in android ops crashes.,"System information
 
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
Yes (as described below)
 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
Ubuntu 16 64 bit
 
- TensorFlow installed from (source or binary): 
Source
 
- TensorFlow version : 
1.2.1 (Perhaps using a newer version may fix the issue)
 
- Bazel version (if compiling from source): 
Build label: 0.5.3
 
- Exact command to reproduce:

`bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a --verbose_failures`


I'm trying to port a encoder-decoder net model to android and I was able to add all the required operations to the core_ops/ extended_ops and compile it.
The last method that needs to be included in the scatter_nd Operation for unpooling (I'm using a custom implementation which uses scatter_nd from here https://github.com/tensorflow/tensorflow/issues/2169).

I included the 

`""scatter_nd_op.h"" in filegroup: ""mobile_srcs"" `

```
""scatter_nd_op.h"",
""scatter_nd_op.cc"", in filegroup: ""android_core_ops""
```

I also commented out the line 

`""scatter_nd_op*"", in exclude[] filegroup : ""android_all_ops""`

and tried to compile.

Error Message:

```
tensorflow/core/kernels/scatter_nd_op.cc:373: error: undefined reference to 'tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, int, int, (tensorflow::scatter_nd_op::UpdateOp)1, 5>::operator()(Eigen::ThreadPoolDevice const&, int, Eigen::array<int, 5u>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>)'
tensorflow/core/kernels/scatter_nd_op.cc:372: error: undefined reference to 'tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, int, int, (tensorflow::scatter_nd_op::UpdateOp)1, 4>::operator()(Eigen::ThreadPoolDevice const&, int, Eigen::array<int, 4u>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>)'
tensorflow/core/kernels/scatter_nd_op.cc:371: error: undefined reference to 'tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, int, int, (tensorflow::scatter_nd_op::UpdateOp)1, 3>::operator()(Eigen::ThreadPoolDevice const&, int, Eigen::array<int, 3u>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>)'
tensorflow/core/kernels/scatter_nd_op.cc:370: error: undefined reference to 'tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, int, int, (tensorflow::scatter_nd_op::UpdateOp)1, 2>::operator()(Eigen::ThreadPoolDevice const&, int, Eigen::array<int, 2u>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>)'
tensorflow/core/kernels/scatter_nd_op.cc:369: error: undefined reference to 'tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, int, int, (tensorflow::scatter_nd_op::UpdateOp)1, 1>::operator()(Eigen::ThreadPoolDevice const&, int, Eigen::array<int, 1u>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>)'
tensorflow/core/kernels/scatter_nd_op.cc:255: error: undefined reference to 'tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, int, int, (tensorflow::scatter_nd_op::UpdateOp)1, 5>::operator()(Eigen::ThreadPoolDevice const&, int, Eigen::array<int, 5u>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>)'
tensorflow/core/kernels/scatter_nd_op.cc:254: error: undefined reference to 'tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, int, int, (tensorflow::scatter_nd_op::UpdateOp)1, 4>::operator()(Eigen::ThreadPoolDevice const&, int, Eigen::array<int, 4u>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>)'
tensorflow/core/kernels/scatter_nd_op.cc:253: error: undefined reference to 'tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, int, int, (tensorflow::scatter_nd_op::UpdateOp)1, 3>::operator()(Eigen::ThreadPoolDevice const&, int, Eigen::array<int, 3u>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>)'
tensorflow/core/kernels/scatter_nd_op.cc:252: error: undefined reference to 'tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, int, int, (tensorflow::scatter_nd_op::UpdateOp)1, 2>::operator()(Eigen::ThreadPoolDevice const&, int, Eigen::array<int, 2u>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>)'
tensorflow/core/kernels/scatter_nd_op.cc:251: error: undefined reference to 'tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, int, int, (tensorflow::scatter_nd_op::UpdateOp)1, 1>::operator()(Eigen::ThreadPoolDevice const&, int, Eigen::array<int, 1u>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, int>, 16, Eigen::MakePointer>)'
```

Do I need to add any more dependencies to the BUILD file. 

Or are there any alternatives to scatter_nd which can be used in Android ?

Thanks"
12211,"Please develop a 32-bit version,thank you very much！！！！！！","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12210,tensorflow/contrib/session_bundle/example/export_half_plus_two.py fails with ValueError: invalid option,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.2.1-0-gb4957ff', '1.2.1')
- **Python version**: Python 2.7.9
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: Not used
- **Exact command to reproduce**: python tensorflow/contrib/session_bundle/example/export_half_plus_two.py


### Describe the problem
The given script is used to generate testdata for savedmodel. 
`python tensorflow/contrib/session_bundle/example/export_half_plus_two.py` Fails with below error

```
Traceback (most recent call last):
  File ""tensorflow/contrib/session_bundle/example/export_half_plus_two.py"", line 159, in <module>
    help=""If true, write v2 checkpoint files.""
  File ""/usr/lib/python2.7/argparse.py"", line 1280, in add_argument
    kwargs = self._get_optional_kwargs(*args, **kwargs)
  File ""/usr/lib/python2.7/argparse.py"", line 1410, in _get_optional_kwargs
    raise ValueError(msg % tup)
ValueError: invalid option string 'bool': must start with a character '-'
```

### Source code / logs
tensorflow/contrib/session_bundle/example/export_half_plus_two.py




"
12208,Document changes in notMNIST data sets,"### Describe the problem
The notMNIST data set used in Assignment 1 and maybe elsewhere in this repo repeatedly changed size over the last 12 months. Last year, in issue #4693, a size of about 50 MB is reported for notMNIST_large.tar.gz. Now this file is 236 MB. People doing the assignment are comparing their classifier accuracy to results published by previous assignment takers and some are talking about publishing research using this data set. If the data set is modified unknowingly to the users, observed accuracy figures are not comparable and can mislead people to wrong conclusions.

@yaroslavvb The files is also bigger now on your personal website. However, the time stamps of the .atr.gz and all files inside are still from 2011.

### Source code / logs
The changes in size can also be tracked in changes to the calls of maybe_download() in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb

### Suggestions

- The .tar.gz files should contain a readme and changelog with version information.
- The filename should include the version number.
- References to the data set in the assignments should be changed from ""the notMNIST dataset"" to ""version 1.23 of the notMNIST dataset"".
- Result tables produced by code should quote the version of the data set.
"
12205,BUG: TypeError in DNNClassifier.eval() when using same name for feature in feature_engineering_fn,"### Describe the problem

If we use the  same key to replace a feature, tensorflow might throw TypeError when evaluating:

eg:
```python
def feature_engineering_fn(features, label):
  features[""x""] = some_func(features[""x""])
```

When `features` is `dict`, it is a mutable object. Hence the bug is caused by `evaluate` method which runs `feature_engineering_fn` again, see [code](https://github.com/facaiy/tensorflow/blob/c7b80d51da4fb6d51ea54a0bdf2601afa379d60c/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L1165).

I'll open a PR later.
"
12204,No registered 'MirrorPad' OpKernel for XLA_CPU_JIT devices,It seems that mirror padding is not supported with XLA. The message is triggered when running tfcompile. Any ideas on when this (or other ops like #11905) might be available?
12203,Why not develop  32-bit？Thank you very much!,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12202,Using keras built-in models,"#### Version
v1.2.0-rc2-21-g12f033d

#### Problem
The Keras built-in models in `tf.contrib.keras.applications.*` cannot be used as a subgraph in TF.

#### Example
https://stackoverflow.com/questions/45585546/error-with-tf-contrib-keras-tf-placeholder

#### Cause
Calling `keras.applications.InceptionV3(weights='imagenet')(input_tensor)` is supposed to load pre-trained weights only for related variables, but it initializes the entire TF graph.
"
12194,Incorrect Command Line in Image Training Tutorial,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12.6
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.2.0-rc2-21-g12f033d 1.2.0
- **Python version**: 3.5.3
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: none
- **Exact command to reproduce**: 
```
bazel build tensorflow/examples/label_image:label_image && \
bazel-bin/tensorflow/examples/label_image/label_image \
--graph=/tmp/output_graph.pb --labels=/tmp/output_labels.txt \
--output_layer=final_result \
--image=$HOME/flower_photos/daisy/21652746_cc379e0eea_m.jpg
```
### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

The [tutorial for image retraining](https://www.tensorflow.org/tutorials/image_retraining) has an incorrect command line, which prevents the label_image classifier from running.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
If you run the command as is, you get the following output:
```E tensorflow/examples/label_image/main.cc:349] Running model failed: Not found: FeedInputs: unable to find feed output input```

### Solution
Please see [this StackOverflow post](https://stackoverflow.com/questions/43022516/tensorflow-inception-feedinputs-unable-to-find-feed-output-input).

The solution is to add the option `--input_layer=Mul` to the command line. The new command line should read:
```
bazel build tensorflow/examples/label_image:label_image && \
bazel-bin/tensorflow/examples/label_image/label_image \
--graph=/tmp/output_graph.pb --labels=/tmp/output_labels.txt \
--input_layer=Mul \
--output_layer=final_result \
--image=$HOME/flower_photos/daisy/21652746_cc379e0eea_m.jpg
```"
12193,Error Building project tf_core_framework,"26>------ Build started: Project: tf_core_framework, Configuration: Debug x64 ------
26>  Generating __force_rebuild
26>
26>  Generating E:/AIMLDL/TensorFlow/tensorflow/tensorflow/core/util/version_info.cc
26>  The system cannot find the path specified.
26>C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(171,5): error MSB6006: ""cmd.exe"" exited with code 3.

When I try to build the tf_label_image_example project, tf_core_framework errors out with code 3.

Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12188,learner.run Couldn't find trained model,"I have built my custom experiment like this:

```
def experiment_fn(run_config, hparams):
    hooks = [
        tf.train.CheckpointSaverHook ( 
            checkpoint_dir = run_config.model_dir,
            save_steps = 5,
        ), 
        
        tf.train.SummarySaverHook ( 
            save_steps = 5, 
            output_dir = run_config.model_dir, 
            scaffold= tf.train.Scaffold(),
            summary_op=tf.summary.merge_all()
        )
    ]
    
    return learn.Experiment (
        estimator = learn.Estimator (
            model_fn = model_fn, 
            config = run_config,
            params = hparams
        ),
        train_input_fn = lambda: input_fun(train_set),
        eval_input_fn = lambda: input_fun(eval_set),
        eval_metrics = model_eval_metrics(),
        train_steps = 20,
        train_monitors = hooks,
        eval_hooks = hooks,
        min_eval_frequency = 1,
        export_strategies = saved_model_export_utils.make_export_strategy (
            serving_input_fn = serving_input_fn,
        )
    )
```

```
learn_runner.run (
    experiment_fn = experiment_fn,
    run_config = tf.contrib.learn.RunConfig (
        model_dir = output_dir,
    ),
    schedule = ""train_and_evaluate"",
    hparams =  tf.contrib.training.HParams (
        ...some parameters...
    )
)
```

When I run it, I have files created in \model with names like 'model.ckpt-6.meta' plus the monitor shows the following results:

> Monitors are deprecated. Please use tf.train.SessionRunHook.
> INFO:tensorflow:step = 1, loss = 0.0437659
> INFO:tensorflow:Saving checkpoints for 1 into model\model.ckpt.
> INFO:tensorflow:Saving checkpoints for 6 into model\model.ckpt.
> INFO:tensorflow:Saving checkpoints for 11 into model\model.ckpt.
> INFO:tensorflow:Saving checkpoints for 16 into model\model.ckpt.
> INFO:tensorflow:Saving checkpoints for 20 into model\model.ckpt.
> INFO:tensorflow:Loss for final step: 0.0438498.

> ---------------------------------------------------------------------------
> NotFittedError                            Traceback (most recent call last)
> <ipython-input-3-4a9282b8a5f5> in <module>()
>     177         learning_rate = 0.01,
>     178         decay_rate = 0.96,
> --> 179         decay_steps = 10
>     180     )
>     181 )
> 
> C:\Program Files\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\learn_runner.py in run(experiment_fn, output_dir, schedule, run_config, hparams)
>     208   schedule = schedule or _get_default_schedule(run_config)
>     209 
> --> 210   return _execute_schedule(experiment, schedule)
>     211 
>     212 
> 
> C:\Program Files\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\learn_runner.py in _execute_schedule(experiment, schedule)
>      45     logging.error('Allowed values for this experiment are: %s', valid_tasks)
>      46     raise TypeError('Schedule references non-callable member %s' % schedule)
> ---> 47   return task()
>      48 
>      49 
> 
> C:\Program Files\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\experiment.py in train_and_evaluate(self)
>     499                                       metrics=self._eval_metrics,
>     500                                       name=eval_dir_suffix,
> --> 501                                       hooks=self._eval_hooks)
>     502     export_results = self._maybe_export(eval_result)
>     503     return eval_result, export_results
> 
> C:\Program Files\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\experiment.py in _call_evaluate(self, _sentinel, input_fn, steps, metrics, name, checkpoint_path, hooks)
>     686                                       name=name,
>     687                                       checkpoint_path=checkpoint_path,
> --> 688                                       hooks=hooks)
>     689 
>     690 
> 
> C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\util\deprecation.py in new_func(*args, **kwargs)
>     287             'in a future version' if date is None else ('after %s' % date),
>     288             instructions)
> --> 289       return func(*args, **kwargs)
>     290     return tf_decorator.make_decorator(func, new_func, 'deprecated',
>     291                                        _add_deprecated_arg_notice_to_docstring(
> 
> C:\Program Files\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py in evaluate(self, x, y, input_fn, feed_fn, batch_size, steps, metrics, name, checkpoint_path, hooks, log_progress)
>     541         checkpoint_path=checkpoint_path,
>     542         hooks=hooks,
> --> 543         log_progress=log_progress)
>     544 
>     545     if eval_results is not None:
> 
> C:\Program Files\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py in _evaluate_model(self, input_fn, steps, feed_fn, metrics, name, checkpoint_path, hooks, log_progress)
>     814       if not latest_path:
>     815         raise NotFittedError(""Couldn't find trained model at %s.""
> --> 816                              % self._model_dir)
>     817       checkpoint_path = latest_path
>     818 
> 
> NotFittedError: Couldn't find trained model at \model.

Why it cannot find the model ?

Thanks for assistance!"
12187,No builds for Linux Py3.6-gpu TF1.3 RC2?,"Using Python 3.6 in Anaconda on Ubuntu 16.04

```
source activate py36env 
pip install --upgrade \
> https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.3.0rc2-cp35-cp35m-manylinux1_x86_64.whl
tensorflow_gpu-1.3.0rc2-cp35-cp35m-manylinux1_x86_64.whl is not a supported wheel on this platform. 
```
I checked the build stream and don't see a Linux PY3.6 GPU being built?

Seems to me that since I have TF 1.2 installed (it had the PY3.6GPU support that new RC's should also include builds for Py3.6/GPU or a lot of people just won't build a separt downgraded test environment for the RC?

Or is the intention to drop support?? (I hope not!)
"
12186,How to run the inceptionv3 on the Knights Landing Intel Xeon Phi?,I have built tensorflow from scratch for the Knights Landing and it seems to give only 4-5images/sec. Can someone give a procedure to run these standard benchmarks?
12185,`numpy >= 1.11.0` is not sufficient (repro script included),"Our pip package includes a dependency on [`numpy >= 1.11.0`][1]. However, when `numpy` version 1.11.0 is already installed and then the user installs TensorFlow, TensorFlow fails to load with the following error:
```
SystemError: initialization of _pywrap_tensorflow_internal raised unreported exception
```
The root cause is:
```
RuntimeError: module compiled against API version 0xb but this version of numpy is 0xa
```

This problem occurs on **Python 3 only** (I only tested Python 3.4).

This is a regression from build 582 to build 583. I include a repro script below.
```sh
$ cat examine.sh
#!/bin/sh
# USAGE: ./examine.sh JENKINS_BUILD_ID
set -eu
virtualenv=""$(mktemp -d)""
virtualenv ""${virtualenv}"" -p python3
. ""${virtualenv}/bin/activate""
pip install numpy==1.11.0
build_id=""$1""
NIGHTLY_URL=""https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/${build_id}/artifact/pip_test/whl/tensorflow-1.head-cp34-cp34m-linux_x86_64.whl""
pip install ""${NIGHTLY_URL}""
python -c 'import tensorflow'
printf 'Done; everything checks out.\n'
```
Removing the `pip install numpy==1.11.0`, or subsequently removing it with `pip uninstall -y numpy==1.11.0`, or installing TensorFlow with `pip install -I ""${NIGHTLY_URL}` all fix the issue.

The change log for those two builds is 349932f4400d15d610f7b6e51923c6a60ddd186b...dff1062bbf53cd8890d8d43ea815c90ba4555ba4, but nothing in there looks particularly suspicious. Perhaps something changed on the Jenkins side?

Here is the result of the repro script on builds 582 and 583:
```sh
$ ./examine.sh 582
Running virtualenv with interpreter /usr/bin/python3
Using base prefix '/usr'
New python executable in /tmp/tmp.IS6cENR7vo/bin/python3
Also creating executable in /tmp/tmp.IS6cENR7vo/bin/python
Installing setuptools, pip, wheel...done.
Collecting numpy==1.11.0
  Using cached numpy-1.11.0-cp34-cp34m-manylinux1_x86_64.whl
Installing collected packages: numpy
Successfully installed numpy-1.11.0
Collecting tensorflow==1.head from https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/582/artifact/pip_test/whl/tensorflow-1.head-cp34-cp34m-linux_x86_64.whl
  Downloading https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/582/artifact/pip_test/whl/tensorflow-1.head-cp34-cp34m-linux_x86_64.whl (46.5MB)
    100% |████████████████████████████████| 46.5MB 33kB/s 
Collecting six>=1.10.0 (from tensorflow==1.head)
  Using cached six-1.10.0-py2.py3-none-any.whl
Collecting protobuf>=3.3.0 (from tensorflow==1.head)
  Using cached protobuf-3.3.0-cp34-cp34m-manylinux1_x86_64.whl
Requirement already satisfied: numpy>=1.11.0 in ./tmp.IS6cENR7vo/lib/python3.4/site-packages (from tensorflow==1.head)
Collecting tensorflow-tensorboard<0.2.0,>=0.1.0 (from tensorflow==1.head)
  Using cached tensorflow_tensorboard-0.1.2-py3-none-any.whl
Requirement already satisfied: wheel>=0.26 in ./tmp.IS6cENR7vo/lib/python3.4/site-packages (from tensorflow==1.head)
Requirement already satisfied: setuptools in ./tmp.IS6cENR7vo/lib/python3.4/site-packages (from protobuf>=3.3.0->tensorflow==1.head)
Collecting werkzeug>=0.11.10 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.head)
  Using cached Werkzeug-0.12.2-py2.py3-none-any.whl
Collecting markdown==2.2.0 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.head)
Collecting bleach==1.5.0 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.head)
  Using cached bleach-1.5.0-py2.py3-none-any.whl
Collecting html5lib==0.9999999 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.head)
Installing collected packages: six, protobuf, werkzeug, markdown, html5lib, bleach, tensorflow-tensorboard, tensorflow
Successfully installed bleach-1.5.0 html5lib-0.9999999 markdown-2.2.0 protobuf-3.3.0 six-1.10.0 tensorflow-1.3.0rc2 tensorflow-tensorboard-0.1.2 werkzeug-0.12.2
Done; everything checks out.

$ ./examine.sh 583
Running virtualenv with interpreter /usr/bin/python3
Using base prefix '/usr'
New python executable in /tmp/tmp.Iwv3luN526/bin/python3
Also creating executable in /tmp/tmp.Iwv3luN526/bin/python
Installing setuptools, pip, wheel...done.
Collecting numpy==1.11.0
  Using cached numpy-1.11.0-cp34-cp34m-manylinux1_x86_64.whl
Installing collected packages: numpy
Successfully installed numpy-1.11.0
Collecting tensorflow==1.head from https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/583/artifact/pip_test/whl/tensorflow-1.head-cp34-cp34m-linux_x86_64.whl
  Downloading https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/583/artifact/pip_test/whl/tensorflow-1.head-cp34-cp34m-linux_x86_64.whl (46.4MB)
    100% |████████████████████████████████| 46.4MB 32kB/s 
Requirement already satisfied: numpy>=1.11.0 in ./tmp.Iwv3luN526/lib/python3.4/site-packages (from tensorflow==1.head)
Collecting protobuf>=3.3.0 (from tensorflow==1.head)
  Using cached protobuf-3.3.0-cp34-cp34m-manylinux1_x86_64.whl
Requirement already satisfied: wheel>=0.26 in ./tmp.Iwv3luN526/lib/python3.4/site-packages (from tensorflow==1.head)
Collecting autograd>=1.1.11 (from tensorflow==1.head)
Collecting six>=1.10.0 (from tensorflow==1.head)
  Using cached six-1.10.0-py2.py3-none-any.whl
Collecting tensorflow-tensorboard<0.2.0,>=0.1.0 (from tensorflow==1.head)
  Using cached tensorflow_tensorboard-0.1.2-py3-none-any.whl
Requirement already satisfied: setuptools in ./tmp.Iwv3luN526/lib/python3.4/site-packages (from protobuf>=3.3.0->tensorflow==1.head)
Collecting scipy>=0.17 (from autograd>=1.1.11->tensorflow==1.head)
  Using cached scipy-0.19.1-cp34-cp34m-manylinux1_x86_64.whl
Collecting future>=0.15.2 (from autograd>=1.1.11->tensorflow==1.head)
Collecting werkzeug>=0.11.10 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.head)
  Using cached Werkzeug-0.12.2-py2.py3-none-any.whl
Collecting markdown==2.2.0 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.head)
Collecting html5lib==0.9999999 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.head)
Collecting bleach==1.5.0 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.head)
  Using cached bleach-1.5.0-py2.py3-none-any.whl
Installing collected packages: six, protobuf, scipy, future, autograd, werkzeug, markdown, html5lib, bleach, tensorflow-tensorboard, tensorflow
Successfully installed autograd-1.1.11 bleach-1.5.0 future-0.16.0 html5lib-0.9999999 markdown-2.2.0 protobuf-3.3.0 scipy-0.19.1 six-1.10.0 tensorflow-1.3.0rc2 tensorflow-tensorboard-0.1.2 werkzeug-0.12.2
RuntimeError: module compiled against API version 0xb but this version of numpy is 0xa
RuntimeError: module compiled against API version 0xb but this version of numpy is 0xa
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/tmp/tmp.Iwv3luN526/lib/python3.4/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/tmp/tmp.Iwv3luN526/lib/python3.4/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/tmp/tmp.Iwv3luN526/lib/python3.4/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/tmp/tmp.Iwv3luN526/lib/python3.4/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/tmp/tmp.Iwv3luN526/lib/python3.4/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/tmp/tmp.Iwv3luN526/lib/python3.4/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
SystemError: initialization of _pywrap_tensorflow_internal raised unreported exception
```

[1]: https://github.com/tensorflow/tensorflow/blob/ed7d08eaf92cb9dbc96a09ec6e08d0a93b70a61b/tensorflow/tools/pip_package/setup.py#L35"
12184,Tensorboard AttributeError: 'SummaryMetadata' object has no attribute 'display_name',"### System information
- No custom code
- **OS: MaxOS Sierra 10.12.6
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: Build label: 0.5.3-homebrew
- **CUDA/cuDNN version**:
- **GPU model and memory**: 
- **Exact command to reproduce**: ./tensorboard --logdir=/Users/Pertis/Syncog-TF/SUD/logs/p1/summaries

### Describe the problem
Tensorflow and Tensorboard built from source.
Tensorboard finds event files, but returns:
AttributeError: 'SummaryMetadata' object has no attribute 'display_name'
Tensorboard installed from prebuilt binaries runs without error

### Source code / logs
See stack trace below:
TensorBoard 0.1.3 at http://Rocs-iMac.fios-router.home:6006 (Press CTRL+C to quit) ^C
(tensorflow) Rocs-iMac:tensorboard Pertis$ ./tensorboard --logdir=/Users/Pertis/Syncog-TF/SUD/logs/p1/summaries
Exception in thread Reloader:
Traceback (most recent call last):
  File ""/Users/Pertis/anaconda/envs/tensorflow/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/Users/Pertis/anaconda/envs/tensorflow/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/private/var/tmp/_bazel_Pertis/77b889388121c5daae674df9c3cd82a3/execroot/org_tensorflow_tensorboard/bazel-out/darwin_x86_64-py3-fastbuild/bin/tensorboard/tensorboard.runfiles/org_tensorflow_tensorboard/tensorboard/backend/application.py"", line 325, in _reload_forever
    reload_multiplexer(multiplexer, path_to_run)
  File ""/private/var/tmp/_bazel_Pertis/77b889388121c5daae674df9c3cd82a3/execroot/org_tensorflow_tensorboard/bazel-out/darwin_x86_64-py3-fastbuild/bin/tensorboard/tensorboard.runfiles/org_tensorflow_tensorboard/tensorboard/backend/application.py"", line 299, in reload_multiplexer
    multiplexer.Reload()
  File ""/private/var/tmp/_bazel_Pertis/77b889388121c5daae674df9c3cd82a3/execroot/org_tensorflow_tensorboard/bazel-out/darwin_x86_64-py3-fastbuild/bin/tensorboard/tensorboard.runfiles/org_tensorflow_tensorboard/tensorboard/backend/event_processing/event_multiplexer.py"", line 195, in Reload
    accumulator.Reload()
  File ""/private/var/tmp/_bazel_Pertis/77b889388121c5daae674df9c3cd82a3/execroot/org_tensorflow_tensorboard/bazel-out/darwin_x86_64-py3-fastbuild/bin/tensorboard/tensorboard.runfiles/org_tensorflow_tensorboard/tensorboard/backend/event_processing/event_accumulator.py"", line 209, in Reload
    self._ProcessEvent(event)
  File ""/private/var/tmp/_bazel_Pertis/77b889388121c5daae674df9c3cd82a3/execroot/org_tensorflow_tensorboard/bazel-out/darwin_x86_64-py3-fastbuild/bin/tensorboard/tensorboard.runfiles/org_tensorflow_tensorboard/tensorboard/backend/event_processing/event_accumulator.py"", line 355, in _ProcessEvent
    value = data_compat.migrate_value(value)
  File ""/private/var/tmp/_bazel_Pertis/77b889388121c5daae674df9c3cd82a3/execroot/org_tensorflow_tensorboard/bazel-out/darwin_x86_64-py3-fastbuild/bin/tensorboard/tensorboard.runfiles/org_tensorflow_tensorboard/tensorboard/data_compat.py"", line 53, in migrate_value
    return handler(value) if handler else value
  File ""/private/var/tmp/_bazel_Pertis/77b889388121c5daae674df9c3cd82a3/execroot/org_tensorflow_tensorboard/bazel-out/darwin_x86_64-py3-fastbuild/bin/tensorboard/tensorboard.runfiles/org_tensorflow_tensorboard/tensorboard/data_compat.py"", line 80, in _migrate_image_value
    display_name=value.metadata.display_name or value.tag,
AttributeError: 'SummaryMetadata' object has no attribute 'display_name'

"
12181,Cuda build fail with 1.3.0,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow
 - issue with the same outcomes is here: https://stackoverflow.com/questions/45266594/tensorflow-with-gpu-cuda8-compile-error-shfl-up

------------------------
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NO

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Debian 8.4

- **TensorFlow installed from (source or binary)**:

Trying to compile it from source.

- **TensorFlow version (use command below)**:
latest git head

- **Python version**: 
2.7.10

- **Bazel version (if compiling from source)**:
5.2

- **CUDA/cuDNN version**:
various, tried CUDA 8.0, 7.5, 7.0 and cuDNN 4.0, 5.0, 5.1

- **Exact command to reproduce**:
git clone https://github.com/tensorflow/tensorflow.git tensorflow-1.3.0
./configure 
WARNING: Running Bazel server needs to be killed, because the startup options are different.
You have bazel 0.5.2- (@non-git) installed.
Please specify the location of python. [Default is /software/python/2.7.10/intel/bin/python]: 
Found possible Python library paths:
/software/python/2.7.10/intel/lib/python2.7/site-packages
/software/python27-modules/software/python/2.7.10/intel/lib/python2.7/site-packages
Please input the desired Python library path to use.  Default is /software/python/2.7.10/intel/lib/python2.7/site-packages
/software/python27-modules/software/python/2.7.10/intel/lib/python2.7/site-packages
Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: 
jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [y/N]: 
No Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [y/N]: 
No Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: 
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: 
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: 
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL support? [y/N]: 
No OpenCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8.0
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /software/cuda/8.0
""Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 5
Please specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /software/cuda/8.0]:/software/cudnn/5.1
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]2.0,3.5
Do you want to use clang as CUDA compiler? [y/N]: 
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Do you wish to build TensorFlow with MPI support? [y/N]: 
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 
Add ""--config=mkl"" to your bazel command to build with MKL support.
Please note that MKL on MacOS or windows is still not supported.
If you would like to use a local MKL instead of downloading, please set the environment variable ""TF_MKL_ROOT"" every time before build.
Configuration finished

bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
After some time of compiling this error is raised:
>./tensorflow/core/util/cuda_kernel_helper.h(620): error: identifier ""__shfl"" is undefined
>
>./tensorflow/core/util/cuda_kernel_helper.h(640): error: identifier ""__shfl_up"" is undefined
>
>./tensorflow/core/util/cuda_kernel_helper.h(660): error: identifier ""__shfl_down"" is undefined
>
>./tensorflow/core/util/cuda_kernel_helper.h(680): error: identifier ""__shfl_xor"" is undefined
>
>4 errors detected in the compilation of ""/tmp/tmpxft_000042b4_00000000->10_resampler_ops_gpu.cu.compute_20.cpp1.ii"".
>ERROR: /scratch/hanousek/tensorflow-1.3.0-rc2/tensorflow/contrib/resampler/BUILD:45:1: output >'tensorflow/contrib/resampler/_objs/python/ops/_resampler_ops_gpu/tensorflow/contrib/resampler/kernels/resampler_ops_gpu.cu.pic.o' was not created.
>ERROR: /scratch/hanousek/tensorflow-1.3.0-rc2/tensorflow/contrib/resampler/BUILD:45:1: not all outputs were created or valid.

More detailed error message is here: [https://pastebin.com/RArJfN3m](url) (expire after a month)
"
12178,ValueError: At least one of the merge inputs is None,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes, it is my code that does not work as expected.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04.1 LTS

- **TensorFlow installed from (source or binary)**:
Source.

- **TensorFlow version (use command below)**:
('v1.2.1-0-gb4957ff', '1.2.1')

- **Python version**:
2.7.12

- **Bazel version (if compiling from source)**:
Build label: 0.4.5
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Mar 16 12:19:38 2017 (1489666778)
Build timestamp: 1489666778
Build timestamp as int: 1489666778

- **CUDA/cuDNN version**:
5

- **GPU model and memory**:
GTX 1080, 8GB.

- **Exact command to reproduce**:
See below.

### Describe the problem

The code below crashes with the error shown below, although I see no reason why it should not work fine.

### Source code / logs
```
$ cat test.py
import tensorflow as tf

with tf.Graph().as_default():
    values = tf.get_variable(name='foo', shape=[13])
    some_values = tf.boolean_mask(values, tf.cast(values, tf.bool))
    
    k = tf.minimum(42, tf.shape(some_values)[0])
    
    top_values = tf.cond(
        k > 0,
        lambda: tf.nn.top_k(some_values, k=k).values,
        lambda: tf.zeros(shape=[0])
    )

    loss = tf.reduce_sum(top_values)
    
    optimizer = tf.train.MomentumOptimizer(1e-3, 0.9)
    train_op = tf.contrib.training.create_train_op(loss, optimizer)

$ python test.py
Traceback (most recent call last):
  File ""test.py"", line 18, in <module>
    train_op = tf.contrib.training.create_train_op(loss, optimizer)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/training/python/training/training.py"", line 439, in create_train_op
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 386, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 540, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 346, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 540, in <lambda>
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_grad.py"", line 75, in _SwitchGrad
    return merge([good_grad, zero_grad], name=""cond_grad"")[0], None
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 396, in merge
    raise ValueError(""At least one of the merge inputs is None: %s"" % inputs)
ValueError: At least one of the merge inputs is None: [<tf.Tensor 'gradients/cond/TopKV2_grad/tuple/control_dependency_1:0' shape=() dtype=int32>, None]
```"
12177,InvalidArgumentError in tensorflow reduce_sum gradient when compiling from sources,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.3.0rc2
- **Python version**: 
3.5.3
- **Bazel version (if compiling from source)**:
cmake 3.9.0
- **CUDA/cuDNN version**:
cpu only
- **GPU model and memory**:
cpu only
- **Exact command to reproduce**:

I compiled tensorflow from source according to the instructions given here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md
I activated support for AVX (that's the reason I want to compile from source). All steps work without problems. I then install the whl with pip in my python environment.

Now, if I run my code with that version of tf, it crashes, but it works if I use a pre-compiled version.

### Describe the problem
A script that runs without problems when I use a pre-built version of tensorflow throws an error when I use the compiled version (see below for trace). I also compiled r1.2 and saw the same problem. Since the code works with the pre-built tf version (1.1), I suspect the problem is related to compiling from source.

### Source code / logs
Error when running with the compiled tf:

```
  File ""test_network.py"", line 35, in <module>
    network = Network(**configuration)
[elided 0 identical lines from previous traceback]
  File ""C:\Users\falke\EclipseWorkspace\nn\src\codebase\network.py"", line 194, in __init__
    self._build_graph()
  File ""C:\Users\falke\EclipseWorkspace\nn\src\codebase\network.py"", line 227, in _build_graph
    self.graph_states = gop(self.graph_states, enc_state)
  File ""C:\Users\falke\EclipseWorkspace\nn\src\codebase\network.py"", line 158, in __call__
    w = tf.exp(tf.reduce_sum(tf.multiply(node_labels, enc_state), axis=2, keep_dims=True))
  File ""C:\Anaconda\envs\tfc\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 1279, in reduce_sum
    name=name)
  File ""C:\Anaconda\envs\tfc\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 2727, in _sum
    keep_dims=keep_dims, name=name)
  File ""C:\Anaconda\envs\tfc\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 328, in apply_op
    op_type_name, name, **keywords)
  File ""C:\Anaconda\envs\tfc\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Anaconda\envs\tfc\lib\site-packages\tensorflow\python\framework\ops.py"", line 2624, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Anaconda\envs\tfc\lib\site-packages\tensorflow\python\framework\ops.py"", line 1210, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): indices[1] is out of range
         [[Node: opt/gradients/node_label_update_1/Sum_grad/DynamicStitch = DynamicStitch[N=2, T=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](opt/gradients/node_label_update_1/Sum_grad/range, opt/gradients/node_label_update_1/Sum_grad/mod, opt/gradients/node_label_update_1/Sum_grad/Shape, opt/gradients/node_label_update_1/Sum_grad/Fill)]]
```

The code causing the error is:
`w = tf.exp(tf.reduce_sum(tf.multiply(node_labels, enc_state), axis=2, keep_dims=True))`
Both `node_labels` and `enc_state` are 40x5x30 dim tensors. And as I already said, there are no issues at all running this code if I don't compile from source."
12169,Request for Tile operation for integer types,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.2.0-5-g435cdfc', '1.2.1')
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0 / 5
- **GPU model and memory**: TitanX 12G
- **Exact command to reproduce**:

### Describe the problem
I need some matrix indices computation using tf.tile function. But now it is only available for float types.
So I have to cast int into float and after computation back into int. It would be great to have ""tile"" function working with tensors with dtype of integer types.  

### Source code / logs
```
import tensorflow as tf

with tf.device('/gpu:0'):
    tt = tf.tile(tf.range(4), [3])

with tf.Session() as sess:
    print(sess.run(tt))

```
```
Caused by op u'Tile', defined at:
  File ""test_tf3.py"", line 6, in <module>
    tt = tf.tile(tf.range(4), [3])
  File ""/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3847, in tile
    name=name)
  File ""/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Tile': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
     [[Node: Tile = Tile[T=DT_INT32, Tmultiples=DT_INT32, _device=""/device:GPU:0""](range, Tile/multiples)]]
```"
12168,*mlocate* should be denpendency,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
  docker centos 7
- **TensorFlow installed from (source or binary)**:
  source
- **TensorFlow version (use command below)**:
  r1.2
- **Python version**: 
  2.7.5
- **Bazel version (if compiling from source)**:
  0.5.3
- **CUDA/cuDNN version**:
  none
- **GPU model and memory**:
  none
- **Exact command to reproduce**:

    ./configure



### Describe the problem

    `mlocate` command should be *claimed* as a dependency in your installation instruction
"
12167,TFRecord usage,"### Describe the problem
Can I get meta data of record in TFRecord file using tf queue pipeline, which type is an int or np.ndarry without a Tensor?

So I can use this meta data to tf.shuffle_batch example, and I do not want use a another Sesson to fecth from Tensors.

May this can be a Feature for tf.

"
12166,build android ,"ERROR: /home/wangmeng/RSTensorFlow_mobile/tensorflow/examples/android/BUILD:67:1: Building tensorflow/examples/android/libtensorflow_demo.jar (23 source files) failed: Worker process sent response with exit code: 1.
tensorflow/examples/android/src/org/tensorflow/demo/StylizeActivity.java:365: error: constructor TensorFlowInferenceInterface in class TensorFlowInferenceInterface cannot be applied to given types;
    inferenceInterface = new TensorFlowInferenceInterface(getAssets(), MODEL_FILE);
                         ^
  required: no arguments
  found: AssetManager,String
  reason: actual and formal argument lists differ in length
tensorflow/examples/android/src/org/tensorflow/demo/StylizeActivity.java:544: error: cannot find symbol
    inferenceInterface.feed(
                      ^
  symbol:   method feed(String,float[],int,int,int,int)
  location: variable inferenceInterface of type TensorFlowInferenceInterface
tensorflow/examples/android/src/org/tensorflow/demo/StylizeActivity.java:546: error: cannot find symbol
    inferenceInterface.feed(STYLE_NODE, styleVals, NUM_STYLES);
                      ^
  symbol:   method feed(String,float[],int)
  location: variable inferenceInterface of type TensorFlowInferenceInterface
tensorflow/examples/android/src/org/tensorflow/demo/StylizeActivity.java:548: error: cannot find symbol
    inferenceInterface.run(new String[] {OUTPUT_NODE}, isDebug());
                      ^
  symbol:   method run(String[],boolean)
  location: variable inferenceInterface of type TensorFlowInferenceInterface
tensorflow/examples/android/src/org/tensorflow/demo/StylizeActivity.java:549: error: cannot find symbol
    inferenceInterface.fetch(OUTPUT_NODE, floatValues);
                      ^
  symbol:   method fetch(String,float[])
  location: variable inferenceInterface of type TensorFlowInferenceInterface
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java:90: error: constructor TensorFlowInferenceInterface in class TensorFlowInferenceInterface cannot be applied to given types;
    c.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);
                           ^
  required: no arguments
  found: AssetManager,String
  reason: actual and formal argument lists differ in length
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java:93: error: cannot find symbol
    final Operation operation = c.inferenceInterface.graphOperation(outputName);
                                                    ^
  symbol:   method graphOperation(String)
  location: variable inferenceInterface of type TensorFlowInferenceInterface
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java:132: error: cannot find symbol
    inferenceInterface.feed(inputName, floatValues, 1, inputSize, inputSize, 3);
                      ^
  symbol:   method feed(String,float[],int,int,int,int)
  location: variable inferenceInterface of type TensorFlowInferenceInterface
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java:137: error: cannot find symbol
    inferenceInterface.run(outputNames, logStats);
                      ^
  symbol:   method run(String[],boolean)
  location: variable inferenceInterface of type TensorFlowInferenceInterface
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java:142: error: cannot find symbol
    inferenceInterface.fetch(outputName, outputs);
                      ^
  symbol:   method fetch(String,float[])
  location: variable inferenceInterface of type TensorFlowInferenceInterface
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:90: error: constructor TensorFlowInferenceInterface in class TensorFlowInferenceInterface cannot be applied to given types;
    d.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);
                           ^
  required: no arguments
  found: AssetManager,String
  reason: actual and formal argument lists differ in length
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:221: error: cannot find symbol
    inferenceInterface.feed(inputName, floatValues, 1, inputSize, inputSize, 3);
                      ^
  symbol:   method feed(String,float[],int,int,int,int)
  location: variable inferenceInterface of type TensorFlowInferenceInterface
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:226: error: cannot find symbol
    inferenceInterface.run(outputNames, logStats);
                      ^
  symbol:   method run(String[],boolean)
  location: variable inferenceInterface of type TensorFlowInferenceInterface
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:233: error: cannot find symbol
    inferenceInterface.fetch(outputNames[0], outputLocationsEncoding);
                      ^
  symbol:   method fetch(String,float[])
  location: variable inferenceInterface of type TensorFlowInferenceInterface
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:234: error: cannot find symbol
    inferenceInterface.fetch(outputNames[1], outputScoresEncoding);
                      ^
  symbol:   method fetch(String,float[])
  location: variable inferenceInterface of type TensorFlowInferenceInterface
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowSingleShotDetector.java:68: error: constructor TensorFlowInferenceInterface in class TensorFlowInferenceInterface cannot be applied to given types;
    d.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);
                           ^
  required: no arguments
  found: AssetManager,String
  reason: actual and formal argument lists differ in length
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowSingleShotDetector.java:98: error: cannot find symbol
    inferenceInterface.feed(inputName, bytePixels, 1, inputSize, inputSize, 3);
                      ^
  symbol:   method feed(String,byte[],int,int,int,int)
  location: variable inferenceInterface of type TensorFlowInferenceInterface
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowSingleShotDetector.java:105: error: cannot find symbol
    inferenceInterface.run(outputNames, logStats);
                      ^
  symbol:   method run(String[],boolean)
  location: variable inferenceInterface of type TensorFlowInferenceInterface
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowSingleShotDetector.java:114: error: cannot find symbol
    inferenceInterface.fetch(outputNames[3], numDetectionsArray);
                      ^
  symbol:   method fetch(String,float[])
  location: variable inferenceInterface of type TensorFlowInferenceInterface
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowSingleShotDetector.java:119: error: cannot find symbol
    inferenceInterface.fetch(outputNames[0], boxes);
                      ^
  symbol:   method fetch(String,float[])
  location: variable inferenceInterface of type TensorFlowInferenceInterface
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowSingleShotDetector.java:120: error: cannot find symbol
    inferenceInterface.fetch(outputNames[1], scores);
                      ^
  symbol:   method fetch(String,float[])
  location: variable inferenceInterface of type TensorFlowInferenceInterface
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowSingleShotDetector.java:121: error: cannot find symbol
    inferenceInterface.fetch(outputNames[2], classes);
                      ^
  symbol:   method fetch(String,float[])
  location: variable inferenceInterface of type TensorFlowInferenceInterface
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java:107: error: constructor TensorFlowInferenceInterface in class TensorFlowInferenceInterface cannot be applied to given types;
    d.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);
                           ^
  required: no arguments
  found: AssetManager,String
  reason: actual and formal argument lists differ in length
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java:154: error: cannot find symbol
    inferenceInterface.feed(inputName, floatValues, 1, inputSize, inputSize, 3);
                      ^
  symbol:   method feed(String,float[],int,int,int,int)
  location: variable inferenceInterface of type TensorFlowInferenceInterface
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java:161: error: cannot find symbol
    inferenceInterface.run(outputNames, logStats);
                      ^
  symbol:   method run(String[],boolean)
  location: variable inferenceInterface of type TensorFlowInferenceInterface
tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java:172: error: cannot find symbol
    inferenceInterface.fetch(outputNames[0], output);
                      ^
  symbol:   method fetch(String,float[])
  location: variable inferenceInterface of type TensorFlowInferenceInterface
Target //tensorflow/examples/android:tensorflow_demo failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 19.243s, Critical Path: 7.19s
FAILED: Build did NOT complete successfully
"
12165,Tiny error about set CLASSPATH  in Deploy Doc,"In https://www.tensorflow.org/deploy/hadoop this page,
When introduce to set CLASSPATH,
`shell CLASSPATH=$($HADOOP_HDFS_HOME}/bin/hadoop classpath --glob)`
There is an extra parenthesis after **$HADOOP_HDFS_HOME**"
12164,Cannot include '*.pb.h' files in tf_tutorials.cmake,"**System information**

Windows10
VisualStudio 2015
TensorFlow 1.3.0
Python 3.5.3
CMake 3.9.0

I am generating C++ tensorflow 'GPU' version of _tf_tutorials_example_trainer_ example as defined in (https://github.com/tensorflow/tensorflow/tree/r0.12/tensorflow/contrib/cmake) using Cmake and MSbuild. 

**Describe the problem**

Build failing due to missing header files ../contrib/boosted_trees/proto/*.ph.h. 

The *.proto files are present, and I can use protoc.exe to manually generate the *.ph.h files.  This removes the 'cannot include' errors.  BUT now there are linker errors when building the tf_tutorials_example_trainer.exe as it can't find the routines/structures defined in the  *.pb.h files.

NOTE: *.proto files in other directories are also not expanded to *.pb.h equivalents.

**Source code / logs**

""c:\Users\Martin Rosevear\tensorflow\tensorflow\contrib\cmake\build\tf_tutorials_example_trainer.vcxproj"" (default target) (1) ->
""C:\Users\Martin Rosevear\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj"" (default target) (104) ->
(ClCompile target) -> 
  C:\Users\Martin Rosevear\tensorflow\tensorflow/contrib/boosted_trees/lib/trees/decision_tree.h(19): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/tree_config.pb.h': No such file or directory (compiling source file C:\Users\Martin Rosevear\tensorflow\tensorflow\contrib\boosted_trees\lib\learner\common\partitioners\example_partitioner.cc) [C:\Users\Martin Rosevear\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
  C:\Users\Martin Rosevear\tensorflow\tensorflow/contrib/boosted_trees/lib/learner/stochastic/stats/node-stats.h(21): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/learner.pb.h': No such file or directory (compiling source file C:\Users\Martin Rosevear\tensorflow\tensorflow\contrib\boosted_trees\lib\learner\stochastic\handlers\categorical-feature-column-handler.cc) [C:\Users\Martin Rosevear\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
  C:\Users\Martin Rosevear\tensorflow\tensorflow/contrib/boosted_trees/lib/learner/stochastic/stats/node-stats.h(21): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/learner.pb.h': No such file or directory (compiling source file C:\Users\Martin Rosevear\tensorflow\tensorflow\contrib\boosted_trees\lib\learner\stochastic\handlers\bias-feature-column-handler.cc) [C:\Users\Martin Rosevear\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
  C:\Users\Martin Rosevear\tensorflow\tensorflow/contrib/boosted_trees/lib/learner/stochastic/stats/node-stats.h(21): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/learner.pb.h': No such file or directory (compiling source file C:\Users\Martin Rosevear\tensorflow\tensorflow\contrib\boosted_trees\lib\learner\stochastic\handlers\dense-quantized-feature-column-handler.cc) [C:\Users\Martin Rosevear\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
  C:\Users\Martin Rosevear\tensorflow\tensorflow/contrib/boosted_trees/lib/utils/dropout_utils.h(21): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/learner.pb.h': No such file or directory (compiling source file C:\Users\Martin Rosevear\tensorflow\tensorflow\contrib\boosted_trees\lib\utils\dropout_utils.cc) [C:\Users\Martin Rosevear\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
  C:\Users\Martin Rosevear\tensorflow\tensorflow/contrib/boosted_trees/lib/learner/stochastic/stats/node-stats.h(21): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/learner.pb.h': No such file or directory (compiling source file C:\Users\Martin Rosevear\tensorflow\tensorflow\contrib\boosted_trees\lib\learner\stochastic\handlers\sparse-quantized-feature-column-handler.cc) [C:\Users\Martin Rosevear\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
  C:\Users\Martin Rosevear\tensorflow\tensorflow/contrib/boosted_trees/lib/trees/decision_tree.h(19): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/tree_config.pb.h': No such file or directory (compiling source file C:\Users\Martin Rosevear\tensorflow\tensorflow\contrib\boosted_trees\lib\trees\decision_tree.cc) [C:\Users\Martin Rosevear\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
  C:\Users\Martin Rosevear\tensorflow\tensorflow/contrib/boosted_trees/lib/models/multiple_additive_trees.h(21): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/tree_config.pb.h': No such file or directory (compiling source file C:\Users\Martin Rosevear\tensorflow\tensorflow\contrib\boosted_trees\lib\models\multiple_additive_trees.cc) [C:\Users\Martin Rosevear\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]

"
12163,tf.cast Illegal instruction,"```
import tensorflow as tf
a = [0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1]

tf.cast(a, tf.int32)
```
output:
 Illegal instruction

```
import tensorflow as tf
a = [0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0]

tf.cast(a, tf.int32)
```
this's ok.

My System Info:
 centos7 Linux bw-dev-xxxx-v01 3.10.0-123.el7.x86_64 #1 SMP Mon Jun 30 12:09:22 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux
 Python 3.6.2
```
Python 3.6.2 (default, Aug  3 2017, 14:43:36)
[GCC 4.8.2 20140120 (Red Hat 4.8.2-16)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> tf.__version__
'1.2.1'
>>>
```

why? please help!
"
12162,build_all_xxx.sh support for Tizen target,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 LTS 64bit
- **TensorFlow installed from (source or binary)**: Yes
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.1.3
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: GTX Titan XP
- **Exact command to reproduce**:


### Describe the problem
I can find appropriate build scripts for iOS, Android, and Linux at the below webpage.
- https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile/

Where can I also find a build script in case that I want to build Tensorflow on the Tizen  target device ?

- Tizen (for platform developers): https://source.tizen.org/
- Tizen (for application developers): https://developer.tizen.org/

### Source code / logs
Nothing
"
12159,TensorFlow Serving - error in documentation,"https://www.tensorflow.org/serving/setup

It seems that TensorFlow serving installation guide has incorrect python package name for API:
```
$  pip install tensorflow-serving-api
Collecting tensorflow-serving-api
  Could not find a version that satisfies the requirement tensorflow-serving-api (from versions: )
No matching distribution found for tensorflow-serving-api

$ pip install tensorflow-serving-client
Collecting tensorflow-serving-client
  Downloading tensorflow_serving_client-0.0.6-py2.py3-none-any.whl
Collecting Pillow (from tensorflow-serving-client)
  Downloading Pillow-4.2.1-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (3.5MB)
    100% |████████████████████████████████| 3.5MB 226kB/s
Collecting grpcio-tools (from tensorflow-serving-client)
  Downloading grpcio_tools-1.4.0-cp36-cp36m-macosx_10_9_intel.whl (3.3MB)
    100% |████████████████████████████████| 3.3MB 208kB/s
Requirement already satisfied: tensorflow in ./miniconda3/envs/rtb/lib/python3.6/site-packages (from tensorflow-serving-client)
Collecting grpcio (from tensorflow-serving-client)
  Downloading grpcio-1.4.0-cp36-cp36m-macosx_10_7_intel.whl (1.5MB)
    100% |████████████████████████████████| 1.5MB 408kB/s
Collecting olefile (from Pillow->tensorflow-serving-client)
Requirement already satisfied: protobuf>=3.3.0 in ./miniconda3/envs/rtb/lib/python3.6/site-packages (from grpcio-tools->tensorflow-serving-client)
Requirement already satisfied: markdown>=2.6.8 in ./miniconda3/envs/rtb/lib/python3.6/site-packages (from tensorflow->tensorflow-serving-client)
Requirement already satisfied: six>=1.10.0 in ./miniconda3/envs/rtb/lib/python3.6/site-packages (from tensorflow->tensorflow-serving-client)
Requirement already satisfied: html5lib==0.9999999 in ./miniconda3/envs/rtb/lib/python3.6/site-packages (from tensorflow->tensorflow-serving-client)
Requirement already satisfied: wheel>=0.26 in ./miniconda3/envs/rtb/lib/python3.6/site-packages (from tensorflow->tensorflow-serving-client)
Requirement already satisfied: bleach==1.5.0 in ./miniconda3/envs/rtb/lib/python3.6/site-packages (from tensorflow->tensorflow-serving-client)
Requirement already satisfied: numpy>=1.11.0 in ./miniconda3/envs/rtb/lib/python3.6/site-packages (from tensorflow->tensorflow-serving-client)
Requirement already satisfied: werkzeug>=0.11.10 in ./miniconda3/envs/rtb/lib/python3.6/site-packages (from tensorflow->tensorflow-serving-client)
Requirement already satisfied: backports.weakref==1.0rc1 in ./miniconda3/envs/rtb/lib/python3.6/site-packages (from tensorflow->tensorflow-serving-client)
Requirement already satisfied: setuptools in ./miniconda3/envs/rtb/lib/python3.6/site-packages/setuptools-27.2.0-py3.6.egg (from protobuf>=3.3.0->grpcio-tools->tensorflow-serving-client)
Installing collected packages: olefile, Pillow, grpcio, grpcio-tools, tensorflow-serving-client
Successfully installed Pillow-4.2.1 grpcio-1.4.0 grpcio-tools-1.4.0 olefile-0.44 tensorflow-serving-client-0.0.6
```

So, `tensorflow-serving-api` should be changed to `tensorflow-serving-client`"
12158,Is `full_matrices=False` working for `tf.svd()`?,"I am working with tensorflow svd decomposition,  and I notice that the svd results do not have any differences for singular matrix no matter I set `full_matrices=False` or to be `True`. For example, 

```
a = tf.Variable([[1.0, 0.0, 0.0], [0.0, 4.0, 0.0], [0.0, 0.0, 0.0]])  # Singular Matrix
s, u, v = tf.svd(a, full_matrices=False)
init_op = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init_op)
sess.run([s, u, v])
```

The results are always same no matter `full_matrices=False` or `True`. However, the online doc says ""If true, compute full-sized u and v. If false (the default), compute only the leading P singular vectors"". So I wonder if there is a bug for this argument. Thanks!
"
12157,Bug - Restoring a graph created by tensorflow.python.tools.optimize_for_inference has errors with RNN models,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.1
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: tensorflow-gpu (1.1.0)
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: NA
- **CUDA/cuDNN version**:
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
- **GPU model and memory**: Tesla K80 24GB
- **Exact command to reproduce**: see below

### Describe the problem
I believe I've found a bug. The freeze and optimize scripts appear to have bugs related to the proper function of RNNs. Creating a simple RNN, running the freeze script and the optimize script, and then attempting to restore and use the optimized graph creates a puzzling series of errors.

1. After running freeze and optimize, the placeholder for sequence lengths has datatype `float32`, instead of `tf.int32`, even though the placeholder specifies that it is `tf.int32`. This breaks evaluating an instance of GRUCell, which expects length to be of type `tf.int32`.

2. If I add a `tf.to_int32()` to coerce the sequence length placeholder to type `tf.int32`, then we obtain a different error, which appears to pertain to the internal operation of the `tf.nn.dynamic_rnn()` function.

### Source code / logs
The code is divided into 2 user-created scripts (and 2 TF-provided scripts are employed along the way). The first of my scripts defines a model and saves it, and is called ""optimize_graph_minimal.py"". Then the freeze and optimize scripts are run. The second of my scripts attempts to restore the model to a new Python session, and this appears to be buggy.

#### This is the code I used to create and save the graph.

```
import numpy as np
import tensorflow as tf
import tensorflow.contrib.rnn as rnn

class tf_rnn_model(object):
    def __init__(self, seq_length=10, num_units=2):
        self.seq_length = seq_length
        self.num_units = num_units
        self.graph_context = tf.Graph()
        self.graph_specification()

    def graph_specification(self):
        with self.graph_context.as_default():
            self.X = tf.placeholder(dtype=tf.float32, shape=[None, self.seq_length, 2], name=""X"")
            self.X_length = tf.placeholder(dtype=tf.int32, shape=[None], name=""X_length"")

            cell = rnn.GRUCell(self.num_units)
            Y, rnn_state = tf.nn.dynamic_rnn(cell=cell,
                                             inputs=self.X,
                                             sequence_length=self.X_length,
                                             dtype=tf.float32,
                                             swap_memory=False)

            self.Y = tf.identity(Y, name=""Y"")
            self.saver = tf.train.Saver()

        return None

    def restore_optimized_graph(self, graph_def_optimized):
        with tf.gfile.GFile(graph_def_optimized, 'rb') as f:
            graph_def_optimized = tf.GraphDef()
            graph_def_optimized.ParseFromString(f.read())

        self.Y, = tf.import_graph_def(graph_def_optimized, return_elements=[""Y:0""])
        self.X = self.graph_context.get_tensor_by_name(""import/X:0"")
        self.X_length = self.graph_context.get_tensor_by_name(""import/X_length:0"")
        tf.global_variables_initializer().run()

        return None

model = tf_rnn_model()
with tf.Session(graph=model.graph_context) as sess:
    sess.run(tf.global_variables_initializer())
    inputs = np.arange(20).reshape([1, 10, 2])
    out = sess.run(fetches=[model.Y], feed_dict={model.X: inputs, model.X_length: [10]})
    print(out)

    tf.train.write_graph(sess.graph_def, ""."", ""toy_graph.pb"")
    model.saver.save(sess, save_path=""toy_saved"")

    print(""These are some helpful things to know for the script."")
    print(""saver.as_saver_def()= %s"" % model.saver.as_saver_def())
```

## Freeze and optimize scripts are executed here.

```
python -m tensorflow.python.tools.freeze_graph \
--input_graph toy_graph.pb \
--input_checkpoint toy_saved \
--output_graph graph_frozen.pb \
--output_node_names=Y \
--filename_tensor_name=save/Const:0 \
--restore_op_name=save/restore_all

python -m tensorflow.python.tools.optimize_for_inference \
--input graph_frozen.pb \
--output graph_optimized.pb \
--input_names=X,X_length \
--output_names=Y
```

#### Attempt to restore and `run` using this script in a new Python session.

```
# This line just imports the model class from the previous Python script because this is a new Python session.
from optimize_graph_minimal import tf_rnn_model
import numpy as np
import tensorflow as tf

model = tf_rnn_model()

with tf.Session(graph=model.graph_context) as sess:
    model.restore_optimized_graph(""graph_optimized.pb"")
    inputs = np.arange(20).reshape([1, 10, 2])
    out = sess.run(fetches=[model.Y], feed_dict={model.X: inputs, model.X_length: [10]})
    print(out)
```
#### The following errors are produced.

1. Without explicitly coercing the sequence length placeholder using `tf.to_int32()`, we get an error indicating that the sequence length tensor is of type `float32` but must be type `int32`.

```
Traceback (most recent call last):
  File ""tf_minimal/optimize_restore_graph.py"", line 14, in <module>
    model.restore_optimized_graph(""graph_optimized.pb"")
  File ""optimize_graph_minimal.py"", line 44, in restore_optimized_graph
    self.Y = tf.import_graph_def(graph_def_optimized, return_elements=[""Y:0""])
  File ""python2.7/site-packages/tensorflow/python/framework/importer.py"", line 388, in import_graph_def
    node, 'Input tensor %r %s' % (input_name, te)))
ValueError: graph_def is invalid at node u'rnn/Shape_1': Input tensor 'X_length:0' Cannot convert a tensor of type float32 to an input of type int32.
```

2. If we change the graph specification to use an explicit coercion to int32 type
`self.X_length = tf.to_int32(tf.placeholder(dtype=tf.int32, shape=[None], name=""X_length""))`
then we get this error instead.

```
2017-08-09 19:19:10.910686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0)
Traceback (most recent call last):
  File ""optimize_restore_graph.py"", line 14, in <module>
    model.restore_optimized_graph(""graph_optimized.pb"")
  File ""optimize_graph_minimal.py"", line 44, in restore_optimized_graph
    self.Y = tf.import_graph_def(graph_def_optimized, return_elements=[""Y:0""])
  File ""python2.7/site-packages/tensorflow/python/framework/importer.py"", line 362, in import_graph_def
    % (input_name,)))
ValueError: graph_def is invalid at node u'rnn/while/gru_cell/gates/gates/concat/axis': More inputs specified ('rnn/while/Switch:1') than the op expects..
```"
12156,tf.pow edge case failure,"The tf.pow() function has an edge case which causes it to hang with no error message.

If you try to evaluate tf.pow(x,y), when x is an integer (and thus the output tensor is also an integer), while y is a negative value, tensorflow hangs trying to cast the fraction as an integer.

Examples;

sess.run(tf.pow([5,2],[-2,3]))
sess.run(tf.pow([5],[-2]))
sess.run(tf.pow(5, -2))
sess.run(tf.pow(tf.constant(5), tf.constant(-2)))"
12154,Feature request: document which inputs have gradients,"For most operations, the documentation does not make clear what gradients are implemented. Including this information for the inputs (e.g. annotating the inputs that do not have gradients implemented) would help the user better understand the resulting graph. 

It appears a related issue was closed, as it was not a feature request. https://github.com/tensorflow/tensorflow/issues/6025"
12150,Unable to load saved model,"I am facing similar issue but for GRU. I am using tensorflow  1.1.0 and I tried dumping the model in different ways: 
a)          saver = tf.train.Saver(tf.global_variables())
        model_exporter = exporter.Exporter(saver)

        # Restore variables from training checkpoint
        # TODO: This restores the most recent checkpoint, but if we use validation to counterract
        #       over-fitting, we may want to restore an earlier checkpoint.
        checkpoint = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)
        checkpoint_path = checkpoint.model_checkpoint_path
        saver.restore(session, checkpoint_path)
        log_info('Restored checkpoint at training epoch %d' % (int(checkpoint_path.split('-')[-1]) + 1))

        # Initialise the model exporter and export the model
        model_exporter.init(session.graph.as_graph_def(),
                            named_graph_signatures = {
                                'inputs': exporter.generic_signature(
                                    { 'input': input_tensor,
                                      'input_lengths': seq_length}),
                                'outputs': exporter.generic_signature(
                                    { 'outputs': decoded})})
        if FLAGS.remove_export:
            actual_export_dir = os.path.join(FLAGS.export_dir, '%08d' % FLAGS.export_version)
            if os.path.isdir(actual_export_dir):
                log_info('Removing old export')
                shutil.rmtree(actual_FLAGS.export_dir)
        try:
            # Export serving model
            model_exporter.export(FLAGS.export_dir, tf.constant(FLAGS.export_version), session)

            # Export graph
            input_graph_name = 'input_graph.pb'
            tf.train.write_graph(session.graph, FLAGS.export_dir, input_graph_name, as_text=False)

            # Freeze graph
            input_graph_path = os.path.join(FLAGS.export_dir, input_graph_name)
            input_saver_def_path = ''
            input_binary = True
            output_node_names = 'output_node'
            restore_op_name = 'save/restore_all'
            filename_tensor_name = 'save/Const:0'
            output_graph_path = os.path.join(FLAGS.export_dir, 'output_graph.pb')
            clear_devices = False
            freeze_graph.freeze_graph(input_graph_path, input_saver_def_path,
                                      input_binary, checkpoint_path, output_node_names,
                                      restore_op_name, filename_tensor_name,
                                      output_graph_path, clear_devices, '')
b)          output_graph_def = graph_util.convert_variables_to_constants(session, session.graph.as_graph_def(), ['output_node'])
        with gfile.FastGFile('./data/ldc93s1/output_graph2.pb', 'wb') as f:
            f.write(output_graph_def.SerializeToString())

but for both the dump I get the following error: -
'rnn/while/multi_rnn_cell/cell_0/gru_cell/gates/r/cond/rnn/while/multi_rnn_cell/cell_0/gru_cell/gates/r/strided_slice/_assign/RefEnter': Input tensor 'rnn/multi_rnn_cell/cell_0/gru_cell/gates/r/pop_mean:0' **Cannot convert a tensor of type float32 to an input of type float32_ref**

Any solution so far? 
I followed the similar bug but that is specifically related to Batch norm #3628 . And what is the reason behind this??
"
12148,Tensorflow Experiment shape mismatch between train set and test set,"Hi guys,

I am not sure if this is a bug or my mistake but it looks like when the experiment evaluates with the testing set it expects it to be of the same shape as the training set.

First let me show you how I feed the experiment with the train set and eval set:

```
def input_fun(data):
        x, y = data
        x, y = np.reshape(x,(1, -1, n_inputs)), np.reshape(y,(1, -1, n_outputs))
        return tf.constant(x, dtype = tf.float32), tf.constant(y, dtype = tf.float32)

    def get_train_inputs():
        return input_fun(train_set)

    def get_test_inputs():
        return input_fun(test_set)
```

when I print my train set and eval set, I have :
   
> (<tf.Tensor 'Const_22:0' shape=(1, 1000, 1) dtype=float32>, <tf.Tensor 'Const_23:0' shape=(1, 1000, 1) dtype=float32>)
>     (<tf.Tensor 'Const_24:0' shape=(1, 100, 1) dtype=float32>, <tf.Tensor 'Const_25:0' shape=(1, 100, 1) dtype=float32>)

Then I build my model: 

```
def model_fn(x, y, mode, params):
       predict = prediction(x)
       loss = None
       train_op = None
       eval_metric_ops = None
    
       if mode == learn.ModeKeys.TRAIN or mode == learn.ModeKeys.EVAL:
          loss = model_loss(y, predict, mode)
          #eval_metric_ops = { ""rmse"":tf.metrics.root_mean_squared_error(tf.cast(y,tf.float32), predict) }
    
       if mode == learn.ModeKeys.TRAIN:
          global_step = tf.train.get_global_step()
          #train_op = model_train_op(loss, params['learning_rate'], global_step, mode)
    
          learning_rate = tf.train.exponential_decay(learning_rate = params[""learning_rate""], 
                                                  global_step = tf.contrib.framework.get_global_step(), 
                                                  decay_steps = 20, 
                                                  decay_rate = 0.96, 
                                                  staircase = True)

           train_op = tf.contrib.layers.optimize_loss(loss = loss,
                                              global_step = tf.contrib.framework.get_global_step(),
                                              learning_rate = learning_rate,
                                              optimizer = ""Adam"")
    
       predictions = {""predictions"": predict}
    
       return model_fn_lib.ModelFnOps(
          mode = mode, 
          predictions = predictions,
          loss = loss, 
          train_op = train_op,
       )
```

Then I define the experiment:

```
def experiment_fn(output_dir):
        model_params = {'learning_rate': 0.01}
        trainingConfig = tf.contrib.learn.RunConfig(save_checkpoints_steps = 4, save_summary_steps = 2)
        export_strategy = saved_model_export_utils.make_export_strategy(serving_input_fn=serving_input_fn, exports_to_keep=None)
        hooks = [
            #tf.train.LoggingTensorHook({'loss'}, every_n_iter = 2),
            tf.train.StepCounterHook(every_n_steps = 2, output_dir = output_dir),
            tf.train.CheckpointSaverHook(output_dir, save_steps = 10, checkpoint_basename = 'model.ckpt'),
            tf.train.SummarySaverHook(
                save_steps = 10, 
                output_dir = output_dir, 
                #summary_op = ['loss'],
                scaffold= tf.train.Scaffold(),
                summary_op=tf.summary.merge_all()
            )
        ]
        return learn.Experiment(
            estimator = learn.Estimator(model_fn = model_fn, 
                                params = model_params, 
                                model_dir = output_dir, 
                                config = trainingConfig),
            train_input_fn = get_train_inputs,
            eval_input_fn = get_test_inputs,
            #eval_metrics = model_eval_metrics(),
            train_steps = 100,
            #train_monitors = hooks,
            eval_hooks = hooks,
            export_strategies = export_strategy
        )
```

Eventually I run this line:

 ```
learn_runner.run(experiment_fn = experiment_fn, 
                     output_dir = outdir)
```

The results are as followed:

> Monitors are deprecated. Please use tf.train.SessionRunHook.
>     INFO:tensorflow:Create CheckpointSaverHook.
>     INFO:tensorflow:Saving checkpoints for 1 into .\model.ckpt.
>     INFO:tensorflow:step = 1, loss = 0.043889


It works fine for the training set, but when it evaluates on the test set I get the following error:

> ValueError: Features are incompatible with given information. Given features: Tensor(""Const:0"", shape=(1, 100, 1), dtype=float32), required signatures: TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(1), Dimension(1000), Dimension(1)]), is_sparse=False).

Do you know where it comes from ?

Thanks!"
12147,EigenAllocator for GPU ran out of memory when allocating 0 ,"Environment info

Operating System: Ubuntu 16.04
Installed version of CUDA and cuDNN: CUDA-8.0, CUDNN 5.1.10
Tensorflow version r1.1

Hello, does anyone encountered this error ?

After some random number of iterations, i'm getting the below exception. Can anyone help me where its going wrong?

: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
: F tensorflow/core/common_runtime/gpu/gpu_device.cc:103] EigenAllocator for GPU ran out of memory when allocating 0. See error logs for more detailed info.
Aborted (core dumped)

Regards,
Sharath"
12143,const variable placement issue in distributed tensorflow,"### Describe the problem
In my model, I use the FTRL Optimizer like below:

`self.optimizer = tf.train.FtrlOptimizer(0.005,
                learning_rate_power=-0.5,
                initial_accumulator_value=0.1,
                l1_regularization_strength=1.0,
                l2_regularization_strength=0.00001)
`

Inside the FtrlOptimizer it will create several const variables for the parameters, such as learning rate, learning rate power, etc. 

When I run the distributed tensorflow job, from the timeline I can see that for each session run I can see that the worker will send the above const variables to all the ps nodes. This is a cost since the variables are const and not needed to sent to ps nodes repeatedly.  

I was wondering is there a way to pin those const variables to the ps and save the transferring cost during each session run.

Thanks.
"
12141,Link to Linux and Mac OS broken in TF for Java README,The link to `Linux `and `Mac OS` under section [Building from Source](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/README.md#building-from-source) is broken. 
12140,[XLA] Assert when running XLA unit test,"If I run the test //tensorflow/compiler/xla/tests:tuple_test_cpu with a VLOG level of >=2 , then I receive the following assert:

```
Check failed: current_id >= 0 (-1 vs. 0)-1: 0x7ff6b960bd60: instruction may not have parent computation
```

I don't think that this is due to any changes that I have in my own repo.

"
12139,Build error with Tensorflow 1.3.0 and cuDNN 7.0,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux - Arch Linux
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.5.3 for Arch Linux
- **CUDA/cuDNN version**: CUDA: 8.0.61, cuDNN: 7.0.1
- **GPU model and memory**: NVIDIA GeForce GTX 960M
- **Exact command to reproduce**: bazel build --config=opt --config=mkl --config=cuda //tensorflow/tools/pip_package:build_pip_package 

### Describe the problem
I can't build tensorflow from source with cuDNN 7, as it throws an error pertaining to cuDNN. I'm currently using a tensorflow 1.2.1 built earlier from sources using cuDNN 6.

### Source code / logs
```
ERROR: /home/rharish/Data/tensorflow/tensorflow/stream_executor/BUILD:39:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed (Exit 1).
tensorflow/stream_executor/cuda/cuda_dnn.cc: In instantiation of 'cudnnStatus_t perftools::gputools::cuda::wrap::WrapperShim__cudnnSetRNNDescriptor::operator()(perftools::gputools::cuda::CUDAExecutor*, Args ...) [with Args = {cudnnRNNStruct*, int, int, cudnnDropoutStruct*, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnDataType_t}]':
tensorflow/stream_executor/cuda/cuda_dnn.cc:1017:50:   required from here
tensorflow/stream_executor/cuda/cuda_dnn.cc:139:38: error: cannot convert 'cudnnRNNStruct*' to 'cudnnHandle_t {aka cudnnContext*}' for argument '1' to 'cudnnStatus_t cudnnSetRNNDescriptor(cudnnHandle_t, cudnnRNNDescriptor_t, int, int, cudnnDropoutDescriptor_t, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnRNNAlgo_t, cudnnDataType_t)'
       cudnnStatus_t retval = ::__name(args...);                    \
                                      ^
tensorflow/stream_executor/cuda/cuda_dnn.cc:233:3: note: in expansion of macro 'PERFTOOLS_GPUTOOLS_CUDNN_WRAP'
   __macro(cudnnSetRNNDescriptor)                              \
   ^
tensorflow/stream_executor/cuda/cuda_dnn.cc:238:1: note: in expansion of macro 'CUDNN_DNN_ROUTINE_EACH_R5'
 CUDNN_DNN_ROUTINE_EACH_R5(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)
 ^
In file included from tensorflow/stream_executor/cuda/cuda_dnn.cc:42:0:
bazel-out/local_linux-py3-opt/genfiles/external/local_config_cuda/cuda/cuda/include/cudnn.h:1553:8: note: class type 'cudnnRNNStruct' is incomplete
 struct cudnnRNNStruct;
        ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'int perftools::gputools::cuda::{anonymous}::CudnnDataTypeToByteSize(cudnnDataType_t)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:858:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In member function 'int perftools::gputools::cuda::CudnnRnnParamsDescriptor::GetRegionCountPerLayer() const':
tensorflow/stream_executor/cuda/cuda_dnn.cc:1200:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnRNNInputMode_t perftools::gputools::cuda::{anonymous}::ToCudnnRnnInputMode(perftools::gputools::dnn::RnnInputMode)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:821:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnDirectionMode_t perftools::gputools::cuda::{anonymous}::ToCudnnRnnDirectionMode(perftools::gputools::dnn::RnnDirectionMode)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:833:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnRNNMode_t perftools::gputools::cuda::{anonymous}::ToCudnnRnnMode(perftools::gputools::dnn::RnnMode)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:845:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnDataType_t perftools::gputools::cuda::{anonymous}::ToCudnnDataType(perftools::gputools::dnn::DataType, perftools::gputools::dnn::DataLayout)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:809:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnConvolutionFwdAlgo_t perftools::gputools::cuda::{anonymous}::ToConvForwardAlgo(perftools::gputools::dnn::AlgorithmType)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:283:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnConvolutionBwdDataAlgo_t perftools::gputools::cuda::{anonymous}::ToConvBackwardDataAlgo(perftools::gputools::dnn::AlgorithmType)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:305:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnConvolutionBwdFilterAlgo_t perftools::gputools::cuda::{anonymous}::ToConvBackwardFilterAlgo(perftools::gputools::dnn::AlgorithmType)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:327:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: At global scope:
tensorflow/stream_executor/cuda/cuda_dnn.cc:128:26: warning: 'tensorflow::thread::ThreadPool* perftools::gputools::cuda::wrap::GetCudaThreadpool()' defined but not used [-Wunused-function]
 static port::ThreadPool* GetCudaThreadpool() {
                          ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
```"
12137,sparse_matmul_op_test fails on ppc64le,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RHEL/SLES ppc64le
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: bazel test  --test_output=errors //tensorflow/core/kernels:sparse_matmul_op_test

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Test fails since the pbroadcast_* functions in tensorflow/core/kernels/sparse_matmul_op.h are not implemented for PowerPC (has SSE versions), resulting in incorrect values received by BroadcastPacketTest function in the test case code (tensorflow/core/kernels/sparse_matmul_op_test.cc) , log of failure in below section

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

INFO: From Testing //tensorflow/core/kernels:sparse_matmul_op_test:
==================== Test output for //tensorflow/core/kernels:sparse_matmul_op_test:
Running main() from test_main.cc
[==========] Running 4 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 4 tests from SparseMatmulOpTest
[ RUN      ] SparseMatmulOpTest.BroadcastPacketTest
[0.170094 0.170094 0.170094 0.170094] != [  0.170094    0.14922 -0.0823886   0.026985], differences: [         0 -0.0208738  -0.252482  -0.143109]
tensorflow/core/kernels/sparse_matmul_op_test.cc:257: Failure
Value of: areApprox(ref, data2, PacketSize)
  Actual: false
Expected: true
[  FAILED  ] SparseMatmulOpTest.BroadcastPacketTest (1 ms)
[ RUN      ] SparseMatmulOpTest.InterleavePacketTest
[       OK ] SparseMatmulOpTest.InterleavePacketTest (0 ms)
[ RUN      ] SparseMatmulOpTest.Bfloat16ExpandTest
[       OK ] SparseMatmulOpTest.Bfloat16ExpandTest (0 ms)
[ RUN      ] SparseMatmulOpTest.Bfloat16LoadTest
[       OK ] SparseMatmulOpTest.Bfloat16LoadTest (0 ms)
[----------] 4 tests from SparseMatmulOpTest (1 ms total)

[----------] Global test environment tear-down
[==========] 4 tests from 1 test case ran. (1 ms total)
[  PASSED  ] 3 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] SparseMatmulOpTest.BroadcastPacketTest

 1 FAILED TEST
================================================================================
Target //tensorflow/core/kernels:sparse_matmul_op_test up-to-date:
  bazel-bin/tensorflow/core/kernels/sparse_matmul_op_test
INFO: Elapsed time: 14.711s, Critical Path: 14.07s
//tensorflow/core/kernels:sparse_matmul_op_test                          FAILED in 1 out of 2 in 0.0s
  /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/testlogs/tensorflow/core/kernels/sparse_matmul_op_test/test.log

Executed 1 out of 1 test: 1 fails locally."
12136,Quantize_training_test fails with matmul operation on Ubuntu 16.04,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**:  'v1.2.1-0-gb4957ff', '1.2.1'
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: No GPU
- **GPU model and memory**: No GPU
- **Exact command to reproduce**: bazel test -c opt //tensorflow/python:quantize_training_test

### The problem
The `testQuantizedSaveRestore` from  `//tensorflow/python:quantize_training_test` is failing on s390x while importing graph [here.](https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/python/training/quantize_training_test.py#L72) 

The error message shown is : 
`ValueError: Shapes must be equal rank, but are 0 and 2 for 'a/Min/AssignValue' (op: 'Assign') with input shapes: [], [1,1].
`
The check for this failure is at [Merge function](https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/core/framework/shape_inference.cc#L374) . This Merge is called by `Assign` Op kernel. 

I tried changing the `math_ops.matmul` operation from the above test to `math_ops.subtract/minimum/multiply`. With these operations the test passes after removing asserts for `'a/Min/Variable:0'` or `'b/read/Max/Variable:0'` etc. I suppose other operations do not create these tensors. 

Could anyone please provide some inputs on this failure? I am not aware about the computations that are happening when graph is imported with the `matmul` operation.

### Source code / logs
```
.E.
======================================================================
ERROR: testQuantizedSaveRestore (__main__.PywrapQuantizeTrainingTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/test/.cache/bazel/_bazel_test/24685d064c07f7346b48c2d13ec3ad69/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/quantize_training_test.runfiles/org_tensorflow/tensorflow/python/training/quantize_training_test.py"", line 73, in testQuantizedSaveRestore
    _ = importer.import_graph_def(result, name='')
  File ""/home/test/.cache/bazel/_bazel_test/24685d064c07f7346b48c2d13ec3ad69/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/quantize_training_test.runfiles/org_tensorflow/tensorflow/python/framework/importer.py"", line 409, in import_graph_def
    ops.set_shapes_for_outputs(op)
  File ""/home/test/.cache/bazel/_bazel_test/24685d064c07f7346b48c2d13ec3ad69/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/quantize_training_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 1873, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/home/test/.cache/bazel/_bazel_test/24685d064c07f7346b48c2d13ec3ad69/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/quantize_training_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 1823, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""/home/test/.cache/bazel/_bazel_test/24685d064c07f7346b48c2d13ec3ad69/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/quantize_training_test.runfiles/org_tensorflow/tensorflow/python/framework/common_shapes.py"", line 610, in call_cpp_shape_fn
    debug_python_shape_fn, require_shape_fn)
  File ""/home/test/.cache/bazel/_bazel_test/24685d064c07f7346b48c2d13ec3ad69/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/quantize_training_test.runfiles/org_tensorflow/tensorflow/python/framework/common_shapes.py"", line 676, in _call_cpp_shape_fn_impl
    raise ValueError(err.message)
ValueError: Shapes must be equal rank, but are 0 and 2 for 'a/Min/AssignValue' (op: 'Assign') with input shapes: [], [1,1].
```
"
12135,tf.shape return wrong shape,"I have a png image of shape `[128,128,3]` 

`tf.shape(r)` returns `Tensor(""load_images/Shape:0"", shape=(3,), dtype=int32)`

while `numpy.shape(r)` returns `(128, 128, 3)`

**tensorflow 1.1.0**"
12134,Nudge function in fake quantization returns non-nudegd scale value ,"The Nudge function(tensorflow/tensorflow/core/kernels/fake_quant_ops_functor.h) aims to keep the real zero value including in quantization input range. 
After min/max values are nudged, the scale keeps its original value. Is it intended to be?
"
12133,[Feature] [Java]: Inspect tensors in a Graph,"Dear Tensorflow maintainers,

First of all i really like it that tensorflow allows easy interop from keras to tensorflow and then allows for interference to Java.

I have noticed that when you load an invalid graph, for example an empty byte array, then it will just accept it without giving an error. An exception will be thrown when we try to run the graph.

Thus i would propose two improvements for the tensorflow java API,
* throw an exception when loading an invalide graph.
* add api functionality to inspect the tensors contained in the graph

I also noticed in the example for java that you wrapped the graph builder in a convenience java class and it seems that this could be implemented in a really clean way in scala, I would gladly offer you my assistance to implement a scala wrapper. 

Kind regards, Boris"
12132,slim.separable_conv2d is too slow,"------------------------
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: custom, yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: from pip
- **TensorFlow version (use command below)**: ('v1.2.0-5-g435cdfc', '1.2.1')
- **Python version**: python2.7
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: CPU version, no CUDA
- **GPU model and memory**: CPU version, no CUDA
- **Exact command to reproduce**:

### Describe the problem
the depthwise+pointwise structure is faster than the traditional convolution layer theoretically, but the implemetation of tensorflow make it slower. it doesn't make sense.
here is part of my network defination:
#net = slim.conv2d(net, 32, [3, 3], scope='conv1-2')
    #end_points['conv1-2'] = net
    net = slim.separable_conv2d(net,None,[3,3],depth_multiplier=1,stride=1,rate=1,normalizer_fn=slim.batch_norm,scope='conv1-2-depthwise')
    end_points['conv1-2-depthwise'] = net
    net = slim.conv2d(net, depth(32), [1, 1], stride=1, normalizer_fn=slim.batch_norm, scope='conv1-2-pointwise')
    end_points['conv1-2-pointwise'] = net

    net = slim.max_pool2d(net, [2, 2], 2, scope='pool1')
    end_points['pool1'] = net # 58*58

    #net = slim.conv2d(net, 48, [3, 3], padding='VALID', scope='conv2')
    #end_points['conv2'] = net
    net = slim.separable_conv2d(net,None,[3,3],depth_multiplier=1,stride=1,rate=1,normalizer_fn=slim.batch_norm,scope='conv2-depthwise')
    end_points['conv2-depthwise'] = net
    net = slim.conv2d(net, depth(48), [1, 1], stride=1, normalizer_fn=slim.batch_norm, scope='conv2-pointwise')
    end_points['conv2-pointwise'] = net

    net = slim.max_pool2d(net, [2, 2], 2, scope='pool2')
    end_points['pool2'] = net # 28*28

i just change the network defination 
from: 
net = slim.conv2d(net, 32, [3, 3], scope='conv1-2')
end_points['conv1-2'] = net
to:
net = slim.separable_conv2d(net,None,[3,3],depth_multiplier=1,stride=1,
         rate=1,normalizer_fn=slim.batch_norm,scope='conv1-2-depthwise')
end_points['conv1-2-depthwise'] = net
net = slim.conv2d(net, depth(32), [1, 1], stride=1, normalizer_fn=slim.batch_norm, 
         scope='conv1-2-pointwise')
end_points['conv1-2-pointwise'] = net
i do not think i am doing something wrong. so where the problem is?
"
12131,ckpt get in win7 anaconda cannot be used in win10？The same CNN structure and params,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12128,Where is device_attributes.pb.h ,"I have to use the  device_attributes.pb.h  and I don't know where it is.

Thank you !"
12126,Where is NumTraits.h ?,https://stackoverflow.com/questions/45579072/compiling-tensorflow-api-c-cc-using-mingw-gcc
12123,Cannot compile on Mac OS X due to BoringSSL,"
------------------------

### System information

```
@:~/projects/tensorflow <master>$ cat tf_env.txt

== cat /etc/issue ===============================================
Darwin mn-mortutay 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64
Mac OS X 10.12.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 8.1.0 (clang-802.0.42)
Target: x86_64-apple-darwin16.5.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

== uname -a =====================================================
Darwin mn-mortutay 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.13.1)
protobuf (3.3.0)
tensorflow (1.2.1)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.2.1
tf.GIT_VERSION = v1.2.0-5-g435cdfc
tf.COMPILER_VERSION = v1.2.0-5-g435cdfc
Sanity check: array([1], dtype=int32)
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""tensorflow/python/pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
ImportError: No module named pywrap_tensorflow_internal


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```

Building from commit:
```
* d74f65bac (Yun Peng, 3 hours ago) Make Windows Bazel GPU build work again (#11901)
```

### Describe the problem
I'm following the instructions for Mac source installation (https://www.tensorflow.org/install/install_sources, ""Prepare environment for Mac OS"" and then ""Build the pip package"") but I'm running into issues with BoringSSL.

It looks like the `-Wunused-but-set-parameter` and `-Wno-free-nonheap-object` flags are causing the compilation to fail, since `clang` does not support them. 

Expected behavior: The build system should detect this and handle it correctly, eg. by not using the un-available flags

Actual behavior: Error / fails to build

### Source code / logs


```
@:~/projects/tensorflow <master>$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
WARNING: /Users/mortutay/projects/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /Users/mortutay/projects/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
INFO: Found 1 target...
INFO: From Compiling external/protobuf_archive/python/google/protobuf/internal/api_implementation.cc:
warning: unknown warning option '-Wunused-but-set-parameter'; did you mean '-Wunused-parameter'? [-Wunknown-warning-option]
warning: unknown warning option '-Wno-free-nonheap-object'; did you mean '-Wno-sequence-point'? [-Wunknown-warning-option]
2 warnings generated.
INFO: From Compiling external/swig/Source/Swig/typemap.c [for host]:
warning: unknown warning option '-Wunused-but-set-parameter'; did you mean '-Wunused-parameter'? [-Wunknown-warning-option]
warning: unknown warning option '-Wno-free-nonheap-object'; did you mean '-Wno-sequence-point'? [-Wunknown-warning-option]
2 warnings generated.
INFO: From Compiling external/grpc/src/core/lib/debug/trace.c:
warning: unknown warning option '-Wunused-but-set-parameter'; did you mean '-Wunused-parameter'? [-Wunknown-warning-option]
warning: unknown warning option '-Wno-free-nonheap-object'; did you mean '-Wno-sequence-point'? [-Wunknown-warning-option]
2 warnings generated.
INFO: From Compiling external/grpc/src/cpp/codegen/codegen_init.cc:
warning: unknown warning option '-Wunused-but-set-parameter'; did you mean '-Wunused-parameter'? [-Wunknown-warning-option]
warning: unknown warning option '-Wno-free-nonheap-object'; did you mean '-Wno-sequence-point'? [-Wunknown-warning-option]
2 warnings generated.
INFO: From Compiling external/grpc/src/core/ext/transport/chttp2/alpn/alpn.c:
warning: unknown warning option '-Wunused-but-set-parameter'; did you mean '-Wunused-parameter'? [-Wunknown-warning-option]
warning: unknown warning option '-Wno-free-nonheap-object'; did you mean '-Wno-sequence-point'? [-Wunknown-warning-option]
2 warnings generated.
ERROR: /private/var/tmp/_bazel_mortutay/dacb21c644505cd819865fa365d2b69e/external/boringssl/BUILD:116:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1).
error: unknown warning option '-Wunused-but-set-parameter'; did you mean '-Wunused-parameter'? [-Werror,-Wunknown-warning-option]
error: unknown warning option '-Wno-free-nonheap-object'; did you mean '-Wno-sequence-point'? [-Werror,-Wunknown-warning-option]
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2.829s, Critical Path: 0.91s
```

"
12121,tf.random_normal_initializer produces inconsistent results with fixed seed on GPU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.0.0, 1.1.0, 1.2.1
- **Python version**: 2.7.10, 3.5.2
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: CUDA release 8.0, V8.0.46; cuDNN 8.0
- **GPU model and memory**: GeForce GTX TITAN X 12GB, GK210GL [Tesla K80] 12GB
- **Exact command to reproduce**: below

### Describe the problem
Setting both graph-level and op-level random seeds to fixed values, I still get inconsistent results initializing variables with `tf.random_normal_initializer`. Here's the code to reproduce the problem

	import numpy as np
	import tensorflow as tf
	
	tf.set_random_seed(0)
	np.random.seed(0)
	shape = (100, 2048)
	seed = 0
	for i in range(100):
	    seed += 1
	    init = tf.random_normal_initializer(stddev=0.02, seed=seed)
	    tf.get_variable(str(i), shape, initializer=init)
	
	session = tf.Session()
	session.run(tf.global_variables_initializer())
	var_dict = {}
	for var in tf.trainable_variables():
	    var_dict[var.name] = session.run(var.name)
	
	np.savez_compressed(""weights.npz"", **var_dict)
	session.close()

Here I initialize 100 variables with `tf.random_normal_initializer` with a fixed op-level seed and with a fixed graph-level seed.

Running this two times I get different results saved in `weights.npz`. Interestingly, this happens only when using GPU and the difference between the saved weights is very slight: usually only some of the variables are different, and they only differ in some small number of positions. 

I experience this problem on Tensorflow 1.0.0 (Python2), 1.1.0, 1.2.1 (Python3) and on two different GPUs. "
12119,Please add predict_proba to estimators class not in TF1.3 DNN,"### Describe the problem

Feature Request:
The TF.learn library in TF1.2 has perdict_proba for getting probability estimates for predictions it is particularly helpful for setting probability levels up or down or just measuring them.. it is a super helpful function for analysis as well as correlations can be drawn to input variables...

There are many use cases where you may want to accept a prediction with less than a 50/50 (think unbalanced classs)
and there are other cases where you want greater certainty,  for some medical decisions or credit scoring..

The DNN class in TF 1.3 with the estimator does not seem to have this function. (while tf.learn dnn in TF1.2 does...)

Estimators seem to be the way of the future, but at least IMHO we need this to make that move..


"
12118,C API Functions Support,"Hi, 

I was wondering what the status is for supporting the creation of functions in the C API. Couldn't this be done in a similar manner to how while loops are currently constructed?

Thanks,
Anthony

P.S. @skye This could also be useful for defining gradient functions for ops not supported by the C API gradients."
12117,Bazel Windows Build: '//tensorflow/tools/proto_text:gen_proto_text_functions' failed to link,"This was first caught by Bazel CI: http://ci.bazel.io/blue/organizations/jenkins/TensorFlow/detail/TensorFlow/1030/pipeline
Reported at https://github.com/bazelbuild/bazel/issues/3524
```
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /OUT:bazel-out/msvc_x64-py3-opt/bin/tensorflow/tools/proto_text/gen_proto_text_functions.exe /MACHINE:X64 /SUBSYSTEM:CONSOLE @bazel-out/msvc_x64-py3-opt/bin/tensorflow/tools/proto_text/gen_proto_text_functions.exe-2.params /DEFAULTLIB:libcmt.lib.
liblib_proto_parsing.a(logging.o) : error LNK2019: unresolved external symbol ""public: unsigned __int64 __cdecl tensorflow::StringPiece::Hasher::operator()(class tensorflow::StringPiece)const "" (??RHasher@StringPiece@tensorflow@@QEBA_KV12@@Z) referenced in function ""protected: struct std::pair<class std::_List_iterator<class std::_List_val<struct std::_List_simple_types<struct std::pair<class tensorflow::StringPiece const ,int> > > >,bool> __cdecl std::_Hash<class std::_Umap_traits<class tensorflow::StringPiece,int,class std::_Uhash_compare<class tensorflow::StringPiece,struct tensorflow::StringPiece::Hasher,struct std::equal_to<class tensorflow::StringPiece> >,class std::allocator<struct std::pair<class tensorflow::StringPiece const ,int> >,0> >::_Insert<struct std::pair<class tensorflow::StringPiece const ,int> &,class std::_List_unchecked_iterator<class std::_List_val<struct std::_List_simple_types<struct std::pair<class tensorflow::StringPiece const ,int> > > > >(struct std::pair<class tensorflow::StringPiece const ,int> &,class std::_List_unchecked_iterator<class std::_List_val<struct std::_List_simple_types<struct std::pair<class tensorflow::StringPiece const ,int> > > >)"" (??$_Insert@AEAU?$pair@$$CBVStringPiece@tensorflow@@H@std@@V?$_List_unchecked_iterator@V?$_List_val@U?$_List_simple_types@U?$pair@$$CBVStringPiece@tensorflow@@H@std@@@std@@@std@@@2@@?$_Hash@V?$_Umap_traits@VStringPiece@tensorflow@@HV?$_Uhash_compare@VStringPiece@tensorflow@@UHasher@12@U?$equal_to@VStringPiece@tensorflow@@@std@@@std@@V?$allocator@U?$pair@$$CBVStringPiece@tensorflow@@H@std@@@4@$0A@@std@@@std@@IEAA?AU?$pair@V?$_List_iterator@V?$_List_val@U?$_List_simple_types@U?$pair@$$CBVStringPiece@tensorflow@@H@std@@@std@@@std@@@std@@_N@1@AEAU?$pair@$$CBVStringPiece@tensorflow@@H@1@V?$_List_unchecked_iterator@V?$_List_val@U?$_List_simple_types@U?$pair@$$CBVStringPiece@tensorflow@@H@std@@@std@@@std@@@1@@Z)
bazel-out/msvc_x64-py3-opt/bin/tensorflow/tools/proto_text/gen_proto_text_functions.exe : fatal error LNK1120: 1 unresolved externals
Target //tensorflow/tools/proto_text:gen_proto_text_functions failed to build
```

Culprit is found by bisecting: tensorflow/tensorflow@072b0c9.
The unresolved symbol `tensorflow::StringPiece::Hasher::operator()` is implemented in `stringpiece.cc`. This change should have introduced a dependency on stringpieces.cc from logging.cc.

However, `//tensorflow/tools/proto_text:gen_proto_text_functions` doesn't contain `stringpiece.cc`
```
$ bazel query 'somepath(//tensorflow/tools/proto_text:gen_proto_text_functions, tensorflow/core/lib/core/stringpiece.h)'
//tensorflow/tools/proto_text:gen_proto_text_functions
//tensorflow/core:lib_proto_parsing
//tensorflow/core:lib/core/stringpiece.h

$ bazel query 'somepath(//tensorflow/tools/proto_text:gen_proto_text_functions, tensorflow/core/lib/core/stringpiece.cc)'
INFO: Empty results
```

The strange thing is this didn't break the Linux build, one possible explanation is that there is an implementation of stringpiece.cc in protobuf, which is a dependency of `//tensorflow/tools/proto_text:gen_proto_text_functions`
```
$ bazel query 'deps(//tensorflow/tools/proto_text:gen_proto_text_functions)' | grep stringpiece.cc
@protobuf_archive//:src/google/protobuf/stubs/stringpiece.cc
```
@learyg @gunan 

"
12115,"XLA feature fix: ""INVALID ARGUMENTS: Unsupported type in DataTypeToPrimitiveType complex64""","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra 10.12
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.3
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: tfcompile ...

Currently, the XLA compiler seems to not support complex dtypes. I ran into this when trying to use the spectral ops (ff2d) in an AOT compilation. Is it possible to fix this? Alternatively, is there a workaround that succeeds now? I thought I could maybe bitcast a float64 to a complex64, but it seems bitcast is also not implemented for XLA. "
12114,"How can train my own model in tensorflow using Flickr  image dataset, How can i convert data into train, val, test modules",
12113,bug with tf.unique where index output is int64,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX El Capitan 10.11.6 
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.3.0-rc1-27-g2784b1c 1.3.0-rc2
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: y, idx = tf.unique(x, out_idx=tf.int64)

### Describe the problem
The API says `tf.unique` supports `tf.int64` index output. However, no OpKernel supports such attributes.

### Source code / logs
To reproduce:

```
import tensorflow as tf
import numpy as np

x = tf.convert_to_tensor(np.array([0,1,2,0,1,2], dtype=np.int64))
y, idx = tf.unique(x, out_idx=tf.int64)
sess = tf.Session()
print(sess.run(idx))
```

Output on Macbook Air:
```
2017-08-08 16:43:20.546840: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-08 16:43:20.546883: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-08 16:43:20.546895: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-08 16:43:20.546905: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Traceback (most recent call last):
  File ""/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1327, in _do_call
    return fn(*args)
  File ""/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1297, in _run_fn
    self._extend_graph()
  File ""/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1358, in _extend_graph
    self._session, graph_def.SerializeToString(), status)
  File ""/Users/dturmukh/anaconda/envs/python3/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'Unique' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT32]
  device='GPU'; T in [DT_INT32]; out_idx in [DT_INT32]
  device='GPU'; T in [DT_INT64]; out_idx in [DT_INT32]

	 [[Node: Unique = Unique[T=DT_INT64, out_idx=DT_INT64](Const)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""unique.py"", line 7, in <module>
    print(sess.run(idx))
  File ""/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'Unique' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT32]
  device='GPU'; T in [DT_INT32]; out_idx in [DT_INT32]
  device='GPU'; T in [DT_INT64]; out_idx in [DT_INT32]

	 [[Node: Unique = Unique[T=DT_INT64, out_idx=DT_INT64](Const)]]

Caused by op 'Unique', defined at:
  File ""unique.py"", line 5, in <module>
    y, idx = tf.unique(x, out_idx=tf.int64)
  File ""/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3927, in unique
    result = _op_def_lib.apply_op(""Unique"", x=x, out_idx=out_idx, name=name)
  File ""/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'Unique' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT32]
  device='GPU'; T in [DT_INT32]; out_idx in [DT_INT32]
  device='GPU'; T in [DT_INT64]; out_idx in [DT_INT32]

	 [[Node: Unique = Unique[T=DT_INT64, out_idx=DT_INT64](Const)]]
```"
12112,tf.contrib.slim.nets moved?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0.44 / 5.1
- **GPU model and memory**: Nvidia Tesla P100
- **Exact command to reproduce**: 
import tensorflow as tf
vgg = tf.contrib.slim.nets.vgg

### Describe the problem
The Problem is that slim does not seem to contain the directory nets (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim/python/slim/nets) anymore even though in the repo it's still there.
It's also still in the slim docs (https://github.com/tensorflow/tensorflow/issues/12112).

### Source code / logs
import tensorflow as tf
vgg = tf.contrib.slim.nets.vgg
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
AttributeError: module 'tensorflow.contrib.slim' has no attribute 'nets'
"
12110,Android example broken with nativeBuildSystem = none,"This commit breaks the android example when nativeBuildSystem = none since it directly uses the native image conversion functions rather than calling the java ones. The native conversions are not implemented when nativeBuildSystem = none.

https://github.com/tensorflow/tensorflow/commit/003deb88b7fb015db86089c2a87b3044cad2c714

To reproduce:

1. Set nativeBuildSystem = none
2. Build example
3. Run on a phone
4. Observe crash on startup and errors about unimplemented methods
"
12109,Compiling Tensorflow Crashes,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16
- TensorFlow installed from (source or binary):
Source
- TensorFlow version (use command below):
1.3-rc2
- Exact command to reproduce:
cpu=armeabi-v7a
bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=$cpu --verbose_failures

I have a VM setup with Ubuntu 16 64bit.
I tried compiling the tensorflow library with Bazel to generate the .so and .jar files.

I have modified the register_types.h file according to 
https://github.com/bmount/tensorflow/commit/24b59a4a7797623a9da9311ee1214ac334478ed3#diff-76e272a58ca1535b3e0ec93499779a14

But I don't think this issue is because of that change.

The Error is

`ERROR: /home/anand/TensorflowAndroidPort/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:4539:1: Linking of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed (Exit 1): arm-linux-androideabi-ar failed: error executing command`
`Target //tensorflow/contrib/android:libtensorflow_inference.so failed to build
INFO: Elapsed time: 2134.608s, Critical Path: 54.46s
FAILED: Build did NOT complete successfully`

With Verbose Failure Option:
`ERROR: /home/anand/TensorflowAndroidPort/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:4539:1: Linking of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed (Exit 1): arm-linux-androideabi-ar failed: error executing command 
  (exec env - \
    PWD=/proc/self/cwd \
  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-ar rcsD bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/libandroid_tensorflow_kernels.lo bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/aggregate_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/bias_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_bfloat.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_bool.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_complex128.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_complex64.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_double.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_float.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_half.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_int16.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_int32.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_int64.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_int8.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_uint16.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_uint8.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/concat_lib_cpu.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/concat_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/constant_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_ops_common.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/dense_update_functor.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/dense_update_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/example_parsing_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/fill_functor.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/function_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/gather_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/identity_n_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/identity_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/immutable_constant_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/matmul_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/no_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/non_max_suppression_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/one_hot_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/pack_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reshape_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reverse_sequence_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/sendrecv_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/sequence_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/shape_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/slice_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/slice_op_cpu_impl_1.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/slice_op_cpu_impl_2.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/slice_op_cpu_impl_3.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/slice_op_cpu_impl_4.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/slice_op_cpu_impl_5.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/slice_op_cpu_impl_6.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/slice_op_cpu_impl_7.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/softmax_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/split_lib_cpu.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/split_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/split_v_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/strided_slice_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/strided_slice_op_inst_0.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/strided_slice_op_inst_1.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/strided_slice_op_inst_2.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/strided_slice_op_inst_3.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/strided_slice_op_inst_4.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/strided_slice_op_inst_5.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/strided_slice_op_inst_6.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/strided_slice_op_inst_7.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/unpack_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/variable_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/argmax_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/avgpooling_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/batch_matmul_op_real.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/batch_norm_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/bcast_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/check_numerics_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/control_flow_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/conv_grad_filter_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/conv_grad_input_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/conv_grad_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/conv_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/conv_ops_fused.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/conv_ops_using_gemm.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/crop_and_resize_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_abs.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_add_1.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_add_2.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_bitwise_and.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_bitwise_or.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_bitwise_xor.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_div.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_equal_to_1.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_equal_to_2.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_exp.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_floor.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_floor_div.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_greater.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_greater_equal.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_invert.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_isfinite.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_less.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_less_equal.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_log.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_logical_and.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_logical_not.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_logical_or.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_maximum.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_minimum.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_mul_1.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_mul_2.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_neg.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_pow.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_reciprocal.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_rsqrt.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_select.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_sigmoid.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_sign.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_sqrt.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_square.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_squared_difference.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_sub.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_tanh.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/deep_conv2d.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/depthwise_conv_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/dynamic_partition_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/fake_quant_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/fifo_queue.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/fused_batch_norm_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/population_count_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/batchtospace_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/ctc_decoder_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/decode_bmp_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/depthtospace_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/dynamic_stitch_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/in_topk_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/logging_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/lrn_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/maxpooling_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/mirror_pad_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/mirror_pad_op_cpu_impl_1.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/mirror_pad_op_cpu_impl_2.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/mirror_pad_op_cpu_impl_3.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/mirror_pad_op_cpu_impl_4.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/mirror_pad_op_cpu_impl_5.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/pad_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/padding_fifo_queue.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/padding_fifo_queue_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/queue_base.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/queue_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/random_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reduction_ops_all.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reduction_ops_any.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reduction_ops_common.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reduction_ops_max.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reduction_ops_mean.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reduction_ops_min.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reduction_ops_prod.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reduction_ops_sum.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/relu_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/resize_bilinear_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/resize_nearest_neighbor_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/restore_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reverse_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/save_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/save_restore_tensor.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/save_restore_v2_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/session_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/softplus_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/softsign_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/spacetobatch_functor.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/spacetobatch_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/spacetodepth_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/sparse_to_dense_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/stack_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/string_join_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/summary_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tensor_array.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tensor_array_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tile_functor_cpu.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tile_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tile_ops_cpu_impl_1.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tile_ops_cpu_impl_2.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tile_ops_cpu_impl_3.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tile_ops_cpu_impl_4.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tile_ops_cpu_impl_5.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tile_ops_cpu_impl_6.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tile_ops_cpu_impl_7.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/topk_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/training_op_helpers.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/training_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/transpose_functor_cpu.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/transpose_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/warn_about_ints.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/where_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/xent_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/dequantize_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/meta_support.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantization_utils.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantize_down_and_shrink_range.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantize_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_activation_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_add_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_batch_norm_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_bias_add_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_concat_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_conv_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_instance_norm.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_matmul_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_mul_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_pooling_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_reshape_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_resize_bilinear_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/requantization_range_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/requantize.o)
Target //tensorflow/contrib/android:libtensorflow_inference.so failed to build
INFO: Elapsed time: 2134.608s, Critical Path: 54.46s
FAILED: Build did NOT complete successfully`

Bazel Version info:

`Build label: 0.5.3
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jul 28 08:34:59 2017 (1501230899)
Build timestamp: 1501230899
Build timestamp as int: 1501230899`

I'm using NDK version 12b with API version 23 and SDK 23 with Build Tools version 25.0.2 (SDK was installed through android studio)
"
12108,tf.string_split didn't behave the same as that split in python,"In python, if we split the following string `'a#'.split('#')`, we will get a list of two elements: `['a', '']`.
While in tensorflow, if we use `tf.string_split`, we will get an sparsetensor with only one elements. 
For reproduction, you can run the following piece of code
```
a = tf.constant(""#2"")
b = tf.string_split([a], ""#"")
with tf.Session() as ss:
    ss.run(tf.local_variables_initializer())
    print(ss.run(b))
```
As a result, when we use `tf.string_split`, we don't know whether the `2` is before or after the `#`. I think such behavior is undesirable since we usually have missing values when decoding csv files."
12106,Build fails for certain GCC paths,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 17.04
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.3-rc2
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
5.2
- **CUDA/cuDNN version**:
8.0 / 5.1.10
- **GPU model and memory**:
Nvidia GTX 1080 Ti
- **Exact command to reproduce**:
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures --spawn_strategy=standalone

### Describe the problem
Build will fail if compiler is not located in specific paths (like `/usr/bin`). Also will happen by compiling with a symbolic link to compiler if the link reside there.

**Steps to reproduce**
Make a symbolic link to GCC and store it somewhere like `/etc/gcc`. run `./configure` and set compiler path to `/etc/gcc` then run bazel build. This is probably why builds failing on certain many Linux distributions and is related to issues like #3550 and many other abandoned ones.

### Source code / logs

`$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures --spawn_strategy=standalone`
```
WARNING: /opt/tensorflow-1.3.0-rc2/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /opt/tensorflow-1.3.0-rc2/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
INFO: Found 1 target...
ERROR: /root/.cache/bazel/_bazel_root/84ac956a7b3384a65a68aa2a845ef1a1/external/lmdb/BUILD.bazel:8:1: C++ compilation of rule '@lmdb//:lmdb' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/84ac956a7b3384a65a68aa2a845ef1a1/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/opt/tensorflow-1.3.0-rc2/cuda-plat-sym \
    CUDNN_INSTALL_PATH=/opt/tensorflow-1.3.0-rc2/cuda-plat-sym \
    GCC_HOST_COMPILER_PATH=/etc/some-gcc \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2 \
    TF_CUDA_VERSION=8.0 \
    TF_CUDNN_VERSION=5.1.10 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-march=native' -MD -MF bazel-out/local_linux-opt/bin/external/lmdb/_objs/lmdb/external/lmdb/mdb.pic.d -fPIC -iquote external/lmdb -iquote bazel-out/local_linux-opt/genfiles/external/lmdb -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -w -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c external/lmdb/mdb.c -o bazel-out/local_linux-opt/bin/external/lmdb/_objs/lmdb/external/lmdb/mdb.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
some-gcc: error trying to exec 'cc1': execvp: No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```"
12101,Build the latest source code will fail under Linux platform,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10 (Artful Aardvark)
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: N/A
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.5.3
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
Build the the TF from the latest version of source code will fail.

### Source code / logs
WARNING: /home/kevin/research/openSource/tensorflow-fork/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/kevin/research/openSource/tensorflow-fork/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (177 packages loaded).
INFO: Found 1 target...
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: no such package '@protobuf//src/google/protobuf': Could not find handler for bind rule //external:protobuf
INFO: Elapsed time: 10.961s, Critical Path: 0.12s
FAILED: Build did NOT complete successfully
"
12099,Cannot import tensorflow 1.0.1 after compiled from source,"

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.0.1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.5.3
- **CUDA/cuDNN version**: x
- **GPU model and memory**: x
- **Exact command to reproduce**: import tensorflow

### Describe the problem
I successfully built tensorflow 1.0.1 from source using optimization flags with the command `bazel build -c opt --copt=-mmmx --copt=-msse --copt=-msse2  -k //tensorflow/tools/pip_package:build_pip_package`. After that, I activated my virtualenv and issued a `pip install /tmp/tensorflow_pkg/tensorflow-1.0.1-cp27-cp27mu-linux_x86_64.whl` to install in the virtual env this compiled from source version. When I try to import tensorflow in the python code, I receive a error (showing it with -v flag from python interpreter activated) showed below in the source code / logs session.

### Source code / logs
```import numpy.ma.core # precompiled from /home/ubuntu/miniconda3/envs/tf_exp/lib/python2.7/site-packages/numpy/ma/core.pyc
# /home/ubuntu/miniconda3/envs/tf_exp/lib/python2.7/site-packages/numpy/ma/extras.pyc matches /home/ubuntu/miniconda3/envs/tf_exp/lib/python2.7/site-packages/numpy/ma/extras.py
import numpy.ma.extras # precompiled from /home/ubuntu/miniconda3/envs/tf_exp/lib/python2.7/site-packages/numpy/ma/extras.pyc
# /home/ubuntu/miniconda3/envs/tf_exp/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.pyc matches /home/ubuntu/miniconda3/envs/tf_exp/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py
import tensorflow.python.pywrap_tensorflow # precompiled from /home/ubuntu/miniconda3/envs/tf_exp/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.pyc
dlopen(""/home/ubuntu/miniconda3/envs/tf_exp/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so"", 102);
Illegal instruction (core dumped)
```
"
12098,Tensorflow GPU import error,"------------------------

### System information
- **OS Platform**: Windows 10 Pro x64
- **TensorFlow installed from**: pip install tensorflow-gpu
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 3.5.3
- **CUDA/cuDNN version**: CUDA8.0 / cuDNN v7
- **GPU model and memory**: GTX 1050 
- **Exact command to reproduce**: ```import tensorflow```

### Describe the problem
It is impossible to import tensorflow in my present environment. I have checked for a compatible GPU and the neccessary dll's. Even though it hasn't been able to import the tensorflow modules.

### Source code / logs
```python
Traceback (most recent call last):
  File ""D:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""D:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 914, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: No se puede encontrar el módulo especificado.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""D:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""D:\Program Files\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""D:\Program Files\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""D:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""D:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 914, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: No se puede encontrar el módulo especificado.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""D:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```
"
12097,I got a type error without rhyme or reason，when i rewrite a _linear function in the rnn_cell_impl.py,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:win64
- **TensorFlow installed from (source or binary)**: pip 
- **TensorFlow version (use command below)**:1.2
- **Python version**:  3.6
- **CUDA/cuDNN version**:ONLY CPU
- **GPU model and memory**:ONLY CPU
- **Exact command to reproduce**: No

### Describe the problem
I want to write a lstm with batch normalization . After , i read the code of BasicLSTMCell , i find i only need to wirte a _linear function acording to this paper https://arxiv.org/pdf/1603.09025.pdf section 3
and the new _batchlinear function is  below here , the only difference between _batchlinear function  and 
_linear function is  the arg mul it's weights separately and do  it's batch normalization .when i build a  multi layer rnn like this 
```
cells       = [BatchLSTMCell(rnn_numhidden,forget_bias=0.,activation=tf.tanh) for _ in range(num_rnn)]
stack       = tf.contrib.rnn.MultiRNNCell(cells,state_is_tuple=True)
net, _      = tf.nn.dynamic_rnn(stack, net, seqlen_batch, dtype=tf.float32)
```
TypeError: int() argument must be a string, a bytes-like object or a number, not 'TensorShape'
### Source code / logs
source code are here
```
def _batchlinear(   args,
                    output_size,
                    bias,
                    xh_epsilon = 1e-3,
                    hh_epsilon = 1e-3,
                    bias_initializer=None,
                    kernel_initializer= None):           
  if args is None or (nest.is_sequence(args) and not args):
    raise ValueError(""`args` must be specified"")
  if not nest.is_sequence(args):
    args = [args]

  # Calculate the total size of arguments on dimension 1.
  total_arg_size = 0
  shapes = [a.get_shape() for a in args]
  for shape in shapes:
    if shape.ndims != 2:
      raise ValueError(""linear is expecting 2D arguments: %s"" % shapes)
    if shape[1].value is None:
      raise ValueError(""linear expects shape[1] to be provided for shape %s, ""
                       ""but saw %s"" % (shape, shape[1]))
    else:
      total_arg_size += shape[1].value

  dtype = [a.dtype for a in args][0]

  # Now the computation.
  scope = vs.get_variable_scope()
  with vs.variable_scope(scope) as outer_scope:
    if len(args) == 1:
        weights_xh = vs.get_variable('W_xh',
            [shapes[0], output_size],
            dtype = dtype,
            initializer=kernel_initializer)
        res = math_ops.matmul(args[0], weights_xh)
    else:
        weights_xh = vs.get_variable('W_xh',
            [shapes[0], output_size],
            dtype = dtype,
            initializer=kernel_initializer)
        xh = math_ops.matmul(args[0], weights_xh)
        xh_scale = vs.get_variable('xh_scale', [output_size], initializer=init_ops.constant_initializer(0.1, dtype=dtype))
        xh_offset = vs.get_variable('xh_offset', [output_size])
        xh_batch_mean, xh_batch_var = nn_impl.moments(xh, [0])
        xh = (xh - xh_batch_mean) / math_ops.sqrt(xh_batch_var + xh_epsilon)
        xh = xh_scale*xh + xh_offset
        if kernel_initializer is None:
            weights_hh = vs.get_variable('W_hh',
                [shapes[0], output_size],
                dtype = dtype)
        hh = math_ops.matmul(args[0],weights_hh)
        hh_scale = vs.get_variable('hh_scale', [output_size], initializer=init_ops.constant_initializer(0.1, dtype=dtype))
        hh_offset = vs.get_variable('hh_offset', [output_size])
        hh_batch_mean, hh_batch_var = nn_impl.moments(hh, [0])
        hh = (hh - hh_batch_mean) / math_ops.sqrt(hh_batch_var + hh_epsilon)
        xh = hh_scale*hh + hh_offset
        res = xh+hh
    if not bias:
      return res
    with vs.variable_scope(outer_scope) as inner_scope:
      inner_scope.set_partitioner(None)
      if bias_initializer is None:
        bias_initializer = init_ops.constant_initializer(0.0, dtype=dtype)
      biases = vs.get_variable(
          'bias', [output_size],
          dtype=dtype,
          initializer=bias_initializer)
    return nn_ops.bias_add(res, biases)
```
```
class BatchLSTMCell(RNNCell):
  """"""Basic LSTM recurrent network cell.

  The implementation is based on: http://arxiv.org/abs/1409.2329.

  We add forget_bias (default: 1) to the biases of the forget gate in order to
  reduce the scale of forgetting in the beginning of the training.

  It does not allow cell clipping, a projection layer, and does not
  use peep-hole connections: it is the basic baseline.

  For advanced models, please use the full @{tf.nn.rnn_cell.LSTMCell}
  that follows.
  """"""

  def __init__(self, num_units, forget_bias=1.0,
               state_is_tuple=True, activation=None, reuse=None):
    """"""Initialize the basic LSTM cell.

    Args:
      num_units: int, The number of units in the LSTM cell.
      forget_bias: float, The bias added to forget gates (see above).
      state_is_tuple: If True, accepted and returned states are 2-tuples of
        the `c_state` and `m_state`.  If False, they are concatenated
        along the column axis.  The latter behavior will soon be deprecated.
      activation: Activation function of the inner states.  Default: `tanh`.
      reuse: (optional) Python boolean describing whether to reuse variables
        in an existing scope.  If not `True`, and the existing scope already has
        the given variables, an error is raised.
    """"""
    super(BatchLSTMCell, self).__init__(_reuse=reuse)
    if not state_is_tuple:
      logging.warn(""%s: Using a concatenated state is slower and will soon be ""
                   ""deprecated.  Use state_is_tuple=True."", self)
    self._num_units = num_units
    self._forget_bias = forget_bias
    self._state_is_tuple = state_is_tuple
    self._activation = activation or math_ops.tanh

  @property
  def state_size(self):
    return (LSTMStateTuple(self._num_units, self._num_units)
            if self._state_is_tuple else 2 * self._num_units)

  @property
  def output_size(self):
    return self._num_units

  def call(self, inputs, state):
    """"""Long short-term memory cell (LSTM).""""""
    sigmoid = math_ops.sigmoid
    # Parameters of gates are concatenated into one multiply for efficiency.
    if self._state_is_tuple:
      c, h = state
    else:
      c, h = array_ops.split(value=state, num_or_size_splits=2, axis=1)

      concat = _batchlinear(args=[inputs, h], output_size=4 * self._num_units, bias=True)

    # i = input_gate, j = new_input, f = forget_gate, o = output_gate
    i, j, f, o = array_ops.split(value=concat, num_or_size_splits=4, axis=1)

    new_c = (
        c * sigmoid(f + self._forget_bias) + sigmoid(i) * self._activation(j))
    new_h = self._activation(new_c) * sigmoid(o)

    if self._state_is_tuple:
      new_state = LSTMStateTuple(new_c, new_h)
    else:
      new_state = array_ops.concat([new_c, new_h], 1)
    return new_h, new_state
```

logs are here 
Traceback (most recent call last):

  File ""<ipython-input-1-21f874a460ec>"", line 1, in <module>
    runfile('C:/Users/Administrator/Test/char_rnn.py', wdir='C:/Users/Administrator/Test')

  File ""C:\ProgramData\Miniconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 688, in runfile
    execfile(filename, namespace)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 101, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/Administrator/Test/char_rnn.py"", line 25, in <module>
    final_logits = charRnn(char_seq_batch,seqlen_batch,charsize=charsize,num_rnn=num_layers,rnn_numhidden = rnn_numhidden)

  File ""C:\Users\Administrator\Test\networks.py"", line 221, in charRnn
    net, _      = tf.nn.dynamic_rnn(stack, net, seqlen_batch, dtype=tf.float32)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\ops\rnn.py"", line 574, in dynamic_rnn
    dtype=dtype)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\ops\rnn.py"", line 737, in _dynamic_rnn_loop
    swap_memory=swap_memory)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2770, in while_loop
    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2599, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2549, in _BuildLoop
    body_result = body(*packed_vars_for_body)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\ops\rnn.py"", line 720, in _time_step
    skip_conditionals=True)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\ops\rnn.py"", line 206, in _rnn_step
    new_output, new_state = call_cell()

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\ops\rnn.py"", line 708, in <lambda>
    call_cell = lambda: cell(input_t, state)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py"", line 181, in __call__
    return super(RNNCell, self).__call__(inputs, state)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\layers\base.py"", line 441, in __call__
    outputs = self.call(inputs, *args, **kwargs)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py"", line 917, in call
    cur_inp, new_state = cell(cur_inp, cur_state)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py"", line 181, in __call__
    return super(RNNCell, self).__call__(inputs, state)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\layers\base.py"", line 441, in __call__
    outputs = self.call(inputs, *args, **kwargs)

  File ""C:\Users\Administrator\Test\networks.py"", line 167, in call
    concat = _batchlinear(args=[inputs, h], output_size=4 * self._num_units, bias=True)

  File ""C:\Users\Administrator\Test\networks.py"", line 80, in _batchlinear
    initializer=kernel_initializer)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 1065, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 962, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 360, in get_variable
    validate_shape=validate_shape, use_resource=use_resource)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 1405, in wrapped_custom_getter
    *args, **kwargs)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py"", line 184, in _rnn_get_variable
    variable = getter(*args, **kwargs)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py"", line 184, in _rnn_get_variable
    variable = getter(*args, **kwargs)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 352, in _true_getter
    use_resource=use_resource)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 653, in _get_single_variable
    shape = tensor_shape.as_shape(shape)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 798, in as_shape
    return TensorShape(shape)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 434, in __init__
    self._dims = [as_dimension(d) for d in dims_iter]

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 434, in <listcomp>
    self._dims = [as_dimension(d) for d in dims_iter]

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 376, in as_dimension
    return Dimension(value)

  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 32, in __init__
    self._value = int(value)

TypeError: int() argument must be a string, a bytes-like object or a number, not 'TensorShape'"
12096,"I configured ops_to_register.h and set #define SHOULD_REGISTER_OP_GRADIENT false, but unexpectedly the generated libtensorflow_inference.so file get larger.","### Describe the problem
 I  configured ops_to_register.h as below:

#define SHOULD_REGISTER_OP(op) true
#define SHOULD_REGISTER_OP_GRADIENT false
#define SHOULD_REGISTER_OP_KERNEL(clz) true

set  SHOULD_REGISTER_OP_GRADIENT false. And then I used command below to generate libtensorflow_inference.so file. 

`bazel build -c opt --copt=""-DSELECTIVE_REGISTRATION"" \                                         
            --copt=""-DSUPPORT_SELECTIVE_REGISTRATION"" \
            //tensorflow/contrib/android:libtensorflow_inference.so \
            --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
            --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a`

I expected that this configuration will reduce gradient operations and make the genereted libtensorflow_inference.so file smaller. But unexpectedly, the generated file is 16M ,while  the .so file generate by command below which will include all operators is 9.7M.

` bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so \
         --crosstool_top=//external:android/crosstool \
         --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
         --cpu=armeabi-v7a`

### Question
I can't understand why the .so file get larger when  I set SHOULD_REGISTER_OP_GRADIENT false than the original .so file which include all operators. Could you please explain it to me ? Thank you very much and hope for your answer."
12095,WARNING:tensorflow:The default stddev value of initializer will change from “1/sqrt(vocab_size)” to “1/sqrt(dimension)” after 2017/02/25,"I got this warning message when set Deep Model.

Warning message

```
WARNING:tensorflow:The default stddev value of initializer will change from ""1/sqrt(vocab_size)"" to ""1/sqrt(dimension)"" after 2017/02/25.
```

My code

```
    deep_columns = [
      tf.contrib.layers.embedding_column(workclass, dimension=8),
      tf.contrib.layers.embedding_column(education, dimension=8),
      tf.contrib.layers.embedding_column(gender, dimension=8),
      tf.contrib.layers.embedding_column(relationship, dimension=8),
      tf.contrib.layers.embedding_column(native_country, dimension=8),
      tf.contrib.layers.embedding_column(occupation, dimension=8),
      age, education_num, capital_gain, capital_loss, hours_per_week
    ]
```

Please advice. Thank you."
12093,Feature Request - Return final loss from tf.estimator.Estimator.train along with self.,"### Describe the problem
Things like hyper opt need to know the loss so that it can effectively pick the best hyper parameters for the model. Right now self is just returned, it is a one line change to the code since the loss is set the exact line before. I would be happy to initiate the PR myself assuming its wanted.

### Source code / logs
From:
```
loss = self._train_model(input_fn=input_fn, hooks=hooks)
logging.info('Loss for final step: %s.', loss)
return self
```

To:
```
loss = self._train_model(input_fn=input_fn, hooks=hooks)
logging.info('Loss for final step: %s.', loss)
return self, loss
```
"
12091,Compiling from source,"I am trying to compile tensorflow from source as the pip version is compiled on older version of cuDNN 

### System information
- **OS Platform and Distribution ** : Linux Ubuntu 16.04
- **TensorFlow installed from** source:
- **TensorFlow version (use command below)**: 
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:  Build label: 0.5.3
- **CUDA/cuDNN version**: CUDA 8, cuDNN 7.0.1
- **GPU model and memory**: GTX 1080 Ti and VRAM 11GB, RAM 32 GB
- **Exact command to reproduce**:
Trying to compile with   :  https://www.tensorflow.org/install/install_sources

### Describe the problem
Unable to compile from source. I tried the binary but it uses an old version of cuDNN I am using 7.0.1

### Source code / logs
ERROR: /home/ntweat/tensorflow/tensorflow/stream_executor/BUILD:39:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python2.7 \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_CUDA_VERSION=8.0 \
    TF_CUDNN_VERSION=7.0.1 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL=0 \
"
12090,imagenet_distributed_train using inception v3 stuck on saving check points forever.,"System information
```bash
== cat /etc/issue ===============================================
Linux ip-172-30-4-87 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15:04:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""7 (Core)""
VERSION_ID=""7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux ip-172-30-4-87 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15:04:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.0)
protobuf (3.3.0)
tensorflow-gpu (1.2.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.2.0
tf.GIT_VERSION = v1.2.0-rc2-21-g12f033d
tf.COMPILER_VERSION = v1.2.0-rc2-21-g12f033d
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Mon Aug  7 21:00:40 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.51                 Driver Version: 375.51                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           On   | 0000:00:1E.0     Off |                    0 |
| N/A   51C    P0    72W / 149W |  10944MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      2886    C   /bin/python                                  10938MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
```
I am running the imagenet_distributed_train.py of inception: https://github.com/tensorflow/models/tree/master/inception, with 16 AWS p2x2 machines. I didn't change any code of inception and follow the guidance to run imagenet_distributed_train using parallel-ssh.

The script I use to run parallel-ssh:
```python
from pssh.pssh_client import ParallelSSHClient
import datetime
from pprint import pprint
from pssh.utils import load_private_key

output_ps = []
output_worker = []
some host ip here
ps = [host1,host2,host3]
worker = [host0,host1,host2,host3,host4,host5,host6,host7,host8,host9,host10,host11,host12,host13,host14,host15]
client_ps = ParallelSSHClient(ps, user='centos')
client_worker = ParallelSSHClient(worker, user='centos')

output_ps = client_ps.run_command('%s', host_args=(
    ('/imagenet/run_ps.sh --job_name ps --task_id 0 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_ps.sh --job_name ps --task_id 1 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_ps.sh --job_name ps --task_id 2 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ))

output_worker = client_worker.run_command( '%s', host_args=(
    ('/imagenet/run_worker.sh --job_name worker --task_id 0 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 1 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 2 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 3 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 4 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 5 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 6 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 7 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 8 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 9 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 10 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 11 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 12 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 13 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 14 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 15 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
       ))

client_ps.join(output_ps)
#client_worker.join(output_worker)
pprint(output_ps.values()[0].exit_code)
#pprint(output_worker.values()[0].exit_code)

for host, host_output in output_ps.items():
    for line in host_output.stdout:
        print(""Host [%s] - %s"" % (host, line))
```
I think this script worked fine because I logged in every machine and checked with ps command and ensured the program was running with correct parameters. Then the program just worked fine but to some point, it started to save checkpoints forever(here is the output of worker0):

```bash
INFO:tensorflow:Worker 0: 2017-08-04 06:46:08.510727: step 2340, loss = 11.22(2.0 examples/sec; 15.788  sec/batch)
INFO:tensorflow:Running Summary operation on the chief.
INFO:tensorflow:Finished running Summary operation.
INFO:tensorflow:Running Summary operation on the chief.
INFO:tensorflow:Finished running Summary operation.
INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt
INFO:tensorflow:Running Summary operation on the chief.
INFO:tensorflow:Finished running Summary operation.
INFO:tensorflow:Worker 0: 2017-08-04 06:53:55.553703: step 2370, loss = 10.30(2.1 examples/sec; 15.573  sec/batch)
INFO:tensorflow:Running Summary operation on the chief.
INFO:tensorflow:Finished running Summary operation.
INFO:tensorflow:Running Summary operation on the chief.
INFO:tensorflow:Finished running Summary operation.
INFO:tensorflow:Worker 0: 2017-08-04 07:01:44.226068: step 2400, loss = 10.84(2.1 examples/sec; 15.421  sec/batch)
INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt
INFO:tensorflow:Running Summary operation on the chief.
INFO:tensorflow:Finished running Summary operation.
INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt
INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt
INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt
INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt
INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt
INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt
INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt
INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt
(same saving checkpoint output forever)
```
I ran nvidia-smi and found the GPU wasn't working and same with other nodes. The output of worker 1-15 just stucked on step 2400 and didn't do any progress. I tried this several time on new set of 16 machines but it all stucked on saving checkpoint forever problem at some time. I guess it might be a bug in tensorflow? Or does this caused network failure? but it didn't retrun any network failure error."
12088,TENSORFLOW CUDNN_HOME NOT USED,"When I build with cmake on windows10, there is always a warning:

CMake Warning:
  Manually-specified variables were not used by the project:

    CUDNN_HOME

If I just ignore it, there will be fatal errors about missing pb.h files. Can anyone help?"
12086,Cannot learn initial_states with batch_sequences_with_states,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 18
- **TensorFlow installed from (source or binary)**: Binary (pip)
- **TensorFlow version (use command below)**: v1.3.0.0rc0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
Using `tf.contrib.training.batch_sequences_with_states`, it seems impossible to learn the `initial_states` passed to it. I've tried using Variables with `trainable=True` with several initializers and while my optimizer picks them up through `tf.trainable_variables()`, their values do not get learned.

I assume this has something to do with TensorFlow continuing to work with the Tensors, not the Variables? Additionally, the initializer is called each run and in TensorBoard it shows that it does not feed into the optimizer, whereas network weights for example would.

Are my presumptions correct, and if not, how do I fix this? If this is a TensorFlow limitation, how could I work around it?

### Source code / logs
```
            initializers = {
                    'lstm_c': train_init,
                    'lstm_h': train_init,
                    'encode_lstm_c': train_init,
                    'encode_lstm_h': train_init,
                    'last_out': train_init,
                    }

            with tf.variable_scope(tf.get_variable_scope(), reuse=not is_training):

                initial_states = { k: tf.get_variable('initial_{}'.format(k), v.shape,
                        dtype=v.dtype,
                        initializer=initializers[k],
                        trainable=is_training and self.learn_initial_states, \
                        collections=[attend.GraphKeys.INITIAL_STATES]) \
                        for k, v in initial_constants.items() }


                # This makes sure we're not dealing with the reference but learned values
                initial_states = { k: v.initialized_value() for k, v in initial_states.items() }
```
"
12084,InvalidArgumentError mobilenet,"im getting the following error while training my own dataset with mobilenet

```
Caused by op 'MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise', defined at:
  File ""tensorflow/examples/image_retraining/retrain.py"", line 1326, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""tensorflow/examples/image_retraining/retrain.py"", line 984, in main
    create_model_graph(model_info))
  File ""tensorflow/examples/image_retraining/retrain.py"", line 282, in create_model_graph
    model_info['resized_input_tensor_name'],
  File ""/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 287, in import_graph_def
    op_def=op_def)
  File ""/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): NodeDef mentions attr 'data_format' not in Op<name=DepthwiseConv2dNative; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=padding:string,allowed=[""SAME"", ""VALID""]>; NodeDef: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read)
	 [[Node: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read)]]


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tensorflow/examples/image_retraining/retrain.py"", line 1326, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""tensorflow/examples/image_retraining/retrain.py"", line 1025, in main
    bottleneck_tensor, FLAGS.architecture)
  File ""tensorflow/examples/image_retraining/retrain.py"", line 476, in cache_bottlenecks
    resized_input_tensor, bottleneck_tensor, architecture)
  File ""tensorflow/examples/image_retraining/retrain.py"", line 418, in get_or_create_bottleneck
    bottleneck_tensor)
  File ""tensorflow/examples/image_retraining/retrain.py"", line 373, in create_bottleneck_file
    str(e)))
RuntimeError: Error during processing file tensorflow/examples/image_retraining/dataset/female/2Q== (1).jpg (NodeDef mentions attr 'data_format' not in Op<name=DepthwiseConv2dNative; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=padding:string,allowed=[""SAME"", ""VALID""]>; NodeDef: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read)
	 [[Node: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read)]]

Caused by op 'MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise', defined at:
  File ""tensorflow/examples/image_retraining/retrain.py"", line 1326, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""tensorflow/examples/image_retraining/retrain.py"", line 984, in main
    create_model_graph(model_info))
  File ""tensorflow/examples/image_retraining/retrain.py"", line 282, in create_model_graph
    model_info['resized_input_tensor_name'],
  File ""/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 287, in import_graph_def
    op_def=op_def)
  File ""/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): NodeDef mentions attr 'data_format' not in Op<name=DepthwiseConv2dNative; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=padding:string,allowed=[""SAME"", ""VALID""]>; NodeDef: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read)
	 [[Node: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read)]]
)

```"
12083,tf.contrib.data.Iterator.make_initializer operation lacks name,"I'm using tf.contrib.data for my input pipeline. Since it feeds the model without placeholders, I need to be able to reload both the model and input pipeline in order to resume training later. However, the make_initializer method of tf.contrib.data.Iterator does not have a name argument like most (all?) other Tensorflow operations, making it impossible (or at best, error-prone) to find these ops in the reloaded graph. Unless I'm mistaken, this seems like a standard use-case and the missing name argument is inconsistent with the rest of Tensorflow."
12082,AttributeError,"Im getting the following error while training mobile net model as

```
 python retrain.py \
    --image_dir ~/dataset --architecture mobilenet_1.0_224
```


Following is the error..

```
 _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""retrain.py"", line 989, in main
    class_count = len(image_lists.keys())
AttributeError: 'NoneType' object has no attribute 'keys'
```"
12081,Tensorflow bezel command line c options does not pass down to gcc if static object is compiled,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
CentOS 7.1
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.3 rc1
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
0.5.1
- **CUDA/cuDNN version**:
n/a
- **GPU model and memory**:
n/a
- **Exact command to reproduce**:
bazel build --copt=""-DEIGEN_USE_MKL_VML"" -c opt //tensorflow/tools/pip_package:build_pip_package -s

### Describe the problem


When I compile TensorFlow with bazel, I found the c option I put in the bazel command line only passed to the object file for a dynamic library (with -fPIC option), not the object file for a static library (without -fPIC).

For example, I ran the following command:

bazel build --copt=""-DEIGEN_USE_MKL_VML"" -c opt //tensorflow/tools/pip_package:build_pip_package -s
I expect -DEIGEN_USE_MKL_VML passed down to gcc, but it does not for *.o. For example, the gcc command line for external/nasm/labels.c is the following:

(cd /nfs/pdx/home/sfu2/.cache/bazel/_bazel_sfu2/fec016c4b4f3097e22950dbc1f4b848d/execroot/private-tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/nfs/pdx/home/sfu2/gcc/install/lib64:/usr/lib64:/usr/local/lib \
    PATH=/nfs/pdx/home/sfu2/bin:/usr/bin:/usr/local/bin/:/usr/lib64/qt-3.3/bin:/nfs/pdx/home/sfu2/perl5/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin \
    PWD=/proc/self/cwd \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -g0 -MD -MF bazel-out/host/bin/external/nasm/_objs/nasm/external/nasm/labels.d -DHAVE_SNPRINTF -iquote external/nasm -iquote bazel-out/host/genfiles/external/nasm -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -w '-std=c99' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nasm/labels.c -o bazel-out/host/bin/external/nasm/_objs/nasm/external/nasm/labels.o)

You can see the c option ""-DEIGEN_USE_MKL_VML"" is not on the command line.

Is it a bug in TensorFlow build script?"
12080,distribute tensorflow programe failed when increase worker number,"I run a distribute tensorflow programe with 5 ps and 20 workers. It work normal。
when I increase the worker number to 50, the programe fail. the error log as follow. 
I don't know how to debug this problem

2017-08-07 03:51:42.191472: E tensorflow/core/distributed_runtime/master.cc:251] Master init: Unavailable: {""created"":""@1502077902.191300911"",""description"":""OS Error"",""errno"":104,""file"":""external/grpc/src/core/lib/iomgr/tcp_posix.c"",""file_line"":229,""grpc_status"":14,""os_error"":""Connection reset by peer"",""syscall"":""recvmsg""}
2017-08-07 03:51:42.735190: E tensorflow/core/distributed_runtime/master.cc:251] Master init: Unavailable: {""created"":""@1502077902.735079403"",""description"":""OS Error"",""errno"":104,""file"":""external/grpc/src/core/lib/iomgr/tcp_posix.c"",""file_line"":229,""grpc_status"":14,""os_error"":""Connection reset by peer"",""syscall"":""recvmsg""}
2017-08-07 03:51:43.224557: E tensorflow/core/distributed_runtime/master.cc:251] Master init: Unavailable: {""created"":""@1502077903.224418940"",""description"":""OS Error"",""errno"":104,""file"":""external/grpc/src/core/lib/iomgr/tcp_posix.c"",""file_line"":229,""grpc_status"":14,""os_error"":""Connection reset by peer"",""syscall"":""recvmsg""}
2017-08-07 03:51:43.630253: I tensorflow/core/distributed_runtime/master.cc:203] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1
2017-08-07 03:51:43.630305: I tensorflow/core/distributed_runtime/master.cc:203] CreateSession still waiting for response from worker: /job:ps/replica:0/task:4
2017-08-07 03:51:45.901058: E tensorflow/core/distributed_runtime/master.cc:251] Master init: Unavailable: {""created"":""@1502077905.900893611"",""description"":""OS Error"",""errno"":104,""file"":""external/grpc/src/core/lib/iomgr/tcp_posix.c"",""file_line"":229,""grpc_status"":14,""os_error"":""Connection reset by peer"",""syscall"":""recvmsg""}
2017-08-07 03:51:49.926346: E tensorflow/core/distributed_runtime/master.cc:251] Master init: Unavailable: {""created"":""@1502077909.926169663"",""description"":""OS Error"",""errno"":104,""file"":""external/grpc/src/core/lib/iomgr/tcp_posix.c"",""file_line"":229,""grpc_status"":14,""os_error"":""Connection reset by peer"",""syscall"":""recvmsg""}
Traceback (most recent call last):
File ""/home/xiangqin.oxq/tensorflow/train_ps.py"", line 117, in <module>
tf.app.run()
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
_sys.exit(main(_sys.argv[:1] + flags_passthrough))
File ""/home/xiangqin.oxq/tensorflow/train_ps.py"", line 106, in main
with sv.prepare_or_wait_for_session(server.target, config=sess_config, max_wait_secs=600) as sess:
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 714, in prepare_or_wait_for_session
max_wait_secs=max_wait_secs)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 384, in wait_for_session
sess)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 467, in _try_run_local_init_op
sess.run(self._local_init_op)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 786, in run
run_metadata_ptr)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 994, in _run
feed_dict_string, options, run_metadata)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1044, in _do_run
target_list, options, run_metadata)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1064, in _do_call
raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnavailableError: {""created"":""@1502077902.191300911"",""description"":""OS Error"",""errno"":104,""file"":""external/grpc/src/core/lib/iomgr/tcp_posix.c"",""file_line"":229,""grpc_status"":14,""os_error"":""Connection reset by peer"",""syscall"":""recvmsg""}
"
12079,My TensorBoard isn't showing any data,"System:
Ubuntu 16.04, Tensorflow built from source for python3.

tensorboard --inspect --logdir=Logdirpath
correctly outputs my logs with e.g.
sessionlog:checkpoint
   first_step           0
   last_step            25302
   max_step             25302

 and 

scalars
   Validation_Accuracy
   batch/fraction_of_240_full
   global_step/sec
   parallel_read/filenames/fraction_of_32_full
   parallel_read/fraction_of_204_full

tensorboard --logdir=Logdirpath outputs 
Starting TensorBoard b'55' at http://*****:6006
(Press CTRL+C to quit)

but opening the link in firefox just shows a blank page. As does the command with --debug
"
12078,Assert randomly fails when training with multiple threads,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04.2 LTS
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.3.0-rc2
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**: `for ((n=0;n<100;n++)); do python mnist_softmax_parallel_issue.py; done`


### Describe the problem
The following script randomly crashes (i.e., sometimes crashes and produces this traceback, most of the times it does not). The script trains the MNIST softmax model in parallel leveraging several threads. 

### Source code / logs

_mnist_softmax_device_issue.py_


```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import sys

from tensorflow.examples.tutorials.mnist import input_data

import tensorflow as tf
import threading
import numpy as np
import json
import os
import time

FLAGS = None

INTER_OP_PARALLELISM = 76
INTRA_OP_PARALLELISM = 1
BATCH_SIZE = 100
ITERATIONS = 1000
TRAINING_THREADS = 46

threads = [None] * TRAINING_THREADS

def train_function(thread_idx, mnist, sess, train_step, x, y_, y):
  iterations = int(ITERATIONS/TRAINING_THREADS)
  for i in range(iterations):
    batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)
    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})

def main(_):
  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)

  x = tf.placeholder(tf.float32, [None, 784])
  W = tf.Variable(tf.zeros([784, 10]))
  b = tf.Variable(tf.zeros([10]))
  y = tf.matmul(x, W) + b

  y_ = tf.placeholder(tf.float32, [None, 10])

  cross_entropy = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))
  train_step = tf.train.GradientDescentOptimizer(0.5, use_locking=True).minimize(cross_entropy)

  sess = tf.InteractiveSession(config=tf.ConfigProto(intra_op_parallelism_threads = INTRA_OP_PARALLELISM, inter_op_parallelism_threads= INTER_OP_PARALLELISM))
  sess.run(tf.global_variables_initializer())

  for i in range(TRAINING_THREADS):
      threads[i] = threading.Thread(target=train_function, args=[i, mnist, sess, train_step, x, y_, y])

  for thread in threads:
      thread.start()
  for thread in threads:
      thread.join()


if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument('--data_dir', type=str, default='mnist-data',
                      help='Directory for storing input data')
  FLAGS, unparsed = parser.parse_known_args()
  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)

```

_Traceback_

`
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h:125: Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::T
ensorEvaluator(const XprType&, const Device&) [with Broadcast = const Eigen::IndexList<Eigen::type2index<1l>, int>; ArgType = const Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long
 int>, 16, Eigen::MakePointer>; Device = Eigen::ThreadPoolDevice; Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::XprType = Eigen::TensorBroadcastingOp<const Eigen::IndexList<Eigen::type2index<1l>, int>, const Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16, Eigen::MakePointer> >]: Assertion input_dims[i] > $' failed.
`
"
12077,Occur error when compile tf_core_gpu_kernels/generated_adjust_hue_op_gpu.cu.cc file in VS2015,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
windows 10
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.3
- **IDE version**: 
vs2015 Debug mode
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
CUDA8.0 cuDNN5.1
- **GPU model and memory**:
4GB
- **Exact command to reproduce**:
D:/tensorflow-r1.3\tensorflow/stream_executor/device_description.h(85): warning : type qualifier on return type is meaningless
35>D:/tensorflow-r1.3\tensorflow/stream_executor/device_description.h(144): warning : type qualifier on return type is meaningless
35>E:/vs2015/VC/bin/../../VC/INCLUDE\xutility(911): **error : calling a __host__ function(""std::_Debug_message"") from a __device__ function(""std::_Debug_lt<const int &, const int &> "") is not allowed**
35>  1 error detected in the compilation of ""C:/Users/hh/AppData/Local/Temp/tmpxft_0000425c_00000000-15_adjust_hue_op_gpu.cu.compute_52.cpp1.ii"".
35>  adjust_hue_op_gpu.cu.cc
35>  CMake Error at tf_core_gpu_kernels_generated_adjust_hue_op_gpu.cu.cc.obj.Debug.cmake:282 (message):
35>    Error generating file
35>    D:/tensorflow-r1.3/CMAKE-GPU/CMakeFiles/tf_core_gpu_kernels.dir/__/__/core/kernels/Debug/tf_core_gpu_kernels_generated_adjust_hue_op_gpu.cu.cc.obj

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I built tensorflow project for GPU version in vs2015, and get CUDA error. It seems like call host function in the device function, but I can't find the place where error occur
"
12073,tf.py_func errors in freeze_graph,"I use `tf.py_func` in my networks, and it works fine in training and test mode.  Then I run `bazel-bin/tensorflow/python/tools/freeze_graph` to freeze the graph and generate xxx.pb file.
I try to restore the xxx.pb and run the model again, error happens.

### Erorr logs
```
2017-08-07 16:04:15.565535: W tensorflow/core/framework/op_kernel.cc:1158] Unknown: exceptions.KeyError: 'pyfunc_0'
Traceback (most recent call last):
  File ""object_detector_pb.py"", line 159, in <module>
    objects = detector.detect(img)
  File ""object_detector_pb.py"", line 118, in detect
    scores, boxes = self.im_detect(image)
  File ""object_detector_pb.py"", line 101, in im_detect
    scores, bbox_pred, rois = self.sess.run([self._cls_prob, self._bbox_pred, self._rois], feed)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 789, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 997, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1132, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1152, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: exceptions.KeyError: 'pyfunc_0'
	 [[Node: detector/MobilenetV1_2/ANCHOR_default/generate_anchors = PyFunc[Tin=[DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_FLOAT], Tout=[DT_FLOAT, DT_INT32], token=""pyfunc_0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](detector/MobilenetV1_2/ANCHOR_default/ToInt32, detector/MobilenetV1_2/ANCHOR_default/ToInt32_1, detector/MobilenetV1_2/ANCHOR_default/generate_anchors/input_2, detector/MobilenetV1_2/ANCHOR_default/generate_anchors/input_3, detector/MobilenetV1_2/ANCHOR_default/generate_anchors/input_4)]]

Caused by op u'detector/MobilenetV1_2/ANCHOR_default/generate_anchors', defined at:
  File ""object_detector_pb.py"", line 143, in <module>
    detector = ObjectDetector(model_path, config_file)
  File ""object_detector_pb.py"", line 76, in __init__
    tf.import_graph_def(graph_def, name=""detector"")
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 311, in import_graph_def
    op_def=op_def)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

UnknownError (see above for traceback): exceptions.KeyError: 'pyfunc_0'
	 [[Node: detector/MobilenetV1_2/ANCHOR_default/generate_anchors = PyFunc[Tin=[DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_FLOAT], Tout=[DT_FLOAT, DT_INT32], token=""pyfunc_0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](detector/MobilenetV1_2/ANCHOR_default/ToInt32, detector/MobilenetV1_2/ANCHOR_default/ToInt32_1, detector/MobilenetV1_2/ANCHOR_default/generate_anchors/input_2, detector/MobilenetV1_2/ANCHOR_default/generate_anchors/input_3, detector/MobilenetV1_2/ANCHOR_default/generate_anchors/input_4)]]
```

I found the `tf.py_func` is not in the graph. How can I define and use the `tf.py_func` in freezed graph?"
12072,Recurrent Reinforcement Learning Issue,"Hi guys,

I am working on a structure with recurrent neural network as the deep part of deep reinforcement learning.

I have been searching all along but I haven't found a way to build this structure by using tensorflow. The recurrent feature makes the objective function iterative. I am wondering if TF can calculate the gradient by itself.

It would be great if I can get to know if this structure can be done or not."
12071,"Numerical instability of gradient calculation of tf.norm (nan at 0, inf for small values) ","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.11.6
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: #v1.2.0-5-g435cdfc    1.2.1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: On CPU
- **GPU model and memory**:
- **Exact command to reproduce**: tf.norm at [0,0] see below for code

```
import numpy as np
import tensorflow as tf
print(tf.GIT_VERSION, ""  "", tf.VERSION) #v1.2.0-5-g435cdfc    1.2.1

X = tf.placeholder(tf.float32, shape=(4,None))
Z = tf.norm(X, ord='euclidean', axis=1, name='logit')
var_grad = tf.gradients(Z, [X])

with tf.Session() as sess:
    X_ = np.array([
        [1],  # Grad OK
        [0],  # Grad NaN
        [1e-16],  # Grad OK
        [1e-19] #Grad Inf
    ], dtype=np.float32)
    sess.run(tf.global_variables_initializer())
    print(sess.run((Z, var_grad), feed_dict={X: X_}))
    # Result:
    #(array([9.99999940e-01, 0.00000000e+00, 9.99999951e-17,
    #        0.00000000e+00], dtype=float32), [array([[1.00000012],
    #                                                 [nan],
    #                                                 [1.],
    #                                                 [inf]], dtype=float32)])
```

### Describe the problem
`nan` is calculated for the gradient of `tf.norm` at zero values. For extremely small values `inf` is calculated. Note that the exact result should be 1 in all cases above. 

Above is a minimal example to reproduce it. The problem occurred in a real world scenario, when implementing a custom loss function (the entropy in https://arxiv.org/abs/1611.01449) and two embeddings where too close to each other (distance practically 0).

### Source code / logs
See above 

#### Output of logfile
```
== cat /etc/issue ===============================================
Darwin Olivers-MBP-5.fritz.box 15.6.0 Darwin Kernel Version 15.6.0: Tue Apr 11 16:00:51 PDT 2017; root:xnu-3248.60.11.5.3~1/RELEASE_X86_64 x86_64
Mac OS X 10.11.6

== are we in docker =========================================  echo == are we in docker ====================================num echo == are we in docker =========================================  ec==  echo == are we in docker =======================================c++ --version

== uname -a =====================================================
Darwin Olivers-MBP-5.fritz.box 15.6.0 Darwin Kernel Version 15.6.0: Tue Apr 11 16:00:51 PDT 2017; root:xnu-3248.60.11.5.3~1/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.13.0)
protobuf (3.3.0)
tensorflow (1.2.1)

== check for virtualenv ==============  echo == check for virtualenv =====on_b echo == check fo sys  echo == check for virtualenv ============== echo == check for virtualenv ============================================

== cat /etc/issue ===============================================
Darwin Olivers-MBP-5.fritz.box 15.6.0 Darwin Kernel Version 15.6.0: Tue Apr 11 16:00:51 PDT 2017; root:xnu-3248.60.11.5.3~1/RELEASE_X86_64 x86_64
Mac OS X 10.11.6

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 7.3.0 (clang-703.0.31)
Target: x86_64-apple-darwin15.6.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin Olivers-MBP-5.fritz.box 15.6.0 Darwin Kernel Version 15.6.0: Tue Apr 11 16:00:51 PDT 2017; root:xnu-3248.60.11.5.3~1/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.13.0)
protobuf (3.3.0)
tensorflow (1.2.1)

== check for virtualenv =========================================
True

== tensorflow import ============================================
tf.VERSION = 1.2.1
tf.GIT_VERSION = v1.2.0-5-g435cdfc
tf.COMPILER_VERSION = v1.2.0-5-g435cdfc
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh.txt: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```"
12070,[XLA] Should we disable `REGISTER_XLA_BACKEND(GPU)` if there is no GPU installed on the machine? ,"I install TensorFlow from source with `XLA` and `TF_CPP_MIN_VLOG_LEVEL` enabled (without `GPU`), after trying `XLA` example, I got these logs:

```shell
...
XLA op registration: device: XLA_CPU_JIT op: AssignAddVariableOp
XLA op registration: device: XLA_GPU_JIT op: AssignAddVariableOp
XLA op registration: device: XLA_CPU_JIT op: _UnsafeReadVariable
XLA op registration: device: XLA_GPU_JIT op: _UnsafeReadVariable
XLA op registration: device: XLA_CPU_JIT op: Unpack
XLA op registration: device: XLA_GPU_JIT op: Unpack
...
```

But there is no `GPU` installed on my machine, so I don't enable `CUDA` option, but it still do

```cpp
REGISTER_XLA_BACKEND(DEVICE_GPU_XLA_JIT, kGpuAllTypes, GpuOpFilter);
```

Should we disable `REGISTER_XLA_BACKEND(GPU)` if there is no `GPU` installed on the machine?

@tatatodd @hawkinsp WDYT?

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
  No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
  Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
  Source
- **TensorFlow version (use command below)**:
  v1.2.1
- **Python version**: 
  2.7
- **Bazel version (if compiling from source)**:
  0.4.5
- **CUDA/cuDNN version**:
  N/A
- **GPU model and memory**:
  N/A
- **Exact command to reproduce**:"
12068,what's the purpose of class StatsPublisherInterface?,"### Describe the problem
Tensorflow Open source version contains the class StatsPublisherInterface and a dummy implementation, NoOpStatsPublisher. I was wondering the design purpose of this class. From the class name it seems that there is some other system that can subscribe the statistics of tensorflow step and display the information in realtime.  Can someone help ex

### Source code / logs
https://github.com/tensorflow/tensorflow/blob/0a4f5b6bda405814af59803829216762e030728d/tensorflow/core/common_runtime/stats_publisher_interface.h
"
12066,Encounter Fatal signal 11 (SIGSEGV) problem doing training on mobile device,"I am stuck on the SIGSEGV problem while trying to do training on a mobile device (Nexus 7), can you help me out? 

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes. I added a couple of lines to the TensorFlowInferenceInterface.java
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.2.1
- **Python version**: 
2.7.12
- **Bazel version (if compiling from source)**:
0.52
- **CUDA/cuDNN version**:
-
- **GPU model and memory**:
-
- **Exact command to reproduce**:
-
### Describe the problem
I am trying to find out the performance of doing training on mobile devices. 
I got the Fatal signal 11 (SIGSEGV) when I am trying to work with a graph including a couple of convolutional layers, FC layers, and cross entropy operation. 

Here is what I did:
I modified the TensorFlowInferenceInterface.java by adding a function for initialization all nodes; then I was able to reproduce the example in the following link in Android.
https://tebesu.github.io/posts/Training-a-TensorFlow-graph-in-C++-API

I try to do the training with a more complicated graph (including a couple of convolutional layers, FC layers and cross entropy operation). I can do the training with this graph using the C++ API.

I first solved the No OpKernel problem. The error message is as follows. 
No OpKernel was registered to support Op 'SparseSoftmaxCrossEntropyWithLogits' with these attrs.

I add the sparse_xent_op.h and the sparse_xent_op.cc files to the BUILD file locates at /tensorflow/tensorflow/core/kernels/. 
I modify the list_op_files.txt locally, adding the sparse_xent_op operator and, rebuild the Tensorflow source. 

The No OpKernel error disappears, but the program stops at runner.runAndFetchMetadata() and throws me an error:
A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x0 in tid 15173 (inference)

### Source code / logs
TensorFlowInferenceInterface.java:

  public void runTarget(String[] outputNames){
    Log.d(TAG, ""start of the runTarget"");
    for (String t : outputNames) {
      runner.addTarget(t);
   
    }
    Log.d(TAG, ""finished adding target"");

      runner.runAndFetchMetadata();
    Log.d(TAG,""runAndFetchMetadata"");
       runner = sess.runner();
    Log.d(TAG,""sess.runner"");

  }

Corresponding Logcat messages:
08-06 16:25:01.900 15099-15173/org.tensorflow.demo D/TensorFlowInferenceInterface: start of the runTarget
08-06 16:25:01.900 15099-15173/org.tensorflow.demo D/TensorFlowInferenceInterface: finished adding target
08-06 16:25:04.576 15099-15173/org.tensorflow.demo A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x0 in tid 15173 (inference)

Crash dump result:

********** Crash dump: **********
Build fingerprint: 'google/razor/flo:6.0/MRA58K/2256973:user/release-keys'
pid: 5414, tid: 5478, name: inference  >>> org.tensorflow.demo <<<
signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0
Stack frame #00 pc 00017664  /system/lib/libc.so (__memcpy_base+91)
Stack frame #01 pc 0072a203  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_inference.so
Stack frame #02 pc 006eea49  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_inference.so
Stack frame #03 pc 006d59d3  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_inference.so
Stack frame #04 pc 006ed9c3  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_inference.so
Stack frame #05 pc 006df991  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_inference.so
Stack frame #06 pc 006dffb5  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_inference.so
Stack frame #07 pc 0009eff9  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_inference.so
Stack frame #08 pc 0009f403  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_inference.so
Stack frame #09 pc 00099085  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_inference.so (Java_org_tensorflow_Session_run+920)
Stack frame #10 pc 000eaa29  /system/lib/libart.so (art_quick_generic_jni_trampoline+40)
Stack frame #11 pc 000e6331  /system/lib/libart.so (art_quick_invoke_stub_internal+64)
Stack frame #12 pc 00402663  /system/lib/libart.so (art_quick_invoke_static_stub+170)
Stack frame #13 pc 001009f4  [stack:5478]
"
12065,Feature Request - Seq2Seq Inference Helper w/o Embeddings,"`tf.contrib.seq2seq` has two `Helper` classes to use during inference, `SampleEmbeddingHelper` and `GreedyEmbeddingHelper`. However, both make use of embeddings, which is unhelpful when building sequence-to-sequence models that operate on non-embedded target sequences (my target sequence already consists of meaningful vectors).

I'd like a new `Helper` class that pipes the output of the decoder RNN at one time step into the decoder RNN at the following time step. It should permit the `start_tokens` to be vectors (tensors?) and the `end_token` to be a vector (tensor?) as well. Right now, I'm attempting to use `ScheduledOutputTrainingHelper` with `sampling_probability` set equal to 1.0, but I'm struggling to get it to work. Something like a simple `OutputInferenceHelper` would be very nice :)

If there already exists an easy way to do what I'm suggesting, please let me know!

"
12063,"The name 'final_result:0' refers to a Tensor which does not exist. The operation, 'final_result', does not exist in the graph.","### System information
Linux Ubuntu 16.04

Tensorflow 1.2.1

Having done a clean install of Tensorflow, I have rebuilt the model using training data which went fine. On classifying I receive:

```
  File ""/home/tass/OpenTass/Tensorflow/TassClassifier.py"", line 235, in classify
    softmaxTensor = sess.graph.get_tensor_by_name('final_result:0')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2733, in get_tensor_by_name
    return self.as_graph_element(name, allow_tensor=True, allow_operation=False)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2584, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2626, in _as_graph_element_locked
    ""graph."" % (repr(name), repr(op_name)))
KeyError: ""The name 'final_result:0' refers to a Tensor which does not exist. The operation, 'final_result', does not exist in the graph.""
```
Any ideas? Thanks in advance."
12061,the efficiency of StagingArea on gpu,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:ubuntu 16.04
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:v1.2.0-2809-g7add4e6 1.2.1-rc2
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**:0.5.2
- **CUDA/cuDNN version**:8.0/6.0
- **GPU model and memory**:8 1080ti cards, each 11172MB
- **Exact command to reproduce**: see below

### Describe the problem
I test the efficiency of StagingArea(as described in https://www.tensorflow.org/performance/performance_models) on GPU compared to direct feed_dict to session.run input method.
The script shows these two methods have almost same speed, all about 34 seconds. But if I put all inputs to StagingArea first, the script just need 23 seconds to run. StagingArea method can't hide data copy time.

### Source code / logs
https://gist.github.com/suiyuan2009/99fb21567e7ce3716ae25772754c7543
"
12060,Tensorflow installation error,"Hi I installed tensorflow for cpu and while trying to run command to check the version of tensor i am prompted error message.  Please help me to get this resolved

Please see below the log:

Microsoft Windows [Version 6.2.9200]
(c) 2012 Microsoft Corporation. All rights reserved.

C:\WINDOWS\system32>python
Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AM
D64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf

Traceback (most recent call last):
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in impor
t_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\__init__.p
y"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow.py"", line 21, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow')
  File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in impor
t_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\__init__.py"", lin
e 24, in <module>
    from tensorflow.python import *
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\__init__.p
y"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in impor
t_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\__init__.p
y"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow.py"", line 21, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow')
  File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in impor
t_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow'


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_st
arted/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>"
12059,tfdbg does not work with sparse tensors,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.2.0-5-g435cdfc', '1.2.1')
- **Python version**:  Python 2.7.6
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0.61 / 5.1.10
- **GPU model and memory**: Nvidia  TITAN X (Pascal) 12G
- **Exact command to reproduce**: python sparse_debug.py --debug

### Describe the problem
There seems to be a bug using tensorflow debugger with sparse tensors. Below is just a simple example  it fails when run with or without the ` --debug` option. It works when LocalCLIDebugWrapperSession line is removed. This prevents the use of the debugger while using sparse_placeholders, unless I'm missing something.
[This issue](https://github.com/tensorflow/tensorflow/issues/6110) also reports the same error, but isn't related to tfdbg.


### Source code / logs
sparse_debug.py
```
import tensorflow as tf
from tensorflow.python import debug as tf_debug
a=tf.sparse_placeholder(tf.float32,shape=(None,5,5),name='tensor1')
b=tf.sparse_placeholder(tf.float32,shape=(None,5,5),name='tensor2')
add=tf.sparse_add(a,b)

sess = tf.Session()
sess = tf_debug.LocalCLIDebugWrapperSession(sess)

a_val=([[0,0,1],[0,0,2]],[1,2],(1,5,5))
b_val=([[0,0,1],[0,0,2]],[1,2],(1,5,5))
res=sess.run(add,feed_dict={a:a_val,b:b_val})
```
Traceback

```
  File ""sparse_debug.py"", line 12, in <module>
    res=sess.run(add,feed_dict={a:a_val,b:b_val})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/wrappers/framework.py"", line 411, in run
    self._run_call_count))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py"", line 189, in on_run_start
    request.feed_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py"", line 487, in _update_run_calls_state
    self._tensor_filters)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/cli_shared.py"", line 273, in get_run_start_intro
    feed_dict_lines.append(feed_key.name)
AttributeError: 'SparseTensor' object has no attribute 'name'
```
"
12058,Couldn't restore attention_ocr checkpoint via saver,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
MacOS Sierra 10.12.5
- **TensorFlow installed from (source or binary)**:
created environment in conda, then installed tf via pip
- **TensorFlow version (use command below)**:
('v1.2.0-rc2-21-g12f033d', '1.2.0')
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
Not installed
- **CUDA/cuDNN version**:
No GPU supported
- **GPU model and memory**:
No
- **Exact command to reproduce**:
python test.py
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Hi, I am trying to use attention_ocr in my own data, a simple test is firstly implemented.
according to the instructions from [How to use a pre-trained model](https://github.com/tensorflow/models/tree/master/attention_ocr#how-to-use-a-pre-trained-model) but somehow failed in restoring the checkpoints without explicit error info.

The following condition has been checked:
1. checkpoint files are complete
2. right path to the checkpoint
3. graphs have been imported from .meta
4. Nothing changes after run saver.train.restore() (predictions remained the same)
5. No error or hints provided

The checkpoint was downloaded as suggested:
```
wget http://download.tensorflow.org/models/attention_ocr_2017_05_17.tar.gz
tar xf attention_ocr_2017_05_17.tar.gz
cd attention_ocr_2017_05_17
ls -lh
```
```
total 64216
-rw-r-----  1 liuhuichuan  staff    14M  5 18 04:07 model.ckpt-399731.data-00000-of-00001
-rw-r-----  1 liuhuichuan  staff   8.2K  5 18 04:07 model.ckpt-399731.index
-rw-r-----  1 liuhuichuan  staff    17M  5 18 04:07 model.ckpt-399731.meta
```

The graphs were successfully imported from .meta, but somehow saver couldn't recognize .index and .data files: 
```
print os.path.exists('../attention_ocr_2017_05_17/model.ckpt-399731.data-00000-of-00001')
print os.path.exists('../attention_ocr_2017_05_17/model.ckpt-399731.index')
print tf.train.get_checkpoint_state('../attention_ocr_2017_05_17/model.ckpt-399731')
```
returns:
```
Ture
Ture
None
```
A very simple test is attempted:
```
saver = tf.train.import_meta_graph('../attention_ocr_2017_05_17/model.ckpt-399731.meta')
with tf.Session() as sess:
    print os.path.exists('./attention_ocr_2017_05_17/model.ckpt-399731.meta')
    print tf.train.get_checkpoint_state('../attention_ocr_2017_05_17/model.ckpt-399731')
    saver.restore(sess,'../attention_ocr_2017_05_17/model.ckpt-399731')
```
returns no error, but still not restored:
```
2017-08-06 16:24:41.346086: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
True
2017-08-06 16:24:41.346124: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-06 16:24:41.346129: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-06 16:24:41.346133: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
None
INFO:tensorflow:Restoring parameters from ../attention_ocr_2017_05_17/model.ckpt-399731
INFO 2017-08-06 16:24:41.000354: tf_logging.py: 82 Restoring parameters from ../attention_ocr_2017_05_17/model.ckpt-399731

Process finished with exit code 0
```

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf
import os
from tensorflow.python.platform import flags
import matplotlib.image as mpimg
import common_flags

FLAGS = flags.FLAGS
common_flags.define()

# yapf: disable
flags.DEFINE_integer('num_batches', 100,
                     'Number of batches to run eval for.')

flags.DEFINE_string('eval_log_dir', '/tmp/attention_ocr/eval',
                    'Directory where the evaluation results are saved to.')

flags.DEFINE_integer('eval_interval_secs', 60,
                     'Frequency in seconds to run evaluations.')

flags.DEFINE_integer('number_of_steps', None,
                     'Number of times to run evaluation.')


# fake a simple test image

raw_image_data = mpimg.imread('A4A8A5910A355-cvt.jpg').reshape((1,150,600,3))
images_placeholder = tf.placeholder(tf.float32,shape = (1,150, 600, 3),name='img_data')

if not tf.gfile.Exists(FLAGS.eval_log_dir):
    tf.gfile.MakeDirs(FLAGS.eval_log_dir)
dataset = common_flags.create_dataset(split_name=FLAGS.split_name)
model = common_flags.create_model(dataset.num_char_classes,
                                    dataset.max_sequence_length,
                                    dataset.num_of_views, dataset.null_code)
endpoints = model.create_base(images_placeholder, labels_one_hot=None)

# start loading attention_ocr model

saver = tf.train.import_meta_graph('../attention_ocr_2017_05_17/model.ckpt-399731.meta')

with tf.Session() as sess:
    # init without checkpoint variables and predict
    init = tf.global_variables_initializer()
    sess.run(init)
    predictions = sess.run(endpoints.predicted_chars, feed_dict={images_placeholder: raw_image_data})
    print predictions

    # restore from checkpoint then predict
    print os.path.exists('./attention_ocr_2017_05_17/model.ckpt-399731.meta')
    print tf.train.get_checkpoint_state('../attention_ocr_2017_05_17/model.ckpt-399731')
    saver.restore(sess,'../attention_ocr_2017_05_17/model.ckpt-399731')
    predictions = sess.run(endpoints.predicted_chars, feed_dict={images_placeholder: raw_image_data})
    print predictions
```

```
INFO 2017-08-06 16:48:44.000554: fsns.py: 130 Using FSNS dataset split_name=train dataset_dir=/Users/liuhuichuan/PycharmProjects/models/attention_ocr/python/datasets/data/fsns
DEBUG 2017-08-06 16:48:44.000556: model.py: 343 images: Tensor(""img_data:0"", shape=(1, 150, 600, 3), dtype=float32)
DEBUG 2017-08-06 16:48:44.000561: model.py: 348 Views=4 single view: Tensor(""AttentionOcr_v1/split:0"", shape=(1, 150, 150, 3), dtype=float32)
DEBUG 2017-08-06 16:48:44.000561: model.py: 191 Using final_endpoint=Mixed_5d
DEBUG 2017-08-06 16:48:46.000492: model.py: 191 Using final_endpoint=Mixed_5d
DEBUG 2017-08-06 16:48:47.000546: model.py: 191 Using final_endpoint=Mixed_5d
DEBUG 2017-08-06 16:48:48.000684: model.py: 191 Using final_endpoint=Mixed_5d
DEBUG 2017-08-06 16:48:49.000862: model.py: 354 Conv tower: Tensor(""AttentionOcr_v1/conv_tower_fn/INCE/InceptionV3/Mixed_5d/concat:0"", shape=(1, 16, 16, 288), dtype=float32)
DEBUG 2017-08-06 16:48:49.000862: model.py: 357 Conv tower w/ encoded coordinates: Tensor(""AttentionOcr_v1/conv_tower_fn/INCE/InceptionV3/Mixed_5d/concat:0"", shape=(1, 16, 16, 288), dtype=float32)
DEBUG 2017-08-06 16:48:49.000869: model.py: 360 Pooled views: Tensor(""AttentionOcr_v1/pool_views_fn/STCK/Reshape:0"", shape=(1, 1024, 288), dtype=float32)
DEBUG 2017-08-06 16:48:49.000869: sequence_layers.py: 421 Use AttentionWithAutoregression as a layer class
DEBUG 2017-08-06 16:48:53.000099: model.py: 363 chars_logit: Tensor(""AttentionOcr_v1/sequence_logit_fn/SQLR/concat:0"", shape=(1, 37, 134), dtype=float32)
2017-08-06 16:50:05.943512: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-06 16:50:05.943528: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-06 16:50:05.943532: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-06 16:50:05.943537: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
INFO:tensorflow:Restoring parameters from ../attention_ocr_2017_05_17/model.ckpt-399731
INFO 2017-08-06 16:50:29.000024: tf_logging.py: 82 Restoring parameters from ../attention_ocr_2017_05_17/model.ckpt-399731
[[  0   0   0 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123
  123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123
  123]]
True
None
[[  0   0   0 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123
  123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123
  123]]

Process finished with exit code 0
```
"
12057,tf.contrib.distributions - Additional Distributions,"Are there plans to add additional distributions in the tf.contrib.distributions module like Scaled Inverse Chi-Sq, LKJ, Gumbel etc?  Could mirror the distribution list supported by STAN
"
12055,Tensorboard Error on windows ,"I want to use tensorboard on windows but when I am going to start it, its shows this error
```
(C:\Users\anura\Anaconda3) C:\Users\anura>tensorboard --logdir foo:C:\Users\anura\Desktop\Python\Tensorboard
Traceback (most recent call last):
  File ""C:\Users\anura\Anaconda3\Scripts\tensorboard-script.py"", line 3, in <module>
    import tensorflow.tensorboard.tensorboard
  File ""C:\Users\anura\Anaconda3\lib\site-packages\tensorflow\tensorboard\tensorboard.py"", line 33, in <module>
    from tensorflow.tensorboard.backend import application
  File ""C:\Users\anura\Anaconda3\lib\site-packages\tensorflow\tensorboard\backend\application.py"", line 47, in <module>
    from tensorflow.tensorboard.plugins.projector import projector_plugin
  File ""C:\Users\anura\Anaconda3\lib\site-packages\tensorflow\tensorboard\plugins\projector\projector_plugin.py"", line 28, in <module>
    from tensorflow.contrib.tensorboard.plugins.projector import PROJECTOR_FILENAME
  File ""C:\Users\anura\Anaconda3\lib\site-packages\tensorflow\contrib\__init__.py"", line 30, in <module>
    from tensorflow.contrib import factorization
  File ""C:\Users\anura\Anaconda3\lib\site-packages\tensorflow\contrib\factorization\__init__.py"", line 24, in <module>
    from tensorflow.contrib.factorization.python.ops.gmm import *
  File ""C:\Users\anura\Anaconda3\lib\site-packages\tensorflow\contrib\factorization\python\ops\gmm.py"", line 27, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import estimator
  File ""C:\Users\anura\Anaconda3\lib\site-packages\tensorflow\contrib\learn\__init__.py"", line 87, in <module>
    from tensorflow.contrib.learn.python.learn import *
  File ""C:\Users\anura\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\__init__.py"", line 23, in <module>
    from tensorflow.contrib.learn.python.learn import *
  File ""C:\Users\anura\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\__init__.py"", line 25, in <module>
    from tensorflow.contrib.learn.python.learn import estimators
  File ""C:\Users\anura\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\__init__.py"", line 297, in <module>
    from tensorflow.contrib.learn.python.learn.estimators.dnn import DNNClassifier
  File ""C:\Users\anura\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\dnn.py"", line 29, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import dnn_linear_combined
  File ""C:\Users\anura\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\dnn_linear_combined.py"", line 31, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import estimator
  File ""C:\Users\anura\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 49, in <module>
    from tensorflow.contrib.learn.python.learn.learn_io import data_feeder
  File ""C:\Users\anura\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\learn_io\__init__.py"", line 21, in <module>
    from tensorflow.contrib.learn.python.learn.learn_io.dask_io import extract_dask_data
  File ""C:\Users\anura\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\learn_io\dask_io.py"", line 26, in <module>
    import dask.dataframe as dd
  File ""C:\Users\anura\Anaconda3\lib\site-packages\dask\dataframe\__init__.py"", line 3, in <module>
    from .core import (DataFrame, Series, Index, _Frame, map_partitions,
  File ""C:\Users\anura\Anaconda3\lib\site-packages\dask\dataframe\core.py"", line 36, in <module>
    pd.computation.expressions.set_use_numexpr(False)
AttributeError: module 'pandas' has no attribute 'computation'

(C:\Users\anura\Anaconda3) C:\Users\anura>
```
when is run localhost:6006 in my chrome browser 

![screenshot 140](https://user-images.githubusercontent.com/15853647/28995526-f9aa1742-7a08-11e7-92d6-f87038c9ea7a.png)
"
12054,Feature request: non-blocking enqueue operations,"As for version 1.2, queue operations involving enqueuing can block if a queue is full or empty. This is very useful for designing input threads feeding the queue from static datasets as it suspends the thread until it can proceed doing its work. However, this behavior might not be desirable or acceptable if instead the data being fed comes from an asynchronous and continuously changing live source at a best effort basis, because blocking the thread implies missing the continuity of the source.

For example, let's say we have a distributed reinforcement learning environment where an external game plays asynchronously and a thread tries its best to read the most recent state of the game and input the actions to take. As a result, this thread keeps producing training batches that are enqueued in a PaddingFIFOQueue, which are consumed by a separate trainer.

In this case, if for any reason the trainer takes too long or has an un expected peak in the number of training batches it receives, the queue will fill and the producer thread(s) will block. Since the source of data (the external game) keeps playing, this means that the next time the producer threads unblock they will probably have lost temporal coherency on the game. In this case, dropping a training batch would be more desirable than blocking the thread until such batch can be enqueued.

For this reason, I'd like to propose the following feature: a new optional argument to the enqueuing methods (enqueue and enqueue_many) that allows to immediately return when the queue is full instead of blocking the thread. This argument would default to the existing behavior (e.g., 'non_blocking=False'), so no code updates are required.

Signaling if/how many elements were successfully enqueued or failed to do so is likely a desirable output of a non-blocking enqueue operation. However, implementing it might not be trivial without altering the function return signature. If we find this is something we should provide, it might be better to instead of adding a new argument simply add new non-blocking versions of the enqueue ops that return a (op, num_elements_discarded) tuple.

Any comments or suggestions are most welcome."
12052,Upgrade to CuDNN 7 and CUDA 9,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows Server 2012
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3.0-rc1
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA V8.0.44, CuDNN 6.0
- **GPU model and memory**: Nvidia GeForce GTX 1080 Ti, 11 GB
- **Exact command to reproduce**: N/A

### Describe the problem
Please upgrade TensorFlow to support CUDA 9 and CuDNN 7. Nvidia claims this will provide a 2x performance boost on Pascal GPUs."
12051,an API to tell TF ABI,"This is a feature request.
TF pip packages might be built with different C++ ABI. The released binaries are built with old ABI. If a user manually compile it with gcc>=5, the default is to use new CXX11 ABI (unless explicitly changed).

As someone who wrote custom ops, this could cause trouble: the op has to be compiled with the same ABI, otherwise there will be issues like #10714 #9137. Therefore the user of my ops would need to be aware of what ABI he's using, and change the flags manually.

I hope there is an API simply tells what ABI should be used when compiling user ops, similar to `tf.sysconfig.get_include()` which tells what path to include."
12047,Could you please provide a TensorFlow.dll 32 bits ? many thanks,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12046,tf.scatter_update to variable pinned on GPU fails,"### System information
- Linux Ubuntu 16.04
- tensorflow-gpu v1.2.0 binary installed from pip
- Python 3.5
- CUDA 8.0, cuDNN v5.1 
- GeForce GTX 1080 Ti, 11GB
- A simple example I came up with which reproduces the error follows below:
**************************************************************************************************
```python
import tensorflow as tf
with tf.device(""/gpu:0""):
    a = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])
    c = tf.Variable([0, 0, 0, 0, 0, 0, 0, 0])
    step_sos = tf.Variable([False, False, True, True, False, False, True, True])
    write_ops = []
    for b in range(8):
        write_ops.append(tf.cond(step_sos[b], lambda: tf.scatter_update(a, b, 0), lambda: a))

    with tf.control_dependencies(write_ops):
       d = tf.assign(c, a)


session_config = tf.ConfigProto(allow_soft_placement=False, log_device_placement=True)

with tf.Session(config=session_config) as sess:
    init_op = tf.global_variables_initializer()
    sess.run(init_op)
    sess.run(d)
    print(sess.run(c))
```
********************************************************************************************************              

### 
When setting allow_soft_placement=True the error is solved but the variable and some operations which are often used are placed in the CPU, which leads to fluctuating GPU utilization.

### Source code / logs
```
Traceback (most recent call last):
  File ""/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1139, in _do_call
    return fn(*args)
  File ""/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1117, in _run_fn
    self._extend_graph()
  File ""/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1166, in _extend_graph
    self._session, graph_def.SerializeToString(), status)
  File ""/usr/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Variable_2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Colocation Debug Info:
Colocation group had the following types and devices: 
Assign: CPU 
Identity: CPU 
VariableV2: CPU 
	 [[Node: Variable_2 = VariableV2[container="""", dtype=DT_BOOL, shape=[8], shared_name="""", _device=""/device:GPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""scatter_test.py"", line 20, in <module>
    sess.run(init_op)
  File ""/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 789, in run
    run_metadata_ptr)
  File ""/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 997, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1132, in _do_run
    target_list, options, run_metadata)
  File ""/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1152, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Variable_2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Colocation Debug Info:
Colocation group had the following types and devices: 
Assign: CPU 
Identity: CPU 
VariableV2: CPU 
	 [[Node: Variable_2 = VariableV2[container="""", dtype=DT_BOOL, shape=[8], shared_name="""", _device=""/device:GPU:0""]()]]

Caused by op 'Variable_2', defined at:
  File ""scatter_test.py"", line 7, in <module>
    step_sos = tf.Variable([False, False, True, True, False, False, True, True])
  File ""/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/ops/variables.py"", line 200, in __init__
    expected_shape=expected_shape)
  File ""/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/ops/variables.py"", line 297, in _init_from_args
    name=name)
  File ""/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/ops/state_ops.py"", line 128, in variable_op_v2
    shared_name=shared_name)
  File ""/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/ops/gen_state_ops.py"", line 684, in _variable_v2
    name=name)
  File ""/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Variable_2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Colocation Debug Info:
Colocation group had the following types and devices: 
Assign: CPU 
Identity: CPU 
VariableV2: CPU 
	 [[Node: Variable_2 = VariableV2[container="""", dtype=DT_BOOL, shape=[8], shared_name="""", _device=""/device:GPU:0""]()]]
```
"
12043,tf.contrib.util.make_ndarray is slow the first time it is run,"### System information

**Output of tf_env_collect.sh:** [tf_env.txt](https://github.com/tensorflow/tensorflow/files/1201133/tf_env.txt)

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.3.0-rc1-479-g82456f9 1.2.1-rc1
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**:
== cuda libs  ===================================================
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/lib/python3.5/dist-packages/torch/lib/libcudart.so.8.0
/usr/local/cuda-7.5/doc/man/man7/libcudart.7
/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7
/usr/local/cuda-7.5/lib/libcudart.so.7.5.18
/usr/local/cuda-7.5/lib/libcudart_static.a
/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18
/usr/local/cuda-7.5/lib64/libcudart_static.a

- **GPU model and memory**:
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 381.22                 Driver Version: 381.22                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1060    Off  | 0000:01:00.0      On |                  N/A |
| N/A   48C    P0    28W /  N/A |    591MiB /  6064MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

```
- **Exact command to reproduce**:
Download  [frame.summary.zip](https://github.com/tensorflow/tensorflow/files/1201101/frame.summary.zip) and extract it to `frame.summary`. Some info about it:

```
tensor_proto dtype: DT_UINT8
tensor_shape {
  dim {
    size: 1059
  }
  dim {
    size: 768
  }
}
```

Run the following:

```python
import time
from google.protobuf import message
import tensorflow as tf

for i in range(2):
  path = 'frame.summary'
  with open(path, 'rb') as summary_file:
    summary_string = summary_file.read()

  summary_proto = tf.Summary()
  summary_proto.ParseFromString(summary_string)
  tensor_proto = summary_proto.value[0].tensor
  a = time.time()
  array = tf.contrib.util.make_ndarray(tensor_proto)
  b = time.time()
  print('b-a', b-a)
```

### Describe the problem
The first time `make_ndarray` is run, it takes a long time. For this isolated example, the output is this on my machine:
```
b-a 2.810185194015503
b-a 0.00030040740966796875
```
In my project, it took 105.96585726737976 for the first run, and 0.0005743503570556641 the second time (and closer to 0.0003 afterwards).

### Source code / logs
Here is the branch I was using : https://github.com/chrisranderson/beholder/tree/bug-report
And the line that took ~105 seconds: https://github.com/chrisranderson/beholder/blob/b54d4203d803492af9852b7c8453e8a1e5342f46/beholder/file_system_tools.py#L34


"
12042,[docs] Broken link on Tool Developer's page,"Source page: https://www.tensorflow.org/extend/tool_developers/

Bad link (`graph_run_run2.pbtxt`): https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/components/tf_tensorboard/test/data/graph_run_run2.pbtxt"
12040,NaNs only on GPU with large convolution kernel,"Consider the following (silly) autoencoder-style network, which performs a strided convolution followed by a transposed convolution:
```
import numpy as np
import tensorflow as tf

n = 100
m = 24
k = 14

W1 = tf.Variable(tf.truncated_normal([1, k, 32, 64], stddev=1e-1,
                                     dtype=tf.float32))
b1 = tf.Variable(tf.truncated_normal([64], stddev=1e-1,
                                     dtype=tf.float32))
W2 = tf.Variable(tf.truncated_normal([1, k, 32, 64], stddev=1e-1,
                                     dtype=tf.float32))
b2 = tf.Variable(tf.truncated_normal([1], stddev=1e-1,
                                     dtype=tf.float32))

x = tf.placeholder(tf.float32, [n, 1, m, 32])
x_1 = tf.nn.tanh(tf.nn.conv2d(x, W1,
                              [1, 1, 8, 1], 'SAME')) + b1
x_2 = tf.nn.conv2d_transpose(x_1, W2, [n, 1, m, 32],
                             [1, 1, 8, 1], 'SAME') + b2

loss = tf.nn.l2_loss(x - x_2)

train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)
check_step = tf.add_check_numerics_ops()

sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())

for i in range(10000):
    _, _, loss_val = \
        sess.run([train_step, check_step, loss],
                 feed_dict={x: np.random.randn(n, 1, m, 32)})

    print(""Iteration: {}; loss: {}"".format(i, loss_val))
```
On the CPU, this runs fine.  Using CUDA with k >= 14, though, I reproducibly get the following error:
```
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:82:00.0
Total memory: 10.91GiB
Free memory: 10.76GiB
2017-08-04 17:09:01.683907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0
2017-08-04 17:09:01.683916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y
2017-08-04 17:09:01.683927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0)
2017-08-04 17:09:03.022214: E tensorflow/core/kernels/check_numerics_op.cc:157] abnormal_detected_host @0x1020d800b00 = {1, 0} gradients/Conv2D_grad/Conv2DBackpropFilter:0
Traceback (most recent call last):
  File ""mintrans.py"", line 35, in <module>
    feed_dict={x: np.random.randn(n, 1, m, 32)})
  File ""/home/sschulze/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 789, in run
    run_metadata_ptr)
  File ""/home/sschulze/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 997, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/sschulze/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1132, in _do_run
    target_list, options, run_metadata)
  File ""/home/sschulze/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1152, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: gradients/Conv2D_grad/Conv2DBackpropFilter:0 : Tensor had NaN values
         [[Node: CheckNumerics_60 = CheckNumerics[T=DT_FLOAT, message=""gradients/Conv2D_grad/Conv2DBackpropFilter:0"", _device=""/job:localhost/replica:0/task:0/gpu:0""](gradients/Conv2D_grad/Conv2DBackpropFilter, ^CheckNumerics_59)]]
  
Caused by op u'CheckNumerics_60', defined at:
  File ""mintrans.py"", line 27, in <module>
    check_step = tf.add_check_numerics_ops()
  File ""/home/sschulze/.local/lib/python2.7/site-packages/tensorflow/python/ops/numerics.py"", line 68, in add_check_numerics_ops
    check_op = [array_ops.check_numerics(output, message=message)]
  File ""/home/sschulze/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 415, in check_numerics
    message=message, name=name)
  File ""/home/sschulze/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/home/sschulze/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/sschulze/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()
  
InvalidArgumentError (see above for traceback): gradients/Conv2D_grad/Conv2DBackpropFilter:0 : Tensor had NaN values
         [[Node: CheckNumerics_60 = CheckNumerics[T=DT_FLOAT, message=""gradients/Conv2D_grad/Conv2DBackpropFilter:0"", _device=""/job:localhost/replica:0/task:0/gpu:0""](gradients/Conv2D_grad/Conv2DBackpropFilter, ^CheckNumerics_59)]]
```

This code is the minimal example that I could reproduce the error with.  I am running tensorflow-gpu 1.2.1, installed via pip on Ubuntu 16.04.2, with Python version 2.7.12.  CUDA is 8.0.61-1 from the nVidia repo, and cuDNN is 5.1.10.  My GPU is an nVidia GTX 1080 Ti with 11172 MiB of memory.

To reproduce the error, run the above code with CUDA enabled.

My suspicion is that when the kernel is too large with respect to the input or the output of the strided convolution, a bug is triggered.  I am not entirely sure how the size of the kernel must relate to the other convolution parameters, but I could produce errors both in the conv2d and the conv2d_transpose op."
12039,Model callbacks don't work in TF r1.2 and Keras functional API (Python 3.6 + windows 10),"Two test cases were constructed in order to show better the problem with ""Model callbacks, which don't work in TF r1.2 and Keras functional API"".

1/ Callbackes were used in the following way:
#------------ checkpoints -------------------------------
import tensorflow.contrib.keras as K
filedest=""./models/weights_model011a.best.hdf5""
checkpoint = K.callbacks.ModelCheckpoint(filedest, monitor='val_acc', verbose=1, 
                             save_best_only=True, mode='max')

checkRLR = K.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.1, 
                                         patience=7,min_lr=1.0e-10)

earlyStop = K.callbacks.EarlyStopping(monitor='val_acc', patience=30)
tbgraph = K.callbacks.TensorBoard(log_dir='./graphs', histogram_freq=1, 
                      write_graph=True, write_images=False, 
                      embeddings_freq=1)

callbacks_list = [checkpoint,earlyStop,checkRLR,tbgraph]

and then:
history = model.fit(X_train, y_train[:11200], validation_data=(X_test, y_tst_cat[:7200]),
          epochs=epochs, batch_size=40,shuffle=True,
          callbacks=callbacks_list, verbose=0)

That resulted in successful run through the training session, but with the following info:
INFO:tensorflow:Summary name embedding/embeddings:0 is illegal; using embedding/embeddings_0 instead.
INFO:tensorflow:Summary name conv1d/kernel:0 is illegal; using conv1d/kernel_0 instead.
INFO:tensorflow:Summary name conv1d/bias:0 is illegal; using conv1d/bias_0 instead.
INFO:tensorflow:Summary name conv1d_1/kernel:0 is illegal; using conv1d_1/kernel_0 instead.
INFO:tensorflow:Summary name conv1d_1/bias:0 is illegal; using conv1d_1/bias_0 instead.
INFO:tensorflow:Summary name conv1d_2/kernel:0 is illegal; using conv1d_2/kernel_0 instead.
INFO:tensorflow:Summary name conv1d_2/bias:0 is illegal; using conv1d_2/bias_0 instead.
INFO:tensorflow:Summary name conv1d_3/kernel:0 is illegal; using conv1d_3/kernel_0 instead.
INFO:tensorflow:Summary name conv1d_3/bias:0 is illegal; using conv1d_3/bias_0 instead.
INFO:tensorflow:Summary name conv1d_4/kernel:0 is illegal; using conv1d_4/kernel_0 instead.
INFO:tensorflow:Summary name conv1d_4/bias:0 is illegal; using conv1d_4/bias_0 instead.
INFO:tensorflow:Summary name dense/kernel:0 is illegal; using dense/kernel_0 instead.
INFO:tensorflow:Summary name dense/bias:0 is illegal; using dense/bias_0 instead.

2/ Callbacks were used with Keras functional API in similar way, described above. The run resulted in similar info message PLUS the following:
C:\path\AppData\Local\Continuum\Miniconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\keras\python\keras\callbacks.py:411: RuntimeWarning: Can save best model only with val_acc available, skipping.
  'skipping.' % (self.monitor), RuntimeWarning)
C:\path\AppData\Local\Continuum\Miniconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\keras\python\keras\callbacks.py:499: RuntimeWarning: Early stopping requires val_acc available!
  RuntimeWarning)
Traceback (most recent call last):

  File ""<ipython-input-1-70e23aa02b0a>"", line 1, in <module>
    runfile('C:/path_to_the files/Test_bugs/NonTagTFmodel_local_v011.py', wdir='C:/path_to_the files/Test_bugs')

  File ""C:\path\AppData\Local\Continuum\Miniconda3\envs\tensorflow\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 880, in runfile
    execfile(filename, namespace)

  File ""C:\path\AppData\Local\Continuum\Miniconda3\envs\tensorflow\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/path/Documents/Python Scripts/Recognition/Run09recipes/Test_bugs/NonTagTFmodel_local_v011.py"", line 149, in <module>
    callbacks=callbacks_list, verbose=0)

  File ""C:\path\AppData\Local\Continuum\Miniconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\keras\python\keras\engine\training.py"", line 1495, in fit
    initial_epoch=initial_epoch)

  File ""C:\path\AppData\Local\Continuum\Miniconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\keras\python\keras\engine\training.py"", line 1158, in _fit_loop
    callbacks.on_epoch_end(epoch, epoch_logs)

  File ""C:\path\AppData\Local\Continuum\Miniconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\keras\python\keras\callbacks.py"", line 96, in on_epoch_end
    callback.on_epoch_end(epoch, logs)

  File ""C:\path\AppData\Local\Continuum\Miniconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\keras\python\keras\callbacks.py"", line 501, in on_epoch_end
    if self.monitor_op(current - self.min_delta, self.best):

TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'

The same code (TF r1.2 and Keras functional API) runs perfectly alright if 'callbacks=None':
history = model.fit(X_train, [Y_tr_hot,y_tr_cat[:11200]], validation_data=(X_test, [Y_ts_hot,y_ts_cat[:7200]]),
          epochs=epochs, batch_size=40, shuffle=True,
          callbacks=None, verbose=1)

What can be the reason for Model callbacks not working???"
12037,Missing tf_python_protos_cc library dependency in tf_tutorials.cmake,"### System information
Windows10
VisualStudio 2017
TensorFlow 1.3.0
Python 3.5.3
CMake 3.8.1

### Describe the problem
I can't build **tf_tutorials_example_trainer** due missing library dependency to tf_python_protos_cc.lib. A lot of linking errors occurred. If the bold line is added into tf_tutorials.cmake then the compilation works:

target_link_libraries(tf_tutorials_example_trainer PUBLIC
    tf_protos_cc
    **tf_python_protos_cc**
    ${tf_core_gpu_kernels_lib}
    ${tensorflow_EXTERNAL_LIBRARIES}
)

CMake is used build the project files for VisualStudio. 

![image](https://user-images.githubusercontent.com/30691148/28971246-95f12174-792b-11e7-8d9f-5bb5542a5ce0.png)

### Source code / logs
`1>------ Build started: Project: tf_tutorials_example_trainer, Configuration: Release x64 ------
1>   Creating library C:/Development/dev/test_projects/deeplearning/tensorflow_build64/Release/tf_tutorials_example_trainer.lib and object C:/Development/dev/test_projects/deeplearning/tensorflow_build64/Release/tf_tutorials_example_trainer.exp
1>ensemble_optimizer_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::DecisionTreeConfig::DecisionTreeConfig(void)"" (??0DecisionTreeConfig@trees@boosted_trees@tensorflow@@QEAA@XZ) referenced in function ""public: static class tensorflow::boosted_trees::trees::DecisionTreeConfig * __cdecl google::protobuf::Arena::CreateMessage<class tensorflow::boosted_trees::trees::DecisionTreeConfig>(class google::protobuf::Arena *)"" (??$CreateMessage@VDecisionTreeConfig@trees@boosted_trees@tensorflow@@@Arena@protobuf@google@@SAPEAVDecisionTreeConfig@trees@boosted_trees@tensorflow@@PEAV012@@Z)
1>prediction_ops.cc.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::DecisionTreeConfig::DecisionTreeConfig(void)"" (??0DecisionTreeConfig@trees@boosted_trees@tensorflow@@QEAA@XZ)
1>training_ops.cc.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::DecisionTreeConfig::DecisionTreeConfig(void)"" (??0DecisionTreeConfig@trees@boosted_trees@tensorflow@@QEAA@XZ)
1>ensemble_optimizer_ops.cc.obj : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::boosted_trees::trees::DecisionTreeConfig::Swap(class tensorflow::boosted_trees::trees::DecisionTreeConfig *)"" (?Swap@DecisionTreeConfig@trees@boosted_trees@tensorflow@@QEAAXPEAV1234@@Z) referenced in function ""public: virtual void __cdecl tensorflow::AddTreesToEnsembleOp::Compute(class tensorflow::OpKernelContext * const)"" (?Compute@AddTreesToEnsembleOp@tensorflow@@UEAAXQEAVOpKernelContext@2@@Z)
1>ensemble_optimizer_ops.cc.obj : error LNK2019: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::DecisionTreeConfig::DecisionTreeConfig(class google::protobuf::Arena *)"" (??0DecisionTreeConfig@trees@boosted_trees@tensorflow@@IEAA@PEAVArena@protobuf@google@@@Z) referenced in function ""public: static class tensorflow::boosted_trees::trees::DecisionTreeConfig * __cdecl google::protobuf::Arena::CreateMessage<class tensorflow::boosted_trees::trees::DecisionTreeConfig>(class google::protobuf::Arena *)"" (??$CreateMessage@VDecisionTreeConfig@trees@boosted_trees@tensorflow@@@Arena@protobuf@google@@SAPEAVDecisionTreeConfig@trees@boosted_trees@tensorflow@@PEAV012@@Z)
1>training_ops.cc.obj : error LNK2001: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::DecisionTreeConfig::DecisionTreeConfig(class google::protobuf::Arena *)"" (??0DecisionTreeConfig@trees@boosted_trees@tensorflow@@IEAA@PEAVArena@protobuf@google@@@Z)
1>ensemble_optimizer_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::DecisionTreeMetadata::DecisionTreeMetadata(void)"" (??0DecisionTreeMetadata@trees@boosted_trees@tensorflow@@QEAA@XZ) referenced in function ""public: static class tensorflow::boosted_trees::trees::DecisionTreeMetadata * __cdecl google::protobuf::Arena::CreateMessage<class tensorflow::boosted_trees::trees::DecisionTreeMetadata>(class google::protobuf::Arena *)"" (??$CreateMessage@VDecisionTreeMetadata@trees@boosted_trees@tensorflow@@@Arena@protobuf@google@@SAPEAVDecisionTreeMetadata@trees@boosted_trees@tensorflow@@PEAV012@@Z)
1>training_ops.cc.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::DecisionTreeMetadata::DecisionTreeMetadata(void)"" (??0DecisionTreeMetadata@trees@boosted_trees@tensorflow@@QEAA@XZ)
1>ensemble_optimizer_ops.cc.obj : error LNK2019: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::DecisionTreeMetadata::DecisionTreeMetadata(class google::protobuf::Arena *)"" (??0DecisionTreeMetadata@trees@boosted_trees@tensorflow@@IEAA@PEAVArena@protobuf@google@@@Z) referenced in function ""public: static class tensorflow::boosted_trees::trees::DecisionTreeMetadata * __cdecl google::protobuf::Arena::CreateMessage<class tensorflow::boosted_trees::trees::DecisionTreeMetadata>(class google::protobuf::Arena *)"" (??$CreateMessage@VDecisionTreeMetadata@trees@boosted_trees@tensorflow@@@Arena@protobuf@google@@SAPEAVDecisionTreeMetadata@trees@boosted_trees@tensorflow@@PEAV012@@Z)
1>training_ops.cc.obj : error LNK2001: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::DecisionTreeMetadata::DecisionTreeMetadata(class google::protobuf::Arena *)"" (??0DecisionTreeMetadata@trees@boosted_trees@tensorflow@@IEAA@PEAVArena@protobuf@google@@@Z)

**...**

1>C:\Development\dev\test_projects\deeplearning\tensorflow_build64\Release\tf_tutorials_example_trainer.exe : fatal error LNK1120: 85 unresolved externals
1>Done building project ""tf_tutorials_example_trainer.vcxproj"" -- FAILED.
========== Build: 0 succeeded, 1 failed, 0 up-to-date, 0 skipped ==========
`
"
12036,refresh model for spark streaming with sv = tf.train.Supervisor() under sv.managed_session(),"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, I made custom distributed mnist code with spark streaming, based on [TensorFlowOnSpark example](https://github.com/yahoo/TensorFlowOnSpark/blob/master/examples/mnist/streaming/mnist_dist.py).
OS Platform and Distribution: CentOS 7
TensorFlow installed from (source or binary): Unmodified source with HDFS enabled
TensorFlow version (use command below): 1.2.1
Python version: Python 3.5.2 :: Anaconda 4.2.0 (64-bit)
Bazel version (if compiling from source): bazel release 0.5.2
CUDA/cuDNN version: 8.0/5.1.10
GPU model and memory: NVIDIA Geforce 1070 8GB (shared by two yarn containers, each has two excuetors)

The code works fine, but with --mode inference a yarn container will restore the model from logdir at the very beginning and use the same model during the whole prediction periode(based on spark logging info for restoring event).

My question is, is there a way that I can reload some new trained model(same architecture) under sv.managed_session() periodically or after new model is generated? Because at the same time, the other yarn container with --mode train produced new model.

Thanks for helping!"
12035,RDMA+verbs stuck in some nodes,"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
Yes, I made custom distributed inception code
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04
TensorFlow installed from (source or binary): Unmodified source with RDMA Verbs enabled
TensorFlow version (use command below): 1.3.0-rc1
Python version: 2.7.12
Bazel version (if compiling from source): 0.5.1
CUDA/cuDNN version: 8.0/5.1.5
GPU model and memory: NVIDIA TITAN Xp PCIe 12GB (4 per node)

The code works for normal grpc, but stuck between some nodes, not between all nodes.
I've tested all nodes with ib_write_bw and ibv_rc_pingpong, communication between all of the nodes works fine."
12033,read batch file using filename queue when fit wide and deep model,"### Describe the problem
I read records from files using typical pipeline:
```Python
filename_queue = tf.train.string_input_producer(file_list, num_epochs=num_epochs)
reader = tf.TextLineReader()
 _, value = reader.read_up_to(filename_queue, num_records=num_records)
data_batch = tf.train.batch([value]...)
record_defaults = [['string'] for _ in range(len(COLUMNS))]
col = tf.decode_csv(records=data_batch,
                    record_defaults=record_defaults,
                    field_delim=',')
```

Then, creates a dictionary mapping from each feature column name (k) to the values of that column stored in a Tensor.

```Python
feature_dict = dict()
label = None
for i, cname in enumerate(COLUMNS):
    c = tf.slice(col, begin=[i, 0], size=[1, -1])
    if cname in CONTINUOUS_COLUMNS:
        c = tf.string_to_number(c, tf.float64)
        c = tf.transpose(c)
        feature_dict[cname] = c
    elif cname in CATEGORICAL_COLUMNS:
        c = dense_to_sparse(c[0])
        feature_dict[cname] = c
    elif cname == 'click':
        label = tf.string_to_number(c[0], tf.int64)
```

Creating threads to prefetch using QueueRunner objects

```Python
model = DNNLinearCombinedClassifier(...)
config = tf.ConfigProto(allow_soft_placement=True)
config.gpu_options.allow_growth = True
with tf.Session(config=config) as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(tf.local_variables_initializer())
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)
    try:
        while not coord.should_stop():
            model.fit(input_fn=lambda: (feature_dict, label), steps=FLAGS.train_step)
        except tf.errors.OutOfRangeError:
            print('Done training, epoch reached')
        finally:
            coord.request_stop()
        coord.join(threads)
```

The error is:

```log
Traceback (most recent call last):
File ""/Users/houxue/workspace/Python/dnn-tf/v2/dnntf.py"", line 262, in <module>
    main(FLAGS)
File ""/Users/houxue/workspace/Python/dnn-tf/v2/dnntf.py"", line 145, in main
    train_v2(FLAGS)
File ""/Users/houxue/workspace/Python/dnn-tf/v2/dnntf.py"", line 128, in train_v2
    model.fit(input_fn=lambda: (feature_dict, label), steps=FLAGS.train_step)
File ""/usr/local/devtools/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 289, in new_func
    return func(*args, **kwargs)
File ""/usr/local/devtools/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 455, in fit
    loss = self._train_model(input_fn=input_fn, hooks=hooks)
File ""/usr/local/devtools/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1003, in _train_model
    config=self._session_config
File ""/usr/local/devtools/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 352, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
File ""/usr/local/devtools/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
File ""/usr/local/devtools/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 470, in __init__
    h.begin()
File ""/usr/local/devtools/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 163, in begin
    for (tag, tensor) in self._tensors.items()}
File ""/usr/local/devtools/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 163, in <dictcomp>
    for (tag, tensor) in self._tensors.items()}
File ""/usr/local/devtools/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 685, in _as_graph_element
    ""to current graph %s."" % (obj, graph))
ValueError: Passed Tensor(""binary_logistic_head/log_loss_with_two_classes/loss:0"", shape=(), dtype=float32) should have graph attribute that is equal to current graph <tensorflow.python.framework.ops.Graph object at 0x1131f3610>.
```
Googleing the error, I found that [ValueError: Passed Tensor(...) should have graph attribute that is equal to current graph](https://stackoverflow.com/questions/42799041/valueerror-passed-tensor-should-have-graph-attribute-that-is-equal-to-curr)
>  Returning the features or labels from a closure fails because a new tf.Graph is created when you call model.fit, so any modifications to the graph (e.g. tf.contrib calls) need to be made from within the input_fn (and therefore after the new graph has been instantiated).                                    -[alcorn](https://stackoverflow.com/users/6536722/alcorn)

How to read batch data using filename queue when fit DNNLinearCombinedClassifier model? Thanks!
"
12032,Save and restore feature request,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/v5.1
- **GPU model and memory**: GeForce GTX TITAN X / 12205MiB
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

 I am sorry to bother you all, here this is not a bug but, in my view, a feature request. 

I have trained a model and initialized a Saver instance by defining


<!-- language: python -->

    value_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='global/old_scope')
    value_list.extend(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='global/actor_critic'))
    saver = tf.train.Saver(value_list, max_to_keep=100)

    with tf.Session(config=tf_configs) as sess:
        coord = tf.train.Coordinator()
        if load_model:
            print('Loading Model...')
            ckpt = tf.train.get_checkpoint_state(model_path)
            saver.restore(sess, ckpt.model_checkpoint_path)
        else:
            sess.run(tf.global_variables_initializer())

And later in a new sub-scope, I added a new layer, with the same `saver` defined above, I trained the model, however, I found that weights of the new layer were not saved.

Here is my network

<!-- language: python -->

    with tf.variable_scope(scope):
        with tf.variable_scope('old_scope'):
            self.inputs = tf.placeholder(shape=[None, 80, 80, 1], dtype=tf.float32)
            self.conv_1 = slim.conv2d(activation_fn=tf.nn.relu, inputs=self.inputs, num_outputs=32,
                                      kernel_size=[8, 8], stride=4, padding='SAME')
            self.conv_2 = slim.conv2d(activation_fn=tf.nn.relu, inputs=self.conv_1, num_outputs=64,
                                      kernel_size=[4, 4], stride=2, padding='SAME')
            self.conv_3 = slim.conv2d(activation_fn=tf.nn.relu, inputs=self.conv_2, num_outputs=64,
                                      kernel_size=[3, 3], stride=1, padding='SAME')
            self.fc = slim.fully_connected(slim.flatten(self.conv_3), 512, activation_fn=tf.nn.elu)

        with tf.variable_scope('added_layer'):
            self.fc_1 = slim.fully_connected(self.fc, 512, activation_fn=tf.nn.elu)

        with tf.variable_scope('actor_critic'):
            # Output layers for policy and value estimations
            self.policy = slim.fully_connected(self.fc_1,
                                             cfg.ACTION_DIM,
                                             activation_fn=tf.nn.softmax, 
                                             biases_initializer=None)
            self.value = slim.fully_connected(self.fc_1,
                                              1,
                                              activation_fn=None,
                                              biases_initializer=None)

And I found that the [`var_list`][1] defines values to be restored and saved. But in my case, there is no checkpoint data of the new layer in the checkpoint file. 

Since before adding the new layer, I have trained the model and save the checkpoint data, and then after adding the new layer, I wanna train the network.

And I can define a new instance of Saver to save the model

`new_saver = tf.train.Saver(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=GLOBAL_SCOPE))`

However, I think it is not an elegant way to do so.

And can you add a feature to restore some values specified by users and also save some specified values when saving?

And in fact, it is a question asked by me on [SO](https://stackoverflow.com/questions/45502149/tensorflow-save-and-restore-model-after-adding-one-layer)

  [1]: https://www.tensorflow.org/api_docs/python/tf/train/Saver#__init__



### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12030,print out 'nan' for simple linear regression model ,"```
root@1cf079dc9729:~/src# python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
('v1.2.0-5-g435cdfc', '1.2.1')
```
The source code is very simple and it is from: https://www.tensorflow.org/get_started/get_started
```
import numpy as np
import tensorflow as tf

# Model parameters
W = tf.Variable([.3], dtype=tf.float32)
b = tf.Variable([-.3], dtype=tf.float32)
# Model input and output
x = tf.placeholder(tf.float32)
linear_model = W * x + b
y = tf.placeholder(tf.float32)
# loss
loss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares
# optimizer
optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss)
# training data
x_train = [1,2,3,4,35]
y_train = [0,-1,-2,-3,-34]
# training loop
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init) # reset values to wrong
for i in range(1000):
  sess.run(train, {x:x_train, y:y_train})

#x_test = [5, 100, 345] #[5,100,345,-99.66]
#y_test = [-4, -99, -344] #[-4,-99,-344,-101.66]
# evaluate training accuracy
curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x:x_train, y:y_train})
print(""W: %s b: %s loss: %s""%(curr_W, curr_b, curr_loss))
```
the output:
```
W: [ nan] b: [ nan] loss: nan
```
if i delete the last element for training data:
```
x_train = [1,2,3,4]
y_train = [0,-1,-2,-3]
```
the output is right:
```
W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11
```"
12028,Tensorboard fails to add summaries without any warning,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12025,Parallelize bottleneck creation in retraining script,"Hi

It might be possible to parallelize the creation of the bottleneck files in the Inception retraining script. Currently it can only do a few images per second and if you have 100s of thousands of images, it takes forever. I could do it myself also and create a pull request, if that's fine.

Thanks"
12024,tf.contrib.distributions.percentile returning AttributeError,"Hello,

I am trying to utilize tf.contrib.distributiions.percentile as described in:
https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/percentile 

- OS Platform and Distribution :  Ubuntu 14.04
- TensorFlow installed from binary
- TensorFlow version : 1.2.0
- Python version: 3.4.5

My invocation is as follows:
pctl_val = tf.contrib.distributions.percentile(grad_values, pctl)

Here is the error message that I get:
AttributeError: 'module' object has no attribute 'percentile'

I understand that the percentile feature was not available in 1.1.0 and it is now available in 1.2.0. Still, why do I get the error?"
12022,Error exporting TensorForestEstimator model for serving,"### Problem
I am trying to host a TensorForestEstimator model on Google Cloud's ML Engine. Everything works right, but at the very end the model fails to export with stack trace:

```
Traceback (most recent call last):
[...]
File ""/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 502, in train_and_evaluate
  export_results = self._maybe_export(eval_result)
File ""/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 597, in _maybe_export
  eval_result=eval_result))
File ""/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/export_strategy.py"", line 87, in export
  return self.export_fn(estimator, export_path, **kwargs)
File ""/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py"", line 412, in export_fn
  checkpoint_path=checkpoint_path)
File ""/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1280, in export_savedmodel
  actual_default_output_alternative_key)
File ""/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py"", line 252, in build_all_signature_defs
  for input_key, inputs in input_alternatives.items()
File ""/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py"", line 254, in <dictcomp>
  in output_alternatives.items()}
File ""/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py"", line 119, in build_standardized_signature_def
  input_tensors, output_tensors)
File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py"", line 146, in predict_signature_def
  signature_constants.PREDICT_METHOD_NAME)
File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py"", line 45, in build_signature_def
  signature_def.outputs[item].CopyFrom(outputs[item])
TypeError: None has type NoneType, but expected one of: bytes, unicode
```

Based on that trace, I'm thinking the error is in the make_export_strategy function with `default_output_alternative_key=None`. So what I did is set `default_output_alternative_key='default'` but then got the error:

```
ValueError: Requested default_output_alternative: default, but available output_alternatives are: [None]
```

So this shows that there are no output alternatives and my model is single-headed. Here is the code:

```
def serving_input_fn():
    feature_placeholders = {
    column['name']: tf.placeholder(dtype=column['dtype'], shape=[None])
    for column in columns_list if column['derived'] == 'N' and column['column_role'] != 'label'
    }

    features = {
        key: tf.expand_dims(tensor, -1)
        for key, tensor in feature_placeholders.items()
    }

    return InputFnOps(
        features=features,
        labels=None,
        default_inputs=feature_placeholders
    )

def get_experiment_fn(args):
    def _experiment(run_config, hparams):
        return Experiment(
            estimator=TensorForestEstimator(
                params=ForestHParams(
                    num_trees=args.num_trees,
                    max_nodes=10000,
                    min_split_samples=2,
                    num_features=7,
                    num_classes=args.num_projections,
                    regression=True
                ),
                model_dir=args.job_dir,
                graph_builder_class=RandomForestGraphs,
                config=run_config,
                report_feature_importances=True,
            ),
            train_input_fn=get_input_fn(
                project_name=args.project,
                data_location=args.train_data,
                dataset_size=args.train_size,
                batch_size=args.train_batch_size
            ),
            train_steps=args.train_steps,
            eval_input_fn=get_input_fn(
                project_name=args.project,
                data_location=args.eval_data,
                dataset_size=args.eval_size,
                batch_size=args.eval_batch_size
            ),
            eval_steps=args.eval_steps,
            eval_metrics=get_eval_metrics(),
            export_strategies=[
                make_export_strategy(
                    serving_input_fn,
                    default_output_alternative_key=None,
                    exports_to_keep=1
                )
            ]
        )
    return _experiment


def main():
    args = get_arg_parser().parse_args()

    learn_runner.run(
        experiment_fn=get_experiment_fn(args),
        run_config=RunConfig(model_dir=args.job_dir),
        hparams=HParams(**args.__dict__)
    )

if __name__ == '__main__':
    main()
```

This seems like a bug, but I could be wrong.

### System information
```
== cat /etc/issue ===============================================
Darwin mbmagenic12 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64
Mac OS X 10.12.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 8.1.0 (clang-802.0.42)
Target: x86_64-apple-darwin16.5.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

== uname -a =====================================================
Darwin mbmagenic12 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.13.1)
protobuf (3.1.0.post1)
tensorflow (1.2.1)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.2.1
tf.GIT_VERSION = v1.2.0-5-g435cdfc
tf.COMPILER_VERSION = v1.2.0-5-g435cdfc
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```
"
12019,foldr is too restrictive:  dimension 0 in both shapes must be equal,"Using TF 1.2.0-rc1 on Ubuntu 16.04.

There are cases where `tf.foldr` should work, but is unable to because of a restriction that the first shape dimensions be identical for all list elements.  Below is a self-contained example that demonstrates the problem:

```
import tensorflow as tf
sess = tf.InteractiveSession()

def concat2(A, B):
    return tf.concat([A, B], axis=0)

A = tf.constant([[10,10]])             # A.shape => (1,2)
B = tf.constant([[20, 20], [30, 30]])  # B.shape => (2,2)

print(concat2(A, B).eval())              # => [[10, 10], [20, 20], [30, 30]]
print(tf.foldr(concat2, [A, B]).eval())  # => ERROR!
```
"
12018,Missing tensorflow/core/debug/debug_service.grpc.pb.h header file,"Hi,

debug_io_utils.h includes a header file tensorflow/core/debug/debug_service.grpc.pb.h, which is missing from the repo. This causes build on Linux failed. 

`// TODO(cais): Support grpc:// debug URLs in open source once Python grpc
//   genrule becomes available. See b/23796275.

#ifndef PLATFORM_WINDOWS
#include ""tensorflow/core/debug/debug_service.grpc.pb.h""
`
Anyone see this issue?

Thanks,
RLE"
12017,stream_executor/platform/mutex.h doesn't compile under C++14,"Please go to Stack Overflow for help and support:

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Mac OS X with clang/llvm 5.0

- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
0.4.5

### Describe the problem
There's an ifdef in mutex.h that uses shared_timed_mutex when compiled as C++14 and up. The file doesn't compile because C++14 requires using condition_variable_any rather than condition_variable with that kind of mutex.

### Source code / logs
```
In file included from external/org_tensorflow/tensorflow/stream_executor/platform/mutex.h:25:
external/org_tensorflow/tensorflow/stream_executor/platform/default/mutex.h:86:26: error: no matching member function for call to 'wait_for'
  std::cv_status s = cv->wait_for(*mu, std::chrono::milliseconds(ms));
                     ~~~~^~~~~~~~
external/org_chromium_clang_mac/include/c++/v1/__mutex_base:404:21: note: candidate function not viable: no known conversion from 'perftools::gputools::mutex_lock' to 'unique_lock<std::__1::mutex> &' for 1st argument
condition_variable::wait_for(unique_lock<mutex>& __lk,
                    ^
external/org_chromium_clang_mac/include/c++/v1/__mutex_base:426:21: note: candidate function template not viable: requires 3 arguments, but 2 were provided
condition_variable::wait_for(unique_lock<mutex>& __lk,
```"
12016,A single value placeholder cause an GPU memory warning.,"I train my model on GTX 980 with 4 GB memory. The tensorflow version is 1.1.0. Python is 3.6.0. 

The example code is on this GitHub link (https://github.com/dennybritz/cnn-text-classification-tf). Since the code is kind of long, I do not want to copy them. IF someone can run it, I will appreciate it.

The problem is when I increase the epoch, It cause a memory error on my GPU. But, when delete the variable placeholder ""self.dropout_keep_prob"", there would be no error message. I do not know why, my code running on CPU is totally fine. 

Below is the warning message:
2017-08-03 15:38:52.537616: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.84GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available."
12015,Missing file pywrap_tensorflow_internal,"
Please close this issue. This issue was due to trying to import tensorflow from the source directory"
12011,Quantized graph not running with commit:bb88ec7ecc4dc7ba72548a5115fb86e20b14de5b,"OS: Ubuntu 16.04 64bits
 Android Version: 7.1 (Nougat)
 NDK Version: android-ndk-r12b

commit bb88ec7ecc4dc7ba72548a5115fb86e20b14de5b
Author: Alan Yee <alyee@ucsd.edu>
Date:   Mon Jul 24 22:46:38 2017 -0700



LOG:

```
native : benchmark_model.cc:405 Input layers: [Variable]
native : benchmark_model.cc:406 Input shapes: [1,227,227,3]
native : benchmark_model.cc:407 Input types: [float]
native : benchmark_model.cc:408 Output layers: [prob]
native : benchmark_model.cc:409 Num runs: [50]
native : benchmark_model.cc:410 Inter-run delay (seconds): [-1.0]
native : benchmark_model.cc:411 Num threads: [16]
native : benchmark_model.cc:412 Benchmark name: []
native : benchmark_model.cc:413 Output prefix: []
native : benchmark_model.cc:414 Show sizes: [0]
native : benchmark_model.cc:415 Warmup runs: [2]
native : benchmark_model.cc:54 Loading TensorFlow.
native : benchmark_model.cc:61 Got config, 0 devices
can't determine number of CPU cores: assuming 4
native : op_kernel.cc:1142 OpKernel ('op: ""BitwiseAnd"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_UINT16 } } }') for unknown op: BitwiseAnd
native : op_kernel.cc:1142 OpKernel ('op: ""BitwiseAnd"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_UINT8 } } }') for unknown op: BitwiseAnd
native : op_kernel.cc:1142 OpKernel ('op: ""BitwiseAnd"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT64 } } }') for unknown op: BitwiseAnd
native : op_kernel.cc:1142 OpKernel ('op: ""BitwiseAnd"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT32 } } }') for unknown op: BitwiseAnd
native : op_kernel.cc:1142 OpKernel ('op: ""BitwiseAnd"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT16 } } }') for unknown op: BitwiseAnd
native : op_kernel.cc:1142 OpKernel ('op: ""BitwiseAnd"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseAnd
native : op_kernel.cc:1142 OpKernel ('op: ""BitwiseXor"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_UINT16 } } }') for unknown op: BitwiseXor
native : op_kernel.cc:1142 OpKernel ('op: ""BitwiseXor"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_UINT8 } } }') for unknown op: BitwiseXor
native : op_kernel.cc:1142 OpKernel ('op: ""BitwiseXor"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT64 } } }') for unknown op: BitwiseXor
native : op_kernel.cc:1142 OpKernel ('op: ""BitwiseXor"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT32 } } }') for unknown op: BitwiseXor
native : op_kernel.cc:1142 OpKernel ('op: ""BitwiseXor"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT16 } } }') for unknown op: BitwiseXor
native : op_kernel.cc:1142 OpKernel ('op: ""BitwiseXor"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseXor
native : op_kernel.cc:1142 OpKernel ('op: ""Invert"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_UINT16 } } }') for unknown op: Invert
native : op_kernel.cc:1142 OpKernel ('op: ""Invert"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_UINT8 } } }') for unknown op: Invert
native : op_kernel.cc:1142 OpKernel ('op: ""Invert"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT64 } } }') for unknown op: Invert
native : op_kernel.cc:1142 OpKernel ('op: ""Invert"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT32 } } }') for unknown op: Invert
native : op_kernel.cc:1142 OpKernel ('op: ""Invert"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT16 } } }') for unknown op: Invert
native : op_kernel.cc:1142 OpKernel ('op: ""Invert"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT8 } } }') for unknown op: Invert
native : op_kernel.cc:1142 OpKernel ('op: ""BitwiseOr"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_UINT16 } } }') for unknown op: BitwiseOr
native : op_kernel.cc:1142 OpKernel ('op: ""BitwiseOr"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_UINT8 } } }') for unknown op: BitwiseOr
native : op_kernel.cc:1142 OpKernel ('op: ""BitwiseOr"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT64 } } }') for unknown op: BitwiseOr
native : op_kernel.cc:1142 OpKernel ('op: ""BitwiseOr"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT32 } } }') for unknown op: BitwiseOr
native : op_kernel.cc:1142 OpKernel ('op: ""BitwiseOr"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT16 } } }') for unknown op: BitwiseOr
native : op_kernel.cc:1142 OpKernel ('op: ""BitwiseOr"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseOr
native : benchmark_model.cc:74 Could not create TensorFlow Session: Not found: Op type not registered 'RoundToSteps' in binary running on localhost. Make sure the Op and Kernel are registered in the binary running in this process.

```

Earlier this error was not getting reported.

Thanks"
12009,Expose Tensorflow Go library as cgo_library rule in Bazel,Currently it's not possible to reference @org_tensorflow//tensorflow/go:go_default_library from a BUILD file. It would be great to have this ability.
12008,"Building 1.2.1 with bazel 0.5.2 fails with ""Genrules without outputs don't make sense""","Hello,

I'm trying to build Tensorflow on a Debian Stretch system usng the distribution provided CUDA packages. I'm using tensorflow 1.2.1 from git clone --recursive and bazel 0.5.2.

The build fails with ""Genrules without outputs don't make sense"" error.
```
ERROR: /home/gandalf/.cache/bazel/_bazel_gandalf/db00e87a820fe7f78d45fd73738fa4bd/external/local_config_cuda/cuda/BUILD:170:12: in outs attribute of genrule rule @local_config_cuda//cuda:cuda-include: Genrules without outputs don't make sense.
```
When looking at the file referenced in the error message I can see the following lines (starting from :170):

```
genrule(
    name = ""cuda-include"",
    outs = [
    ],
    cmd = """"""
    """""",
)

genrule(
    name = ""cuda-nvvm"",
    outs = [
    ],
    cmd = """"""
    """""",
)

genrule(
    name = ""cuda-extras"",
    outs = [
    ],
    cmd = """"""
    """""",
)

genrule(
    name = ""cuda-lib"",
    outs = [
      ""lib/libcuda.so"",
      ""lib/libcudart.so.8.0"",
      ""lib/libcudart_static.a"",
      ""lib/libcublas.so.8.0"",
      ""lib/libcusolver.so.8.0"",
      ""lib/libcurand.so.8.0"",
      ""lib/libcufft.so.8.0"",
      ""lib/libcudnn.so.6"",
      ""lib/libcupti.so.8.0"",    ],
    cmd = """"""
ln -s /usr/lib/x86_64-linux-gnu/nvidia/current/libcuda.so.375.82 $(@D)/lib/libcuda.so && ln -s /usr/lib/x86_64-linux-gnu/libcudart.so.8.0.44 $(@D)/lib/libcudart.so.8.0 && ln -s /usr/lib/x86_64-linux-gnu/libcudart_static.a $(@D)/lib/libcudart_static.a && ln -s /usr/lib/x86_64-linux-gnu/libcublas.so.8.0.45 $(@D)/lib/libcublas.so.8.0 && ln -s /usr/lib/x86_64-linux-gnu/libcusolver.so.8.0.44 $(@D)/lib/libcusolver.so.8.0 && ln -s /usr/lib/x86_64-linux-gnu/libcurand.so.8.0.44 $(@D)/lib/libcurand.so.8.0 && ln -s /usr/lib/x86_64-linux-gnu/libcufft.so.8.0.44 $(@D)/lib/libcufft.so.8.0 && ln -s /usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21 $(@D)/lib/libcudnn.so.6 && ln -s /usr/lib/x86_64-linux-gnu/libcupti.so.8.0.44 $(@D)/lib/libcupti.so.8.0    """""",
)

genrule(
    name = ""cudnn-include"",
    outs = [
      ""include/cudnn.h"",    ],
    cmd = """"""
ln -s /usr/include/cudnn.h $(@D)/cudnn.h    """""",
)
```
So indeed, there's genrules section with empty output but I have absolutely no idea if it's a problem or not and how I'm supposed to fix it.

Thanks in advance,

Best regards, Adam.

Here is the complete build log:
```
debian/configure-expect.sh 2.7 ""-mavx -mavx2 -mfma -msse4.1 -msse4.2"" /usr/lib/x86_64-linux-gnu/
spawn ./configure
Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python2.7
Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]
/usr/lib/python2.7/dist-packages/
Do you wish to build TensorFlow with MKL support? [y/N] N
No MKL support will be enabled for TensorFlow
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: -mavx -mavx2 -mfma -msse4.1 -msse4.2
Do you wish to use jemalloc as the malloc implementation? [Y/n] Y
jemalloc enabled
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] Y
Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] Y
Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] N
No XLA JIT support will be enabled for TensorFlow
Do you wish to build TensorFlow with VERBS support? [y/N] Y
VERBS support will be enabled for TensorFlow
Do you wish to build TensorFlow with OpenCL support? [y/N] N
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] Y
CUDA support will be enabled for TensorFlow
Do you want to use clang as CUDA compiler? [y/N] N
nvcc will be used as CUDA compiler
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 
Please specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/lib/x86_64-linux-gnu/
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/gcc
Please specify the cuDNN version you want to use. [Leave empty to use system default]: 
Please specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/lib/x86_64-linux-gnu/]: 
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
3.0,3.5,3.7,5.0,5.2,5.3,6.0,6.1
[Default is: ""3.5,5.2""]: Extracting Bazel installation...
.........
INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.
Configuration finished
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
____Loading package: tensorflow/tools/pip_package
____Loading package: @local_config_cuda//crosstool
____Loading package: @bazel_tools//tools/jdk
____Loading package: @local_jdk//
____Loading package: @bazel_tools//tools/cpp
____Loading package: @local_config_cc//
____Loading complete.  Analyzing...
____Loading package: tensorflow/contrib/graph_editor
____Loading package: tensorflow/contrib/tensor_forest
____Loading package: tensorflow/python/saved_model
____Loading package: tensorflow/contrib/slim/python/slim/data
____Loading package: tensorflow/contrib/signal
____Loading package: @six_archive//
____Downloading http://mirror.bazel.build/github.com/google/gemmlowp/archive/a6f29d8ac48d63293f845f2253eccbf86bc28321.tar.gz: 379,293 bytes
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 1,652,655 bytes
____Downloading http://mirror.bazel.build/github.com/google/re2/archive/b94b7cd42e9f02673cd748c1ac1d16db4052514c.tar.gz: 177,937 bytes
____Downloading http://mirror.bazel.build/github.com/google/gemmlowp/archive/a6f29d8ac48d63293f845f2253eccbf86bc28321.tar.gz: 553,707 bytes
____Downloading http://mirror.bazel.build/github.com/nanopb/nanopb/archive/1251fa1065afc0d62f635e0f63fec8276e14e13c.tar.gz: 80,756 bytes
____Loading package: @com_googlesource_code_re2//
____Downloading http://mirror.bazel.build/github.com/google/gemmlowp/archive/a6f29d8ac48d63293f845f2253eccbf86bc28321.tar.gz: 675,655 bytes
____Downloading http://mirror.bazel.build/github.com/nanopb/nanopb/archive/1251fa1065afc0d62f635e0f63fec8276e14e13c.tar.gz: 150,238 bytes
____Downloading http://mirror.bazel.build/github.com/glennrp/libpng/archive/v1.2.53.zip: 103,443 bytes
____Loading package: @org_pythonhosted_markdown//
____Downloading http://mirror.bazel.build/github.com/nanopb/nanopb/archive/1251fa1065afc0d62f635e0f63fec8276e14e13c.tar.gz: 214,048 bytes
____Downloading http://mirror.bazel.build/ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz: 393,473 bytes
____Downloading http://mirror.bazel.build/pypi.python.org/packages/b7/7f/44d3cfe5a12ba002b253f6985a4477edfa66da53787a2a838a40f6415263/Werkzeug-0.11.10.tar.gz: 556,541 bytes
____Loading package: @nanopb_git//
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 2,943,035 bytes
____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 556,541 bytes
____Downloading http://mirror.bazel.build/pypi.python.org/packages/b7/7f/44d3cfe5a12ba002b253f6985a4477edfa66da53787a2a838a40f6415263/Werkzeug-0.11.10.tar.gz: 889,455 bytes
____Loading package: @zlib_archive//
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 3,168,497 bytes
____Downloading http://mirror.bazel.build/github.com/google/highwayhash/archive/dfcb97ca4fe9277bf9dc1802dd979b071896453b.tar.gz: 121,218 bytes
____Loading package: @org_pocoo_werkzeug//
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 3,416,647 bytes
____Downloading http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz: 384,964 bytes
____Loading package: @fft2d//
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 3,610,913 bytes
____Downloading http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz: 688,416 bytes
____Downloading http://mirror.bazel.build/github.com/google/farmhash/archive/92e897b282426729f4724d91a637596c7e2fe28f.zip: 88,003 bytes
____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 411,906 bytes
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 3,800,925 bytes
____Downloading http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz: 912,460 bytes
____Downloading http://mirror.bazel.build/github.com/google/farmhash/archive/92e897b282426729f4724d91a637596c7e2fe28f.zip: 156,067 bytes
____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 536,690 bytes
____Loading package: @libxsmm_archive//
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 3,946,979 bytes
____Downloading http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz: 1,177,310 bytes
____Downloading http://mirror.bazel.build/github.com/jemalloc/jemalloc/archive/4.4.0.tar.gz: 166,595 bytes
____Loading package: @nccl_archive//
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 4,098,705 bytes
____Downloading http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz: 1,561,904 bytes
____Downloading http://mirror.bazel.build/github.com/jemalloc/jemalloc/archive/4.4.0.tar.gz: 312,649 bytes
____Downloading http://mirror.bazel.build/github.com/libjpeg-turbo/libjpeg-turbo/archive/1.5.1.tar.gz: 295,629 bytes
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 4,214,981 bytes
____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 1,361,965 bytes
____Downloading http://mirror.bazel.build/github.com/google/farmhash/archive/92e897b282426729f4724d91a637596c7e2fe28f.zip: 418,397 bytes
____Downloading http://mirror.bazel.build/curl.haxx.se/download/curl-7.49.1.tar.gz: 1,068,439 bytes
____Downloading http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz: 2,164,554 bytes
____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 1,079,468 bytes
____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 1,597,353 bytes
____Downloading http://mirror.bazel.build/github.com/libjpeg-turbo/libjpeg-turbo/archive/1.5.1.tar.gz: 712,521 bytes
____Loading package: @eigen_archive//
____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 1,747,661 bytes
____Downloading http://mirror.bazel.build/github.com/libjpeg-turbo/libjpeg-turbo/archive/1.5.1.tar.gz: 868,501 bytes
____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 1,900,805 bytes
____Downloading http://mirror.bazel.build/github.com/libjpeg-turbo/libjpeg-turbo/archive/1.5.1.tar.gz: 1,024,481 bytes
____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 2,053,949 bytes
____Downloading http://mirror.bazel.build/github.com/libjpeg-turbo/libjpeg-turbo/archive/1.5.1.tar.gz: 1,180,461 bytes
____Loading package: @jpeg//
____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 2,075,220 bytes
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 307,790 bytes
____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 2,337,549 bytes
____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 2,238,290 bytes
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 676,470 bytes
____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 2,470,841 bytes
____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 2,415,540 bytes
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 1,026,716 bytes
____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 2,604,133 bytes
____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 2,572,938 bytes
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 1,423,756 bytes
____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 2,751,605 bytes
____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 2,772,876 bytes
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 1,928,564 bytes
____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 2,967,141 bytes
____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 3,038,042 bytes
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 2,583,680 bytes
____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 3,239,397 bytes
____Loading package: @curl//
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 3,414,628 bytes
____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 3,781,074 bytes
____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 4,302,898 bytes
____Loading package: @grpc//
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 926,641 bytes
____Loading package: @boringssl//
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 2,126,269 bytes
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 3,469,115 bytes
____Loading package: @protobuf//
____Loading package: @local_config_cuda//cuda
____Loading package: tools/defaults
____Loading package: tensorflow/tools/graph_transforms
____Loading package: tensorflow/contrib
____Loading package: tensorflow/cc
ERROR: /home/gandalf/.cache/bazel/_bazel_gandalf/db00e87a820fe7f78d45fd73738fa4bd/external/local_config_cuda/cuda/BUILD:170:12: in outs attribute of genrule rule @local_config_cuda//cuda:cuda-include: Genrules without outputs don't make sense.
____Downloading http://mirror.bazel.build/ufpr.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz: 1,153,203 bytes
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.
____Elapsed time: 10.746s
debian/rules:29: recipe for target 'build-py2-2.7' failed
```"
12005,Warning when running rnn_commons.select_last_activations (TensorFlow 1.2.1),"I see the following warning:
/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory."
12004,"Where does ""unknown error"" message come from ?","Hi,

While doing my training of a faster-rcnn model, I get an ""unknown error"" message but the training seems to work fine.

I just want to know where in Tensorflow's source code there is a ""print('unknown error')"" ? I don't seem to find that anywhere.

Thanks you guys,"
12003,top_k doesn't order lower index first if two elements are equals,"# Bug
If two elements are equal, ```top_k``` is not ordering them by index value.

# Reproducing the error
Here is the command I am running
```
import tensorflow as tf
sess = tf.Session()
 sess.run(tf.nn.top_k(tf.convert_to_tensor([0, 0, 0, 0]), k=3))
```

And here is the output for Tensorflow 1.3.0-rc0
```
TopKV2(values=array([0, 0, 0], dtype=int32), indices=array([1, 3, 0], dtype=int32))
```
compared to version 1.2.1
```
TopKV2(values=array([0, 0, 0]), indices=array([0, 1, 2]))
```

# Setup
Running ```tf_env_collect.sh```:
```

== cat /etc/issue ===============================================
Linux 0187adfde6c8 4.4.0-47-generic #68-Ubuntu SMP Wed Oct 26 19:39:52 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.2 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux 0187adfde6c8 4.4.0-47-generic #68-Ubuntu SMP Wed Oct 26 19:39:52 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.12.1)
protobuf (3.3.0)
tensorflow (1.3.0rc0)
tensorflow-tensorboard (0.1.2)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.3.0-rc0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: line 90: nvidia-smi: command not found

== cuda libs  ===================================================
```
"
12002,tf.nn.sparse_softmax_cross_entropy_with_logits() seems to return bad values !,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: from pip
- **TensorFlow version (use command below)**: 'v1.2.0-5-g435cdfc', '1.2.1'
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: cudNN v8.0
- **GPU model and memory**: 2* NVidia GeForce 1080Ti (11Go each)
- **Exact command to reproduce**: Following code

### Describe the problem
It seems that `tf.nn.sparse_softmax_cross_entropy_with_logits() ` and `tf.nn.softmax_cross_entropy_with_logits()` are returning bad values. According to this [stackOverflow post](https://stackoverflow.com/questions/36078411/tensorflow-are-my-logits-in-the-right-format-for-cross-entropy-function/36086477#36086477), `tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels)` is almost equivalent to `-tf.reduce_sum(labels * tf.log(tf.nn.softmax(logits) + 1e-10), 1))`.
But when I'm using the provided optimized function, I don't get the same results. It appears that its come from the log function when logits is equal to 0. But I've read that `tf.nn.sparse_softmax_cross_entropy_with_logits()` handle that case, and that's the case, cause I would have some *Nan* output. But instead I have huge numbers, so I (naturally) thought that to avoid *log(0)* a small constant must have been added to the problematic numbers. So I tried to reproduce this tip (with 1e-10) and I don't still have the same result. So I tried to read the code [here](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/ops/nn_ops.py) to understand what's going on. But I can't find in the repo the gen_nn_ops module to understand why this function returns such a ""strange"" result. I will be pleased to contribute to understand what happens (and to correct it, if needed of course !).
Here is a really simple piece of code to reproduce the ""error"" (if it's indeed one). And we have the same behavior with the sparse version of the function (with proper labels).
Thanks, 
### Source code / logs
```
graph = tf.Graph()

features = np.array([[6.83324017e-02, 4.55211316e-01,-1.41892820e-01, 6.41751984e-01, -5.45895865e-01, 5.38657679e-01, 1.93379897e-01, 1.60154529e-01, 1.57859872e-02, 1.36758294e-02, 4.40859703e+00, 4.96067050e+03, -5.95230431e+01, 2.29624126e+00, 4.02069655e+00], [8.82284599e-01, 6.42900440e-01,-4.27639642e-02, 1.83567706e-01, 7.52404702e-01, -6.32605771e-01, 5.40391531e-01, 5.84584613e-01,-7.15044264e-03,-8.23328268e-02, 6.29273115e+00,-4.32369561e+01, 7.07259958e+00,-1.02810233e+00,-7.04034886e-01], [5.48660773e-01, 8.08794529e-01,-5.96924524e-02,-7.26052964e-01,-2.70772000e-02, 6.87105464e-01, 5.68913359e-01, 4.76252594e-01, 4.14203699e-02,-5.79935485e-03, 9.40232256e+00,-2.01665599e+04, 1.34500232e+01,-2.24989629e-01, 2.52753983e-01], [7.46613308e-01, 8.23272733e-01,-1.04753678e-01, 7.87653516e-01, 5.33736860e-01, 3.07777360e-01, 8.51814816e-01, 7.29870149e-01,-5.67521706e-03, 2.37203887e-02, 6.33280960e+00, 4.08845288e+05, 4.48007235e+01, 5.33139458e-02, 2.37384134e-02], [4.47498908e-01, 1.49080014e-01,-9.07106172e-03,-2.67174181e-01,-5.21700457e-01, 8.10213916e-01, 9.18038857e-01, 8.36740457e-01,-7.64173908e-03,-1.18870530e-02, 6.18394833e+00, 7.37307204e+01,-5.58432681e+01, 3.83996968e-01, 9.18497562e-01], [4.71607629e-01, 1.31179570e-01,-4.56846546e-02,-9.27597302e-01,-3.63639607e-01,-8.56123912e-02, 3.32925650e-01, 2.86999292e-01,-1.37396795e-01,-2.39745171e-01, 6.28318531e+00,-9.03421275e+04,-9.83543039e+03,-1.09839821e+00, 1.05041514e+00], [4.71613040e-01, 1.31166299e-01,-4.56797268e-02,-9.27775404e-01,-3.64117510e-01,-8.15551274e-02, 3.32854008e-01, 2.86979856e-01,-1.36950051e-01,-2.39623484e-01, 6.28318531e+00,-2.85787226e+05, 1.02588457e+05,-1.09795489e+00, 1.05020120e+00], [1.72510574e-01, 3.40244123e-02,-1.78258372e-01,-1.78623912e-01, 9.82406854e-01,-5.45001987e-02, 6.49133952e-01, 4.58514334e-01,-1.05587941e-01,-1.50382361e-01, 6.56445597e+00,-7.39915259e+01,-3.39043636e+01, 8.32312454e-01, 1.66266815e+00]])

labels = np.array([[ 1., 0.], [ 0., 1.], [ 0., 1.], [ 1., 0.], [ 0., 1.], [ 0., 1.], [ 0., 1.], [ 1., 0.]])

totalLoss = 0
totalTest = 0

with graph.as_default():
    x = tf.placeholder(""float"", [None, 15], name = ""x"")
    y = tf.placeholder(""int64"", [None, 2], name = ""y"")

    h1 = tf.Variable(tf.truncated_normal([15, 100], stddev = 0.1), name = ""h1"") 
    out = tf.Variable(tf.truncated_normal([100, 2], stddev = 0.1), name = ""out"")
    b1 =  tf.Variable(tf.truncated_normal([100], stddev = 0.1), name = ""b1"")
    bout = tf.Variable(tf.truncated_normal([2], stddev = 0.1), name = ""bout"")


    def model(x):
        layer_1 = tf.add(tf.matmul(x, h1), b1)
        layer_1 = tf.nn.relu(layer_1)

        out_layer = tf.matmul(layer_1, out) + bout
        return out_layer

    logits = model(x)
    
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y))
    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)

    with tf.Session(graph = graph) as session:
        for i in xrange(10):
            tf.global_variables_initializer().run()
            
            _ = session.run(optimizer, feed_dict = {x : features, y : labels})
            l = session.run(loss, feed_dict = {x : features, y : labels})
            test = session.run(tf.reduce_mean(-tf.reduce_sum(labels * tf.log(tf.nn.softmax(logits) + 1e-10), 1)), feed_dict = {x : features})
            totalLoss += l 
            totalTest += test

print(""mathematical : "", totalTest *1. /10)
print(""sparse_softmax_cross_entropy : "", totalLoss *1. /10)
```
"
12001,Feature Request: Add separable_conv2d_transpose operation,"Some recent papers (e.g.) have shown that transposed separable convolutions can be a great choice for decoders in encoder decoder architectures.

Can you add a seperable_conv2d_transpose operation comparable to the conv2d_transpose operation?"
12000,error C2678: binary '<': no operator found which takes a left-hand operand of type IndicesRowIterator,"If you build a debug version of the current tensorflow version 1.3.0 on Windows  with CMAKE  following error is occurred:

(ClCompile target) ->
  C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\VC\Tools\MSVC\14.10.25017\include\xutility(978): error C2678: binary '<': no operator found which
takes a left-hand operand of type 'tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator' (or there is no acceptable conversion) (compiling sou
rce file C:\Development\dev\test_projects\deeplearning\tensorflow\tensorflow\tensorflow\contrib\boosted_trees\lib\utils\sparse_column_iterable.cc) [C:\Development\de
v\test_projects\deeplearning\tensorflow_build64\tf_core_kernels.vcxproj]

The reason is **IndicesRowIterator::operator<()** is missing.
Adding these lines 

  **bool operator<( const IndicesRowIterator& other ) const {
     QCHECK_LT( iter_, other.iter_ );
     return ( row_idx_ < other.row_idx_ );
  }**

solves the problem.

###System information
tensorflow 1.3.0
Windows 10
VisualStudio 2017
CMake 3.8.1



"
11998,tensorflow multi label classification,"How does tensorflow CNN implement multi label classification, specifically how to input multi tags and output multi label prediction results?"
11996,tensorflow Makefile build does not support building arm64-v8a static library for Android,"Currently tensorflow has only support build static library in `armeabi-v7a` for Android.  

And the build script has not support the other platforms yet as we can see in:
 [`tensorflow/tensorflow/contrib/makefile/Makefile`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/Makefile#L219)

Our team would like to build all the architecture for Android if possible, and we tried to fixed the `makefile` that expect to link with our Android project but got many linking errors except `armeabi-v7a`.

So, I was just wondering if tensorflow will support those platform: `arm64-v8a`, `x86`, `x88_64`?

"
11995,Errors,"Hi I am running a simple tensor flow script and I am getting the following errors
 - OS is macOS
-Tensorflow binary 1.2.1 version
-python version 3.5 

The thing is it runs on the Jupyter notebook buy when I run python add.py I get the following errors

```
`File ""trail1.py"", line 4, in <module>
    import tensorflow as tf
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/google/protobuf/descriptor.py"", line 37, in <module>
    import six
  File ""/Users/neutrino/six.py"", line 1, in <module>
    from tensorflow.contrib.tensorboard.plugins import projector
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/__init__.py"", line 22, in <module>
    from tensorflow.contrib import bayesflow
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/bayesflow/__init__.py"", line 24, in <module>
    from tensorflow.contrib.bayesflow.python.ops import entropy
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/bayesflow/python/ops/entropy.py"", line 23, in <module>
    from tensorflow.contrib.bayesflow.python.ops.entropy_impl import *
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/bayesflow/python/ops/entropy_impl.py"", line 29, in <module>
    from tensorflow.contrib.bayesflow.python.ops import monte_carlo_impl as monte_carlo
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/bayesflow/python/ops/monte_carlo_impl.py"", line 28, in <module>
    from tensorflow.python.framework import ops
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 30, in <module>
    from tensorflow.core.framework import attr_value_pb2
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/core/framework/attr_value_pb2.py"", line 8, in <module>
    from google.protobuf import reflection as _reflection
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/google/protobuf/reflection.py"", line 58, in <module>
    from google.protobuf.internal import python_message as message_impl
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/google/protobuf/internal/python_message.py"", line 62, in <module>
    from google.protobuf.internal import decoder
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/google/protobuf/internal/decoder.py"", line 87, in <module>
    if six.PY3:
AttributeError: module 'six' has no attribute 'PY3'`
```"
11994,tensorflow-1.2.1：TypeError: sampled_loss() got an unexpected keyword argument 'logits',"# the code is:
    def sampled_loss(labels, inputs):
            labels = tf.reshape(labels, [-1, 1])
            # We need to compute the sampled_softmax_loss using 32bit floats to
            # avoid numerical instabilities.
            local_w_t = tf.cast(w_t, tf.float32)
            local_b = tf.cast(b, tf.float32)
            local_inputs = tf.cast(inputs, tf.float32)
            return tf.cast(
                tf.nn.sampled_softmax_loss(
                    weights=local_w_t,
                    biases=local_b,
                    labels=labels,
                    inputs=local_inputs,
                    num_sampled=num_samples,
                    num_classes=self.target_vocab_size),tf.float32)
      
           softmax_loss_function = sampled_loss


Traceback (most recent call last):
  File ""predict.py"", line 177, in <module>
    tf.app.run()
  File ""/home/amax/tensorflow/1.2.1/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""predict.py"", line 159, in main
    decode()
  File ""predict.py"", line 40, in decode
    model = create_model(sess, True)
  File ""/home/BD/bd_chenyi/nlp_code/textsum/textsum.py"", line 144, in create_model
    forward_only=forward_only)
  File ""/home/BD/bd_chenyi/nlp_code/textsum/seq2seq_model.py"", line 167, in __init__
    softmax_loss_function=softmax_loss_function)
  File ""/home/amax/tensorflow/1.2.1/local/lib/python2.7/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py"", line 1221, in model_with_buckets
    softmax_loss_function=softmax_loss_function))
  File ""/home/amax/tensorflow/1.2.1/local/lib/python2.7/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py"", line 1134, in sequence_loss
    softmax_loss_function=softmax_loss_function))
  File ""/home/amax/tensorflow/1.2.1/local/lib/python2.7/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py"", line 1089, in sequence_loss_by_example
    crossent = softmax_loss_function(labels=target, logits=logit)
TypeError: sampled_loss() got an unexpected keyword argument 'logits'"
11991, ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.4
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**: 0.5.3
- **CUDA/cuDNN version**: 6.0.21
- **GPU model and memory**:
- **Exact command to reproduce**: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

### Describe the problem

I know someone had reported the same issue. Their solution is to roll back bazel to 0.5.2. But it does not work on my machine. Does anyone know how to fix this problem without the need of rolling back bazel? Thx!

> tensorflow-git$ sudo bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package 
> WARNING: ignoring http_proxy in environment.
> .......
> ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
> 	File ""/home/intel/DevLib/tensorflow-git/third_party/gpus/cuda_configure.bzl"", line 1039
> 		_create_local_cuda_repository(repository_ctx)
> 	File ""/home/intel/DevLib/tensorflow-git/third_party/gpus/cuda_configure.bzl"", line 976, in _create_local_cuda_repository
> 		_host_compiler_includes(repository_ctx, cc)
> 	File ""/home/intel/DevLib/tensorflow-git/third_party/gpus/cuda_configure.bzl"", line 145, in _host_compiler_includes
> 		get_cxx_inc_directories(repository_ctx, cc)
> 	File ""/home/intel/DevLib/tensorflow-git/third_party/gpus/cuda_configure.bzl"", line 120, in get_cxx_inc_directories
> 		set(includes_cpp)
> depsets cannot contain mutable items
> WARNING: Target pattern parsing failed.
> ERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
> 	File ""/home/intel/DevLib/tensorflow-git/third_party/gpus/cuda_configure.bzl"", line 1039
> 		_create_local_cuda_repository(repository_ctx)
> 	File ""/home/intel/DevLib/tensorflow-git/third_party/gpus/cuda_configure.bzl"", line 976, in _create_local_cuda_repository
> 		_host_compiler_includes(repository_ctx, cc)
> 	File ""/home/intel/DevLib/tensorflow-git/third_party/gpus/cuda_configure.bzl"", line 145, in _host_compiler_includes
> 		get_cxx_inc_directories(repository_ctx, cc)
> 	File ""/home/intel/DevLib/tensorflow-git/third_party/gpus/cuda_configure.bzl"", line 120, in get_cxx_inc_directories
> 		set(includes_cpp)
> depsets cannot contain mutable items
> INFO: Elapsed time: 4.064s
> FAILED: Build did NOT complete successfully (0 packages loaded)
>     currently loading: tensorflow/tools/pip_package
> 
"
11990,"Python crashes with ""Check failed"" error when fitting TensorForestEstimator","### Problem

I am experiencing an issue trying to use `TensorForestEstimator`. I am trying to predict 7 output labels by inputting 7 features. That is, my `num_classes=7` and `num_features=7` in my hyperparameters. The shape of `features` and `labels` is `(484876, 7)`.

Here is an example of the format of my input data:

```
f1       f2     f3    f4      f5    f6    f7   l1       l2       l3       l4       l5       l6       l7
39000.0  120.0  65.0  1000.0  25.0  0.69  3.94 39000.0  39959.0  42099.0  46153.0  49969.0  54127.0  55911.0
32000.0  185.0  65.0  1000.0  75.0  0.46  2.19 32000.0  37813.0  43074.0  48528.0  54273.0  60885.0  63810.0 
30000.0  185.0  65.0  1000.0  25.0  0.41  1.80 30000.0  32481.0  35409.0  39145.0  42750.0  46678.0  48595.0
```

When calling `fit()`,  each of the trees get properly trained (see logs below), but when it goes to the`CheckpointSaverHook` phase, Python crashes with the following error:

```
2017-08-02 22:55:00.690746: F tensorflow/contrib/tensor_forest/kernels/count_extremely_random_stats_op.cc:404] 
Check failed: column < num_classes_ (39001 vs. 7)

Process finished with exit code 134 (interrupted by signal 6: SIGABRT)
```

I might be wrong, but this seems like a bug since TensorFlow is training each the `TensorForestEstimator` trees, but crashing upon saving. Also the number 39001 is not related to my data since the  shape for both `feature` and `label` is `(484876, 7)`.

Here is the code to reproduce:

```
import tensorflow as tf
import os

from tensorflow.contrib.tensor_forest.client.random_forest import TensorForestEstimator, TensorForestLossHook
from tensorflow.contrib.tensor_forest.python.tensor_forest import ForestHParams, RandomForestGraphs

tf.logging.set_verbosity('INFO')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'

def get_training_data():
    training_file = ""data.txt""
    data = pd.read_csv(training_file, sep='\t')

    X = np.array(data.drop('Result', axis=1), dtype=np.float32)

    y = []
    # All 7 labels are stored in a string initially, so we parse each label to float and store in an array
    for e in data.ResultStr:
        y.append(list(np.array(str(e).replace('[', '').replace(']', '').split(','))))

    y = np.array(y, dtype=np.float32)

    features = tf.constant(X) # Also tried with tf.Variable
    labels = tf.constant(y) # Also tried with tf.Variable

    return features, labels

# Main
hyperparameters = ForestHParams(
    num_trees=100,
    max_nodes=10000,
    bagging_fraction=1.0,
    num_splits_to_consider=7,
    feature_bagging_fraction=1.0,
    max_fertile_nodes=0,
    split_after_samples=250,
    min_split_samples=5,
    valid_leaf_threshold=1,
    dominate_method='bootstrap',
    dominate_fraction=0.99,
    num_classes=7, 
    num_features=7
)

estimator = TensorForestEstimator(
    params=hyperparameters,
    device_assigner=None,
    model_dir=None,
    graph_builder_class=RandomForestGraphs,
    config=None,
    weights_name=None,
    keys_name=None,
    feature_engineering_fn=None,
    early_stopping_rounds=100,
    num_trainers=1,
    trainer_id=0,
    report_feature_importances=False,
    local_eval=False
)

estimator.fit(
    input_fn=lambda: get_training_data(),
    max_steps=100,
    monitors=[
        TensorForestLossHook(
            early_stopping_rounds=30
        )
    ]
)
```
Here is the verbose logging output:
```
INFO:tensorflow:Using default config.
WARNING:tensorflow:Using temporary folder as model directory: /var/folders/s3/k08tc0v94j10xn6fk3wz79zh0000gn/T/tmp2RnVSx
INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1121fe390>, '_model_dir': '/var/folders/s3/k08tc0v94j10xn6fk3wz79zh0000gn/T/tmp2RnVSx', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_tf_random_seed': None, '_environment': 'local', '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_tf_config': gpu_options {
  per_process_gpu_memory_fraction: 1
}
, '_evaluation_master': '', '_master': ''}
INFO:tensorflow:Constructing forest with params = 
INFO:tensorflow:{'valid_leaf_threshold': 1, 'split_after_samples': 250, 'num_output_columns': 7, 'feature_bagging_fraction': 1.0, 'split_initializations_per_input': 1, 'bagged_features': None, 'min_split_samples': 5, 'max_nodes': 10000, 'num_features': 7, 'num_trees': 100, 'num_splits_to_consider': 7, 'base_random_seed': 0, 'num_outputs': 1, 'dominate_fraction': 0.99, 'max_fertile_nodes': 5000, 'bagged_num_features': 7, 'dominate_method': 'bootstrap', 'bagging_fraction': 1.0, 'regression': False, 'num_classes': 6}
INFO:tensorflow:training graph for tree: 0
INFO:tensorflow:training graph for tree: 1
...
INFO:tensorflow:training graph for tree: 98
INFO:tensorflow:training graph for tree: 99
INFO:tensorflow:Create CheckpointSaverHook.
2017-08-02 22:53:25.218258: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-02 22:53:25.218298: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-02 22:53:25.218304: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-02 22:53:25.218308: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-08-02 22:55:00.690746: F tensorflow/contrib/tensor_forest/kernels/count_extremely_random_stats_op.cc:404] Check failed: column < num_classes_ (39001 vs. 7)

Process finished with exit code 134 (interrupted by signal 6: SIGABRT)
```

### System information: tf_env.txt

```

== cat /etc/issue ===============================================
Darwin 127001.local 16.6.0 Darwin Kernel Version 16.6.0: Fri Apr 14 16:21:16 PDT 2017; root:xnu-3789.60.24~6/RELEASE_X86_64 x86_64
Mac OS X 10.12.5

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 8.1.0 (clang-802.0.42)
Target: x86_64-apple-darwin16.6.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin 127001.local 16.6.0 Darwin Kernel Version 16.6.0: Fri Apr 14 16:21:16 PDT 2017; root:xnu-3789.60.24~6/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.12.1)
protobuf (3.3.0)
tensorflow (1.2.1)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.2.1
tf.GIT_VERSION = v1.2.0-5-g435cdfc
tf.COMPILER_VERSION = v1.2.0-5-g435cdfc
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```"
11988,AttentionWrapper is not compatible with dynamic_rnn,"### System information
- **TensorFlow version (v1.2.0-rc2-21-g12f033d 1.2.0)**:
- **Python 3**

### Describe the problem

The return of `zero_state` function of  `tensorflow.contrib.seq2seq.AttentionWrapper` is not compatible with `tensorflow.nn.dynamic_rnn` function.  It seems that `if output.shape.ndims == 0:` in `_copy_one_through` function does not work. Once those scalar elements in return tuple are modified to have the shape like [batch_size, ...] the error disappears.

### Code to reproduce the bug

```
import tensorflow as tf

batch_size=2
hidden_units = 3
attention_size = 4
max_len = 5

inputs = tf.constant(1.,shape=(batch_size,max_len,hidden_units))
context = tf.constant(1.,shape=(batch_size,max_len,hidden_units))
input_lens = (2,4)

cell = tf.contrib.rnn.LSTMCell(hidden_units)
am = tf.contrib.seq2seq.BahdanauAttention(attention_size, context)
wrapper = tf.contrib.seq2seq.AttentionWrapper(cell,am)
tf.nn.dynamic_rnn(wrapper,inputs,input_lens,dtype=tf.float32)
```
### Log
```
Traceback (most recent call last):
  File ""test.py"", line 15, in <module>
    tf.nn.dynamic_rnn(wrapper,inputs,input_lens,dtype=tf.float32)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py"", line 574, in dynamic_rnn
    dtype=dtype)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py"", line 737, in _dynamic_rnn_loop
    swap_memory=swap_memory)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2770, in while_loop
    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2599, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2549, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py"", line 720, in _time_step
    skip_conditionals=True)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py"", line 210, in _rnn_step
    final_output_and_state = _copy_some_through(new_output, new_state)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py"", line 182, in _copy_some_through
    for state, new_state in zip(flat_state, flat_new_state)]
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py"", line 182, in <listcomp>
    for state, new_state in zip(flat_state, flat_new_state)]
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py"", line 171, in _copy_one_through
    return array_ops.where(copy_cond, output, new_output)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py"", line 2328, in where
    return gen_math_ops._select(condition=condition, t=x, e=y, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 2145, in _select
    name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2508, in create_op
    set_shapes_for_outputs(ret)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1873, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1823, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py"", line 610, in call_cpp_shape_fn
    debug_python_shape_fn, require_shape_fn)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py"", line 676, in _call_cpp_shape_fn_impl
    raise ValueError(err.message)
ValueError: Shapes must be equal rank, but are 0 and 1 for 'rnn/while/Select_4' (op: 'Select') with input shapes: [2], [], [].
```"
11986,The FixedLenFeature of parse_example?,"View[ API DOC](https://www.tensorflow.org/versions/master/api_docs/python/tf/parse_example).

The description maybe wrong?
`Each FixedLenFeature df maps to a Tensor of the specified type (or tf.float32 if not specified) and shape (serialized.size(),) + df.shape.`

But the example shows:
For dense results in two serialized Examples:

```
[
  features {
    feature { key: ""age"" value { int64_list { value: [ 0 ] } } }
    feature { key: ""gender"" value { bytes_list { value: [ ""f"" ] } } }
   },
   features {
    feature { key: ""age"" value { int64_list { value: [] } } }
    feature { key: ""gender"" value { bytes_list { value: [ ""f"" ] } } }
  }
]
```
We can use arguments:

```
example_names: [""input0"", ""input1""],
features: {
    ""age"": FixedLenFeature([], dtype=tf.int64, default_value=-1),
    ""gender"": FixedLenFeature([], dtype=tf.string),
}
```
And the expected output is:
```

{
  ""age"": [[0], [-1]],
  ""gender"": [[""f""], [""f""]],
}
```
The shape of output is (2, 1) not equal to (2, ) + (), where  (2,) is `(serialized.size(),)` and () is `df.shape`.
"
11985,windows bazel build failed: undeclared inclusion ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
2ab9cb209babf905091dcff2f7cdce38da616cfe
- **Python version**: 
3.5.3
- **Bazel version (if compiling from source)**:
0.5.3
- **CUDA/cuDNN version**:
None
- **GPU model and memory**:
None
- **Exact command to reproduce**:
bazel --output_base C:\t  build  //tensorflow/tools/pip_package:build_pip_package


### Describe the problem
ERROR: C:/os/tensorflow/tensorflow/core/BUILD:1271:1: undeclared inclusion(s) in rule '//tensorflow/core:lib_internal':
this rule is missing dependency declarations for the following files included by 'tensorflow/core/framework/variant_tensor_data.cc':
  'C:/os/tensorflow/tensorflow/core/framework/tensor.h'
  'C:/os/tensorflow/tensorflow/core/framework/allocator.h'
  'C:/os/tensorflow/tensorflow/core/framework/numeric_types.h'
  'C:/os/tensorflow/tensorflow/core/framework/type_traits.h'
  'C:/os/tensorflow/tensorflow/core/framework/variant.h'
  'C:/os/tensorflow/tensorflow/core/framework/type_index.h'
  'C:/os/tensorflow/tensorflow/core/framework/tensor_shape.h'
  'C:/os/tensorflow/tensorflow/core/framework/tensor_types.h'
  'C:/os/tensorflow/tensorflow/core/framework/types.h'
  'C:/os/tensorflow/tensorflow/core/framework/bfloat16.h'

### Source code / logs"
11984,Save seq2seq model error (tensowflow 1.0.0),"1. I had trained a seq2seq model with tensorflow 1.0.0 (cpu version)

  install: pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.0-cp35-cp35m-win_amd64.whl 

2. Save the model. 
save_path = saver.save(session, 'model/model.ckpt')

3. load the model. 
new_saver = tf.train.import_meta_graph('model/model.ckpt.meta')


when I load the model,there is a error as follows:

File ""D:\python3.5.2\lib\site-packages\tensorflow\python\training\saver.py"", line 1577, in import_meta_graph**kwargs)
File ""D:\python3.5.2\lib\site-packages\tensorflow\python\framework\meta_graph.py"", line 498, in import_scoped_meta_graphproducer_op_list=producer_op_list)
File ""D:\python3.5.2\lib\site-packages\tensorflow\python\framework\importer.py"", line 259, in import_graph_def raise ValueError('No op named %s in defined operations.' % node.op) ValueError: No op named attn_mul_fun_f32f32 in defined operations.
what's the reason for the error?

I found the function(attn_mul_fun) from the file D:\python3.5.2\Lib\site-packages\tensorflow\contrib\seq2seq\python\ops\attention_decoder_fn.py

I don't know what's the connection between attn_mul_fun_f32f32 and attn_mul_fun, how to fix the error when loading the model?"
11982,Feature Request: Kill session->run process,"For mobile & embedded devices `session->run` is typically initiated through a user interaction. If the user presses the back button or continues to another screen before `session->run` finishes, the process is still lingering in the background wasting resources. Since these devices are relatively low powered, it would be great if we could cancel/kill the process when it's not needed anymore.


"
11978, tf.nn.weighted_cross_entropy_with_logits  is bad," tf.nn.weighted_cross_entropy_with_logits
(1 - z) * x + l * (log(1 + exp(-abs(x))) + max(-x, 0)) 
where l = (1 + (q - 1) * z)

but tf.nn.sigmoid_cross_entropy_with_logits
max(x, 0) - x * z + log(1 + exp(-abs(x)))

if q=1 weighted_cross_entropy_with_logits is equvalent sigmoid_cross_entropy_with_logits


(1 - z) * x + l * (log(1 + exp(-abs(x))) + max(-x, 0)) {l=1}- (max(x, 0) - x * z + log(1 + exp(-abs(x)))) ==
x+max(-x, 0)-max(x, 0) <> 0


 tf.nn.weighted_cross_entropy_with_logits must be
   (- z * x) + l * (log(1 + exp(-abs(x))) + max(x, 0) "
11977,Multiple runs of Configure with MKL enabled leads to cyclic symlinks,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Centos 7
- **TensorFlow installed from (source or binary)**: Compilation from source
- **TensorFlow version (use command below)**: v1.2.1 and v1.3.0-r1
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: 0.5.2
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: ./configure [enter] [enter] y y [enter till complete], twice

### Describe the problem
Currently the way the configure script is written for the MKL decision branch it does not ensure no cyclic symbolic links are created for libdl.so.2

This becomes a problem if MKL has been downloaded already upon a re-run of the configure script and the first line of the locate output happens to be the symlink located in the third_party/mkl directory. If that occurs then the `ln -sf` command will create a cyclic symlink.

This can be fixed either by checking that ""$loc"" is not the same string as ""$PWD""/third_party/mkl/libdl.so.2 or by checking for the existence of the symlink before the `ln` command.


### Source code / logs
configure, line 276, 277.
`loc=$(locate -e libdl.so.2 | sed -n 1p)` <- can end up returning the destination file if it already exists

`ln -sf $loc third_party/mkl/libdl.so.2` <- if ""$loc"" == ""$PWD""/third_party/mkl/libdl.so.2, creates cyclic link."
11976,How to Retrain Final Layer for New Categories,Could there be another or newer classification algorithm for image classification like ResNet etc? There are a lot of new neural nets with less classification error than Inception. ImageNet results: [http://image-net.org/challenges/LSVRC/2017/results](INCR)
11975,CMake build with -Dtensorflow_BUILD_ALL_KERNELS=OFF does not work,"Problem:

When building Tensorflow via CMake, specifying option tensorflow_BUILD_ALL_KERNELS=OFF leads to failed linker step due to unresolved external functions, because many important files (that are not kernels, but are necessary) are not included in build's list of sources (like ops_util.cc, but also other files).

Reproduction:

Follow instruction in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md, specify -Dtensorflow_BUILD_ALL_KERNELS=OFF to cmake."
11974,tf.reshape does not accept Dimension objects for the shape parameter,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  1.2.1
- **Python version**: 3.6.1 (Anaconda 4.4.0 64-bit)
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 
- **GPU model and memory**: GTX 780
- **Exact command to reproduce**: tf.reshape()

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

tf.reshape does not accept a list with mixed Integer and Dimension() as elements for the shape parameter. It should accept shapes that have Dimension as elements since tensor shapes consist of dimensions. Specifically, tf.tensor.shape returns a list of Dimensions, therefore using a similar object to specify a shape in tf.reshape should not cause an error.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Example code:

```
X = tf.placeholder(""float"", [1,784])
X = tf.reshape(X, [1, X.shape[-1]])
```

results in

```
TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [1, Dimension(784)]. Consider casting elements to a supported type.
```

This is rectified by casting X.shape[-1] to int before passing to tf.reshape.


A similar unrelated issue is that X.shape returns (784,) while it should return (1,784). This requires reshaping to turn the placeholder back into a 2D tensor. I haven't determined if this is a bug but it occurs when the tensor is explicitly specified as a 2D tensor so it is probably worth changing.

"
11971,API request: MonitoredSession.run_without_hooks,"Currently it is not possible to run a `MonitoredSession` without also calling the `before_run` and `after_run` methods of session hooks. But sometimes it is necessary. For example, I want to modify certain variables in the graph, but sometimes the `SummarySaverHook` decides it is time to collect summary, right when I am loading new values into the dependent variables, and then `Tensorboad` shows inconsistent results.

A `run_without_hooks` method will be immensely helpful. "
11970,Error when using dataset.map() with num_threads != None,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes (code attached below)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: --
- **CUDA/cuDNN version**: 8/5.1
- **GPU model and memory**: GeForce GTX 1080, 8GB
- **Exact command to reproduce**: just run the script below

### Describe the problem
When using the `tf.contrib.data` API and applying a function to a dataset via `dataset.map(my_function, num_threads=2)`, the following error occurs:
>TypeError: Input 'output_buffer_size' of 'ParallelMapDataset' Op has type int32 that does not match expected type of int64.

Please note that `num_threads=2` is necessary to cause the error.
From what I can see, in `MapDataset::make_dataset_resource()` (file `dataset_ops.py`) the `if self._num_threads is None:` triggers the call to `gen_dataset_ops.parallel_map_dataset` which then raises the error. I suspect a cast from int32 to int64 got lost somewhere in that function.

### Source code / logs

```
import tensorflow as tf
from functools import partial

files = [ 'some_filename' ]

keys_to_features = {
    'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),
    'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'),
    'image/class/label': tf.FixedLenFeature([], tf.int64, default_value=-1),
    'image/height': tf.FixedLenFeature([], tf.int64),
    'image/width': tf.FixedLenFeature([], tf.int64),
}

items_to_handlers = {
    'image': tf.contrib.slim.tfexample_decoder.Image('image/encoded', 'image/format'),
    'label': tf.contrib.slim.tfexample_decoder.Tensor('image/class/label'),
}

decoder = tf.contrib.slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)
map_decode = partial(decoder.decode, items=['label', 'image'])

dataset = tf.contrib.data.TFRecordDataset(files)
dataset = dataset.map(map_decode, num_threads=2)
it = dataset.make_one_shot_iterator()
```

Full traceback:
```
Traceback (most recent call last):

  File ""<ipython-input-8-7ba67ea47c21>"", line 44, in <module>
    it = dataset.make_one_shot_iterator()

  File ""C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\data\python\ops\dataset_ops.py"", line 413, in make_one_shot_iterator
    _make_dataset.add_to_graph(ops.get_default_graph())

  File ""C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\function.py"", line 619, in add_to_graph
    self._create_definition_if_needed()

  File ""C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\data\python\framework\function.py"", line 167, in _create_definition_if_needed
    outputs = self._func(*inputs)

  File ""C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\data\python\ops\dataset_ops.py"", line 411, in _make_dataset
    return self.make_dataset_resource()

  File ""C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\data\python\ops\dataset_ops.py"", line 1466, in make_dataset_resource
    output_shapes=nest.flatten(self.output_shapes))

  File ""C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\gen_dataset_ops.py"", line 414, in parallel_map_dataset
    output_shapes=output_shapes, name=name)

  File ""C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 513, in apply_op
    (prefix, dtypes.as_dtype(input_arg.type).name))

TypeError: Input 'output_buffer_size' of 'ParallelMapDataset' Op has type int32 that does not match expected type of int64.
```"
11969,tensorflow_gpu-1.3.0rc1-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.,"I am trying to install TF 1.3.0rc1 in CentOS 7.3 with Nvidia GPUs for python3.6. When I installed with python2.7, it was fine.

`sudo pip3.6 install -U http://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.3.0rc1-cp27-none-linux_x86_64.whl`

I get this error with python3.6

My pip3.6 has a slight difference with sudo

Without sudo, when I run pip3.6 --version, I get

> pip 9.0.1 from /usr/local/lib/python3.6/site-packages (python 3.6)

With sudo I get

> pip 9.0.1 from /usr/lib/python3.6/site-packages (python 3.6)

Any workaround for this?"
11967,Training_ops_test assertion failures on Ubuntu 16.04,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:  'v1.2.1-0-gb4957ff', '1.2.1'
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: No GPU
- **GPU model and memory**: No GPU
- **Exact command to reproduce**: bazel test -c opt //tensorflow/python:training_ops_test

### The problem
The `//tensorflow/python:training_ops_test` is failing on s390x with AssertionError in `_testTypesForFtrl` method. I have added test log below.

A single value differs as below in the result:
```
lhs =  [-2.9473114]
rhs =  [-2.94730377]
```

@gunan, I saw your suggestion in [issue](https://github.com/tensorflow/tensorflow/issues/8945#issuecomment-291763267), that the test tolerance bounds are too tight.

So I tried increasing tolerance to `4e-06 ` as below and test passes.

```
- self.assertAllClose(linear_update, linear.eval())
+ self.assertAllClose(linear_update, linear.eval(), rtol=4e-06, atol=4e-06)
```  
Is this tolerance level acceptable?    

   
    
    

### Source code / logs   
```
AssertionError:
Not equal to tolerance rtol=1e-06, atol=1e-06
None
(mismatch 1.0%)
 x: array([  1.020000e+02,   1.038411e+02,   1.050863e+02,   1.055917e+02,
         1.053070e+02,   1.042043e+02,   1.022649e+02,   9.947507e+01,
         9.582399e+01,   9.130299e+01,   8.590485e+01,   7.962347e+01,...
 y: array([  1.020000e+02,   1.038411e+02,   1.050863e+02,   1.055917e+02,
         1.053070e+02,   1.042043e+02,   1.022649e+02,   9.947507e+01,
         9.582399e+01,   9.130299e+01,   8.590485e+01,   7.962347e+01,...

----------------------------------------------------------------------

Ran 9 tests in 4.704s

FAILED (failures=1)
not close where =  (array([19]),)
not close lhs =  [-2.9473114]
not close rhs =  [-2.94730377]
not close dif =  [  7.62939453e-06]
not close tol =  [  3.94730387e-06]
dtype = float32, shape = (100,)
```"
11965,Moving average and moving variance in Batchnorm aren't updated,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**:  3.5.3
- **Bazel version (if compiling from source)**: None
- **CUDA/cuDNN version**: 8/5.1
- **GPU model and memory**: GeForce 1080
- **Exact command to reproduce**:

### Describe the problem
I'm using the slim wrapper, which in turn returns an instance of BatchNormalization from layers/normalisation.py. All paramers are set to default, except for scale which is set to True (i.e. adding the gamma scaler). After training, when looking the at the learned parameters, I notice that all the moving means in the network are still 0 while all the moving variances are 1, i.e. they weren't updated. 

Both variables don't show up in tf.trainable_variables() which might explain the lack of updates. However, since these are not actually learned but rather calculated, I'm not sure whether they would be updated by the optimiser.
"
11964,Fail to restore the model after upgraded to version 1.2.1,"I trained my model in tensorflow version 1.1.0, and test or restore it successfully. But after I upgraded tensorflow to version 1.2.1,  I fail to restore the same model  with the same code!

### Error Log 
```
restore from ckpt: ./Models/dl_fast/TextRecognize/model-105000
2017-08-02 15:18:09.050018: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key bidirectional_rnn/bw/lstm_cell/bias not found in checkpoint
2017-08-02 15:18:09.050057: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key bidirectional_rnn/fw/lstm_cell/kernel not found in checkpoint
2017-08-02 15:18:09.050751: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key bidirectional_rnn/fw/lstm_cell/bias not found in checkpoint
2017-08-02 15:18:09.050753: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key bidirectional_rnn/bw/lstm_cell/kernel not found in checkpoint
Traceback (most recent call last):
  File ""dl_fast_recognizer.py"", line 151, in <module>
    text_recognizer_model=text_recognizer_model
  File ""dl_fast_recognizer.py"", line 37, in __init__
    self.text_recognizer = Predictor('dl_fast.yml', text_recognizer_model)
  File ""../Recognition/predictor.py"", line 45, in __init__
    saver.restore(self.sess, ckpt)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1548, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 789, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 997, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1132, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1152, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Key bidirectional_rnn/bw/lstm_cell/bias not found in checkpoint
	 [[Node: save/RestoreV2_6 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_6/tensor_names, save/RestoreV2_6/shape_and_slices)]]

Caused by op u'save/RestoreV2_6', defined at:
  File ""dl_fast_recognizer.py"", line 151, in <module>
    text_recognizer_model=text_recognizer_model
  File ""dl_fast_recognizer.py"", line 37, in __init__
    self.text_recognizer = Predictor('dl_fast.yml', text_recognizer_model)
  File ""./Recognition/predictor.py"", line 43, in __init__
    saver = tf.train.Saver(tf.global_variables(),max_to_keep=100)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1139, in __init__
    self.build()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1170, in build
    restore_sequentially=self._restore_sequentially)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 691, in build
    restore_sequentially, reshape)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 407, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 247, in restore_op
    [spec.tensor.dtype])[0])
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 640, in restore_v2
    dtypes=dtypes, name=name)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

NotFoundError (see above for traceback): Key bidirectional_rnn/bw/lstm_cell/bias not found in checkpoint
	 [[Node: save/RestoreV2_6 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_6/tensor_names, save/RestoreV2_6/shape_and_slices)]]
```

###  Restore Code
```
        with self.sess.as_default():
            with self.model.graph.as_default(), tf.device('/cpu:0'):
                self.sess.run(tf.global_variables_initializer())

                ckpt = tf.train.latest_checkpoint(model_path)
                print('restore from ckpt: {}'.format(ckpt))
                saver = tf.train.Saver(tf.global_variables(),max_to_keep=100)
                if ckpt:
                    saver.restore(self.sess, ckpt)
                    print('restored from ckpt: {}'.format(ckpt))
                else:
                    print('cannot restore')
```

Is it a bug of tensorflow or my code? How to fix it?"
11963,Infiniband with tensorflow,"-----------------------
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
commit 3bee923c9
- **Bazel version (if compiling from source)**:
0.4.5
- **CUDA/cuDNN version**:
cuda 8.0/cudnn 5.1.5
- **GPU model and memory**:
Titan Xp
- **Exact command to reproduce**:

### Describe the problem
I tried to use Infiniband with tensorflow using ` server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index, protocol='grpc+verbs')`.
But the session is not hanged.
I used S1, and S2, the cluster is configured with the Infiniband ip address. Is there something that I missed?


ibdev2netdev 
S1
mlx5_0 port 1 ==> ib0 (Down)
mlx5_1 port 1 ==> ib1 (Up)
S2
mlx5_0 port 1 ==> ib0 (Up)
mlx5_1 port 1 ==> ib1 (Up)

### Source code / logs"
11962,Some gpu tests are failing on Windows in Bazel build,"They are marked as `no_windows_gpu` in https://github.com/tensorflow/tensorflow/pull/11901, but we need to investigate why they are now failing.
```
//py_test_dir/tensorflow/python/kernel_tests:metrics_test               TIMEOUT in 1 out of 3 in 6541.4s
//py_test_dir/tensorflow/python/kernel_tests:norm_op_test               TIMEOUT in 20 out of 20 in 6379.0s
//py_test_dir/tensorflow/python:math_grad_test                          TIMEOUT in 566.2s
//py_test_dir/tensorflow/python:math_ops_test                           TIMEOUT in 512.5s
//py_test_dir/tensorflow/python:special_math_ops_test                   TIMEOUT in 88.8s
//py_test_dir/tensorflow/python/kernel_tests:cholesky_op_test            FAILED in 5 out of 5 in 409.8s
//py_test_dir/tensorflow/python/kernel_tests:linalg_ops_test             FAILED in 559.0s
//py_test_dir/tensorflow/python/kernel_tests:reduction_ops_test          FAILED in 4 out of 4 in 455.1s
//py_test_dir/tensorflow/python:session_clusterspec_prop_test            FAILED in 226.5s
//py_test_dir/tensorflow/python:session_list_devices_test                FAILED in 182.7s
```"
11961,timeout breaks FIFOQueue,"### System information
-  Linux Ubuntu 14.04
-  TensorFlow installed from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.1-cp27-none-linux_x86_64.whl
-  Tensorflow-gpu 1.2.1
-  Python version 2.7
-  CUDA 8.0 /cuDNN 5.1.10
-  GPU - Nvidia GTX 1080

### Describe the problem
I use thread to add my training data to a FIFO Queue. There is always an thread error occurring in the process of my training.

### Source code / logs
Here is my source code relevant to the error:
```js
with tf.Session() as sess:
    self.q = tf.FIFOQueue(self.queue_capacity, [tf.float32, tf.float32, train_label_dtype], shapes=[(self.train_batch_size, self.im_height, self.im_width, self.num_tensor_channels), (self.train_batch_size, self.pose_dim), (self.train_batch_size,)])
    self.enqueue_op = self.q.enqueue([self.train_data_batch, self.train_poses_batch, self.train_labels_batch])
    self.train_labels_node = tf.placeholder(train_label_dtype, (self.train_batch_size,))
    self.input_im_node, self.input_pose_node, self.train_labels_node = self.q.dequeue()

    self.queue_thread = threading.Thread(target=self._load_and_enqueue)
    self.queue_thread.start()

    for step in training_range:
    	self._check_dead_queue()
    	""""""run optimization""""""
    	_, l, lr, predictions, batch_labels, output, train_images, conv1_1W, conv1_1b, pose_node = self.sess.run(
    		[optimizer, loss, learning_rate, train_predictions, self.train_labels_node, self.train_net_output, self.input_im_node, self.weights.conv1_1W, self.weights.conv1_1b, self.input_pose_node], options=GeneralConstants.timeout_option)

def _load_and_enqueue(self):
        """""" Loads and Enqueues a batch of images for training """"""

        train_data = np.zeros(
            [self.train_batch_size, self.im_height, self.im_width, self.num_tensor_channels]).astype(np.float32)
        train_poses = np.zeros([self.train_batch_size, self.pose_dim]).astype(np.float32)
        label_data = np.zeros(self.train_batch_size).astype(self.numpy_dtype)

        while not self.term_event.is_set():
            time.sleep(self.cfg['queue_sleep'])

            # Omit the code for geeting data

            # send data to queue
            if not self.term_event.is_set():
                try:
                    self.sess.run(self.enqueue_op, feed_dict={self.train_data_batch: train_data,
                                                              self.train_poses_batch: train_poses,
                                                              self.train_labels_batch: label_data})
                except:
                    pass
        del train_data
        del train_poses
        del label_data
        self.dead_event.set()
        logging.info('Queue Thread Exiting')
        self.queue_thread_exited = True

```
When I train the model for some time, there will be an error:
```js
INFO:root:Step 41817 (epoch 3.35), 0.1 s
INFO:root:Minibatch loss: 1.256, learning rate: 0.000857
INFO:root:Minibatch error: 26.562%
INFO:root:Step 41818 (epoch 3.35), 0.1 s
INFO:root:Minibatch loss: 1.790, learning rate: 0.000857
INFO:root:Minibatch error: 59.375%
INFO:root:Step 41819 (epoch 3.35), 0.1 s
INFO:root:Minibatch loss: 0.868, learning rate: 0.000857
INFO:root:Minibatch error: 3.125%
2017-08-02 07:26:54.769345: W tensorflow/core/kernels/queue_base.cc:302] _0_data_queue/fifo_queue: Skipping cancelled dequeue attempt with queue not closed
2017-08-02 07:26:54.769537: W tensorflow/core/framework/op_kernel.cc:1158] Cancelled: Dequeue operation was cancelled
	 [[Node: data_queue/fifo_queue_Dequeue = QueueDequeueV2[component_types=[DT_FLOAT, DT_FLOAT, DT_INT64], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](data_queue/fifo_queue)]]
2017-08-02 07:26:54.769608: W tensorflow/core/framework/op_kernel.cc:1158] Cancelled: Dequeue operation was cancelled
	 [[Node: data_queue/fifo_queue_Dequeue = QueueDequeueV2[component_types=[DT_FLOAT, DT_FLOAT, DT_INT64], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](data_queue/fifo_queue)]]
2017-08-02 07:26:54.769715: W tensorflow/core/framework/op_kernel.cc:1158] Cancelled: Dequeue operation was cancelled
	 [[Node: data_queue/fifo_queue_Dequeue = QueueDequeueV2[component_types=[DT_FLOAT, DT_FLOAT, DT_INT64], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](data_queue/fifo_queue)]]
2017-08-02 07:26:54.769747: W tensorflow/core/framework/op_kernel.cc:1158] Cancelled: Dequeue operation was cancelled
	 [[Node: data_queue/fifo_queue_Dequeue = QueueDequeueV2[component_types=[DT_FLOAT, DT_FLOAT, DT_INT64], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](data_queue/fifo_queue)]]
Traceback (most recent call last):
  File ""training.py"", line 269, in <module>
    sgdOptimizer.optimize()
  File ""/home/liuquande/Desktop/SenceTime/gqcnn/test/gqcnn/sgd_optimizer.py"", line 254, in optimize
    [optimizer, loss, learning_rate, train_predictions, self.train_labels_node, self.train_net_output, self.input_im_node, self.weights.conv1_1W, self.weights.conv1_1b, self.input_pose_node], options=GeneralConstants.timeout_option)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 789, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 997, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1132, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1152, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.DeadlineExceededError: Timed out waiting for notification
INFO:root:Queue Thread Exiting
INFO:rospy.core:signal_shutdown [atexit]
```"
11958,[Bug?]Session Hang during training with 'mnist_replica.py' and learning rate as a placeholder,"### System information
 OS: Mac OS 
 TF: 1.2.0, pip install
 Python: 2.7.9
also tested on RHEL with TF 1.1 and Python 2.7.13, the problem still exists. I don't think it is system specific.


### Describe the problem
I am training the 'mnist-replica.py' provided on github. link is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py) 
and set up two workers and one parameter server on my laptop.
I made only one modification to the code:
 use learning_rate as a placeholder rather than a fixed value and feed 0.01 to it each step.
When I train it asynchronously, it is ok to go, but the session hangs in .run() after one or two steps when I train it in synchronous.

Can anyone figure out the reason of this?

### Source code / logs
the source code is 
[here](https://drive.google.com/open?id=0Bw4fA7bI0IScTVlsT3FidS0tMXM). just provide it for ease. It is almost the same as the original one."
11957,cudnn failed with GTX960,"I am trying to use conv2d (for cnn) with GPU, and there are one GTX 960 and Quadro 600 on my computer. I set up the `CUDA_VISIBLE_DEVICES=0` to use GTX 960, but I get
```
$ ./mnist-train /home/qinka/ out.p10 1000 +RTS -N
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX 960
major: 5 minor: 2 memoryClockRate (GHz) 1.1775
pciBusID 0000:28:00.0
Total memory: 1.95GiB
Free memory: 1.91GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960, pci bus id: 0000:28:00.0)
W tensorflow/core/framework/op_kernel.cc:993] Unimplemented: Conv2DBackprop for GPU is not currently supported without cudnn
	 [[Node: Conv2DBackpropFilter_152 = Conv2DBackpropFilter[T=DT_FLOAT, data_format=""NHWC"", padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](_recv_Placeholder_0_0/_141, Shape_151/_185, Reshape_146)]]
W tensorflow/core/framework/op_kernel.cc:993] Unimplemented: Conv2DBackprop for GPU is not currently supported without cudnn
	 [[Node: Conv2DBackpropFilter_152 = Conv2DBackpropFilter[T=DT_FLOAT, data_format=""NHWC"", padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](_recv_Placeholder_0_0/_141, Shape_151/_185, Reshape_146)]]
W tensorflow/core/framework/op_kernel.cc:993] Unimplemented: Conv2DBackprop for GPU is not currently supported without cudnn
	 [[Node: Conv2DBackpropFilter_152 = Conv2DBackpropFilter[T=DT_FLOAT, data_format=""NHWC"", padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](_recv_Placeholder_0_0/_141, Shape_151/_185, Reshape_146)]]
W tensorflow/core/framework/op_kernel.cc:993] Unimplemented: Conv2DBackprop for GPU is not currently supported without cudnn
	 [[Node: Conv2DBackpropFilter_152 = Conv2DBackpropFilter[T=DT_FLOAT, data_format=""NHWC"", padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](_recv_Placeholder_0_0/_141, Shape_151/_185, Reshape_146)]]
mnist-train: TensorFlowException TF_UNIMPLEMENTED ""Conv2DBackprop for GPU is not currently supported without cudnn\n\t [[Node: Conv2DBackpropFilter_152 = Conv2DBackpropFilter[T=DT_FLOAT, data_format=\""NHWC\"", padding=\""VALID\"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=false, _device=\""/job:localhost/replica:0/task:0/gpu:0\""](_recv_Placeholder_0_0/_141, Shape_151/_185, Reshape_146)]]""
```


### System information
- I Just using Haskell's binding, but I don't think that is key to this problem, and the libtensorflow.so is the gpu one.
- system
```
uname -a
Linux ETVP-z400 4.10.0-26-generic #30-Ubuntu SMP Tue Jun 27 09:30:12 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
lsb_release -a
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 17.04
Release:	17.04
Codename:	zesty
```
- Using libtensorflow.so download from official site
- tensorflow 1.0
- Using Haskell's binding
- CUDA-8.0 & cuDNN-5.1
```
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
```
- gpu:
```
$ nvidia-smi
Wed Aug  2 09:06:00 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro 600          Off  | 0000:0F:00.0     Off |                  N/A |
| 34%   52C    P0    N/A /  N/A |      0MiB /   963MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 960     Off  | 0000:28:00.0     Off |                  N/A |
|  0%   35C    P0    24W / 120W |      0MiB /  1996MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
```
"
11956,Feature Request: Correlation Layer ,"There is a surging interest in Geometric Computer Vision and a large number of recent papers leveraging an operation(with small variations) dubbed ""Correlation"" layer. There is a CUDA kernel for this operation in the FlowNet paper's author's fork of Caffe. Is there plan on including it in Tensorflow? I couldn't locate a relevant issue anywhere and this is why I am raising this issue. "
11955,Incorrect tfBinaryURL for installing with Anaconda on Linux,"There is incorrect tfBinaryURL at tensorflow.org.

In case of Installing with Anaconda in Linux, the example shows the following command for Python 2.7.
(tensorflow)$ pip install --ignore-installed --upgrade \
 https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.1-cp34-cp34m-linux_x86_64.whl
But this is for Python 3.4 so correct command is below.
(tensorflow)$ pip install --ignore-installed --upgrade \
 https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.1-cp27-none-linux_x86_64.whl
"
11954,Dilated convolution does not preserve tensor shape,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 14.04.5 LTS
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
('unknown', '1.3.0-rc0')
- **Python version**: 
2.7.12

- **Exact command to reproduce**:
```
input_tensor = tf.placeholder(tf.float32, (10, None, 256, 3))

dilated = tf.nn.convolution(input_tensor,
                            tf.zeros((3, 1, 3, 16)),
                            dilation_rate=[2, 1],
                            padding='SAME')

print(dilated.get_shape()) # Displays: [10, ?, ?, 16], expected [10, ?, 256, 16]
```

### Describe the problem
The documentation for tf.nn.convolution has the spatial dimensions of the output given as:

```
If padding == ""SAME"": output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides[i])
```

Which suggests that input_spatial_shape[0] should not affect output_spatial_shape[1], as is the case in the code block above.

This problem arises when using dilated convolutions as part of a larger model containing recurrent layers, in which one spatial dimension is left undefined to allow for unrolling the recurrent layers out during training along the undefined dimension.

This might be related to a [previously fixed problem with undefined batch sizes](https://github.com/tensorflow/tensorflow/issues/4742).
"
11953,Is `tf.matrix_inverse()` supported for TF v.1.2?,"I notice that the Tensorlfow computation op `tf.matrix_inverse()` was not supported running on GPU in the previous versions, and I wonder if it's supported for Tensorflow v.1.2 now. If it's still not, can we make a feature request for it? Thanks!
"
11950,Issues with tensordot and axes is list of ints,"```
== cat /etc/issue ===============================================
Linux node014-jupyter-20170708-132328 4.4.0-1022-aws #31-Ubuntu SMP Tue Jun 27 11:27:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.2 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux node014-jupyter-20170708-132328 4.4.0-1022-aws #31-Ubuntu SMP Tue Jun 27 11:27:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.0)
protobuf (3.2.0)
tensorflow-gpu (1.2.1)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.2.1
tf.GIT_VERSION = v1.2.0-5-g435cdfc
tf.COMPILER_VERSION = v1.2.0-5-g435cdfc
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Tue Aug  1 17:13:34 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |
| N/A   59C    P0    67W / 149W |      0MiB / 11439MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
```
### Describe the problem
`np.tensordot` works for a given set of settings but `tf.tensordot` does not.

### Source code / logs
This works:
```python
np.tensordot(np.ones((3,3)), np.array([2, 3, 1])[None, None], axes=[1, 2])
```
but this does not:
```python
with tf.Session() as sess:
    sess.run(
         tf.tensordot(tf.ones((3,3), dtype=tf.float32), tf.constant([2, 3, 1], dtype=tf.float32)[None, None], axes=[1, 2])
    )
```
I get:
```py3tb
TypeError                                 Traceback (most recent call last)
<ipython-input-1-e93670defe29> in <module>()
      2 with tf.Session() as sess:
      3     sess.run(
----> 4         tf.tensordot(tf.ones((3,3), dtype=tf.float32), tf.constant([2, 3, 1], dtype=tf.float32)[None, None], axes=[1, 2])
      5     )

~/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py in tensordot(a, b, axes, name)
   2415     a = ops.convert_to_tensor(a, name=""a"")
   2416     b = ops.convert_to_tensor(b, name=""b"")
-> 2417     a_axes, b_axes = _tensordot_axes(a, axes)
   2418     a_reshape, a_free_dims, a_free_dims_static = _tensordot_reshape(a, a_axes)
   2419     b_reshape, b_free_dims, b_free_dims_static = _tensordot_reshape(b, b_axes,

~/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py in _tensordot_axes(a, axes)
   2403       a_axes = axes[0]
   2404       b_axes = axes[1]
-> 2405       if len(a_axes) != len(b_axes):
   2406         raise ValueError(
   2407             ""Different number of contraction axes 'a' and 'b', %s != %s."",

TypeError: object of type 'int' has no len()
```"
11948,Memory leak in Java API when using GPU,"### System information
- **Custom code**: https://github.com/riklopfer/TensorflowJavaGpuMemoryTest
- **OS**: CentOS 7
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: n/a
- **Python version**: n/a
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: GeForce GTX 1080
- **Exact command to reproduce**: see https://github.com/riklopfer/TensorflowJavaGpuMemoryTest

### Describe the problem
Main memory on the machine is continuously consumed when running on the GPU. Memory consumption hovers around 600M when running on the CPU.

### Source code / logs
see: https://github.com/riklopfer/TensorflowJavaGpuMemoryTest"
11947,No module named 'tensorflow.contrib.keras.datasets',"In the documentation, in order to import Keras dataset, you have to

```python
import tensorflow.contrib.keras.datasets
``` 
But instead, it only works when

```python
import tensorflow.contrib.keras.python.keras.datasets
``` 
I think this problem would occur not just for datasets but anything else in Keras.
"
11944,Feature Request: Video Decoder,"A video decoder would make it much easier to work with video. For instance, something like tf.image.decode_jpeg, but for mp4 videos (or some other format perhaps)."
11943,Consoles freezes while reading an image. ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1
- **Python version**:  3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 13:51:32)  GCC 4.4.7 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 5.1
- **GPU model and memory**: GTX-1050 4GB


### Describe the problem

Consoles freezes while reading an image. 

### Source code / logs

```python
import tensorflow as tf
image_filename = ""/home/kaiyin/PycharmProjects/tensorflow-for-machine-intelligence/images/chapter-05-object-recognition-and-classification/working-with-images/test-input-image.jpg""
filename_queue = tf.train.string_input_producer(tf.train.match_filenames_once(image_filename))
image_reader = tf.WholeFileReader()
_, image_file = image_reader.read(filename_queue)
image = tf.image.decode_jpeg(image_file)
sess = tf.InteractiveSession()
sess.run(image)
```

Also tried the non-interactive session:

```python
import tensorflow as tf
with tf.Session() as sess:
    image_filename = ""/home/kaiyin/PycharmProjects/tensorflow-for-machine-intelligence/images/chapter-05-object-recognition-and-classification/working-with-images/test-input-image.jpg""
    filename_queue = tf.train.string_input_producer(tf.train.match_filenames_once(image_filename))
    image_reader = tf.WholeFileReader()
    _, image_file = image_reader.read(filename_queue)
    image = tf.image.decode_jpeg(image_file)
    sess.run(image)
```

Error:

```
2017-08-01 17:39:36.997011: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-01 17:39:36.997023: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-01 17:39:36.997026: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-01 17:39:36.997028: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-01 17:39:36.997031: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-08-01 17:39:37.078504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-08-01 17:39:37.078697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: GeForce GTX 1050
major: 6 minor: 1 memoryClockRate (GHz) 1.493
pciBusID 0000:01:00.0
Total memory: 3.95GiB
Free memory: 1.84GiB
2017-08-01 17:39:37.078705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2017-08-01 17:39:37.078708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2017-08-01 17:39:37.078715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0)
```

After showing the message above, the console is not responsive any more. 
"
11942,Got `ValueError: Both labels and logits must be provided` while both labels and logits have been provided. ,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1
- **Python version**:  3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 13:51:32)  GCC 4.4.7 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 5.1
- **GPU model and memory**: GTX-1050 4GB


### Describe the problem

Got `ValueError: Both labels and logits must be provided` while both labels and logits have been provided. 

### Source code / logs

```python
# Softmax example in TF using the classical Iris dataset
# Download iris.data from https://archive.ics.uci.edu/ml/datasets/Iris

import tensorflow as tf
print(tf.__version__)
import os

def combine_inputs(X):
    res =  tf.matmul(X, W) + b
    res = tf.identity(res, name=""linear_out"")


def inference(X):
    return tf.nn.softmax(combine_inputs(X), ""softmax_out"")


def loss(X, Y):
    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=combine_inputs(X), name=""softmax_entropy""), name=""loss"")


def read_csv(batch_size, file_name, record_defaults):
    filename_queue = tf.train.string_input_producer([os.path.dirname(__file__) + ""/"" + file_name])
    reader = tf.TextLineReader(skip_header_lines=1)
    key, value = reader.read(filename_queue)
    # decode_csv will convert a Tensor from type string (the text line) in
    # a tuple of tensor columns with the specified defaults, which also
    # sets the data type for each column
    decoded = tf.decode_csv(value, record_defaults=record_defaults)
    # batch actually reads the file and loads ""batch_size"" rows in a single tensor
    return tf.train.shuffle_batch(decoded,
                                  batch_size=batch_size,
                                  capacity=batch_size * 50,
                                  min_after_dequeue=batch_size)


def inputs():
    sepal_length, sepal_width, petal_length, petal_width, label =\
        read_csv(100, ""iris.csv"", [[0.0], [0.0], [0.0], [0.0], [""""]])
    # convert class names to a 0 based class index.
    label_number = tf.to_int32(tf.argmax(tf.to_int32(tf.stack([
        tf.equal(label, [""Iris-setosa""]),
        tf.equal(label, [""Iris-versicolor""]),
        tf.equal(label, [""Iris-virginica""])
    ])), 0))
    # Pack all the features that we care about in a single matrix;
    # We then transpose to have a matrix with one example per row and one feature per column.
    features = tf.transpose(tf.stack([sepal_length, sepal_width, petal_length, petal_width]))
    return features, label_number


def train(total_loss):
    learning_rate = 0.01
    return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)


def evaluate(sess, X, Y):
    predicted = tf.cast(tf.arg_max(inference(X), 1), tf.int32)
    print(sess.run(tf.reduce_mean(tf.cast(tf.equal(predicted, Y), tf.float32))))


# Launch the graph in a session, setup boilerplate
with tf.Graph().as_default():
    with tf.Session() as sess:
        # this time weights form a matrix, not a column vector, one ""weight vector"" per class.
        W = tf.Variable(tf.zeros([4, 3]), name=""weights"")
        # so do the biases, one per class.
        b = tf.Variable(tf.zeros([3], name=""bias""))
        tf.global_variables_initializer().run()
        X, Y = inputs()
        total_loss = loss(X, Y)
        train_op = train(total_loss)
        summary_out = os.path.join(os.path.dirname(__file__), ""tf_summary"")
        import shutil
        shutil.rmtree(summary_out)
        writer = tf.summary.FileWriter(summary_out, graph=sess.graph)
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess=sess, coord=coord)
        # actual training loop
        training_steps = 1000
        for step in range(training_steps):
            sess.run([train_op])
            # for debugging and learning purposes, see how the loss gets decremented thru training steps
            if step % 10 == 0:
                print(""loss: "", sess.run([total_loss]))
        evaluate(sess, X, Y)
        coord.request_stop()
        coord.join(threads)
        writer.close()
```

Error:

```
/home/kaiyin/miniconda3/envs/tf/bin/python3.6 /home/kaiyin/PycharmProjects/tensorflow-mnist/learntf_008_softmax.py
2017-08-01 15:12:57.469597: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-01 15:12:57.469610: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-01 15:12:57.469613: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-01 15:12:57.469616: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-01 15:12:57.469618: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-08-01 15:12:57.554651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-08-01 15:12:57.554839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: GeForce GTX 1050
major: 6 minor: 1 memoryClockRate (GHz) 1.493
pciBusID 0000:01:00.0
Total memory: 3.95GiB
Free memory: 1.97GiB
2017-08-01 15:12:57.554848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2017-08-01 15:12:57.554852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2017-08-01 15:12:57.554859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0)
Traceback (most recent call last):
  File ""/home/kaiyin/PycharmProjects/tensorflow-mnist/learntf_008_softmax.py"", line 69, in <module>
    total_loss = loss(X, Y)
  File ""/home/kaiyin/PycharmProjects/tensorflow-mnist/learntf_008_softmax.py"", line 17, in loss
    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(None, Y, combine_inputs(X)), name=""loss"")
  File ""/home/kaiyin/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1555, in softmax_cross_entropy_with_logits
    labels, logits)
  File ""/home/kaiyin/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1512, in _ensure_xent_args
    raise ValueError(""Both labels and logits must be provided."")
ValueError: Both labels and logits must be provided.
```
"
11941,"Where can I find ""gen_checkpoint_ops""","_**When I was debuging inception v4, there has a message:**_
""22 from tensorflow.contrib.framework.python.ops import gen_checkpoint_ops
     23 from tensorflow.contrib.util import loader
     24 from tensorflow.python.framework import dtypes

ImportError: cannot import name gen_checkpoint_ops
"".
_**But I can't find a module or package named ""gen_checkpoint_ops"".**_
_**I don't know how to resolve this problem.**_ 
#  Help!
"
11938,I am unable to extract the hidden layer values from Seq2Seq model in Tensorflow for NMt,"Has anyone tried to extract the hidden layer values since the code explicitly doesn't specify the hidden layer  in translate.py or in seq2seq_model.py. ?

tensorflow version: 1.0
OS : CentOS
Python: 2.7

Is there any other program i need to look into for printing the hidden layer values?

"
11937,TPU support,"I wanna add support for my Tensor Processing Unit chip in TensorFlow. 

My TPU is implemented as an accelerator for ARM v7 32bit processor and implements multiple arithmetic kernels, similar to GPU. It implements a simple memory mapped interface, SGDMA and vector instructions over tensors. I added vector extension to GCC 7.1.1 and can run bare metal C++ nets on embedded Ubuntu 16.04

I also checked the TF port  for Raspberry Pi 3, but it looks outdated and barely supported.

I'm not currently aware about the scope of work, but believe that should not be that complex, given open examples from GPU vendors and already existing port for Google TPU

Anyone interested in joining this project is highly welcome! Advise, links and code examples are much appreciated

Thank you"
11936,"does libtensorflow_inference.so contains both  ""code for training .pb file""  and ""code for analyzing .pb file""?","### Describe the problem
 I'd like to import ""libtensorflow_inference.so"" to an android app to and make it possible to make some machine learning in the app. But the size of ""libtensorflow_inference.so""  file is too large which is 9.8M .

Because  I just want to import generated models(.pb file) to predict and don't need to train models in android app. Could you please tell me which code is used to import and analyze the .pb file and witch code is used to train a .pb file in tensorflow project?  

does libtensorflow_inference.so contains both  ""code for training .pb file""  and ""code for analyzing .pb file""? Is possible to remove the ""code for training .pb file"" to minimize its size  if it contains that?  


"
11933,"built 1.3.0 from source, got version 1.2.0 ","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:ubuntu 16.04
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:v1.2.0-2651-g82456f9 1.2.1-rc1
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: 0.5.2
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: titanx, 12GB
- **Exact command to reproduce**: no

### Describe the problem
`git log` shows latest commit is `82456f9fee7c4b5e9beb100e59ba8dc5eb688b28`, my python install package file is named `tensorflow-1.3.0rc1-cp35-cp35m-linux_x86_64.whl`, but `python3 -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` output `v1.2.0-2651-g82456f9 1.2.1-rc1`. I use python help to see some api doc, and find they are same as [1.3.0-rc1 ](https://github.com/tensorflow/tensorflow/releases/tag/v1.3.0-rc1) says.
"
11931,"Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/tensorflow/Graph : Unsupported major.minor version 52.0","When I use tf java API, there is a problem Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/tensorflow/Graph. It seems that the jdk version 1.7 can not user the lib. So, I want to know whether can I use java api with jdk1.7 and how?
------------------------
"
11930,No OpKernel was registered to support Op 'Dequantize' with these attrs,"Im getting the following error while running quantized graph

```
   raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'Dequantize' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

	 [[Node: final_training_ops/weights/final_weights/read/_1__cf__1 = Dequantize[T=DT_QUINT8, mode=""MIN_FIRST""](final_training_ops/weights/final_weights/read/_1__cf__1_quantized_const, final_training_ops/weights/final_weights/read/_1__cf__1_quantized_min, final_training_ops/weights/final_weights/read/_1__cf__1_quantized_max)]]

```"
11929,How to make full use of multicore(only cpu mode) on android phone?,"
### System information
 Platform : android 6.0, arm64-v8a
 TensorFlow installed from source(cpu mode only):
 TensorFlow version (1.2.0)
  Bazel version (0.5.2)

### Describe the problem
When I trained a model by using google object-detection api, I put it to my android env, and use c++ api to run the pb file model on my phone, it has a slow running speed, and only 2 cpus are fully utilized when there are 8 cpus on my device. What should I do to make full use of the remaining cpus on my mobile phone? 

I use c++ api to run the model, here is my config to session of tensorflow, but it is useless.Could some one help me?

### Source code / logs
tensorflow::SessionOptions options;
tensorflow::ConfigProto &config = options.config;
config.mutable_device_count()->insert(google::protobuf::MapPair<std::string, google::protobuf::int32>(""cpu"",8));
config.set_intra_op_parallelism_threads(8);

"
11928,tf-r1.3: show Warning Info when saving checkpoint,"I met some warn info show above when saving checkpoint: ([all code here](https://github.com/bluekingdom/my-tf-learning-code/blob/master/face-generate-example/main.py))

```python
saver.save(sess, ""models/model"", global_step=train_batch_idx)
```

log like this:

> WARNING:tensorflow:Error encountered when serializing model_variables. Type is unsupported, or the types of the items don't match field type in CollectionDef. 'Tensor' object has no attribute 'to_proto'


this log occur in r1.3, but do not occur in r1.2 .

tf do save a checkpoint. But I am not sure the checkpoint has been saved correctly when this log occur. "
11926,How to understand multithreading in tf queue?,"Tensorflow provides us two ways to implement reading data. 
The first way, use many reader, such as `tf.TextLineReader`, one reader per thread.
The second way, use just one reader and multithreading of enqueue ops, for example we can use `tf.train.shuffle_batch` and set `num_threads` greater than 1.

I can't understand the second way, we just have one reader to load data(perhaps a thread), why we need so many threads to enqueue? And for the first way, we should use `tf.train.shuffle_batch_join`, there is no `num_threads` parameter we can set, so I think the first way is understandable. Can anyone give me some explanation for why we need one reader but many threads to enqueue?"
11923,Race condition in add_arg_scope causes silent incorrect behavior,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
Linux Mint 17.3 Rosa
- **TensorFlow installed from (source or binary)**: 
binary
- **TensorFlow version (use command below)**: 
v1.2.0-5-g435cdfc 1.2.1
- **Python version**: 
3.6.1
- **Bazel version (if compiling from source)**: 
N/A
- **CUDA/cuDNN version**: 
CUDA Version 8.0.44
cudnn 5.1.5
- **GPU model and memory**: 
GTX 970 4GB
- **Exact command to reproduce**:
python add_arg_scope.py

### Describe the problem

There is a race condition in `tensorflow.contrib.framework.python.ops.add_arg_scope` where it doesn't reliably extract the arg list. Sometimes the list is incorrect and sometimes when a function is redefined or reloaded, the old arg list is returned. 

In practice, even a function that isn't reloaded can get the wrong arg list. The behavior is strange and seems to depend on the contents of the function and not just its args.

This may be causing errors since `arg_scope` silently ignores any arguments not in the argspec of the ops it's given.

### Source code / logs
```python
import tensorflow as tf
from tensorflow.contrib.framework.python.ops import add_arg_scope, arg_scope, arg_scoped_arguments

# initial definition
@add_arg_scope
def foo(x='x', y='y'):
    if x:
        pass
    if y:
        pass

for i in range(50):
    # redefine the function with different args
    @add_arg_scope
    def foo(a=1, b=2, c=3, d=4, e=5, f=6, g=7, h=8):
        pass
    
    print(arg_scoped_arguments(foo))
```

sample output (it isn't always regular):

('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
('x', 'y')
('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
('x', 'y')
('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
('x', 'y')
('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
('x', 'y')
('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
('x', 'y')
('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
...
"
11916,Docker installation: version compatibility issues,"I am trying to setup tensorflow in Windows 10 machine inside Docker.

The command I ran is:
`docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow`

Error response:
`C:\Program Files\Docker Toolbox\docker.exe: Error response from daemon: client is newer than server (client API version: 1.24, server API version: 1.22).`

Surprisingly, I did not get a lot many threads on this issue."
11915,Out of memory due to cuda_host_bfc allocator,"Hi,

I am getting OOM errors when I try to move data from GPUs to CPU. CPU has 0.5TB memory and when I allocate a large chunk of CPU memory directly there is no problem. However, for some reason cuda_host_bfc allocator sets the limit to 68719476736 which is incidentally 0 in int32. It looks like there is a bug here. Any thoughts?

Thanks

p.s. I disable the auto_gc collector on purpose (with get_raw_handle), so I can quickly clear the GPU memory.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 3.16.43-2+deb8u2 (2017-06-26) x86_64 GNU/Linux
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.2.0-2010-g8605f7a  1.2.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.5.2
- **CUDA/cuDNN version**: 8.0.61
- **GPU model and memory**: 4 x Tesla P100-SXM2-16GB 
- **Exact command to reproduce**: python cuda_host_bfc_oom_bug.py > out.log 2> out.err

### Source code
cuda_host_bfc_oom_bug.py:

```
import tensorflow as tf
if __name__ == '__main__':
    n_gpus = 4
    n_rep = 10
    tensor_shape = [1000,1000,250,4] # 4GB objects
    x_handle_ops = []
    coefs = []
    for i_gpu in range(n_gpus):
        with tf.device('/gpu:%i' % i_gpu):
            coef = tf.placeholder(shape=[], dtype=tf.float32)
            coefs.append(coef)
            x = coef * tf.ones(tensor_shape,dtype=tf.float32)
            x_handle_op = tf.get_session_handle(x)
            x_handle_ops.append(x_handle_op)
    config = tf.ConfigProto(device_count = {'GPU': n_gpus},
                            use_per_session_threads=0,
                            log_device_placement=0,
                            allow_soft_placement=0)
    with tf.Session(config=config) as sess:
        for rep in range(n_rep):
            x_handles = sess.run(x_handle_ops, feed_dict=dict(zip(coefs, range(n_gpus))))
            x_handles = [x_handle.get_raw_handle() for x_handle in x_handles]
            x_vars = []
            x_placeholders = []
            x_del_placeholders = []
            x_deleters = []
            for i_gpu in range(n_gpus):
                x_placeholder, x_val = tf.get_session_tensor(x_handles[i_gpu], tf.float32)
                x_del_placeholder, x_deleter = tf.delete_session_tensor(x_handles[i_gpu])
                x_placeholders.append(x_placeholder)
                x_del_placeholders.append(x_del_placeholder)
                x_deleters.append(x_deleter)
                x_val.set_shape(tensor_shape)
                with tf.device('/cpu:0'):
                    x_copy = tf.identity(x_val)
                    with tf.variable_scope('node_%i' % i_gpu, reuse=False):
                        with tf.variable_scope('var_%i' % rep, reuse=False):
                            x_var = tf.get_variable('x',
                                                    initializer=x_copy,
                                                    dtype=tf.float32)
                    x_vars.append(x_var)
            sess.run(tf.variables_initializer(x_vars),
                     feed_dict=dict(zip(x_placeholders, x_handles)))
            sess.run(x_deleters,
                     feed_dict=dict(zip(x_del_placeholders, x_handles)))
```

### Traceback

```
2017-07-31 11:35:11.906820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB
major: 6 minor: 0 memoryClockRate (GHz) 1.4805
pciBusID 0000:05:00.0
Total memory: 15.89GiB
Free memory: 15.61GiB
2017-07-31 11:35:12.433035: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x18b9550 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2017-07-31 11:35:12.434348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: 
name: Tesla P100-SXM2-16GB
major: 6 minor: 0 memoryClockRate (GHz) 1.4805
pciBusID 0000:06:00.0
Total memory: 15.89GiB
Free memory: 15.61GiB
2017-07-31 11:35:13.070865: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x2d0b7c0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2017-07-31 11:35:13.073944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 2 with properties: 
name: Tesla P100-SXM2-16GB
major: 6 minor: 0 memoryClockRate (GHz) 1.4805
pciBusID 0000:84:00.0
Total memory: 15.89GiB
Free memory: 15.61GiB
2017-07-31 11:35:13.832006: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x2d0f150 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2017-07-31 11:35:13.834336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 3 with properties: 
name: Tesla P100-SXM2-16GB
major: 6 minor: 0 memoryClockRate (GHz) 1.4805
pciBusID 0000:85:00.0
Total memory: 15.89GiB
Free memory: 15.61GiB
2017-07-31 11:35:13.846700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1 2 3 
2017-07-31 11:35:13.846721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y Y Y Y 
2017-07-31 11:35:13.846728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   Y Y Y Y 
2017-07-31 11:35:13.846734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 2:   Y Y Y Y 
2017-07-31 11:35:13.846741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 3:   Y Y Y Y 
2017-07-31 11:35:13.846753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:05:00.0)
2017-07-31 11:35:13.846761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:06:00.0)
2017-07-31 11:35:13.846769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0000:84:00.0)
2017-07-31 11:35:13.846775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla P100-SXM2-16GB, pci bus id: 0000:85:00.0)
2017-07-31 11:36:02.747322: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (cuda_host_bfc) ran out of memory trying to allocate 3.72GiB.  Current allocation summary follows.
2017-07-31 11:36:02.747386: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747402: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747414: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747425: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747436: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747455: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747466: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747477: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747488: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747500: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747511: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747522: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747533: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747544: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747555: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747566: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747577: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747588: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747599: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747610: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747621: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.747641: I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 3.72GiB was 256.00MiB, Chunk State: 
2017-07-31 11:36:02.747653: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1172d000000 of size 4294967296
2017-07-31 11:36:02.747663: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1182d800000 of size 4294967296
2017-07-31 11:36:02.747672: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1192d800000 of size 4000000000
2017-07-31 11:36:02.747681: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11a1beb2800 of size 4589934592
2017-07-31 11:36:02.747690: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11b2d800000 of size 4000000000
2017-07-31 11:36:02.747698: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11c1beb2800 of size 4000000000
2017-07-31 11:36:02.747707: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11d0a565000 of size 4000000000
2017-07-31 11:36:02.747715: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11df8c17800 of size 5179869184
2017-07-31 11:36:02.747724: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11f2d800000 of size 4000000000
2017-07-31 11:36:02.747732: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1201beb2800 of size 4000000000
2017-07-31 11:36:02.747740: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1210a565000 of size 4000000000
2017-07-31 11:36:02.747749: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x121f8c17800 of size 4000000000
2017-07-31 11:36:02.747757: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x122e72ca000 of size 4000000000
2017-07-31 11:36:02.747765: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x123d597c800 of size 4000000000
2017-07-31 11:36:02.747773: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x124c402f000 of size 4000000000
2017-07-31 11:36:02.747781: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x125b26e1800 of size 6359738368
2017-07-31 11:36:02.747789: I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size: 
2017-07-31 11:36:02.747802: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 11 Chunks of size 4000000000 totalling 40.98GiB
2017-07-31 11:36:02.747812: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 4294967296 totalling 8.00GiB
2017-07-31 11:36:02.747822: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 4589934592 totalling 4.27GiB
2017-07-31 11:36:02.747831: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 5179869184 totalling 4.82GiB
2017-07-31 11:36:02.747840: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 6359738368 totalling 5.92GiB
2017-07-31 11:36:02.747850: I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 64.00GiB
2017-07-31 11:36:02.747863: I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats: 
Limit:                 68719476736
InUse:                 68719476736
MaxInUse:              68719476736
NumAllocs:                      32
MaxAllocSize:           6359738368

2017-07-31 11:36:02.747876: W tensorflow/core/common_runtime/bfc_allocator.cc:277] *************************************************x***********************************************xxx
2017-07-31 11:36:02.747900: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1000,1000,250,4]
2017-07-31 11:36:02.747946: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (cuda_host_bfc) ran out of memory trying to allocate 3.72GiB.  Current allocation summary follows.
2017-07-31 11:36:02.747991: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748006: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748024: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748035: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748046: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748058: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748069: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748080: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748091: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748102: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748113: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748124: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748135: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748146: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748157: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748168: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748179: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748191: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748202: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748216: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748227: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748241: I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 3.72GiB was 256.00MiB, Chunk State: 
2017-07-31 11:36:02.748252: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1172d000000 of size 4294967296
2017-07-31 11:36:02.748261: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1182d800000 of size 4294967296
2017-07-31 11:36:02.748270: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1192d800000 of size 4000000000
2017-07-31 11:36:02.748281: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11a1beb2800 of size 4589934592
2017-07-31 11:36:02.748290: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11b2d800000 of size 4000000000
2017-07-31 11:36:02.748298: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11c1beb2800 of size 4000000000
2017-07-31 11:36:02.748306: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11d0a565000 of size 4000000000
2017-07-31 11:36:02.748317: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11df8c17800 of size 5179869184
2017-07-31 11:36:02.748325: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11f2d800000 of size 4000000000
2017-07-31 11:36:02.748334: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1201beb2800 of size 4000000000
2017-07-31 11:36:02.748342: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1210a565000 of size 4000000000
2017-07-31 11:36:02.748350: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x121f8c17800 of size 4000000000
2017-07-31 11:36:02.748358: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x122e72ca000 of size 4000000000
2017-07-31 11:36:02.748366: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x123d597c800 of size 4000000000
2017-07-31 11:36:02.748374: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x124c402f000 of size 4000000000
2017-07-31 11:36:02.748382: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x125b26e1800 of size 6359738368
2017-07-31 11:36:02.748390: I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size: 
2017-07-31 11:36:02.748402: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 11 Chunks of size 4000000000 totalling 40.98GiB
2017-07-31 11:36:02.748412: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 4294967296 totalling 8.00GiB
2017-07-31 11:36:02.748421: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 4589934592 totalling 4.27GiB
2017-07-31 11:36:02.748431: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 5179869184 totalling 4.82GiB
2017-07-31 11:36:02.748440: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 6359738368 totalling 5.92GiB
2017-07-31 11:36:02.748450: I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 64.00GiB
2017-07-31 11:36:02.748462: I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats: 
Limit:                 68719476736
InUse:                 68719476736
MaxInUse:              68719476736
NumAllocs:                      32
MaxAllocSize:           6359738368

2017-07-31 11:36:02.748475: W tensorflow/core/common_runtime/bfc_allocator.cc:277] *************************************************x***********************************************xxx
2017-07-31 11:36:02.748500: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1000,1000,250,4]
2017-07-31 11:36:02.748547: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (cuda_host_bfc) ran out of memory trying to allocate 3.72GiB.  Current allocation summary follows.
2017-07-31 11:36:02.748593: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748607: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748618: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748629: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748641: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748652: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748663: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748674: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748685: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748696: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748707: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748719: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748730: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748741: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748824: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748841: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748861: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748873: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748884: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748895: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748906: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.748920: I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 3.72GiB was 256.00MiB, Chunk State: 
2017-07-31 11:36:02.748931: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1172d000000 of size 4294967296
2017-07-31 11:36:02.748940: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1182d800000 of size 4294967296
2017-07-31 11:36:02.748948: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1192d800000 of size 4000000000
2017-07-31 11:36:02.748957: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11a1beb2800 of size 4589934592
2017-07-31 11:36:02.748965: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11b2d800000 of size 4000000000
2017-07-31 11:36:02.748973: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11c1beb2800 of size 4000000000
2017-07-31 11:36:02.748981: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11d0a565000 of size 4000000000
2017-07-31 11:36:02.748990: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11df8c17800 of size 5179869184
2017-07-31 11:36:02.748998: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11f2d800000 of size 4000000000
2017-07-31 11:36:02.749006: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1201beb2800 of size 4000000000
2017-07-31 11:36:02.749014: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1210a565000 of size 4000000000
2017-07-31 11:36:02.749022: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x121f8c17800 of size 4000000000
2017-07-31 11:36:02.749030: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x122e72ca000 of size 4000000000
2017-07-31 11:36:02.749038: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x123d597c800 of size 4000000000
2017-07-31 11:36:02.749046: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x124c402f000 of size 4000000000
2017-07-31 11:36:02.749054: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x125b26e1800 of size 6359738368
2017-07-31 11:36:02.749062: I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size: 
2017-07-31 11:36:02.749074: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 11 Chunks of size 4000000000 totalling 40.98GiB
2017-07-31 11:36:02.749084: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 4294967296 totalling 8.00GiB
2017-07-31 11:36:02.749094: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 4589934592 totalling 4.27GiB
2017-07-31 11:36:02.749103: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 5179869184 totalling 4.82GiB
2017-07-31 11:36:02.749116: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 6359738368 totalling 5.92GiB
2017-07-31 11:36:02.749126: I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 64.00GiB
2017-07-31 11:36:02.749139: I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats: 
Limit:                 68719476736
InUse:                 68719476736
MaxInUse:              68719476736
NumAllocs:                      32
MaxAllocSize:           6359738368

2017-07-31 11:36:02.749152: W tensorflow/core/common_runtime/bfc_allocator.cc:277] *************************************************x***********************************************xxx
2017-07-31 11:36:02.749174: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1000,1000,250,4]
2017-07-31 11:36:02.749222: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (cuda_host_bfc) ran out of memory trying to allocate 3.72GiB.  Current allocation summary follows.
2017-07-31 11:36:02.749266: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749282: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749293: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749305: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749316: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749327: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749338: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749349: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749360: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749371: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749382: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749393: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749404: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749422: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749434: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749445: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749456: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749468: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749479: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749490: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749501: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
2017-07-31 11:36:02.749515: I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 3.72GiB was 256.00MiB, Chunk State: 
2017-07-31 11:36:02.749526: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1172d000000 of size 4294967296
2017-07-31 11:36:02.749535: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1182d800000 of size 4294967296
2017-07-31 11:36:02.749544: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1192d800000 of size 4000000000
2017-07-31 11:36:02.749552: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11a1beb2800 of size 4589934592
2017-07-31 11:36:02.749561: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11b2d800000 of size 4000000000
2017-07-31 11:36:02.749569: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11c1beb2800 of size 4000000000
2017-07-31 11:36:02.749578: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11d0a565000 of size 4000000000
2017-07-31 11:36:02.749586: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11df8c17800 of size 5179869184
2017-07-31 11:36:02.749610: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x11f2d800000 of size 4000000000
2017-07-31 11:36:02.749617: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1201beb2800 of size 4000000000
2017-07-31 11:36:02.749625: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1210a565000 of size 4000000000
2017-07-31 11:36:02.749632: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x121f8c17800 of size 4000000000
2017-07-31 11:36:02.749640: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x122e72ca000 of size 4000000000
2017-07-31 11:36:02.749647: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x123d597c800 of size 4000000000
2017-07-31 11:36:02.749655: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x124c402f000 of size 4000000000
2017-07-31 11:36:02.749665: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x125b26e1800 of size 6359738368
2017-07-31 11:36:02.749673: I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size: 
2017-07-31 11:36:02.749684: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 11 Chunks of size 4000000000 totalling 40.98GiB
2017-07-31 11:36:02.749694: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 4294967296 totalling 8.00GiB
2017-07-31 11:36:02.749702: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 4589934592 totalling 4.27GiB
2017-07-31 11:36:02.749711: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 5179869184 totalling 4.82GiB
2017-07-31 11:36:02.749719: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 6359738368 totalling 5.92GiB
2017-07-31 11:36:02.749728: I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 64.00GiB
2017-07-31 11:36:02.749740: I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats: 
Limit:                 68719476736
InUse:                 68719476736
MaxInUse:              68719476736
NumAllocs:                      32
MaxAllocSize:           6359738368

2017-07-31 11:36:02.749753: W tensorflow/core/common_runtime/bfc_allocator.cc:277] *************************************************x***********************************************xxx
2017-07-31 11:36:02.749773: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1000,1000,250,4]
Traceback (most recent call last):
  File ""/home/mbasbug/workspace/svn/lcctf/test_handle2.py"", line 44, in <module>
    feed_dict=dict(zip(x_placeholders, x_handles)))
  File ""/tensorflow/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/tensorflow/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/tensorflow/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/tensorflow/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[1000,1000,250,4]
	 [[Node: node_2/var_3/x/Assign = Assign[T=DT_FLOAT, _class=[""loc:@node_2/var_3/x""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](node_2/var_3/x, GetSessionTensor_14/_99)]]

Caused by op u'node_2/var_3/x/Assign', defined at:
  File ""/home/mbasbug/workspace/svn/lcctf/test_handle2.py"", line 41, in <module>
    dtype=tf.float32)
  File ""/tensorflow/tensorflow/python/ops/variable_scope.py"", line 1065, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)
  File ""/tensorflow/tensorflow/python/ops/variable_scope.py"", line 962, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)
  File ""/tensorflow/tensorflow/python/ops/variable_scope.py"", line 367, in get_variable
    validate_shape=validate_shape, use_resource=use_resource)
  File ""/tensorflow/tensorflow/python/ops/variable_scope.py"", line 352, in _true_getter
    use_resource=use_resource)
  File ""/tensorflow/tensorflow/python/ops/variable_scope.py"", line 725, in _get_single_variable
    validate_shape=validate_shape)
  File ""/tensorflow/tensorflow/python/ops/variables.py"", line 199, in __init__
    expected_shape=expected_shape)
  File ""/tensorflow/tensorflow/python/ops/variables.py"", line 320, in _init_from_args
    validate_shape=validate_shape).op
  File ""/tensorflow/tensorflow/python/ops/state_ops.py"", line 274, in assign
    validate_shape=validate_shape)
  File ""/tensorflow/tensorflow/python/ops/gen_state_ops.py"", line 45, in assign
    use_locking=use_locking, name=name)
  File ""/tensorflow/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/tensorflow/tensorflow/python/framework/ops.py"", line 2627, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/tensorflow/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1000,1000,250,4]
	 [[Node: node_2/var_3/x/Assign = Assign[T=DT_FLOAT, _class=[""loc:@node_2/var_3/x""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](node_2/var_3/x, GetSessionTensor_14/_99)]]
```


"
11912,A potential null pointer deference bug in GraphProperties class,"Dear developers,

I am studying the code of Tensorflow and found a potential null pointer dereference bug in tensorflow/core/grappler/costs/graph_properties.cc

### The problem

The function `updateEnter()` at line 70 of tensorflow/core/grappler/costs/graph_properties.cc calls the `getContext()` function of the ShapeRefiner class, which could return a null pointer (line 79 of tensorflow/core/common_runtime/shape_refiner.h) and get stored in the variable `enter_ctx`. At line 73 of the `updateEnter()` function, `enter_ctx` is dereferenced in the for loop condition without being checked against null. If the null pointer dereference is triggered, the program might crash.

I noticed that for another call of the `getContext()` function at line 255 of tensorflow/core/grappler/costs/graph_properties.cc, the function return value, which gets stored in the variable `qctx`, is checked against null, indicating that `getContext()` can indeed return null pointer.

### Source code

I am analyzing the latest version of Tensorflow as of July 31, 2017, and the two relevant files are:
(1) https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/graph_properties.cc (commit 4432623 on 1 July)

(2) https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/shape_refiner.h (commit e85d3df  on 30 June)

Hope my report helps :)

"
11911,tensorflow-1.2.1 import tensorflow Segmentation fault,"hi ,
I installed tensorflow-1.2.1 in my machine, and met a segment fault as below. 

```
stm-zk5:~ # python
Python 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
>>> import tensorflow
段错误
```

### System information
```
stm-zk5:~ #  cat /etc/redhat-release
CentOS Linux release 6.2 (Final)

```

###  Installing 
I do : 
`pip install --ignore-installed --upgrade  https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.1-cp27-none-linux_x86_64.whl`

so , would someone help me to solve this problem ?🤕 ，thx !"
11910,RandomShuffleQueue seems not random when one class one file.,"I split cifar-10 dataset to ten files, where each file contains one class of samples. When I use the split files   as the input of a classification model and use RandomShuffleQueue to handle the input, data in one batch  is not an approximate uniform sampling of the ten classes. The hisogram of the labels in one batch indicates RandomShuffleQueue seems not do a real random dequeue operation. Can anyone explain the details of RandomShuffleQueue dequeen operation? Thanks.
![screenshot from 2017-07-31 21-39-45](https://user-images.githubusercontent.com/8766653/28781032-5187f2bc-763b-11e7-91a8-24a1c0e2ce2b.png)

"
11909,Numeric instability of tf model when run o CPU,"I'm having problem with running a model on CPU. I trained it on GPU and it works just great on GPU with no issues whatsoever yet when run on CPU it computes differently and quickly falls into returning just NaNs.

I used Keras to construct the model which looks like this:
```
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input (InputLayer)               (None, 500)           0                                            
____________________________________________________________________________________________________
embedding_1 (Embedding)          (None, 500, 60)       300000      input[0][0]                      
____________________________________________________________________________________________________
conv1d_1 (Conv1D)                (None, 500, 128)      7808        embedding_1[0][0]                
____________________________________________________________________________________________________
conv1d_2 (Conv1D)                (None, 500, 128)      15488       embedding_1[0][0]                
____________________________________________________________________________________________________
conv1d_3 (Conv1D)                (None, 500, 128)      23168       embedding_1[0][0]                
____________________________________________________________________________________________________
conv1d_4 (Conv1D)                (None, 500, 128)      30848       embedding_1[0][0]                
____________________________________________________________________________________________________
conv1d_5 (Conv1D)                (None, 500, 32)       9632        embedding_1[0][0]                
____________________________________________________________________________________________________
conv1d_6 (Conv1D)                (None, 500, 32)       11552       embedding_1[0][0]                
____________________________________________________________________________________________________
global_average_pooling1d_1 (Glob (None, 128)           0           conv1d_1[0][0]                   
____________________________________________________________________________________________________
global_average_pooling1d_2 (Glob (None, 128)           0           conv1d_2[0][0]                   
____________________________________________________________________________________________________
global_average_pooling1d_3 (Glob (None, 128)           0           conv1d_3[0][0]                   
____________________________________________________________________________________________________
global_average_pooling1d_4 (Glob (None, 128)           0           conv1d_4[0][0]                   
____________________________________________________________________________________________________
global_average_pooling1d_5 (Glob (None, 32)            0           conv1d_5[0][0]                   
____________________________________________________________________________________________________
global_average_pooling1d_6 (Glob (None, 32)            0           conv1d_6[0][0]                   
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 576)           0           global_average_pooling1d_1[0][0] 
                                                                   global_average_pooling1d_2[0][0] 
                                                                   global_average_pooling1d_3[0][0] 
                                                                   global_average_pooling1d_4[0][0] 
                                                                   global_average_pooling1d_5[0][0] 
                                                                   global_average_pooling1d_6[0][0] 
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 576)           0           concatenate_1[0][0]              
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 576)           2304        dropout_1[0][0]                  
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 300)           173100      batch_normalization_1[0][0]      
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 300)           0           dense_1[0][0]                    
____________________________________________________________________________________________________
batch_normalization_2 (BatchNorm (None, 300)           1200        dropout_2[0][0]                  
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 24)            7224        batch_normalization_2[0][0]      
====================================================================================================
Total params: 582,324
Trainable params: 580,572
Non-trainable params: 1,752
```

I used following code to debug it:
```
from keras import backend as K
import numpy as np

inp = model.input                                           # input placeholder
outputs = [layer.output for layer in model.layers]          # all layer outputs
functor = K.function([inp]+ [K.learning_phase()], outputs ) # evaluation function


# Testing
#test = np.random.random(model.input_shape[1])[np.newaxis,...]
x = numpy.asarray(tknzr.texts_to_sequences([str(x) for x in X]))
x = sequence.pad_sequences(x, maxlen=500)
layer_outs = functor([x, 1.])
i=0;
for l in layer_outs:
    print (model.layers[i].name)
    i+=1
    print (l)
```

I fed it with the very simple input just to show the differences.
The results on CPU are quickly starting to diverge from the values obtained on GPU. On GPU it looks like:
```
input
[[    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.    27.    13.     3.   233.  2459.     5.   689.]]
embedding_1
[[[ -3.76313413e-03   3.56433578e-02  -2.13286784e-02 ...,   4.60807569e-02
     1.64003521e-02   4.84493654e-03]
  [ -3.76313413e-03   3.56433578e-02  -2.13286784e-02 ...,   4.60807569e-02
     1.64003521e-02   4.84493654e-03]
  [ -3.76313413e-03   3.56433578e-02  -2.13286784e-02 ...,   4.60807569e-02
     1.64003521e-02   4.84493654e-03]
  ...,
  [ -6.31065369e+00  -6.24338293e+00   2.05590415e+00 ...,   9.74539161e-01
    -1.58533490e+00  -2.32490934e-02]
  [ -1.79465318e+00   7.12853432e+00  -9.40279942e-03 ...,  -8.01290665e+01
     1.42804585e+01  -2.88563843e+02]
  [ -8.28844547e+00  -1.10367613e+01   7.69042683e+00 ...,   1.41618121e+00
     1.20486283e+00   1.61890488e+01]]]
conv1d_1
[[[  0.00000000e+00   0.00000000e+00   5.55836601e+01 ...,   0.00000000e+00
     0.00000000e+00   0.00000000e+00]
  [  0.00000000e+00   0.00000000e+00   5.55836601e+01 ...,   0.00000000e+00
     0.00000000e+00   0.00000000e+00]
  [  0.00000000e+00   0.00000000e+00   5.55836601e+01 ...,   0.00000000e+00
     0.00000000e+00   0.00000000e+00]
  ...,
  [  0.00000000e+00   4.61124207e+02   0.00000000e+00 ...,   8.68512988e-01
     2.12859364e+01   3.05228138e+01]
  [  0.00000000e+00   0.00000000e+00   5.23455273e+03 ...,   0.00000000e+00
     0.00000000e+00   0.00000000e+00]
  [  1.76998459e+02   1.92677148e+03   0.00000000e+00 ...,   0.00000000e+00
     0.00000000e+00   3.46375618e+01]]]
conv1d_2
[[[     0.            294.24438477      0.         ...,    145.89456177
       88.91067505      0.        ]
  [     0.            294.24438477      0.         ...,    145.89456177
       88.91067505      0.        ]
  [     0.            294.24438477      0.         ...,    145.89456177
       88.91067505      0.        ]
  ...,
  [     0.          12730.79589844      0.         ...,   7905.12939453
     1696.24743652      0.        ]
  [     0.              0.          33181.72265625 ...,   4034.12573242
        0.              0.        ]
  [     0.            880.89868164      0.         ...,    102.81720734
      112.0918808       0.        ]]]
conv1d_3
[[[  0.00000000e+00   2.28604034e+02   0.00000000e+00 ...,   0.00000000e+00
     1.81265926e+01   1.41597879e+00]
  [  0.00000000e+00   2.57441193e+02   0.00000000e+00 ...,   0.00000000e+00
     0.00000000e+00   0.00000000e+00]
  [  0.00000000e+00   2.57441193e+02   0.00000000e+00 ...,   0.00000000e+00
     0.00000000e+00   0.00000000e+00]
  ...,
  [  4.42018008e+04   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00
     0.00000000e+00   0.00000000e+00]
  [  7.06631055e+03   4.40339990e+03   0.00000000e+00 ...,   0.00000000e+00
     0.00000000e+00   0.00000000e+00]
  [  0.00000000e+00   1.90917981e+03   0.00000000e+00 ...,   0.00000000e+00
     0.00000000e+00   1.50991055e+04]]]

```

Whereas on CPU it looks like:
```
input
[[    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.    27.    13.     3.   233.  2459.     5.   689.]]
embedding_1
[[[ -3.76313413e-03   3.56433578e-02  -2.13286784e-02 ...,   4.60807569e-02
     1.64003521e-02   4.84493654e-03]
  [ -3.76313413e-03   3.56433578e-02  -2.13286784e-02 ...,   4.60807569e-02
     1.64003521e-02   4.84493654e-03]
  [ -3.76313413e-03   3.56433578e-02  -2.13286784e-02 ...,   4.60807569e-02
     1.64003521e-02   4.84493654e-03]
  ...,
  [ -6.31065369e+00  -6.24338293e+00   2.05590415e+00 ...,   9.74539161e-01
    -1.58533490e+00  -2.32490934e-02]
  [ -1.79465318e+00   7.12853432e+00  -9.40279942e-03 ...,  -8.01290665e+01
     1.42804585e+01  -2.88563843e+02]
  [ -8.28844547e+00  -1.10367613e+01   7.69042683e+00 ...,   1.41618121e+00
     1.20486283e+00   1.61890488e+01]]]
conv1d_1
[[[  0.00000000e+00   0.00000000e+00   5.55836601e+01 ...,   0.00000000e+00
     0.00000000e+00   0.00000000e+00]
  [  0.00000000e+00   0.00000000e+00   5.55836601e+01 ...,   0.00000000e+00
     0.00000000e+00   0.00000000e+00]
  [  0.00000000e+00   0.00000000e+00   5.55836601e+01 ...,   0.00000000e+00
     0.00000000e+00   0.00000000e+00]
  ...,
  [  0.00000000e+00   4.61124207e+02   0.00000000e+00 ...,   8.68513644e-01
     2.12859344e+01   3.05228119e+01]
  [  0.00000000e+00   0.00000000e+00   5.23455322e+03 ...,   0.00000000e+00
     0.00000000e+00   0.00000000e+00]
  [  1.76998474e+02   1.92677136e+03   0.00000000e+00 ...,   0.00000000e+00
     0.00000000e+00   3.46375618e+01]]]
conv1d_2
[[[            nan    294.24438477      0.         ...,    145.89456177
       88.91065979      0.        ]
  [            nan    294.24438477      0.         ...,    145.89456177
       88.91065979      0.        ]
  [            nan    294.24438477      0.         ...,    145.89456177
       88.91065979      0.        ]
  ...,
  [            nan  12730.79394531      0.         ...,   7905.12939453
     1696.24743652      0.        ]
  [            nan      0.          33181.7265625  ...,   4034.12597656
        0.              0.        ]
  [            nan    880.89862061      0.         ...,    102.81723785
      112.0918808       0.        ]]]
conv1d_3
[[[  0.00000000e+00   2.28604034e+02   0.00000000e+00 ...,              nan
     1.81265984e+01   1.41597819e+00]
  [  0.00000000e+00   2.57441193e+02   0.00000000e+00 ...,              nan
     0.00000000e+00   0.00000000e+00]
  [  0.00000000e+00   2.57441193e+02   0.00000000e+00 ...,              nan
     0.00000000e+00   0.00000000e+00]
  ...,
  [  4.42018008e+04   0.00000000e+00   0.00000000e+00 ...,              nan
     0.00000000e+00   0.00000000e+00]
  [  7.06631006e+03   4.40339746e+03   0.00000000e+00 ...,              nan
     0.00000000e+00   0.00000000e+00]
  [  0.00000000e+00   1.90918152e+03   0.00000000e+00 ...,              nan
     0.00000000e+00   1.50991055e+04]]]
```

Note the NaNs in the last shown layer though the values started to diverge much quicker.  The first difference is in conv1d_1 where it computes on the CPU:
```
  [  0.00000000e+00   4.61124207e+02   0.00000000e+00 ...,   8.68513644e-01
     2.12859344e+01   3.05228119e+01]
  [  0.00000000e+00   0.00000000e+00   5.23455322e+03 ...,   0.00000000e+00
     0.00000000e+00   0.00000000e+00]
  [  1.76998474e+02   1.92677136e+03   0.00000000e+00 ...,   0.00000000e+00
     0.00000000e+00   3.46375618e+01]]]
```
where is should:
```
  [  0.00000000e+00   4.61124207e+02   0.00000000e+00 ...,   8.68512988e-01
     2.12859364e+01   3.05228138e+01]
  [  0.00000000e+00   0.00000000e+00   5.23455273e+03 ...,   0.00000000e+00
     0.00000000e+00   0.00000000e+00]
  [  1.76998459e+02   1.92677148e+03   0.00000000e+00 ...,   0.00000000e+00
     0.00000000e+00   3.46375618e+01]]]
```

Note: 8.68513644e-01 vs 8.68512988e-01 and following...

Unfortunately it renders perfectly working model into a worthless pile of weights that produces a vector full of NaNs :(
Does anyone have any idea of what could go wrong?

For the record, the model's  weights are:
```
[array([[ -3.76313413e-03,   3.56433578e-02,  -2.13286784e-02, ...,
          4.60807569e-02,   1.64003521e-02,   4.84493654e-03],
       [ -1.49039514e+03,   1.54171045e+03,   3.97881046e-02, ...,
         -5.78388453e+00,   2.43417993e-02,  -1.80144477e+00],
       [ -7.30040884e+00,  -6.69775963e-01,  -2.57102852e+01, ...,
          1.29456043e+01,   2.43919420e+00,  -2.05847025e-02],
       ..., 
       [ -2.07939720e+00,   1.65184236e+00,   5.03200665e-02, ...,
         -9.66950320e-03,  -3.28505486e-02,  -1.60967112e-01],
       [  2.00684814e+01,  -4.48382616e+00,   9.10888481e+00, ...,
          5.54858351e+00,   1.92856118e-01,  -3.27318764e+00],
       [  1.32376838e+00,  -8.82686138e+00,  -5.80457896e-02, ...,
          4.60517597e+00,   1.85110033e-01,  -7.66239548e+00]], dtype=float32), array([[[ -1.99184626e-01,  -6.24039583e-02,  -5.06775379e-02, ...,
           1.50278226e-01,   1.72856063e-01,   9.54210311e-02],
        [  8.40011835e-02,  -1.04346856e-01,  -1.50010496e-01, ...,
          -6.32669926e-02,   3.96224223e-02,  -4.31677438e-02],
        [ -8.77222940e-02,   5.68446703e-02,   5.21141998e-02, ...,
          -4.07455601e-02,  -1.41425105e-02,  -1.18918315e-01],
        ..., 
        [  3.44693642e+01,   9.11418945e-02,   1.19048271e+01, ...,
           1.09344691e-01,   1.87920347e-01,   1.70073420e-01],
        [  1.95824668e-01,   7.89859332e-03,   9.49118510e-02, ...,
          -3.29354890e-02,   9.31546241e-02,  -1.15318425e-01],
        [  5.89678064e-02,   1.41023742e-02,   1.11247830e-01, ...,
          -9.51733999e-03,  -1.24835804e-01,  -1.12498775e-01]]], dtype=float32), array([  1.22442953e-02,  -1.09238578e-02,  -2.47095129e-04,
         1.17717348e-02,  -3.92556889e-03,  -5.25355758e-03,
         6.99595024e-04,  -1.10572961e-03,  -1.23395994e-02,
                    nan,  -1.07570309e-02,              nan,
        -4.08349838e-03,              nan,  -4.71794903e-02,
                    nan,  -8.35025460e-02,   4.59762150e-03,
        -1.97616145e-02,  -4.07275278e-03,              nan,
        -2.22547930e-02,  -1.18769081e-02,  -4.17554192e-03,
        -1.11441838e-03,  -4.26767347e-03,  -7.31970591e-04,
         1.48863369e-03,  -8.20796192e-03,  -5.81618951e-05,
        -9.59698856e-03,   1.10042130e-03,  -2.86204537e-04,
        -1.64017070e-03,  -3.14154982e-04,   9.65116825e-03,
        -3.80394212e-03,  -1.14975907e-02,  -1.14118000e-02,
        -6.63896417e-03,  -1.02947457e-02,  -9.60258301e-03,
        -1.01221241e-02,  -3.38621135e-03,  -9.19509027e-03,
                    nan,  -2.48736516e-03,  -2.78100204e-02,
        -6.50373427e-03,  -1.12824216e-02,  -1.99352647e-03,
        -2.70526763e-03,  -3.97901144e-03,  -8.55945144e-03,
        -2.47867964e-03,  -7.23447651e-03,  -1.89489545e-03,
                    nan,   2.34743766e-03,  -1.76552832e-02,
        -1.13650626e-02,  -8.72352626e-03,  -1.39924623e-02,
        -9.96223418e-04,   1.19819669e-02,  -1.65980589e-02,
        -4.43597836e-03,  -1.32924877e-03,   5.39703481e-03,
        -5.20827901e-03,   2.09201197e-03,  -9.91921872e-04,
        -4.76511335e-03,  -2.82827131e-02,  -3.15303504e-02,
         7.75449863e-03,  -2.50613503e-03,   5.26739890e-03,
        -9.09337029e-03,  -4.06189971e-02,  -2.45879288e-03,
        -6.14755554e-03,  -4.51790220e-05,  -1.96866468e-02,
        -6.05669746e-04,  -1.08566531e-03,  -2.80132494e-03,
         1.35421744e-02,  -1.56419240e-02,  -9.22969636e-03,
        -1.79088686e-03,  -6.13477267e-03,   3.58441076e-03,
        -7.55022466e-03,   7.20449630e-03,   9.64386389e-03,
        -2.01407354e-02,   3.64139187e-03,  -8.29497911e-03,
        -9.83765721e-03,  -6.11743657e-03,   1.19033596e-03,
         3.14918067e-03,   9.48947109e-03,  -1.80235994e-03,
        -3.43534909e-03,  -9.94156580e-03,  -1.02966810e-02,
        -1.11212442e-02,  -1.83315631e-02,   1.77474413e-02,
        -5.63624455e-03,   1.24310488e-02,  -7.89506827e-03,
         5.76251280e-03,   3.04201362e-03,  -9.81117040e-03,
        -2.30816868e-03,  -1.17079932e-02,  -3.04500442e-02,
         1.04938364e-02,  -1.00002484e-02,  -5.61673800e-03,
        -1.49540678e-02,  -4.57247645e-02,  -2.97049657e-02,
        -1.49060250e-03,  -4.22246335e-03], dtype=float32), array([[[ -5.67133713e+00,   4.49010801e+00,  -3.19202495e+00, ...,
           8.94742459e-02,   5.86951338e-02,  -4.10384983e-02],
        [  1.06694847e-01,  -1.19669974e-01,   1.73447832e-01, ...,
          -4.61597033e-02,   7.81161070e+00,  -9.81731415e-02],
        [  3.78304683e-02,   6.29944146e-01,   5.74416220e-01, ...,
          -3.67318280e-02,   1.13924325e-01,  -5.70971444e-02],
        ..., 
        [  3.19900885e-02,  -1.08785249e-01,   1.04148674e+01, ...,
          -1.18532476e+01,   1.17602840e-03,   6.20494075e-02],
        [  1.15054836e+01,   1.16688490e-01,   1.07223682e+01, ...,
           8.48904774e-02,   3.37078786e+00,   1.72410712e-01],
        [ -6.89478666e-02,  -5.79792976e-01,   1.74256325e+00, ...,
           3.34965885e-02,   9.88691524e-02,  -7.72289336e-02]],

       [[ -4.56401205e+00,  -1.70166504e+00,  -4.00359268e+01, ...,
          -1.60190001e-01,   7.67084712e-04,  -6.53180247e-03],
        [  1.53721035e-01,   4.46713977e-02,  -4.76150699e-02, ...,
          -8.13984126e-02,  -4.89648283e-02,  -5.83464168e-02],
        [ -7.04783201e-02,  -1.08484268e-01,   2.35563189e-01, ...,
           4.65062571e+00,   8.50204751e-02,   5.00446837e-03],
        ..., 
        [ -1.37988487e-02,  -4.87490520e-02,   8.56983513e-02, ...,
           9.54836905e-02,   7.68516883e-02,   4.86173406e-02],
        [  1.28548712e-01,   2.28847917e-02,   9.46553797e-02, ...,
          -3.44736390e-02,   5.21991348e+00,  -5.61943185e-03],
        [ -8.80202651e-02,   6.42642140e-01,  -1.28573000e-01, ...,
          -6.45207912e-02,   3.76606435e-02,   6.43695444e-02]]], dtype=float32), array([             nan,  -1.16861016e-02,   1.06510427e-02,
        -1.00857085e-02,  -3.16432514e-03,   5.28910290e-03,
        -3.37183755e-03,  -5.42259403e-03,  -1.71696593e-03,
         6.63183979e-04,  -1.14967767e-02,  -1.24864047e-02,
         8.57937802e-03,  -8.48316122e-03,  -1.21079152e-02,
        -3.48650361e-03,  -1.25225978e-02,   6.70570601e-03,
         5.04712947e-03,   6.85022306e-03,              nan,
         3.81274032e-03,              nan,   9.87051427e-03,
         7.05182599e-03,  -4.13431972e-03,  -7.64692994e-03,
        -5.04446309e-03,   9.21117794e-03,  -8.85901693e-03,
         5.76300500e-03,   1.10929953e-02,  -4.79056872e-02,
         2.45477562e-03,   5.19062718e-03,  -1.07234372e-02,
         1.28654959e-02,  -2.64256280e-02,  -7.82623887e-03,
         1.58936554e-03,   1.94144645e-03,  -9.01379809e-03,
        -7.94421230e-03,  -9.22279246e-03,   1.28184948e-02,
         8.28053430e-03,  -1.91762717e-03,  -7.50862667e-03,
         5.61357196e-03,  -1.29625732e-02,  -9.95500293e-03,
        -1.55696608e-02,  -1.02757392e-02,  -8.53954721e-03,
        -5.20441961e-03,  -6.20087667e-04,  -4.11103945e-03,
        -5.66026429e-05,  -4.81751822e-02,  -1.19420085e-02,
        -1.14781095e-03,   1.82268117e-02,  -4.04010108e-03,
        -3.97138792e-04,   1.23193730e-02,  -5.21201408e-03,
        -9.81894135e-03,   2.68496876e-03,   2.54207547e-03,
        -8.91120639e-03,  -1.21979211e-02,  -1.08713415e-02,
         5.36955427e-04,   6.40868861e-03,   1.28432624e-02,
         1.19460337e-02,   1.27818873e-02,  -1.24154398e-02,
         1.63561814e-02,  -8.23633932e-03,  -4.27962840e-03,
        -1.16055720e-02,  -1.09054958e-02,   5.52257756e-03,
         1.22179277e-02,  -1.70459468e-02,   1.45145757e-02,
        -2.81992904e-03,   1.21280942e-02,  -4.49353713e-04,
         5.24557894e-04,  -9.54091083e-03,  -1.64333021e-03,
        -1.64759357e-03,  -2.43817759e-03,  -1.37002217e-02,
         3.06799076e-03,   1.03644496e-02,   1.23127848e-02,
        -8.38474371e-04,  -8.28741584e-03,  -4.09937929e-03,
         2.41700746e-03,  -7.54709821e-03,  -4.19640495e-03,
        -6.47489447e-03,  -1.27521744e-02,   1.19293816e-02,
        -1.73897448e-03,  -1.77855510e-02,   9.67702712e-04,
        -1.86440512e-03,  -1.19959423e-02,   2.26790202e-03,
         1.28469197e-02,   1.03012251e-04,  -7.98405055e-03,
         3.69331776e-03,  -2.29048952e-02,  -2.48414255e-03,
        -4.03112406e-03,  -1.02750035e-02,   9.63258557e-03,
        -9.45603568e-03,  -9.57334414e-03,  -3.22731212e-03,
         1.06190313e-02,  -3.17891054e-02], dtype=float32), array([[[ -1.61977196e+00,  -7.39741027e-02,   3.43860276e-02, ...,
          -8.21563378e-02,  -3.90652902e-02,   4.53755051e-01],
        [  1.00042380e-01,  -7.75875032e-01,  -1.36523447e+01, ...,
           1.06767498e-01,  -2.50063915e+01,  -7.44080126e-01],
        [  3.38117170e+00,   3.06568718e+00,   3.14826891e-02, ...,
           2.25776862e-02,  -9.88149464e-01,  -4.45492458e+00],
        ..., 
        [  9.40100002e+00,  -5.44576719e-02,   2.02827530e+01, ...,
           1.68967806e-02,   1.54567789e-02,   5.55494428e-01],
        [  4.40017819e-01,  -2.21299052e+00,  -2.51157563e-02, ...,
          -7.26501718e-02,   3.84688005e-02,  -3.95980949e+01],
        [  8.49678397e-01,  -3.78436089e-01,  -7.45657778e+00, ...,
          -1.15545931e+01,   7.55678117e-02,  -5.93765163e+00]],

       [[ -1.09657323e+00,  -3.41127610e+00,  -2.64547355e-02, ...,
          -1.05148535e+01,  -2.34183741e+00,   6.30887461e+00],
        [  1.08152907e-02,  -5.99248447e-02,  -4.27437496e+00, ...,
           9.48090839e+00,  -1.31728077e+01,  -1.99199450e+00],
        [  4.63601875e+00,   8.85547578e-01,   8.49755383e+00, ...,
           4.01883647e-02,  -5.40873070e+01,  -8.10760593e+00],
        ..., 
        [  1.63729572e+00,   1.07432418e-01,   5.13684988e+00, ...,
           9.78041458e+00,   1.18023157e+01,  -1.92248020e+01],
        [  2.18332148e+00,   2.69323587e-01,   5.46097644e-02, ...,
          -6.50963048e-03,   1.79492927e+00,  -4.89809561e+00],
        [ -9.75892162e+00,  -5.55556975e-02,  -4.50018120e+00, ...,
          -5.22859767e-02,  -9.14887714e+00,   6.72427177e-01]],

       [[ -1.28133631e+00,  -1.15324832e-01,   4.84604597e+00, ...,
           8.53913128e-01,  -4.12158108e+00,   3.20745516e+00],
        [  5.03665566e-01,  -6.51536644e-01,  -1.99298458e+01, ...,
          -4.51152563e+00,  -3.04961085e+00,  -7.68840981e+00],
        [  8.86888266e-01,   1.83413160e+00,   1.15422651e-01, ...,
           3.19552784e+01,  -1.02603951e+01,   5.06386347e-02],
        ..., 
        [ -3.43910828e-02,   8.03594384e-03,  -6.82456121e-02, ...,
          -1.32336438e+00,   9.69354534e+00,   1.43188864e-01],
        [  1.94831336e+00,   7.06176579e-01,  -1.22930088e+01, ...,
           3.19514275e+01,  -3.98706011e-02,   1.26935318e+02],
        [ -1.76737213e+02,   6.87187672e-01,   8.70995164e-01, ...,
           5.70003223e+00,  -1.45483494e+00,   2.32137394e+01]]], dtype=float32), array([ -3.26319179e-03,   1.00775659e-02,   1.24532217e-02,
        -5.61445113e-03,   1.23609733e-02,   2.54739518e-03,
         9.75468103e-03,   2.58134492e-03,              nan,
        -6.60398882e-03,  -6.77689118e-03,              nan,
        -6.14733994e-03,  -1.29952980e-02,  -8.97676032e-03,
        -8.59634299e-03,              nan,              nan,
        -2.95513729e-03,   1.18087698e-02,  -6.92531653e-03,
        -3.81476502e-03,   4.01055673e-03,  -1.87762303e-03,
                    nan,  -8.76397081e-03,   3.31577263e-03,
        -7.15260301e-03,              nan,  -9.94501822e-03,
         8.71287752e-03,   2.85709789e-03,   1.01934960e-02,
         5.10863215e-03,   8.00231192e-03,   1.30663458e-02,
                    nan,  -2.66836630e-03,  -5.84459817e-03,
        -9.70377401e-03,  -5.44005353e-03,  -1.94316905e-03,
                    nan,   6.81864610e-03,  -3.55707551e-03,
                    nan,              nan,              nan,
         7.12703820e-03,  -6.48293830e-03,  -5.05002216e-03,
        -5.49217407e-03,  -6.58929441e-03,   3.22370703e-04,
                    nan,  -4.25100280e-03,  -1.21072698e-02,
        -4.67845378e-03,   1.12837339e-02,              nan,
        -5.03929658e-03,  -7.51174660e-03,  -1.92871108e-03,
        -7.48544186e-03,  -8.55511334e-03,              nan,
         1.22985756e-02,  -9.50173382e-03,              nan,
        -7.84607977e-03,              nan,              nan,
         2.10464559e-03,  -2.24326691e-03,   8.51140916e-03,
         8.25999025e-03,   1.06525635e-02,   1.98795414e-03,
        -4.74094879e-03,  -6.73289353e-04,  -6.25591958e-03,
         5.25373733e-03,  -1.02844061e-02,  -1.83374190e-03,
         3.19173466e-03,  -1.91105681e-03,  -1.02188773e-02,
         1.23149445e-02,   8.52429774e-04,   3.51795199e-04,
         3.75404325e-03,              nan,  -3.79106204e-05,
         8.06951057e-03,              nan,  -2.73329369e-03,
        -1.09396372e-02,              nan,   6.13762764e-03,
        -2.86296476e-04,   1.28226317e-02,  -3.92029062e-03,
                    nan,              nan,              nan,
         3.60080437e-03,              nan,   8.31638649e-03,
        -6.92761596e-03,              nan,  -1.35375967e-03,
         1.14998326e-03,   3.87885934e-03,  -2.76077818e-03,
        -6.00072276e-03,  -5.45156840e-03,  -4.58881212e-03,
        -4.49480955e-03,              nan,   1.20337764e-02,
        -7.77600612e-03,              nan,  -1.05871297e-02,
        -6.26315596e-03,  -6.49133092e-03,              nan,
        -4.12164629e-03,  -1.56654476e-03], dtype=float32), array([[[  9.79139938e+01,   6.50725784e+01,  -2.97711365e+02, ...,
          -5.06063690e+01,  -2.56701141e+02,  -3.16261673e+01],
        [ -1.51150751e+01,  -1.56986221e+02,   1.86006622e+02, ...,
          -3.05852753e+02,  -7.03881741e+00,   4.88232346e+01],
        [  4.23546844e+02,  -5.02201891e+00,   8.98978577e+01, ...,
           1.63829773e+02,  -1.78304108e+02,  -1.39583206e+02],
        ..., 
        [ -2.89913425e+01,  -3.32941986e+02,  -3.21323486e+02, ...,
           2.13999649e+02,   4.47361803e+00,   1.70371838e+01],
        [ -4.92524338e+00,   2.07512909e+02,   1.72584229e+01, ...,
          -6.62303925e+01,   3.61030388e+01,   1.54256191e+01],
        [ -1.19239189e+02,   8.50090885e+00,  -1.25474405e+01, ...,
           2.98828545e+01,  -2.49162731e+01,   1.00076149e+02]],

       [[  3.35705017e+02,  -3.44381237e+00,  -7.37603149e+01, ...,
           1.34216904e+02,  -2.61415283e+02,   3.10259857e+02],
        [ -2.39945786e+02,   1.41973862e+02,   1.38914764e+02, ...,
           8.80347639e-02,  -2.73359741e+02,   2.96114716e+02],
        [  1.49376358e+02,   4.49186897e+01,   2.00130661e+02, ...,
           9.72330994e+02,  -1.42100105e+01,  -6.58187927e+02],
        ..., 
        [  2.92934055e+01,  -6.77076578e-02,   7.36021271e+01, ...,
           3.20087932e-02,   1.15392891e+02,  -9.85379791e+01],
        [ -9.74929581e+01,   4.77340408e-02,  -1.07744355e-02, ...,
          -2.54074135e+01,   2.96503563e+01,   2.98027916e+01],
        [  4.05228519e+00,   5.41728783e+01,  -1.24106041e+02, ...,
           4.01345432e-01,  -7.40911961e+00,  -2.39570007e+02]],

       [[ -1.43696884e+02,   1.93436527e+01,  -2.52150314e+02, ...,
          -1.49458786e+02,   3.11022583e+02,  -1.83917038e+02],
        [  2.52352753e+01,  -1.12239952e+02,   4.66152840e+01, ...,
          -7.23185272e+01,  -1.00572090e+01,   7.35368805e+01],
        [ -3.30404144e+02,  -1.20080490e+02,   1.45473413e+01, ...,
           4.90700653e+02,   3.48218232e-02,   2.57165771e+02],
        ..., 
        [ -7.71125412e+01,  -7.23573542e+00,   8.59337151e-02, ...,
          -6.72250519e+01,  -8.87803650e+01,   6.36297760e+01],
        [  4.29378033e+00,  -8.53653107e+01,  -5.06322823e+01, ...,
          -7.49750900e+01,  -6.81370010e+01,   3.04435005e+01],
        [ -1.38102093e+01,   6.35605591e+02,   1.25637688e+02, ...,
           3.12087307e+01,   2.33276215e+01,  -9.11381607e+01]],

       [[  1.19963753e+02,   1.25849480e+02,   1.68638840e+01, ...,
           3.44137230e+01,  -8.09484329e+01,  -2.24104404e+01],
        [  3.10278137e+02,   5.95514870e+01,   4.31755753e+01, ...,
           3.43056755e+01,   8.27812862e+00,   1.58922095e-02],
        [  5.67219238e+01,  -4.74581299e+01,  -7.78491516e+01, ...,
           3.26776505e+01,  -2.56649200e+02,   1.01390474e-01],
        ..., 
        [ -1.75327194e+02,  -4.58938560e+01,   6.34404242e-01, ...,
          -3.87439613e+01,  -7.91211367e+00,  -8.94857712e+01],
        [  7.56060104e+01,   1.56444870e+02,  -2.60908478e+02, ...,
           1.68213852e+02,   5.67190979e+02,   4.63721466e+00],
        [ -5.50123482e+01,   1.95592957e+01,  -7.65384293e+01, ...,
           1.04166008e+02,   8.10883789e+01,   5.42043671e-02]]], dtype=float32), array([-0.00303183,  0.00052687,         nan, -0.00276367, -0.00478679,
        0.00573551,         nan, -0.00471952,  0.00341398,  0.00209433,
               nan,         nan,         nan,         nan,  0.01182292,
        0.00603716,  0.0003571 ,  0.00553664, -0.01277675,  0.00630027,
        0.01577267, -0.00024848, -0.00744215,         nan,  0.00770565,
        0.0108814 , -0.00526975,         nan,         nan, -0.00473122,
               nan, -0.00862104,  0.01198716,         nan, -0.00185599,
        0.01280991, -0.00830201,  0.0110528 ,         nan,  0.00829809,
               nan,         nan,         nan,         nan,  0.00017231,
        0.01054838, -0.00451169, -0.00078534,  0.01054926,  0.00073076,
               nan,  0.00404945, -0.01106562, -0.00458357, -0.00236127,
       -0.00407398, -0.0123809 ,  0.01148851,         nan,  0.00426665,
               nan,  0.00728147,  0.00257697,         nan,         nan,
        0.0035989 ,  0.01292581,         nan, -0.00101485,         nan,
        0.00752651,  0.012406  , -0.00747198,         nan,         nan,
        0.00028732,         nan,         nan, -0.00726563,         nan,
               nan, -0.00202225, -0.01011117, -0.00427364,  0.01191354,
               nan, -0.00349384, -0.00765174,         nan,         nan,
       -0.0057144 ,         nan,  0.01273667,  0.00771824,  0.00584222,
        0.00526332,  0.01213388,         nan, -0.00803087,         nan,
       -0.01128767,         nan,  0.00313656,         nan,         nan,
               nan, -0.00105658,         nan, -0.00222129,  0.00126256,
       -0.00483474,  0.00181649,         nan,  0.01274615, -0.0035703 ,
        0.01252608,  0.00395002,  0.00793831,  0.00475878,         nan,
        0.00117349,         nan, -0.00574742, -0.00536615,  0.00187141,
       -0.00195374,  0.01258923,  0.00935928], dtype=float32), array([[[ -1.45296729e+00,  -1.85899353e+01,   1.85860240e+00, ...,
           1.94930897e+01,  -1.47400606e+00,   7.29043732e+01],
        [  2.35075054e+01,  -1.03659582e+01,   9.96242142e+00, ...,
           2.11247150e-02,   2.00708313e+01,  -6.38016558e+00],
        [ -4.31753695e-01,   4.08484497e+01,   1.61803894e+01, ...,
          -6.04199505e+00,   6.41501397e-02,   9.52337086e-01],
        ..., 
        [ -2.21003017e+01,  -1.18904546e-01,  -1.55363083e-01, ...,
          -2.38874340e+00,  -4.60414673e+02,  -9.21500702e+01],
        [ -2.44020879e-01,  -3.07760040e+02,   2.20665344e+02, ...,
           1.41504839e-01,   6.73939667e+01,   7.69760971e+01],
        [ -6.71635568e-02,  -3.54697342e+01,   2.49731827e+01, ...,
           6.77688932e+00,  -1.29692221e+01,   7.68926392e+01]],

       [[ -8.56240845e+00,  -4.88954186e-01,   9.49547005e+00, ...,
          -1.01107016e-01,  -6.38556331e-02,   1.50050968e-01],
        [ -7.05387020e+00,   1.79977454e-02,   2.79764473e-01, ...,
          -2.17094421e+00,   9.58358228e-01,   3.31778831e+01],
        [ -1.36463273e+00,   6.40443116e-02,   7.02730036e+00, ...,
          -3.51816406e+01,  -5.56423092e+00,   1.91442057e-01],
        ..., 
        [ -1.49651198e-02,  -2.20929161e-02,   1.22701049e+00, ...,
           2.77249590e-02,   6.97759092e-02,  -5.58288574e+01],
        [  7.06718369e+01,  -3.51797342e-02,  -6.28136826e+01, ...,
           4.55904268e-02,  -1.19507370e+02,  -1.35787094e+00],
        [  2.48525453e+00,  -8.05968463e-01,  -3.65290785e+00, ...,
           2.53703240e-02,   8.64613712e-01,  -4.69191790e+00]],

       [[  9.13305879e-01,  -2.86780268e-01,   8.63831234e+00, ...,
          -1.64243639e-01,   4.79303040e-02,  -1.34833694e+00],
        [  4.19871140e+00,   2.35252619e-01,   5.80482446e-02, ...,
           1.02745809e-01,  -6.02718890e-02,   2.01890364e-01],
        [ -1.49204123e+00,  -3.45432043e-01,   1.39521313e+00, ...,
           1.53291887e-02,   3.16416211e-02,   1.99632749e-01],
        ..., 
        [  3.85478997e+00,  -4.91042061e+01,   1.10719744e-02, ...,
          -4.59006042e+01,   2.95897903e+01,   3.59708995e-01],
        [  9.65245056e+00,   2.97684765e+01,  -8.52102280e+00, ...,
           5.24145317e+00,   6.61757469e-01,  -7.93726778e+00],
        [ -1.76229572e+00,  -3.71834159e-01,   3.96519870e-01, ...,
           4.75902596e+01,  -1.80244904e+01,  -5.53226042e+00]],

       [[ -5.77183228e+01,   8.02351151e+01,  -8.58829956e+01, ...,
           1.63730297e+01,  -7.77448273e+00,   3.93538404e+00],
        [  1.75697727e+01,  -3.36315422e+01,  -4.92112541e+00, ...,
          -9.84368744e+01,   1.35106974e+01,  -1.07538238e-01],
        [ -2.01689930e+01,  -5.16754723e+01,  -3.77257729e+01, ...,
          -3.77422869e-01,  -1.33594424e-02,   6.95656281e+01],
        ..., 
        [  2.45753989e-01,  -1.22433269e+00,  -2.04659209e-01, ...,
          -4.91189808e-02,   2.15090290e-01,   4.25090647e+00],
        [  4.65992361e-01,  -2.33611241e-02,   1.78628671e+00, ...,
          -6.76035285e-02,   7.90065527e-01,   1.43017685e+00],
        [ -9.79941368e+00,   3.30868387e-03,  -8.15405846e+00, ...,
          -1.62937809e-02,  -2.52626985e-01,  -8.99091840e-01]],

       [[ -4.05211896e-02,   2.87306875e-01,   8.19576073e+00, ...,
           9.16721573e+01,   1.19460239e+01,  -1.11535025e+01],
        [  6.69084883e+00,  -2.85312481e+01,  -1.31200695e+01, ...,
          -2.19567269e-01,   3.48937774e+00,   1.40147152e+01],
        [  9.00131524e-01,   3.83975296e+01,  -8.44452381e-01, ...,
          -2.13302597e-01,  -3.27482596e-02,   3.68042976e-01],
        ..., 
        [  4.49458542e+01,  -1.46812403e+00,   1.56103373e+00, ...,
          -3.38927956e+01,   1.97558090e-01,  -1.12726234e+02],
        [  3.74297786e+00,  -2.31305864e-02,  -1.06778107e+01, ...,
           3.03148955e-01,   2.86338590e-02,  -5.93775749e+00],
        [ -1.43102903e+01,  -3.51021767e+01,  -7.25477314e+00, ...,
           7.86737502e-02,  -4.04430218e-02,  -1.03887154e+02]]], dtype=float32), array([ 0.00721407, -0.01080991,  0.00202953,         nan, -0.00871999,
       -0.00979585, -0.0051407 ,         nan,  0.01252265, -0.00181422,
       -0.00819429,         nan,  0.00209477,  0.00645181, -0.00805668,
               nan, -0.00669367,  0.0101859 , -0.00478634,         nan,
               nan, -0.01159984, -0.00527002,  0.01011875,         nan,
        0.01290197,         nan, -0.00313366,  0.01090364,         nan,
               nan, -0.00534851], dtype=float32), array([[[ -8.29690695e-01,   1.51981525e-02,   1.80010319e-01, ...,
           5.66750526e-01,  -4.03445870e-01,   2.89482236e-01],
        [ -1.01449096e+00,   2.62717247e+00,   1.47955343e-02, ...,
          -2.47280979e+00,   3.46152306e-01,  -1.45272362e+00],
        [ -5.35625994e-01,   9.26369488e-01,   1.48123455e+00, ...,
           2.37583661e+00,  -9.48905349e-01,   4.46271926e-01],
        ..., 
        [ -4.07793236e+00,   9.04592097e-01,   4.52610940e-01, ...,
           7.79820025e-01,   8.25595617e-01,  -2.73653299e-01],
        [  1.01398051e+00,   2.77464420e-01,  -4.08575125e-02, ...,
          -7.94092119e-02,  -4.73003477e-01,  -3.22694874e+00],
        [  1.34389770e+00,   7.39578426e-01,  -1.99999583e+00, ...,
           1.59634852e+00,  -4.43105774e+01,   3.48869658e+00]],

       [[  4.49796051e-01,   1.62432404e+01,   2.99024284e-01, ...,
           2.39469528e+00,   3.63545680e+00,   1.33222399e+01],
        [ -1.25804222e+00,   5.93616903e-01,  -2.24094465e-01, ...,
           7.14776611e+00,  -6.07950096e+01,  -3.97061610e+00],
        [  5.73225451e+00,   3.44473243e-01,   4.89822502e+01, ...,
          -1.02140198e+01,  -8.63099694e-02,   1.03545511e+00],
        ..., 
        [ -2.90418053e+00,   1.66334426e+00,   5.34012127e+00, ...,
           5.08385658e+00,  -8.84643495e-01,   9.50575471e-02],
        [  2.43404126e+00,   3.19609120e-02,   1.15552902e+00, ...,
          -1.97065187e+00,  -2.18373016e-02,  -1.87884569e+00],
        [ -1.10491252e+00,  -7.00622857e-01,  -2.75327936e-02, ...,
          -1.67351559e-01,  -3.56536369e+01,   4.31534672e+00]],

       [[ -2.98323005e-01,   1.95138149e+01,  -6.61930680e-01, ...,
          -8.12640414e-02,   1.63551956e-01,  -1.92686641e+00],
        [ -2.47889729e+01,   2.75876876e-02,  -2.04139900e+00, ...,
          -4.11038667e-01,   2.69464999e-01,  -8.74109194e-02],
        [  5.85711956e+00,   3.21791115e+01,   3.46343189e-01, ...,
           4.58834410e+00,   6.28389931e+01,  -8.28920782e-01],
        ..., 
        [ -4.79541969e+00,   1.23082197e+00,   2.88488054e+00, ...,
           4.17237043e+00,   9.18720588e-02,   2.71237111e+00],
        [ -6.11321092e-01,   9.34896886e-01,   3.61941648e+00, ...,
          -2.09522057e+00,  -4.29104716e-02,   2.33770385e-01],
        [ -2.10204840e+00,   2.41703320e+00,   4.08353806e-01, ...,
          -1.11578882e+00,   6.10901237e-01,   3.12420082e+00]],

       [[  3.17858648e+00,  -1.15199342e-01,   7.99448907e-01, ...,
          -6.94288686e-02,  -3.78224194e-01,  -2.11228147e-01],
        [ -1.01672611e+01,   2.29932928e+00,  -8.23774040e-02, ...,
           1.10687828e+01,  -5.56976497e-01,  -4.18549194e+01],
        [  4.57473211e-02,  -1.62644148e+00,   1.10198345e+01, ...,
           1.72234133e-01,   4.72544543e-02,   9.63192642e-01],
        ..., 
        [  1.93458311e-02,   4.66402806e-02,   1.37197465e-01, ...,
           2.74113345e+00,   1.18698873e-01,   1.47488102e-01],
        [ -3.48606199e-01,   7.13406861e-01,   8.62872303e-01, ...,
           3.86153483e+00,  -3.22620296e+00,   1.43851032e+01],
        [ -1.28992152e+00,  -3.58179271e-01,  -1.59407692e+01, ...,
          -2.38189086e-01,   6.28528297e-01,   1.44906259e+00]],

       [[  1.38401061e-01,   1.56333103e+01,   3.94406766e-02, ...,
          -7.97948956e-01,  -8.32110122e-02,  -2.61989832e-01],
        [ -1.69780898e+00,   1.22613108e+00,   1.13238543e-01, ...,
          -5.42741203e+00,   5.35991639e-02,  -1.43280470e+00],
        [ -4.69797134e-01,  -1.67963159e+00,   8.70674551e-01, ...,
          -5.95084608e-01,  -4.11402397e-02,   5.14732115e-03],
        ..., 
        [ -1.86762452e+00,  -2.84470558e-01,   4.23804569e+00, ...,
           1.27319336e+01,  -1.56363716e+01,   2.92534500e-01],
        [ -5.73735118e-01,  -8.57480049e-01,   3.85818887e+00, ...,
          -7.57689953e+00,   1.69330168e+00,   1.80779419e+01],
        [ -3.45315456e+00,  -1.05621368e-02,  -4.53398257e-01, ...,
           2.06666350e+00,   7.84944892e-02,   3.17968488e+00]],

       [[ -4.06911325e+00,   1.65015548e-01,  -1.14256665e-01, ...,
           8.55081439e-01,  -1.74662575e-01,  -1.13633204e+00],
        [ -4.77266431e-01,   1.00441754e+00,   4.50561941e-01, ...,
          -7.69126475e-01,  -5.32188594e-01,  -8.28091562e-01],
        [  5.19197750e+00,  -2.57612914e-01,   4.44432318e-01, ...,
           1.72394907e+00,  -9.16882098e-01,  -1.48226023e+00],
        ..., 
        [ -2.42185307e+01,  -6.19122810e+01,   1.09572971e+00, ...,
           1.69736347e+01,   1.08710063e+00,   9.96017396e-01],
        [ -2.44375134e+01,   8.00353050e-01,   5.87797318e+01, ...,
          -9.42115688e+00,   9.32907388e-02,  -3.28873873e+00],
        [ -9.75492287e+00,  -4.38124895e-01,  -4.97199476e-01, ...,
          -3.35016060e+00,  -6.08749568e-01,   6.51416397e+00]]], dtype=float32), array([-0.00091459, -0.00620937, -0.00180343,  0.01052671,  0.00611617,
        0.00269699, -0.00578218,  0.01065915, -0.00869957,  0.01294277,
       -0.00639585, -0.0065877 , -0.00597737, -0.00342507, -0.0046289 ,
       -0.00190541,  0.00475265,  0.00023435,  0.00337634, -0.00540435,
       -0.00607256, -0.00419708,  0.01165059,  0.01108866, -0.00320541,
        0.01025646,  0.00874494,  0.01261826, -0.00963546,  0.00792126,
       -0.00331757,  0.01240073], dtype=float32), array([ 0.92324513,  0.91189975,  0.68247992,  0.611655  ,  1.11188841,
        1.24183393,  0.78877187,  0.94186455,  1.27621102,  0.95670766,
        1.2502116 ,  0.95994872,  0.80079126,  1.22881901,  1.41063344,
        1.07295191,  1.24689925,  0.9035126 ,  1.27278912,  1.34401119,
        1.23818433,  1.2192806 ,  1.19316161,  1.38558877,  1.18151259,
        0.70934945,  1.14419794,  0.70987469,  0.9374221 ,  1.27000499,
        1.3482002 ,  0.78130794,  1.02721739,  1.192191  ,  0.91440892,
        0.62458515,  0.81259495,  0.72212356,  1.28875387,  0.71674573,
        0.77234554,  0.79238915,  1.33870089,  0.8651765 ,  1.2067343 ,
        1.30049622,  0.77098888,  1.29051161,  1.23092282,  0.62862951,
        1.18639112,  1.28262627,  0.94983268,  0.65229762,  1.37430549,
        0.82133031,  1.33457494,  1.38797319,  0.78886795,  1.26026869,
        0.86639565,  0.66859865,  1.36260259,  1.06364739,  0.8305313 ,
        1.14732647,  1.18739545,  0.99207777,  0.7789138 ,  0.63479638,
        0.75854886,  1.22457278,  0.87885702,  1.41933584,  1.28638506,
        0.71523923,  1.14043164,  0.81472951,  1.39107215,  1.17304611,
        1.30299258,  0.81510711,  0.84077132,  1.22211564,  0.9681775 ,
        0.9281354 ,  1.23058033,  0.79885459,  1.24401665,  0.8758862 ,
        1.25450766,  0.84477127,  0.86915898,  0.63901693,  0.62361622,
        0.88031119,  1.35723126,  0.84122556,  0.76101047,  0.9330076 ,
        0.72560656,  0.67724234,  0.86384225,  1.01724434,  0.93573648,
        0.76780319,  1.31037223,  1.07925224,  0.81563425,  1.30601621,
        0.94119728,  1.2825371 ,  0.97992492,  1.22965431,  0.53950727,
        1.23549151,  0.59556764,  1.27590203,  1.25413597,  1.23823452,
        0.70959306,  0.62548274,  0.87274724,  1.23207903,  1.36108971,
        1.29653132,  1.18485701,  1.07082903,  0.82756746,  0.74832225,
        0.86865455,  0.93214756,  1.01476288,  0.9942596 ,  0.80549979,
        0.97460073,  0.92048162,  0.77916354,  0.78707469,  0.83674943,
        1.01871479,  0.78273112,  0.81403154,  0.88662291,  0.77351379,
        0.87571979,  0.92801058,  0.74901462,  0.77704167,  0.83975536,
        1.22084785,  1.0269258 ,  0.7466014 ,  1.32603633,  0.73203731,
        0.88649112,  0.7401551 ,  0.87404191,  0.89444137,  0.81067967,
        1.4649024 ,  0.75762224,  0.93519962,  0.74893814,  0.95110834,
        1.280568  ,  0.87244642,  0.7525841 ,  0.94060266,  0.77005023,
        1.25788486,  0.94583333,  0.87172806,  1.08644712,  0.91021127,
        0.86235201,  1.05104804,  1.22341335,  0.86605495,  1.27367413,
        0.76196283,  0.85116506,  0.70819497,  0.78221506,  1.0321089 ,
        1.12106144,  1.44357586,  0.75676209,  1.01206946,  1.02126384,
        1.32443583,  0.99048209,  1.03620136,  0.83227968,  0.79153019,
        0.82600629,  0.84851086,  0.81706792,  1.24497879,  1.26714015,
        0.77497232,  0.72850466,  0.80097783,  1.12743843,  0.81188363,
        0.77680135,  0.9681018 ,  0.86912555,  0.82433802,  1.08783257,
        0.77873075,  1.08205795,  0.73998272,  1.29796326,  1.03962433,
        0.79080981,  0.87378049,  1.25883913,  0.92012763,  0.99345338,
        1.16669178,  0.82504064,  0.91854829,  1.05217016,  1.02248108,
        0.82858986,  1.02929378,  1.09943044,  0.87474597,  0.8325696 ,
        0.75225061,  0.82186985,  0.88399786,  0.99284208,  0.77929509,
        0.80184114,  0.92587566,  1.37938547,  1.10543239,  0.79352069,
        0.80869478,  0.85203063,  0.89897668,  0.75929564,  0.86093128,
        0.87196112,  1.38894391,  0.86825073,  0.74169791,  0.76852977,
        0.91970271,  0.72150081,  0.73341006,  0.84463739,  0.83543485,
        1.34363723,  0.83324951,  0.92552757,  1.02850258,  0.71077508,
        0.93726856,  1.04080963,  0.85020447,  0.81692302,  0.82796645,
        0.6400485 ,  0.62768459,  0.947245  ,  0.88588703,  0.93196899,
        0.93230802,  0.95469648,  0.92390162,  0.84833068,  0.94646561,
        0.89167571,  0.63733917,  0.9116236 ,  0.82794297,  0.91070575,
        0.8822006 ,  0.98996753,  0.88207477,  0.88646972,  0.8521868 ,
        0.89941269,  0.77922833,  0.84261811,  0.88188922,  1.05048823,
        0.93200248,  0.98309511,  1.02783   ,  0.8090719 ,  0.70652324,
        0.73236096,  0.90449786,  0.87350255,  0.84028536,  1.10842824,
        0.9247309 ,  0.96409667,  0.7645945 ,  0.97348124,  0.83638608,
        0.98999095,  0.85562307,  0.95512122,  0.72635466,  0.88122612,
        0.91798496,  0.70071429,  0.82288027,  1.11360848,  0.95942098,
        1.11058652,  0.92326015,  0.98234093,  0.72201574,  0.87033051,
        0.77146524,  1.00500035,  0.72814441,  0.719607  ,  1.03981459,
        0.68174195,  1.01526725,  0.86931908,  0.82885873,  0.9725039 ,
        0.89089197,  1.05959928,  0.8540464 ,  0.90424043,  0.81220067,
        0.84160411,  0.6650098 ,  0.95070642,  0.90326285,  0.81896961,
        0.98918557,  0.89003968,  0.90541989,  0.92553186,  0.87018538,
        0.73151058,  0.85450697,  0.924833  ,  0.67955649,  0.8936466 ,
        0.96428728,  1.00697994,  1.1095928 ,  0.96397698,  0.91212779,
        0.98802084,  0.87135857,  0.72244912,  0.88761735,  0.9826352 ,
        0.90775639,  0.814991  ,  0.97274172,  0.87164116,  0.87200755,
        0.87039524,  0.86348069,  0.60782027,  0.99143809,  0.82865047,
        0.76348358,  0.82481432,  1.00521755,  0.95388275,  0.93471348,
        0.80328858,  0.97608858,  0.90786624,  0.78293693,  0.9387188 ,
        1.05676043,  0.94996649,  0.90885735,  0.76560307,  0.84717047,
        0.79366243,  0.98399621,  0.71567577,  0.66029119,  0.85207433,
        1.00126958,  0.69157439,  0.9107157 ,  0.85799575,  0.92462587,
        0.88748997,  0.93318915,  0.99592036,  0.7729736 ,  0.75125623,
        0.7421515 ,  0.7371279 ,  0.8069917 ,  0.89432222,  0.87229955,
        0.87646377,  0.64453566,  0.91080266,  0.97707778,  0.68118942,
        0.60430533,  0.92588139,  0.8567751 ,  0.93447566,  0.94679332,
        0.67847568,  0.83453822,  0.91654968,  0.92968297,  0.69324344,
        0.75036299,  0.77830368,  0.92005759,  0.87036562,  0.9363668 ,
        0.88637435,  0.80963498,  0.76762754,  0.92501837,  0.9008624 ,
        0.88113987,  0.83168811,  0.94614244,  0.82872683,  0.7752685 ,
        0.69449437,  0.60647815,  0.87106222,  0.6449452 ,  0.91489285,
        0.83349478,  0.76603216,  0.85015899,  0.89259475,  0.96830028,
        0.84974003,  0.77153814,  0.89535224,  0.9256832 ,  0.8008877 ,
        0.80640543,  0.83236629,  0.71014738,  0.76220351,  0.80116212,
        0.84920239,  0.71114242,  0.91479725,  0.78226274,  0.94460845,
        0.94747585,  0.97313702,  0.59300864,  0.92920971,  0.96162492,
        0.75395739,  0.55505848,  0.84639651,  0.64516205,  0.89154035,
        0.87502444,  0.62925559,  0.96463913,  0.91576618,  0.65746689,
        0.68906087,  0.66600591,  0.78059584,  0.94482565,  0.83121347,
        0.93048126,  0.89082116,  0.73019499,  0.89990819,  0.82093835,
        0.77312094,  0.87820232,  0.88318926,  0.94799238,  0.75950092,
        0.8085044 ,  0.6941905 ,  0.80920118,  0.93610948,  0.57750458,
        0.73357487,  0.8286469 ,  0.90382361,  0.6823318 ,  0.9106403 ,
        0.93044192,  0.87453932,  0.86136389,  0.78060842,  0.92122746,
        1.00319707,  0.68839288,  0.64943296,  0.79142654,  0.86752546,
        0.70153332,  0.91624457,  0.94828254,  0.83849299,  0.79837543,
        0.87679446,  0.8695612 ,  0.86987436,  0.86089462,  0.92754692,
        0.6679092 ,  0.66677892,  0.95932406,  0.97985977,  0.9611063 ,
        0.85794193,  0.87591904,  0.94417387,  0.94389588,  0.89519042,
        0.74272192,  1.01115835,  0.95259851,  0.86745769,  0.92278951,
        0.72078449,  0.93177384,  0.91099179,  0.94293749,  0.7468816 ,
        0.91201252,  0.590123  ,  0.92652237,  0.92053473,  0.98247588,
        0.86670363,  0.89657253,  0.95276397,  1.09993672,  0.84992331,
        0.61647707,  1.0624218 ,  0.9760294 ,  0.84544462,  0.93171477,
        0.66878033,  1.00695813,  0.97923803,  0.6804266 ,  0.95166844,
        0.99310899,  1.08601403,  1.04561758,  1.13575077,  0.66119641,
        1.04587078,  0.97103989,  1.0984236 ,  1.10031962,  0.91157776,
        1.19642639,  0.7539537 ,  0.85630423,  0.75334775,  0.83235133,
        0.94932777], dtype=float32), array([ 0.21513477,  0.25235781, -0.01164734,  0.00812957,  0.0469191 ,
        0.08426256,  0.02624334,  0.18139027,  0.13159387,  0.08998515,
        0.14502546, -0.04822301,  0.01167602,  0.10503929,  0.14026111,
        0.04108373,  0.03667439,  0.20382529,  0.18656163,  0.01321671,
        0.08295929,  0.16116051,  0.0947966 , -0.04603532,  0.09742413,
       -0.07467403,  0.24833094,  0.08291947,  0.01852527,  0.14373298,
        0.05248534,  0.20675589,  0.13275369,  0.07756286,  0.41444346,
       -0.07379562,  0.03522586,  0.1551806 ,  0.12376447, -0.08178429,
        0.07966474,  0.16586709,  0.05122001,  0.00604945,  0.22456609,
        0.13572276,  0.03780139,  0.17800449,  0.20268792,  0.01079178,
        0.09938767,  0.18528275,  0.20099923,  0.06780296,  0.00305275,
        0.09349702,  0.13340999,  0.18285613, -0.03720064,  0.20336342,
        0.15889709, -0.044036  ,  0.19432218,  0.0673499 ,  0.08878616,
       -0.00126485,  0.16699122,  0.21126172,  0.12083656, -0.0176493 ,
        0.16961034,  0.07944823,  0.03459396,  0.19645755,  0.13866211,
        0.06702986,  0.00886732, -0.02252088,  0.19451457,  0.16268517,
        0.15687318,  0.00331449,  0.09069867,  0.25872436, -0.12164863,
        0.16358218, -0.00682826,  0.03762042,  0.15828508,  0.07392085,
        0.13210936,  0.06433506,  0.18269709, -0.00348008,  0.04271064,
        0.04527429,  0.17402175,  0.12602551, -0.03505278,  0.14879747,
        0.00327524, -0.00505609,  0.00789965,  0.24499258,  0.18764867,
        0.02787114,  0.12370888,  0.18646622,  0.08006537,  0.31765482,
        0.09984814,  0.22597547,  0.10502543,  0.10769939, -0.08103713,
        0.14144586, -0.05742846,  0.19790782,  0.14855613,  0.1048301 ,
        0.0265718 , -0.09612875,  0.06837879,  0.09417342,  0.10076804,
        0.21487097,  0.11882482,  0.1344099 ,  0.01039768,  0.01046966,
        0.12080838,  0.17098039,  0.23318933,  0.13605478, -0.01817689,
        0.08002071,  0.07361093, -0.07623066,  0.06926445,  0.17542176,
        0.14412902,  0.22412172,  0.05991406,  0.09023374,  0.15624791,
        0.08426542,  0.17060365, -0.07120038, -0.05670423,  0.03806323,
        0.10895428,  0.31922793,  0.11434312,  0.10919707,  0.14502959,
        0.1610183 ,  0.00681128, -0.08269095,  0.24219224,  0.05926536,
        0.14226501,  0.14173776,  0.06112398,  0.03121125,  0.12308633,
        0.09533465, -0.02837656, -0.02005078,  0.11338665, -0.01487743,
        0.10338601,  0.1129363 ,  0.19798772,  0.1456272 ,  0.19310625,
        0.10701656,  0.25977972,  0.15427302,  0.05986759,  0.22551021,
        0.09458067,  0.10495061,  0.09350866,  0.06876841,  0.1423347 ,
        0.309156  ,  0.18146063,  0.06937418,  0.25378922, -0.03416175,
        0.02536071,  0.09677454,  0.14686505,  0.19095816,  0.07507709,
        0.18157071,  0.3017295 ,  0.10140018,  0.16234942,  0.19093814,
        0.09805069,  0.0550074 ,  0.13936727,  0.1347481 ,  0.19129212,
        0.09916603,  0.11527727,  0.12738715, -0.04907956,  0.20409778,
       -0.05126302,  0.07003123,  0.07933259,  0.15789157,  0.16495503,
        0.0382163 ,  0.16530749,  0.09226055,  0.29120147,  0.21301824,
        0.04310212, -0.05778586,  0.00651748,  0.22950931,  0.17440093,
        0.08809182,  0.09717909,  0.08513435,  0.19778129, -0.01846456,
       -0.05024394,  0.1230069 ,  0.04975546,  0.08784546,  0.00666083,
        0.11113517,  0.23783061,  0.17325942,  0.12386075,  0.0473346 ,
        0.03064803,  0.17498848,  0.24725163,  0.06897967,  0.03023582,
        0.13333851,  0.13429347, -0.02490993,  0.10004763, -0.00632021,
        0.01631161, -0.02700441,  0.17108987,  0.13485585,  0.05454494,
        0.09426814,  0.16256647,  0.05861462,  0.09804872, -0.03055473,
        0.14543988,  0.05096266,  0.10780343, -0.0242221 ,  0.00553415,
        0.0134938 , -0.02397695,  0.06497488,  0.02675983, -0.17460628,
       -0.04864439,  0.21374881,  0.12452196, -0.07948656,  0.31697804,
       -0.11074805,  0.03184726,  0.11444523, -0.01209346,  0.37230873,
        0.05219135,  0.15265796,  0.01635455,  0.16254823, -0.05262664,
        0.1656882 ,  0.04892838,  0.04358613, -0.01831526,  0.14575137,
       -0.04428541,  0.06114314,  0.01055301,  0.0806742 , -0.03822853,
        0.0024281 ,  0.23819065,  0.14820386,  0.15432395,  0.06820562,
        0.14084032,  0.0214096 ,  0.28960019,  0.03336281, -0.02258998,
       -0.11918389,  0.12191802,  0.12011044, -0.0383323 ,  0.09663165,
       -0.04327046,  0.23696648,  0.03797962,  0.16609034,  0.00134832,
        0.12600201,  0.06433844,  0.2159729 ,  0.12042333,  0.184242  ,
        0.07827154, -0.07142582,  0.13497566,  0.12408919,  0.08139872,
        0.08778475,  0.05593739, -0.07034422, -0.02145396,  0.1858903 ,
        0.13324609,  0.21348356,  0.06271571,  0.24681482,  0.03502885,
        0.09164923, -0.09670884,  0.13099472, -0.06682523,  0.06847583,
        0.15508497, -0.03456884,  0.17532405,  0.06655746,  0.06141814,
        0.09758208, -0.08340237, -0.07836234, -0.04713979,  0.26549336,
        0.08241519,  0.26755059,  0.15660191,  0.0140149 ,  0.06114695,
        0.1236652 ,  0.20641492, -0.0086607 ,  0.25696442,  0.16652805,
       -0.0189483 ,  0.08821324, -0.0561601 ,  0.09646437,  0.05534786,
        0.06144878,  0.08153529, -0.03351871,  0.18770348,  0.21821322,
        0.00923865,  0.05793909, -0.03316918, -0.01503262, -0.08808464,
        0.06295279,  0.03633495,  0.13560405, -0.09101282,  0.0784924 ,
        0.13943812,  0.07187323,  0.07224862,  0.00436772,  0.1421919 ,
        0.13261436, -0.04384675,  0.06922174, -0.05535286,  0.01790292,
        0.02594931,  0.03137456,  0.14343001,  0.13875815,  0.18751529,
        0.18230127,  0.16239385, -0.04877168,  0.05408951,  0.13819633,
        0.04689167,  0.00860408, -0.05521136,  0.08899331,  0.12256701,
        0.09942619,  0.05431732,  0.01246066,  0.2459348 ,  0.03505556,
        0.00352251,  0.10704872,  0.13250764,  0.09410823,  0.21223374,
       -0.06725354,  0.20313561,  0.05718818,  0.15772417,  0.08655146,
        0.18685333,  0.24251266,  0.04728935,  0.08336292, -0.01416809,
        0.11174773,  0.00846751,  0.09750774,  0.17642273,  0.20093188,
        0.08958908,  0.06244315,  0.06603803,  0.12445851,  0.10244825,
        0.03678581, -0.03146201,  0.07758824,  0.00737987,  0.05445781,
        0.08941389,  0.15300757,  0.01156699,  0.18133481,  0.02707617,
        0.01896188,  0.00373267,  0.05308403,  0.0893611 ,  0.07354195,
        0.2719883 ,  0.06033295,  0.11146184, -0.0334769 ,  0.17474252,
        0.0170496 , -0.03238125,  0.07332145,  0.00221918,  0.10075361,
        0.14380831,  0.05464551,  0.08960186,  0.08277151,  0.15450172,
        0.07375888, -0.00090652,  0.0227281 ,  0.03489426, -0.00458596,
        0.15148938, -0.02450214,  0.21064126,  0.20087112, -0.08557961,
        0.03035768,  0.00841392,  0.15346257,  0.09999306,  0.15065986,
        0.13216971,  0.14718296,  0.1095356 ,  0.00530122,  0.2183297 ,
        0.11066475,  0.25523499,  0.20002022, -0.11860739,  0.06214419,
       -0.01568105,  0.27799031,  0.14767797,  0.10479995,  0.10038255,
        0.04629319,  0.09810003,  0.07883403, -0.00560048,  0.23447773,
        0.2438803 ,  0.20737502,  0.13416626,  0.28730634,  0.05690536,
       -0.06244919,  0.03087391, -0.00481488,  0.20564254,  0.12845384,
        0.06240393,  0.13889894,  0.14797753,  0.15924339,  0.20609157,
        0.12777169,  0.16959696, -0.0650304 ,  0.23496245,  0.09176328,
        0.10268437, -0.01722782,  0.11177473,  0.01012664,  0.05449714,
        0.02712695,  0.24581461,  0.19195949,  0.06926737,  0.06519087,
        0.14836258, -0.01333803,  0.13641383,  0.06160649,  0.29746476,
        0.03355586,  0.06637412,  0.15873741,  0.04920393, -0.0238049 ,
       -0.083992  , -0.06237444,  0.04802573,  0.07770825,  0.10175433,
        0.11417089,  0.17024639,  0.21109106,  0.20851119,  0.23614784,
       -0.00223412,  0.0691645 ,  0.10060007,  0.18423948,  0.12191705,
       -0.05729315,  0.00972765,  0.13062176,  0.03442303,  0.14358045,
        0.21901843,  0.0838166 ,  0.19728287,  0.02123089, -0.03895708,
        0.2586911 ,  0.28226635,  0.13332252, -0.10844424,  0.13561204,
        0.19466975,  0.0516293 ,  0.07846542,  0.06355014,  0.05162555,
        0.17901592], dtype=float32), array([  5.29013176e+01,   3.68298279e+02,   1.58932693e+02,
         3.13076721e+02,   1.72377571e-01,   1.25030443e-01,
         1.27369705e+02,   2.41604366e+01,   1.39345556e-01,
         5.12570478e-02,   1.63797006e-01,   2.24519502e-02,
         5.02344131e+01,   3.71608138e-03,   1.11178495e-01,
         5.23512065e-03,   9.21985656e-02,   4.72296448e+01,
         2.08315969e-01,   1.15286283e-01,   3.14273615e-03,
         1.53550237e-01,   1.37613684e-01,   1.96638629e-01,
         1.86812878e-01,   5.63835632e+02,   1.55304492e-01,
         8.92956314e+01,   7.52598206e+02,   1.51206240e-01,
         1.17755756e-01,   5.77467918e+01,   2.37712509e+02,
         1.44288942e-01,   8.16054993e+01,   3.33516815e+02,
         7.57081528e+01,   7.09654770e+01,   1.40709385e-01,
         6.22298622e+01,   7.43702087e+01,   1.32977905e+02,
         1.41323179e-01,   5.85510597e+01,   2.54116893e-01,
         2.97157839e-03,   7.30314697e+02,   1.31483257e-01,
         2.38155365e+01,   3.59857605e+02,   1.24962874e-01,
         1.42724335e-01,   2.33147354e+01,   6.62400131e+01,
         1.57808691e-01,   8.03918228e+01,   1.26261771e-01,
         3.26975831e-03,   5.21077766e+01,   2.89877045e+02,
         4.42206497e+01,   1.89649010e+01,   1.23163067e-01,
         1.12137894e+02,   7.10229568e+01,   6.70116699e+02,
         1.77359521e-01,   6.12951698e+01,   8.30807571e+01,
         4.12999512e+02,   4.19509552e+02,   1.13279961e-01,
         1.69872162e+02,   1.50238186e-01,   1.35774821e-01,
         7.70510330e+01,   1.30124778e-01,   8.54601288e+01,
         1.39189273e-01,   1.56786576e-01,   1.49796903e-01,
         1.60520996e+02,   4.16353516e+02,   1.85329527e-01,
         1.60984680e+02,   1.20601280e+02,   1.14089131e-01,
         1.61643646e+02,   1.85202137e-01,   5.05330544e+01,
         1.30995050e-01,   2.30380801e+04,   3.75291656e+02,
         7.00347748e+01,   2.31097015e+02,   6.78052444e+01,
         1.08042598e-01,   6.86257095e+01,   8.28457642e+01,
         4.24259071e+01,   2.31480072e+02,   1.39489395e+02,
         1.76346302e+01,   3.17238495e+02,   4.91219177e+01,
         1.58652771e+02,   1.40976071e-01,   7.54999466e+01,
         2.15405838e+02,   1.68837905e-01,   8.79115601e+01,
         1.47948965e-01,   9.01728745e+01,   1.67503044e-01,
         5.95936133e+04,   1.90694198e-01,   4.04954987e+01,
         1.24202691e-01,   1.15148850e-01,   1.81677163e-01,
         3.56195259e+01,   8.88218918e+01,   9.05546112e+01,
         1.79674968e-01,   1.14744417e-01,   1.20044470e-01,
         1.58811241e-01,   2.82923549e-01,   4.59666539e-04,
         6.35243164e+02,   6.48225220e+02,   4.00935181e+02,
         1.01392296e+02,   7.27658264e+02,   9.59493179e+01,
         6.60831421e+02,   1.60758087e+02,   2.94946747e+02,
         2.69049683e+02,   2.02686279e+02,   4.14429108e+02,
         2.34200882e+02,   5.25431702e+02,   2.19347366e+02,
         1.22508453e+02,   2.69395874e+02,   1.75890030e+02,
         1.75082626e+02,   1.97738479e-03,   6.86364746e+02,
         7.31368004e-07,   3.50071350e+02,   3.18454803e+02,
         9.92241949e-02,   3.27594757e+02,   6.58869202e+02,
         2.37637222e+02,   1.56215729e+02,   1.98578064e+02,
         1.89657013e+02,   9.23661217e-02,   2.23594482e+02,
         3.97281372e+02,   4.90556152e+02,   8.02289734e+02,
         1.02253422e-01,   2.50522797e+02,   4.50917358e+02,
         7.23524658e+02,   3.86128876e+02,   2.12493181e-01,
         1.42017792e+02,   1.15965393e+02,   7.42478027e+01,
         6.34436707e+02,   8.22630066e+02,   1.59623749e+02,
         1.54434860e-01,   1.92384872e+02,   1.13033667e-01,
         4.16869354e+02,   3.87392700e+02,   6.75103638e+02,
         1.33600723e+02,   4.61286163e+01,   5.69286194e+01,
         1.07615136e-01,   4.87922241e+02,   1.09750562e+03,
         2.57252930e+02,   9.13989022e-02,   3.43379810e+03,
         3.40020416e+02,   3.12283386e+02,   4.95360107e+02,
         5.79436890e+02,   5.34526550e+02,   4.00680420e+02,
         2.36818880e-01,   1.32217571e-01,   1.29922986e+03,
         6.07961243e+02,   8.96932007e+02,   9.17435303e+02,
         4.57122894e+02,   5.17504395e+02,   6.48910522e+02,
         3.30657166e+02,   1.75505783e+02,   2.80984344e+02,
         1.74758804e+02,   2.08989716e+02,   6.61557556e+02,
         9.93769243e-02,   8.19757080e+01,   5.00759491e+02,
         6.07085571e+02,   6.29775635e+02,   4.57315735e+02,
         9.63794174e+01,   7.09169617e+01,   6.01780151e+02,
         3.06086140e+01,   2.90993958e+02,   2.32205658e+02,
         8.25352295e+02,   3.10921841e+01,   1.76693085e+02,
         1.46209703e+05,   3.44789154e+02,   5.03325165e+02,
         5.48049453e+04,   1.00943123e+02,   6.24874420e+01,
         6.31356812e+02,   1.50489899e+02,   1.10429832e+02,
         1.20430648e-01,   4.27515198e+02,   1.77617310e+02,
         2.28568527e+02,   1.99913818e+02,   2.26644638e+02,
         2.15966812e+02,   2.24030502e+02,   1.01033301e+03,
         1.19160265e-01,   3.25658752e+02,   4.94112366e+02,
         3.66817261e+02,   4.21857330e+02,   3.44939087e+02,
         1.80054428e+02,   6.33526184e+02,   5.76601624e+02,
         8.18330422e-02,   7.79961914e+02,   9.31102234e+02,
         3.59356964e+02,   9.81664297e+04,   6.03922791e+02,
         6.64522949e+02,   1.92235596e+03,   1.43135742e+03,
         1.06152905e+03,   9.14043281e+04,   7.49058594e+04,
         1.84903784e-05,   8.60325439e+02,   1.34697617e+02,
         1.54310120e+02,   1.94524988e+03,   3.11508593e-05,
         1.79827976e+00,   4.11105988e+02,   1.35709229e+03,
         2.41047754e+04,   4.64194244e+02,   2.31814990e+03,
         2.46742065e+02,   2.20986143e-01,   2.88030457e+02,
         2.32577686e+03,   4.62291473e+02,   3.78230071e+00,
         7.18686157e+02,   4.25875946e+02,   4.60051636e+02,
         1.18072180e+03,   9.04094925e+01,   3.29402496e+02,
         3.36625244e+02,   7.97540299e-04,   7.79839050e+02,
         9.12524609e+04,   1.38928953e+05,   5.20552185e+02,
         7.71098877e+02,   1.30964404e+03,   4.08867432e+02,
         4.43175415e+02,   1.75749694e-04,   2.26342630e+00,
         5.86277398e-04,   5.19271057e+02,   6.75552597e+01,
         1.29423145e+03,   1.35464111e+02,   2.08412227e+04,
         4.68114471e+02,   3.89939669e-05,   9.54547500e+04,
         2.03766650e+03,   1.17901642e+02,   5.55593811e+02,
         3.91676003e-04,   1.22785324e+02,   3.87807983e+02,
         1.00350266e+05,   9.06684326e+02,   8.55342407e+02,
         5.38626546e-06,   5.68653687e+02,   1.19883648e+05,
         3.63806159e-08,   3.39570703e+04,   1.63535748e-04,
         6.88466579e-02,   8.01904297e+02,   3.44243506e+03,
         2.89363708e+02,   2.69104736e+02,   1.01401208e+03,
         8.34419495e+02,   4.95362549e+02,   9.31297485e+02,
         9.64531797e+04,   2.63933258e+02,   3.45696808e+02,
         9.18420715e+02,   2.12337509e+02,   1.17244080e+03,
         7.64988770e+02,   1.53447974e+03,   7.86241516e+02,
         8.23347109e+04,   8.06836487e+02,   7.87529815e-03,
         2.33504156e+05,   7.90595459e+02,   4.00759396e-04,
         6.31914185e+02,   1.96726654e+02,   8.74345642e-05,
         7.12424844e+04,   1.28315112e+03,   1.38505359e+03,
         2.76464375e+04,   2.04282532e+01,   2.29135986e+02,
         2.07299232e+00,   1.08086279e+03,   4.14172973e-04,
         1.24269482e+03,   7.39336304e+02,   1.50823947e-02,
         7.26363623e+03,   6.34344188e+05,   2.47653137e+02,
         1.41914075e+03,   1.41438049e+03,   1.21932325e+06,
         6.47305359e+02,   5.80179321e+02,   2.17027217e-03,
         3.71571719e+04,   1.24817664e+03,   4.98597219e-04,
         1.77981030e+03,   3.95596405e+02,   1.22562109e+03,
         4.32819914e-04,   4.60019958e+02,   1.41190100e+03,
         9.41332129e+03,   1.09653809e+04,   3.15492647e-03,
         1.54994078e+05,   5.23385391e+04,   9.22623145e+03,
         4.98499488e-04,   3.13364938e+05,   7.49179346e+03,
         1.00305547e+04,   1.82671659e-02,   3.42344213e+00,
         4.30668592e+00,   4.16045514e-04,   1.03824775e+04,
         1.23922656e+05,   1.04631006e+04,   1.73410047e+05,
         8.55786406e+04,   6.04137158e+03,   2.45309277e+04,
         8.89998340e+03,   2.56647750e+05,   1.70954451e-01,
         2.58657148e+04,   4.36246688e+05,   5.87789141e+04,
         1.23386428e-01,   3.66295959e+02,   1.85506699e+04,
         3.94269142e+01,   1.44688359e+05,   1.45298477e+04,
         6.21279478e+00,   6.63667529e+03,   1.65185953e+05,
         1.97631230e+04,   4.81436328e+03,   1.40221679e+00,
         9.09212012e+03,   2.44030178e-01,   1.94093132e+01,
         3.32279022e+02,   2.92363472e+01,   1.05144541e+04,
         7.98557910e+03,   1.18249258e+04,   1.39883062e+05,
         1.07809453e+04,   9.80455469e+03,   1.64741135e+02,
         5.09485875e+05,   2.78596344e+05,   6.52749902e+03,
         5.84930195e+04,   7.87608398e+03,   1.88581758e+04,
         1.18888516e+04,   1.52794098e+02,   1.30843457e+04,
         2.18986487e+00,   1.14318955e+04,   3.12385375e+05,
         7.70872116e-01,   1.69423599e+01,   4.86643375e+05,
         8.11400391e+03,   1.74439545e+01,   3.48252281e+05,
         1.87914600e+01,   4.64235254e+03,   1.01091338e+04,
         1.47928438e+04,   2.79598942e+01,   2.45827949e+04,
         7.65991553e+03,   2.00238895e+00,   3.10605913e-01,
         7.81298594e+04,   2.86300537e+02,   1.99413319e+01,
         2.27015254e+04,   2.19956250e+05,   1.09991318e+04,
         1.81497953e+05,   1.63340271e+00,   9.53981641e+03,
         5.85154609e+04,   1.06890038e-01,   5.10188141e+02,
         4.31207094e+05,   4.47639453e+04,   5.45675688e+05,
         8.08595508e+03,   1.01950957e+04,   1.00513789e+04,
         1.57426543e+04,   1.10389585e+01,   8.30430566e+03,
         7.10388303e-01,   1.07709609e+05,   1.03372437e+02,
         1.12308057e+04,   8.98787415e+02,   3.63446414e-01,
         1.23672510e+04,   5.79760107e+03,   2.11794297e+04,
         1.09236570e+05,   1.37908730e+04,   6.53607500e+05,
         9.86244531e+03,   1.82566872e+01,   5.03345312e+03,
         2.09913812e+05,   1.89372016e+05,   1.18128057e+04,
         3.34898711e+04,   5.14254453e+04,   2.74777679e+02,
         8.40752637e+03,   1.13671128e-09,   1.40161156e+05,
         2.76869344e+05,   8.98101270e+03,   7.23281738e+03,
         2.95665844e+05,   8.28646289e+03,   1.26285315e+03,
         5.59499707e+03,   9.33632507e+02,   4.17510059e+03,
         2.74917554e+03,   1.77040662e+03,   1.05599475e+03,
         1.28719270e+00,   4.38650781e+04,   2.99222598e+04,
         2.16534888e+03,   1.85546465e-04,   4.12549011e+02,
         5.87204468e+02,   2.01756494e+03,   2.98604941e+00,
         1.06492188e+03,   4.36787695e+03,   1.74426062e+05,
         2.23833093e-04,   9.97683883e-01,   3.52064771e+03,
         1.63819739e+03,   1.69842798e+03,   9.01733971e+00,
         3.50203955e+03,   3.52579713e+00,   6.51048320e+04,
         1.78168835e+03,   3.99957578e+04,   4.32379171e-03,
         5.37734473e+03,   4.18319733e+02,   2.20450635e+03,
         3.08879224e+03,   3.07227051e+03,   9.02617264e+01,
         2.16442773e+03,   1.82850219e+05,   9.95169144e+01,
         5.80445801e+03,   1.53985059e+03,   1.54938342e+03,
         1.65538812e+05,   3.12529388e+02,   4.37371704e+02,
         1.26923102e+05,   2.81344757e+02,   9.64415222e+02,
         1.80920517e+02,   3.56961670e+02,   3.33044281e+01,
         2.57207938e+05,   5.01052734e+02,   3.10455902e+02,
         2.39062057e+02,   6.41142807e+01,   2.90034570e+03,
         1.02433784e+02,   1.55525141e+05,   3.53992871e+03,
         7.01231875e+04,   4.46191943e+03,   2.23219946e+03], dtype=float32), array([  6.56725879e+03,   1.77622344e+05,   3.18103145e+04,
         2.41600844e+05,   7.54997134e-02,   3.44786420e-02,
         2.81869980e+04,   9.40850647e+02,   4.77921106e-02,
         2.52545327e-01,   4.29868847e-02,   4.87780310e-02,
         5.01154639e+03,   1.92321301e-03,   3.98116410e-02,
         1.80527964e-03,   2.74859406e-02,   6.12046777e+03,
         6.90781176e-02,   4.14036177e-02,   1.30904000e-03,
         4.53408845e-02,   4.51114476e-02,   7.10743815e-02,
         1.46589354e-01,   6.51259562e+05,   8.97554904e-02,
         2.29651914e+04,   1.86265762e+06,   6.77037165e-02,
         3.93539295e-02,   7.69707666e+03,   1.15184508e+05,
         5.41263446e-02,   1.62433037e+04,   1.89716875e+05,
         1.77742266e+04,   8.33265039e+03,   6.64749071e-02,
         6.58396240e+03,   1.62296406e+04,   3.88601484e+04,
         6.64772242e-02,   1.10456992e+04,   2.13279590e-01,
         8.68510571e-04,   1.13872125e+06,   3.98312919e-02,
         1.28476770e+03,   3.61016469e+05,   4.65687551e-02,
         4.18955944e-02,   6.57615173e+02,   1.13869990e+04,
         5.44546731e-02,   1.42056416e+04,   5.52582741e-02,
         1.30865094e-03,   7.35240332e+03,   1.80242859e+05,
         4.18325000e+03,   7.03952148e+02,   4.66260165e-02,
         2.31643262e+04,   1.05575215e+04,   8.50128125e+05,
         1.16116919e-01,   9.12049805e+03,   1.63498652e+04,
         2.34684406e+05,   6.22357688e+05,   3.49865668e-02,
         6.18355312e+04,   7.85629079e-02,   6.14080355e-02,
         1.22910215e+04,   3.67580876e-02,   1.55048467e+04,
         5.30930012e-02,   8.40283632e-02,   1.06895901e-01,
         7.93618516e+04,   2.74736500e+05,   7.57213831e-02,
         4.64126719e+04,   2.31482207e+04,   3.84591371e-02,
         5.47133594e+04,   8.00890550e-02,   5.03970410e+03,
         4.58921753e-02,   1.46375392e+08,   2.80774281e+05,
         1.01872686e+04,   5.96545547e+04,   1.04652324e+04,
         3.27632017e-02,   1.18556875e+04,   1.89156836e+04,
         2.46120850e+03,   1.59834141e+05,   3.15576582e+04,
         7.31501404e+02,   2.08305359e+05,   3.41946118e+03,
         6.01592695e+04,   5.18381707e-02,   1.47427959e+04,
         6.23734414e+04,   6.84033409e-02,   1.15035801e+04,
         6.99213520e-02,   1.47401875e+04,   1.05047785e-01,
         9.51572160e+08,   8.03274214e-02,   1.84672205e+03,
         3.45594361e-02,   2.39903089e-02,   8.19043294e-02,
         4.81728857e+03,   1.53356064e+04,   1.94567285e+04,
         7.26292431e-02,   4.16457020e-02,   5.72962649e-02,
         4.70973030e-02,   1.26155123e-01,   1.16627999e-02,
         4.57733250e+05,   8.70579375e+05,   3.12152844e+05,
         2.06735000e+04,   1.46355475e+06,   1.32723682e+04,
         1.20402750e+06,   7.92076250e+04,   2.58246297e+05,
         1.94664422e+05,   6.44915039e+04,   4.05692750e+05,
         1.48230453e+05,   5.79201688e+05,   1.42105141e+05,
         2.67509238e+04,   1.76364828e+05,   5.60092383e+04,
         2.95030586e+04,   1.36257365e-01,   1.16693838e+06,
         2.95148027e-07,   3.00831656e+05,   2.38597672e+05,
         4.44938764e-02,   3.45833750e+05,   1.12516375e+06,
         1.30464727e+05,   3.98782891e+04,   6.22198086e+04,
         8.91793672e+04,   3.92676927e-02,   1.57008156e+05,
         2.81842125e+05,   4.45475688e+05,   1.59314875e+06,
         4.76403832e-02,   1.17160281e+05,   3.40225875e+05,
         9.16864875e+05,   5.81662656e+04,   8.15862641e-02,
         3.22163008e+04,   5.35540234e+04,   9.53132617e+03,
         1.11055862e+06,   1.36656112e+06,   1.36821266e+05,
         4.82784398e-02,   4.62502773e+04,   4.28558923e-02,
         6.30767438e+05,   3.69278219e+05,   1.00224300e+06,
         2.20270254e+04,   1.01537031e+04,   5.39678906e+03,
         4.43979800e-02,   3.88127719e+05,   2.80469900e+06,
         1.40893828e+05,   2.74416450e-02,   2.72335420e+07,
         3.32222344e+05,   2.69213844e+05,   6.23885250e+05,
         9.10986062e+05,   8.97708188e+05,   1.72020688e+05,
         1.38641834e-01,   6.81143105e-02,   4.46147500e+06,
         7.65871188e+05,   2.02227762e+06,   2.60089325e+06,
         5.93394500e+05,   7.20972062e+05,   6.46433562e+05,
         1.10797102e+05,   6.97056562e+04,   1.41376344e+05,
         1.23979359e+05,   1.06408672e+05,   7.05495438e+05,
         4.74322885e-02,   1.13823115e+04,   4.88212188e+05,
         9.90942438e+05,   1.10041038e+06,   7.28137875e+05,
         2.49954980e+04,   9.77100391e+03,   9.24850250e+05,
         2.94580493e+03,   2.37913234e+05,   1.15229695e+05,
         1.78638825e+06,   1.75375525e+03,   4.82480898e+04,
         5.82342861e+09,   3.84025562e+05,   6.34016688e+05,
         1.00981203e+09,   2.61069648e+04,   6.54391016e+03,
         1.09530300e+06,   6.80156797e+04,   1.77738086e+04,
         3.50512266e-02,   5.02550062e+05,   6.86984922e+04,
         1.49380781e+05,   1.04811117e+05,   9.95024766e+04,
         1.27687305e+05,   8.33934062e+04,   1.56131538e+06,
         5.89982830e-02,   3.29635719e+05,   5.61151062e+05,
         4.29612250e+05,   5.54313312e+05,   3.80469906e+05,
         5.28989180e+04,   6.95368688e+05,   6.17497062e+05,
         2.92687602e-02,   1.27342300e+06,   1.50443600e+06,
         2.13756984e+05,   2.53844838e+09,   7.35767062e+05,
         1.05289362e+06,   9.28481700e+06,   4.52070850e+06,
         6.73601600e+06,   2.18595379e+09,   1.47850125e+09,
         1.71202549e-03,   1.80524650e+06,   2.56987285e+04,
         4.60143047e+04,   7.32025000e+06,   3.79995140e-03,
         2.19851523e+04,   4.11323219e+05,   2.88072800e+06,
         1.51594736e+08,   3.60528156e+05,   1.35424120e+07,
         1.17154414e+05,   1.08761269e+02,   1.45091250e+05,
         1.43434010e+07,   3.64486219e+05,   5.40734766e+03,
         9.25089625e+05,   2.75619531e+05,   3.94881312e+05,
         3.56706200e+06,   2.73784824e+04,   2.91637406e+05,
         4.86528625e+05,   5.73409259e-01,   1.92818112e+06,
         2.16013850e+09,   5.11070362e+09,   7.83293125e+05,
         2.06747762e+06,   6.55942600e+06,   2.85351781e+05,
         3.14499688e+05,   2.41670907e-02,   2.38311670e+03,
         6.28750086e-01,   4.12087344e+05,   1.33410449e+04,
         3.38972725e+06,   2.91469961e+04,   1.12226760e+08,
         4.78038969e+05,   3.31788673e-03,   2.37769984e+09,
         1.20188740e+07,   2.40251035e+04,   6.18608938e+05,
         3.60166915e-02,   3.55320977e+04,   2.31391766e+05,
         2.64104653e+09,   1.58894662e+06,   1.91879550e+06,
         2.25834758e-03,   5.89070750e+05,   3.75344998e+09,
         9.73730963e-08,   3.01804736e+08,   3.02301832e-02,
         1.52924747e+01,   8.82315125e+05,   3.52988080e+07,
         1.43543703e+05,   1.60795438e+05,   1.78715288e+06,
         1.46677988e+06,   2.76105188e+05,   2.22614450e+06,
         2.42787763e+09,   1.42952625e+05,   1.60724656e+05,
         1.49458050e+06,   1.46047344e+05,   2.76745200e+06,
         1.17231375e+06,   2.76334150e+06,   1.24400888e+06,
         1.80372915e+09,   9.92899875e+05,   3.15896893e+00,
         1.41939425e+10,   1.26099962e+06,   1.14125289e-01,
         1.10785950e+06,   4.45960156e+04,   2.58721467e-02,
         1.35744973e+09,   3.68730100e+06,   5.46133250e+06,
         2.02761856e+08,   5.48238477e+04,   1.03384750e+06,
         3.93404443e+03,   2.16825900e+06,   2.67905354e-01,
         2.80396725e+06,   1.45971425e+06,   8.95416927e+00,
         2.49809760e+07,   1.06471219e+11,   1.15776516e+05,
         5.06676450e+06,   4.31984050e+06,   3.86451997e+11,
         5.98350312e+05,   1.38594500e+06,   1.67351604e+00,
         3.68885536e+08,   3.60366800e+06,   4.16667581e-01,
         7.80595050e+06,   3.08655125e+05,   5.23512200e+06,
         2.79271722e-01,   3.34777125e+05,   3.99767675e+06,
         1.09258584e+08,   1.97274624e+08,   3.16495838e+01,
         6.13972480e+09,   9.94024960e+08,   1.00567448e+08,
         2.10300946e+00,   2.53692457e+10,   1.26955896e+08,
         2.01173968e+08,   1.02804810e+02,   1.66388156e+05,
         1.26302492e+05,   1.50857806e+00,   8.21985440e+07,
         4.11486976e+09,   2.26806864e+08,   7.90428621e+09,
         1.93021158e+09,   5.40854800e+07,   3.57005472e+08,
         1.18079800e+08,   1.70579804e+10,   3.29691968e+03,
         1.74607526e+09,   4.95405629e+10,   1.14627213e+09,
         1.24449133e+03,   7.17408750e+06,   6.19507968e+08,
         2.00560975e+06,   5.47243674e+09,   4.73584448e+08,
         1.47082984e+05,   6.36617560e+07,   7.41921485e+09,
         2.01332688e+08,   3.39079960e+07,   6.99887354e+03,
         1.16970744e+08,   9.32351367e+03,   3.48604125e+05,
         1.90951060e+07,   2.54747875e+05,   2.30601856e+08,
         1.21317712e+08,   2.79921376e+08,   4.98754202e+09,
         1.43456912e+08,   2.03330944e+08,   4.65217700e+06,
         6.72014336e+10,   2.00895427e+10,   9.09570800e+07,
         9.75377280e+08,   7.56335920e+07,   5.70409152e+08,
         2.47780272e+08,   3.27594625e+06,   4.14175232e+08,
         4.35209805e+04,   1.88619648e+08,   2.56333128e+10,
         1.60992012e+04,   2.72850000e+05,   6.20860948e+10,
         7.28272800e+07,   2.82648975e+06,   3.16500439e+10,
         1.31043250e+06,   2.41057440e+07,   1.43970016e+08,
         4.99416992e+08,   1.01184600e+06,   3.68047898e+09,
         1.20624256e+08,   2.43332812e+04,   8.15136328e+03,
         1.60642598e+09,   2.73450200e+06,   2.38838203e+05,
         5.12689408e+08,   1.25280942e+10,   1.84381472e+08,
         8.63307469e+09,   6.08516094e+04,   1.62093040e+08,
         2.82314291e+09,   1.32952258e+03,   1.62178140e+07,
         4.81663345e+10,   1.21457367e+10,   7.68626033e+10,
         8.02472080e+07,   2.93666976e+08,   1.49485664e+08,
         4.27040096e+08,   4.29687125e+05,   1.28158904e+08,
         8.97039355e+03,   3.02248934e+09,   1.03696210e+07,
         2.00021152e+08,   2.98153600e+07,   1.00804482e+04,
         9.39922560e+08,   4.73496560e+07,   2.76551731e+09,
         3.77426304e+09,   4.35165472e+08,   1.09742006e+11,
         2.15231920e+08,   3.09728500e+06,   3.24399940e+07,
         1.22994432e+10,   1.08861686e+10,   2.79660576e+08,
         9.37724736e+08,   1.02720493e+09,   2.17777050e+06,
         1.54798112e+08,   1.00629443e-08,   5.19118694e+09,
         1.98968300e+10,   1.33751944e+08,   9.35871520e+07,
         2.28296315e+10,   9.38741040e+07,   3.17882250e+06,
         8.31164240e+07,   1.46775562e+06,   6.01824640e+07,
         1.75525580e+07,   5.50511050e+06,   2.18296225e+06,
         1.07290801e+04,   4.99514304e+08,   2.29417152e+08,
         8.99616100e+06,   1.12912692e-01,   2.42840719e+05,
         1.13189812e+06,   7.25623850e+06,   8.52977930e+03,
         2.10469600e+06,   4.55028320e+07,   7.80608819e+09,
         2.87974954e-01,   8.70923340e+02,   2.96686880e+07,
         7.41351750e+06,   3.24923525e+06,   5.72499062e+04,
         2.77035780e+07,   4.16185693e+03,   1.09000768e+09,
         5.04991100e+06,   4.49137312e+08,   4.01553993e+01,
         5.70331200e+07,   4.28945562e+05,   1.06906340e+07,
         2.11904900e+07,   2.61208980e+07,   3.01649570e+04,
         9.72933000e+06,   8.66692198e+09,   8.41177266e+04,
         1.02964984e+08,   7.68585550e+06,   7.45714400e+06,
         6.98213274e+09,   2.38885984e+05,   1.02112962e+06,
         4.27480960e+09,   3.26569906e+05,   1.77353412e+06,
         5.66584922e+04,   1.22934812e+06,   4.77232617e+03,
         1.71293143e+10,   5.42164000e+05,   1.90681484e+05,
         1.16951258e+05,   1.15400928e+04,   2.35181320e+07,
         2.42172090e+04,   6.32247757e+09,   2.86867180e+07,
         1.27844915e+09,   4.70369600e+07,   1.15285590e+07], dtype=float32), array([[-0.04769275,  0.069238  , -0.00462658, ..., -0.08036393,
        -0.05525096, -0.00468105],
       [ 0.0016573 ,  0.00850066, -0.07921569, ..., -0.08608776,
         0.01443016,  0.05530938],
       [-0.01991984, -0.06943925, -0.02920824, ..., -0.01871235,
         0.06272089,  0.0801362 ],
       ..., 
       [ 0.02875734, -0.02720934, -0.04642835, ..., -0.02848272,
         0.07837038, -0.00966376],
       [-0.02375437,  0.0391067 , -0.07079554, ...,  0.02161885,
         0.08407549, -0.00626772],
       [ 0.00421856, -0.04084457, -0.13998236, ..., -0.03473885,
        -0.00353694, -0.03559897]], dtype=float32), array([-0.04281327,  0.01216352, -0.04227917,  0.00750893, -0.01155498,
       -0.10221472, -0.1214705 , -0.13738319,  0.05585212, -0.15259944,
       -0.06488334, -0.06068194, -0.02622675, -0.08340055, -0.11234758,
       -0.02888185, -0.04345198,  0.02169439,  0.02590524, -0.04059396,
       -0.06806512,  0.10729537, -0.18412711, -0.18169376,  0.00365732,
       -0.13488761, -0.02717731, -0.00244496,  0.06433829, -0.11677473,
       -0.01639851, -0.03303393, -0.02688541,  0.04380631, -0.00033412,
        0.02736358,  0.05983658,  0.02838399, -0.17597876,  0.04450758,
        0.00798095,  0.01551016,  0.04845837,  0.036789  , -0.01422661,
        0.01329577, -0.04310149, -0.03296043, -0.09829519, -0.01581104,
       -0.03864533,  0.07951146, -0.03805969, -0.02215071, -0.03068955,
        0.01017272, -0.04202718,  0.02586712,  0.03728069, -0.165975  ,
       -0.04635691,  0.07902595,  0.01165657,  0.0273642 , -0.05795354,
        0.01866121, -0.03403607, -0.00425029, -0.01559693, -0.12403741,
        0.04839318, -0.00974488, -0.07547898, -0.17886929, -0.00342553,
       -0.05279821,  0.02622259, -0.12527826,  0.00409175,  0.01612878,
       -0.00909053, -0.04103935, -0.04583444,  0.04986244, -0.05952312,
       -0.0086245 , -0.06176755,  0.03476633, -0.06459755, -0.03509617,
       -0.11843704, -0.16909009,  0.05126577,  0.05601902, -0.02887684,
        0.00571529, -0.02757956,  0.01492239,  0.04096997,  0.00517426,
       -0.09337299, -0.10719658,  0.02460384,  0.01244749, -0.06192895,
        0.05721438, -0.09381114, -0.02609875, -0.07876792, -0.10560443,
        0.02266885, -0.04450804, -0.18668133, -0.093195  , -0.06256788,
        0.06355501, -0.0322827 ,  0.04231226,  0.00050422,  0.0583259 ,
       -0.03034916, -0.09061082,  0.02164552, -0.0799282 ,  0.0472419 ,
       -0.04796644,  0.00187331, -0.12954311, -0.01791267, -0.07327066,
       -0.09112413, -0.04374129, -0.050823  ,  0.00941752, -0.06980613,
       -0.02574478, -0.2491547 , -0.1225718 , -0.06167391,  0.04720496,
       -0.11496516,  0.00070918, -0.11957023,  0.05516461,  0.00511729,
       -0.02611975, -0.0194354 , -0.00131737, -0.00630221, -0.05635685,
       -0.11458802,  0.014326  , -0.02076014, -0.06513453, -0.14417651,
       -0.06692553, -0.02526882,  0.03328592, -0.04254598, -0.02013853,
       -0.04922098, -0.09137172, -0.01722721, -0.06816894, -0.0259109 ,
       -0.01487811, -0.05329435, -0.0315166 , -0.07660367,  0.02827635,
       -0.15094604,  0.01775419,  0.03518398,  0.02181572, -0.02780074,
       -0.02563138, -0.04151968, -0.07021532,  0.0409584 , -0.1852023 ,
       -0.03211535, -0.00469262,  0.01153537, -0.04135527, -0.08311858,
       -0.0674094 , -0.02219914, -0.12330494, -0.02713043,  0.01014902,
       -0.03629309, -0.04927927, -0.12445687, -0.00322825, -0.02708133,
       -0.05924047, -0.04556635,  0.00729214, -0.08192374,  0.01589071,
       -0.00263936,  0.01880972, -0.19471456,  0.01881176, -0.01513805,
       -0.03726128,  0.01239012, -0.13099153, -0.07545547, -0.06204348,
       -0.07222242, -0.05149313, -0.08179797,  0.05838239, -0.03805961,
       -0.05964502, -0.04124784, -0.13633013,  0.02435996, -0.01846947,
       -0.14851616, -0.0667586 ,  0.04615407, -0.10091442, -0.04483977,
       -0.02565717,  0.00274817, -0.04863312,  0.01534853, -0.010924  ,
        0.01179601, -0.05243185, -0.0294415 , -0.07210685, -0.05832606,
       -0.04773422,  0.01780162, -0.02805614, -0.10551364, -0.05942029,
       -0.08747935,  0.03103399,  0.00280847,  0.04558571, -0.0268664 ,
       -0.05908133, -0.02920071, -0.05185449, -0.05839225, -0.00542642,
       -0.00445425, -0.00277337, -0.09241011, -0.1817136 , -0.02085226,
        0.02530239, -0.02431361, -0.02471693, -0.10894374, -0.11241273,
       -0.05965532, -0.04742953, -0.0284279 ,  0.01174705, -0.0459118 ,
       -0.08097897, -0.09698882,  0.01121806, -0.01535511, -0.10678045,
       -0.03162777, -0.03982999,  0.00554388,  0.00179311, -0.07819723,
       -0.17190899, -0.03242657, -0.03468801,  0.01528743, -0.06650851,
        0.0190498 , -0.01271552, -0.03651275, -0.11232013, -0.05504088,
       -0.03477527, -0.05536038, -0.23618491, -0.14842159, -0.14404929,
       -0.09345041,  0.00977631, -0.01399187, -0.01725124,  0.00727773,
        0.02457467,  0.01258498, -0.05015456, -0.08532293, -0.11646874], dtype=float32), array([ 0.87176639,  0.93463355,  1.05634081,  0.99340051,  1.0030396 ,
        0.99227065,  1.07705748,  1.1993829 ,  0.91247821,  1.12197948,
        1.05622685,  1.04016495,  0.96569908,  1.08073354,  1.09519255,
        1.03536379,  0.94631213,  0.99006516,  0.98306018,  1.03557265,
        1.06246316,  0.90030062,  1.14284647,  1.11873901,  0.95603698,
        1.08676267,  1.01677489,  0.93079323,  1.01469243,  1.00669801,
        0.97528404,  0.99322295,  1.08552587,  0.96151406,  0.97922933,
        0.94758064,  0.94741124,  1.00225997,  0.98095411,  0.94945347,
        1.07860887,  0.93168348,  0.91968876,  0.92890793,  0.97574204,
        1.19683862,  0.98550284,  0.9843815 ,  1.09685063,  0.91151792,
        1.01290405,  0.89867115,  1.06257439,  0.94521379,  1.12563264,
        0.91047913,  0.90380287,  1.03080583,  1.01078689,  1.06201088,
        1.14023697,  0.93335462,  0.91904873,  0.86279535,  1.11908197,
        0.92907482,  1.08038664,  1.01428044,  0.95855457,  1.0398047 ,
        0.95748085,  1.03991616,  0.95625305,  1.09788918,  1.0463078 ,
        0.97386622,  0.96178865,  1.22783625,  0.91931695,  0.99592876,
        1.11665428,  1.02903068,  0.89683247,  0.98548013,  1.11801124,
        1.04297292,  0.95071673,  0.94948286,  1.0440582 ,  1.09026253,
        1.15528381,  1.07348859,  1.1052078 ,  1.00697756,  1.02022457,
        0.96277684,  0.91864967,  1.20153558,  1.06757641,  0.99407727,
        1.10569465,  1.24604774,  0.93461794,  0.93011755,  1.11834908,
        1.00567818,  1.09637153,  0.91049117,  1.06878328,  1.00739777,
        1.03471887,  1.10538578,  1.10195613,  1.06552505,  1.05732918,
        0.98002946,  0.93754977,  0.94159895,  0.99337161,  0.86666179,
        0.96387154,  1.04916489,  1.10477781,  1.29178476,  0.9523856 ,
        0.94455338,  0.96811038,  1.18081236,  0.93842161,  1.01636326,
        0.98871404,  0.95899242,  0.91982192,  0.95779598,  1.10919225,
        1.0903461 ,  1.08609915,  1.0404433 ,  0.96200049,  0.89642465,
        1.12744498,  1.01105285,  1.07991314,  0.93388069,  1.00590289,
        0.92252302,  0.99469012,  0.8675974 ,  1.08350885,  0.95367026,
        1.10333335,  0.91295874,  1.20902872,  0.95180273,  1.04733539,
        1.1583041 ,  0.93378431,  0.94954377,  1.08910704,  0.94165903,
        1.0202949 ,  1.02597845,  0.94184482,  0.95451266,  1.10715771,
        1.03222644,  0.98962945,  1.05778515,  0.99307364,  0.87771612,
        1.09873402,  0.92734551,  1.01758647,  0.9831512 ,  1.20215917,
        0.92206228,  1.0119828 ,  0.98569953,  0.96064812,  1.16997325,
        0.93985933,  0.99224442,  0.93972802,  1.02293193,  0.98777872,
        1.06081343,  0.94340861,  1.07796156,  0.99668258,  0.95350105,
        1.10059285,  0.90825182,  1.05968821,  1.07461452,  0.92003191,
        1.00912607,  0.91402423,  0.93052262,  0.97435498,  0.88573182,
        1.00183964,  0.93959558,  1.01498902,  0.97465479,  1.06606197,
        1.11691356,  0.96486741,  1.0107826 ,  1.05988026,  1.12390065,
        1.11638784,  1.15644038,  1.00571382,  0.93838054,  0.96072191,
        0.94599783,  1.12893295,  1.1230365 ,  0.88623613,  1.17280185,
        1.07552314,  0.97262222,  1.01057529,  1.0089252 ,  1.10838652,
        0.90431851,  0.92546988,  1.02088475,  0.97890371,  0.91935927,
        1.09139442,  1.05249536,  0.98707712,  0.99708903,  0.9559375 ,
        0.97608489,  0.99335748,  1.00746489,  1.16643882,  1.06395233,
        1.08416784,  1.05056548,  0.94751292,  1.02552378,  0.93117982,
        0.99507368,  0.98581773,  0.96943289,  1.1225698 ,  1.04918706,
        1.1006186 ,  1.04070008,  0.97809011,  1.12040222,  1.02568603,
        1.03338635,  0.97896522,  1.07827854,  1.08026075,  1.02890062,
        1.13584721,  0.94108975,  0.93482053,  0.96129233,  1.01920295,
        1.08889484,  1.06581569,  0.97552782,  0.90931129,  1.14291739,
        0.94304538,  0.86557966,  1.08406758,  1.025388  ,  1.01034272,
        1.11826336,  1.16416299,  0.90759176,  0.99313092,  1.04457438,
        0.94098908,  0.92880487,  1.15274727,  1.05287194,  0.97517741,
        0.92147702,  0.92111331,  1.23986077,  1.11298263,  1.00378525,
        1.11741269,  0.93636656,  0.88912165,  1.1682471 ,  0.94735444,
        0.93080258,  0.97969508,  1.1736064 ,  0.94570416,  1.08708167], dtype=float32), array([ 0.00105192,  0.1075988 , -0.03282657,  0.03583052, -0.09719235,
        0.10098293, -0.07340404,  0.03293641,  0.065913  , -0.03426898,
        0.05921084,  0.05246812,  0.03162163,  0.05474028,  0.08947311,
        0.01194484, -0.07671205,  0.0631092 ,  0.07323366,  0.0793419 ,
       -0.04232517,  0.08939039, -0.02628171, -0.00756457,  0.05183554,
       -0.12899128, -0.0649024 , -0.04325962, -0.02710859, -0.09890834,
        0.07437202, -0.07601923, -0.11400878,  0.04129992,  0.08282767,
        0.08283643, -0.05385976, -0.03914045, -0.12124286,  0.03088612,
        0.03944312, -0.09534793,  0.06200753, -0.01494889, -0.0153994 ,
        0.1093872 ,  0.00861222, -0.00998991, -0.09372575,  0.07211181,
       -0.00128495,  0.050912  , -0.08632752, -0.04408937, -0.03642046,
       -0.06745459,  0.05850372, -0.01044809,  0.05830123, -0.09542446,
        0.0233317 ,  0.07115889,  0.08189265,  0.01408185, -0.08020137,
        0.06224453, -0.02057605, -0.07207014,  0.03757225, -0.10853849,
        0.04602966,  0.04792535, -0.0802042 , -0.10549779,  0.06760156,
       -0.03262317, -0.00384597, -0.1187655 ,  0.0324718 ,  0.02293467,
        0.07470402, -0.00473271, -0.05201067,  0.00340916, -0.07972595,
       -0.06715551, -0.04450863, -0.01131769,  0.11351115,  0.02656087,
        0.03106523,  0.05401589,  0.01870393, -0.10399695,  0.07486138,
        0.10062269,  0.00317511,  0.07580896, -0.05176559, -0.07679165,
        0.03077197, -0.06874954,  0.04182817, -0.00021885,  0.00777751,
       -0.00386811, -0.06315943, -0.02873971, -0.09663004, -0.08720001,
        0.05925692, -0.03412884, -0.09969616, -0.10604034, -0.09998439,
        0.0165443 , -0.05464287,  0.02691977,  0.03697906,  0.05903339,
        0.00373324,  0.03420821,  0.05734671, -0.08574876,  0.03399953,
        0.04508731,  0.03038682, -0.07434025,  0.02477123, -0.06707739,
        0.00520484, -0.05636477, -0.0444801 ,  0.04341142, -0.11206874,
        0.04344311, -0.02316927, -0.03908125, -0.09280184, -0.10791238,
       -0.03243571,  0.03864868,  0.02267985,  0.02916247, -0.03753981,
       -0.07467007, -0.07640166,  0.08990327, -0.07352129, -0.00924637,
       -0.07852473, -0.08742116,  0.04097183,  0.04054951, -0.06483995,
        0.04285421, -0.05109795,  0.08268256,  0.14438044,  0.07374576,
       -0.05631001, -0.03801958,  0.0659643 ,  0.03456411,  0.10542175,
       -0.04591293, -0.04668155, -0.05933658, -0.07865544,  0.09005108,
       -0.09962866,  0.00698445, -0.00094486,  0.01116829,  0.00763724,
       -0.08532009, -0.05005287, -0.08039762,  0.07219297, -0.10870604,
        0.05073224, -0.05017154,  0.07427768,  0.02236671, -0.0603907 ,
        0.04971685, -0.04218143, -0.1090948 , -0.11825089,  0.04080078,
        0.037702  , -0.05006854, -0.01802806, -0.01644355,  0.06621083,
       -0.04078118,  0.01575663,  0.06474546, -0.07074669,  0.04355057,
        0.0229033 ,  0.04121767,  0.07368467, -0.056196  ,  0.05615422,
        0.0617256 ,  0.07434787,  0.04947244, -0.02962077,  0.10924897,
        0.096898  , -0.06690084, -0.11307403, -0.06736805,  0.04571798,
       -0.0564863 , -0.08323038,  0.12810753,  0.08982489,  0.12296901,
       -0.12568548, -0.09132   ,  0.01673149,  0.01547858,  0.08331845,
       -0.03638335,  0.04122225,  0.05720593,  0.05331337, -0.00744243,
        0.03796619,  0.07079853, -0.05719955, -0.07888139, -0.0270002 ,
        0.01639183, -0.01545628, -0.06905092, -0.07906485, -0.0905702 ,
       -0.08095042, -0.03649227, -0.0561366 , -0.08276156,  0.0895219 ,
        0.02497489, -0.06564935, -0.07411634,  0.0231353 , -0.04208171,
       -0.10946783, -0.02992818, -0.00608243, -0.09065883,  0.07754683,
       -0.04593742, -0.0468175 ,  0.09627265,  0.07830583, -0.03170425,
       -0.07183324, -0.05017394, -0.05594379, -0.01742457,  0.02904778,
       -0.08004445,  0.09018414,  0.03322495, -0.03285638,  0.0429967 ,
        0.05266179, -0.01548739, -0.1070734 ,  0.01433286,  0.13911456,
        0.05454878, -0.04002381,  0.03345147,  0.02733856, -0.06829462,
        0.04791171,  0.07573467, -0.08281509, -0.06768331,  0.07354324,
       -0.03069789,  0.06341367, -0.01724629,  0.14409827,  0.0323025 ,
        0.03999797,  0.03899879,  0.03868518,  0.06184839, -0.01204476,
        0.07907918, -0.01912301,  0.02162754, -0.06985147,  0.12258166], dtype=float32), array([ 1.58741736,  0.83046049,  0.49278846,  0.66355884,  1.35883749,
        0.38173139,  0.47138804,  0.43110391,  1.80101788,  0.35205626,
        0.54460412,  0.56293535,  1.7665    ,  0.285238  ,  0.55082303,
        0.68496764,  1.15205204,  0.50222516,  0.63375694,  0.6813013 ,
        0.50210482,  0.77778488,  0.45924455,  0.31066194,  1.13828421,
        0.37113306,  0.64763331,  1.06094396,  1.37464523,  1.39178431,
        2.22035289,  0.89710766,  0.5950098 ,  1.93965924,  0.48359066,
        0.89201689,  0.96112823,  1.27325726,  0.22908144,  0.64389646,
        0.64823657,  0.64834362,  1.7579354 ,  0.7597174 ,  0.39607292,
        0.99729741,  1.41259146,  2.59125447,  0.70576864,  1.13219118,
        1.09767771,  1.41419458,  0.60766506,  0.53881752,  0.56293666,
        0.89360815,  0.6194123 ,  0.93562204,  1.02484131,  0.40006694,
        0.59434503,  1.91167617,  2.34298611,  1.53296733,  0.75691116,
        0.89003527,  0.61783952,  0.81138378,  1.64371562,  0.35430235,
        1.34343517,  0.46472439,  1.466272  ,  0.33931288,  0.47045052,
        0.51161039,  1.53993666,  0.47928429,  1.95763087,  1.77608705,
        0.57341701,  0.50211096,  0.50817102,  1.14388573,  0.56272131,
        0.58736807,  1.19793773,  0.65035653,  0.53783292,  0.7115159 ,
        0.37626556,  0.35046262,  1.07449734,  0.89419812,  0.51580989,
        2.02214575,  0.72173238,  0.92564106,  0.95200509,  1.1591835 ,
        0.44617423,  0.52955109,  0.6664145 ,  1.86956346,  0.50393599,
        0.97062725,  0.3529757 ,  1.66795802,  0.43336105,  0.32954726,
        0.64877206,  0.57209402,  0.35647091,  0.50420207,  0.53642792,
        1.07529521,  0.74749094,  1.5213623 ,  1.32599914,  0.66948628,
        1.7666868 ,  0.38221186,  0.77170169,  0.64681786,  1.28200209,
        1.39162767,  1.65997553,  0.45464477,  1.46914268,  1.43028915,
        0.49979123,  1.82406127,  1.66969693,  0.8893832 ,  0.29520938,
        0.54352462,  0.18788485,  0.45428935,  0.51939195,  1.681422  ,
        0.44644737,  0.47918871,  0.38756549,  1.18757832,  0.5216372 ,
        1.55489326,  0.57255518,  1.36661553,  0.77448469,  0.97621095,
        0.54026169,  0.840128  ,  0.68622601,  1.18797326,  0.51033252,
        0.55350024,  0.62252474,  1.17693615,  0.57898831,  1.31683397,
        0.83644539,  0.65491253,  1.74810565,  0.5600279 ,  0.55141366,
        0.71456146,  1.75648773,  0.41260162,  1.00958979,  0.98862243,
        0.43453613,  1.54910636,  2.59015441,  1.96257162,  0.71596181,
        1.56761777,  0.87569118,  1.15905666,  1.1935333 ,  0.42628354,
        0.78581393,  1.07278121,  1.99670708,  1.12757969,  0.57352549,
        0.6526348 ,  1.93397689,  0.5525946 ,  0.82916188,  1.3793608 ,
        0.58608443,  0.54909378,  0.35643783,  0.67872632,  0.62405974,
        1.2140013 ,  1.59627366,  1.17699718,  1.11011863,  1.01137257,
        1.27424181,  0.61385417,  0.46987227,  0.9759801 ,  0.68343341,
        0.53237557,  0.6793381 ,  0.40485594,  0.54794008,  0.55342531,
        0.58519548,  0.4213807 ,  0.53137642,  0.96648145,  0.47418526,
        1.018273  ,  0.42210507,  0.57122999,  1.42477369,  0.69322109,
        0.4378559 ,  1.0747174 ,  0.66998529,  0.63857651,  0.4999207 ,
        1.47753966,  1.40612018,  0.50477082,  1.63557124,  0.66427088,
        1.08031964,  0.44587383,  1.56299174,  2.19856787,  0.33994609,
        1.22838247,  1.09135532,  0.5140515 ,  0.53565055,  0.54737002,
        0.49584809,  1.10968137,  0.45442119,  0.63489652,  2.29406357,
        0.57527053,  1.13013721,  1.46603131,  0.56037319,  1.05982244,
        0.8541944 ,  0.64797914,  1.46328151,  2.06331658,  1.18198347,
        1.14532745,  1.42629218,  0.55044883,  0.56926167,  0.22092894,
        0.47292885,  1.37812161,  1.44022894,  0.65858173,  0.48263684,
        0.52305466,  0.51428097,  0.83840215,  1.32551217,  0.47651047,
        1.59928846,  0.68244648,  0.84140319,  0.9045108 ,  0.43812665,
        0.29034892,  0.37342277,  1.70375502,  1.29215324,  0.62352604,
        2.20307136,  1.07089543,  0.6581471 ,  0.37593204,  1.44360352,
        1.69619274,  0.68847704,  0.31797802,  0.46018013,  1.0798614 ,
        0.53224015,  1.01210105,  1.73447263,  0.58204806,  1.09177899,
        0.87288809,  0.60713041,  0.56325537,  1.52981806,  0.43680874], dtype=float32), array([  8.50832558,   2.1948173 ,   1.47156119,   0.86793345,
         7.56267262,   0.52743137,   0.85967022,   0.62174785,
        10.84791374,   0.64472038,   0.82473201,   0.98221791,
         9.53833866,   0.35674441,   0.88372427,   1.6631583 ,
         5.36925888,   0.80712402,   1.09701276,   1.23362648,
         1.11603653,   1.11818266,   1.0072875 ,   0.60041934,
         4.53394604,   0.74621695,   1.43284988,   2.08176494,
         4.83439207,   7.614923  ,  16.18638992,   2.83269095,
         1.31545651,  12.51511383,   0.61761016,   2.36314249,
         2.0805409 ,   4.92221832,   0.31338164,   1.32354605,
         1.58822548,   1.2469064 ,   9.65171814,   2.08367968,
         0.60319811,   2.24880552,   7.51644802,  23.40749359,
         1.50852871,   4.15203428,   4.4296093 ,   6.12872124,
         1.60210764,   0.97422212,   0.90519387,   2.52471852,
         1.20459402,   3.34684873,   3.56357598,   0.69929022,
         1.34190381,  11.26649666,  16.90973663,   7.20012379,
         1.80595815,   2.31266427,   1.50483   ,   1.73913181,
        10.19339943,   0.5774774 ,   5.92594194,   0.72164756,
         8.07087803,   0.5097748 ,   0.71391815,   1.14628077,
         8.90550423,   0.81803834,  13.52776527,  11.54738617,
         1.974316  ,   0.96892279,   0.83481431,   6.48693037,
         0.94531   ,   1.09711421,   5.2431798 ,   1.1112318 ,
         0.92293882,   1.50891697,   0.50405174,   0.46424854,
         2.94919944,   2.2419188 ,   0.82716382,  13.67724323,
         1.24847305,   1.8736645 ,   2.58528948,   4.62311459,
         0.76796061,   1.13415301,   1.42504084,  12.19272518,
         0.82796019,   2.61683321,   0.7400111 ,   9.48672771,
         0.95302379,   0.55652112,   0.94581527,   1.13498831,
         0.6069448 ,   1.30285597,   1.0415802 ,   4.11059332,
         2.08264923,   8.45399284,   5.44919157,   1.7273649 ,
        11.37794971,   0.57353026,   2.17279673,   1.65181482,
         5.30215359,   7.01115894,   9.79617119,   0.8279829 ,
         7.78252077,   8.06094265,   0.84786183,  11.35456276,
         9.68571854,   2.36841798,   0.35148033,   0.7865324 ,
         0.2493078 ,   0.92990696,   1.05748153,   8.88444996,
         0.96934944,   0.59279555,   0.61410642,   5.03259468,
         1.89128411,   9.22836876,   1.73151112,   6.26345968,
         1.92788625,   3.81693125,   1.41330862,   2.70214033,
         1.33875644,   5.00235558,   1.16972899,   1.40213168,
         0.86328924,   5.56713915,   1.25635326,   6.66478014,
         2.57906127,   1.41762722,  11.49940205,   1.33765638,
         1.02641749,   1.54289806,  11.9556179 ,   0.60686398,
         3.93033481,   3.6158402 ,   0.97632676,   8.70140362,
        22.98396301,  12.16709232,   2.21411014,   9.55394077,
         1.70245385,   5.27477169,   4.74877644,   1.01315928,
         1.61747551,   3.68872881,  12.4381752 ,   4.22976732,
         1.97770393,   1.44307554,  13.54532623,   3.2463553 ,
         1.74849665,   5.55321932,   1.15992761,   0.85573345,
         0.5789004 ,   1.21010411,   1.30119526,   5.43155909,
         8.7329483 ,   5.60314178,   4.95734215,   3.36326861,
         6.02288437,   1.08533168,   0.75165719,   3.02577829,
         1.48395681,   1.01436627,   1.15552723,   0.49574229,
         0.94032437,   0.78715765,   1.16505706,   0.76025128,
         0.9624024 ,   2.56594992,   0.70326924,   4.19901848,
         0.61401999,   1.07683265,   6.89854956,   1.37091768,
         0.78282034,   4.47012424,   1.0680604 ,   1.19640577,
         0.61692232,   7.67789888,   6.89219618,   0.96714169,
         9.25763035,   1.96722925,   3.51632738,   0.71485782,
         8.45778179,  18.48162079,   0.45194376,   5.02067232,
         3.45283651,   0.99955332,   1.52892911,   1.08288383,
         1.09432375,   4.1948781 ,   0.84037805,   1.27640307,
        17.65620422,   1.79641867,   4.39294624,   8.48449612,
         1.24177361,   2.94928694,   2.24756908,   1.34676874,
         7.93221474,  17.47136497,   5.24520254,   5.34199238,
         7.50105429,   0.9347291 ,   1.20254219,   0.328188  ,
         1.29352927,   7.25458622,   7.36333513,   1.24048805,
         0.62080872,   1.03044081,   0.72553873,   1.86116457,
         5.35340405,   0.86338186,   9.29167747,   1.67049587,
         2.16708922,   2.42692399,   0.71743947,   0.49024102,
         0.76039803,   9.41758251,   5.52104521,   0.99792773,
        16.67811775,   3.89839673,   2.56528449,   0.60313815,
         7.14359474,  10.68365765,   1.57200813,   0.75602102,
         0.75752109,   4.691257  ,   0.80761522,   3.18514919,
         8.88020992,   1.16923141,   4.12701082,   2.37390041,
         1.1352998 ,   0.97557211,   9.01546001,   0.71548557], dtype=float32), array([[ 0.01159549,  0.03520122, -0.01593706, ...,  0.02099873,
        -0.1069295 ,  0.04460481],
       [-0.01836603,  0.14524864, -0.04236738, ..., -0.03102439,
         0.04471595,  0.03375527],
       [ 0.13399771,  0.06113688,  0.05495071, ...,  0.02231013,
        -0.12279303,  0.06027269],
       ..., 
       [ 0.10680949,  0.06601008, -0.094094  , ..., -0.0516983 ,
        -0.16597772, -0.0913434 ],
       [ 0.06883431, -0.01142251, -0.03041806, ...,  0.05104952,
        -0.07710856, -0.0078621 ],
       [ 0.01068867,  0.03643836, -0.07430133, ..., -0.13916948,
         0.02187075, -0.1535801 ]], dtype=float32), array([-0.01494258, -0.05401299, -0.13052739,  0.07408305, -0.02245741,
       -0.08479612, -0.04052221, -0.10860188, -0.10309392, -0.05075988,
        0.05745222, -0.05651722,  0.08794922,  0.00829016,  0.00629124,
       -0.02105193, -0.0215585 , -0.02681834, -0.08603896,  0.0633048 ,
       -0.00288377, -0.07038491,  0.06781502, -0.09325983], dtype=float32)]
```"
11908,Query in lower-level versions of Get/Put involved in EncodeFixed/DecodeFixed,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 'v1.2.1-0-gb4957ff', '1.2.1'
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: No GPU
- **GPU model and memory**: No GPU
- **Exact command to reproduce**: Run tests from core module.

### The problem
While checking [coding_test.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/core/coding_test.cc) , came across a sub test - `TEST(Coding, EncodingOutput)` which tests that encoding routines generate little-endian encodings. 
This test is passing on a big endian machine. So while debugging realized that the `EncodeFixed/DecodeFixed` functions from [coding.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/core/coding.cc) and [raw_coding.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/core/raw_coding.h) encode/decode `buf` values on big endian incorrectly i.e. the character buffer writing happens in the same way as on a little-endian machine.


After I made the required changes to correct the same, although 1 test of my interest(`TEST(TensorBundleTest, Checksum)`) now passes, I could see many others failing on big endian with errors like:
1. `Data loss: block checksum mismatch: perhaps your file is in a different file format and you need to use a different restore operator?`
2. `""Data loss: corrupted record at 19""`
3. `Segmentation fault`
4. `Invalid argument: sample_rate must be in (0, 2^32), got: 0`

So looks like this change is breaking too many things here. 

Can anyone help in understanding why the correction is causing so many issues? 
Will this change involve too many changes in TensorFlow code to support big endian?
"
11906,Wrong conv2d outputs for tensors of certain sizes,"### System information
- **Custom code**: yes
- **OS Platform and Distribution**: Windows 7 Professional, SP1
- **TensorFlow installed from**: binary
- **TensorFlow version**: tensorflow-gpu 1.3.0rc0
- **Python version**: 3.5.2
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: Nvidia Titan X (Pascal), 12GB
- **Exact command to reproduce**: conv2d

Images (tensors produced from numpy 2d arrays) of certain sizes fail to be properly convolved with conv2d function. Only the stripe on top of the image is processed correctly. The rest appears empty or contains a kind of structured noise. Sometimes it also shifted and wrapped around the image borders.

https://nbviewer.jupyter.org/github/adibrov/jupyterNotebooks/blob/master/bug_tf.ipynb"
11905,No registered 'ResizeBilinear' OpKernel for XLA_CPU_JIT,"Similar to https://github.com/tensorflow/tensorflow/issues/11890, `tf.image.resize_images` and its siblings haven't been made available for XLA yet. Is there a timeline for when core ops will be supported by XLA? Is there a short instruction somewhere on how to implement ops kernels for the XLA bridge so we could do pull requests as needed to speed up development?

Related: https://github.com/tensorflow/tensorflow/issues/11275"
11904,Unclear about how to integrate AttentionWrapper with BeamSearchDecoder,"TF Version: 1.2.1

I cannot find information about how to integrate AttentionWrapper with BeamSearchDecoder on website / nmt tutorial, in particular how to feed the beam-search-tiled (```tf.contrib.seq2seq.tile_batch```) states to the attention-cloned (```zero_state(...).clone(...)```) states.

In my attempt, there seems to be an inconsistency between the requirement of the ```batch_size``` of the zero states generated by ```AttentionWrapper```  in the training stage and predicting stage. In training stage, initial state of decoder requires batch size, however in predicting stage it requires (batch_size * beam_width).
the minimal code to demonstrate this problem is:
``` python
import tensorflow as tf
from tensorflow.python.layers.core import Dense

BEAM_WIDTH = 5
BATCH_SIZE = 128

# INPUTS
X = tf.placeholder(tf.int32, [BATCH_SIZE, None])
Y = tf.placeholder(tf.int32, [BATCH_SIZE, None])
X_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])
Y_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])

# ENCODER         
encoder_out, encoder_state = tf.nn.dynamic_rnn(
    cell = tf.nn.rnn_cell.BasicLSTMCell(128), 
    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),
    sequence_length = X_seq_len,
    dtype = tf.float32)

# ATTENTION
attention_mechanism = tf.contrib.seq2seq.LuongAttention(
    num_units = 128, 
    memory = encoder_out,
    memory_sequence_length = X_seq_len)

decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
    cell = tf.nn.rnn_cell.BasicLSTMCell(128),
    attention_mechanism = attention_mechanism,
    attention_layer_size = 128)

# DECODER COMPONENTS
Y_vocab_size = 10000
decoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))
projection_layer = Dense(Y_vocab_size)

# TRAINING DECODER
training_helper = tf.contrib.seq2seq.TrainingHelper(
    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),
    sequence_length = Y_seq_len,
    time_major = False)
training_decoder = tf.contrib.seq2seq.BasicDecoder(
    cell = decoder_cell,
    helper = training_helper,
    initial_state = decoder_cell.zero_state(BATCH_SIZE,tf.float32).clone(cell_state=encoder_state),
    output_layer = projection_layer)
training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(
    decoder = training_decoder,
    impute_finished = True,
    maximum_iterations = tf.reduce_max(Y_seq_len))
training_logits = training_decoder_output.rnn_output

# PREDICTING_DECODER
predicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(
    cell = decoder_cell,
    embedding = decoder_embedding,
    start_tokens = tf.tile(tf.constant([1], dtype=tf.int32), [BATCH_SIZE]),
    end_token = 2,
    initial_state = decoder_cell.zero_state(BATCH_SIZE * BEAM_WIDTH,tf.float32).clone(
                    cell_state=tf.contrib.seq2seq.tile_batch(encoder_state, BEAM_WIDTH)),
    beam_width = BEAM_WIDTH,
    output_layer = projection_layer,
    length_penalty_weight = 0.0)
predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(
    decoder = predicting_decoder,
    impute_finished = False,
    maximum_iterations = 2 * tf.reduce_max(Y_seq_len))
predicting_logits = predicting_decoder_output.predicted_ids[:, :, 0]
```
the error message is:
```
Traceback (most recent call last):
  File ""test.py"", line 58, in <module>
    initial_state = decoder_cell.zero_state(BATCH_SIZE*BEAM_WIDTH,tf.float32).clone(
  File ""/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py"", line 659, in zero_state
    message=error_message)]):
  File ""/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/check_ops.py"", line 317, in assert_equal
    _assert_static(condition_static, data)
  File ""/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/check_ops.py"", line 100, in _assert_static
    raise ValueError('\n'.join(data_static))
ValueError: When calling zero_state of AttentionWrapper attention_wrapper: Non-matching batch sizes between the memory (encoder output) and the requested batch size.  Are you using the BeamSearchDecoder?  If so, make sure your encoder output has been tiled to beam_width via tf.contrib.seq2seq.tile_batch, and the batch_size= argument passed to zero_state is batch_size * beam_width.
Condition x == y did not hold element-wise:
x (AttentionWrapperZeroState_1/assert_equal/x:0) = 
640
y (AttentionWrapperZeroState_1/assert_equal/y:0) = 
128
```"
11903,Missing input file mpi:mpio.h,"### System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution: **Linux Ubuntu 16.04)**
- TensorFlow installed from: **source**
- TensorFlow version: **master**
- Python version: **3.5.2**
- Bazel version (if compiling from source): **0.5.2**
- CUDA/cuDNN version: **8.0**
- GPU model and memory: **K80**
- Exact command to reproduce:

```bash
#!/usr/bin/env bash
# Only the compilation step for tensorflow is in this script, for clarity.

git clone https://github.com/tensorflow/tensorflow
cd ./tensorflow
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-8.0/targets/x86_64-linux/lib:/usr/lib/x86_64-linux-gnu/
export PYTHON_BIN_PATH=""/home/ubuntu/anaconda3/bin/python""
export PYTHON_LIB_PATH=""/home/ubuntu/anaconda3/lib/python3.6/site-packages""
export TF_NEED_JEMALLOC=1
export TF_NEED_GCP=0
export TF_NEED_HDFS=0
export TF_ENABLE_XLA=0
export TF_NEED_VERBS=0
export TF_NEED_OPENCL=0
export TF_NEED_CUDA=1
export TF_CUDA_CLANG=0
export TF_NEED_MPI=1
export MPI_HOME=""/usr/lib/openmpi""
export GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
export CUDA_VERSION='8.0'
export CUDNN_VERSION='6'
export CUDNN_INSTALL_PATH=/usr/local/cuda
export CUDA_COMPUTE_CAPABILITIES='3.7'
export CUDA_PATH='/usr/local/cuda'
export CUDA_PATH_LINUX='/opt/cuda'
yes """" | ./configure
bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=cuda -k //tensorflow/tools/pip_package:build_pip_package && \
bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
```

### Describe the problem

When turning on `mpi`, the compile fails with the following error:

```error
INFO: Found 1 target...
ERROR: missing input file '//third_party/mpi:mpicxx.h'.
ERROR: missing input file '//third_party/mpi:mpi.h'.
ERROR: missing input file '//third_party/mpi:mpio.h'.
ERROR: /home/ubuntu/scripts/tensorflow/third_party/mpi/BUILD:18:1: //third_party/mpi:mpi: missing input file '//third_party/mpi:mpicxx.h'.
ERROR: /home/ubuntu/scripts/tensorflow/third_party/mpi/BUILD:18:1: //third_party/mpi:mpi: missing input file '//third_party/mpi:mpio.h'.
ERROR: /home/ubuntu/scripts/tensorflow/third_party/mpi/BUILD:18:1: //third_party/mpi:mpi: missing input file '//third_party/mpi:mpi.h'.
```

### Source code / logs
before running the build script I install bazel 0.5.2 (0.5.3 breaks the build)"
11902,//tensorflow/python/kernel_tests:denormal_test test failure on ppc64le (error : Arrays are not equal),"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
    Ubuntu 16.04 (ppc64le)
- **TensorFlow installed from (source or binary)**:
     Installed from source
- **TensorFlow version (use command below)**:
    ('v1.2.1-0-gb4957ff', '1.2.1')
- **Python version**: 
     Python 2.7.5
- **Bazel version (if compiling from source)**:
      0.4.5-2017-07-13 (@037b9b9)
- **CUDA/cuDNN version**:
     NA
- **GPU model and memory**:
      NA
- **Exact command to reproduce**:
bazel test --test_output=errors //tensorflow/python/kernel_tests:denormal_test


### Describe the problem
Following code returning incorrect results -
https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/python/kernel_tests/denormal_test.py#L36-L44

```
def _flushDenormalsTest(self, use_gpu, dtypes):
    with self.test_session(use_gpu=use_gpu):
      array_ops.identity(7).eval()
      for dtype in dtypes:
        tiny = np.finfo(dtype).tiny
        # Small shape to test main thread, large shape to test thread pool
        for shape in (), (1 << 20,):
          flush = 0.1 * constant_op.constant(tiny, shape=shape)
          self.assertAllEqual(flush.eval(), np.zeros(shape))
```

I have printed the values of `flush.eval() `and `np.zeros(shape)` , see below :
`flush.eval()` o/p =  `1.17549463108e-39`
`np.zeros(shape)` o/p = `0.0`

Getting failure `1.17549463108e-39` vs expected `0.0`.  I am not able to understand why we are getting flush.eval() o/p `1.17549463108e-39`  and not  `0.0`.  But I tried changing the  comparison function from assertAllEqual to assertAllClose and test is passing successfully. Is it OK to merge this changes or ? I would like to hear comment on this.Thanks!

This test passing successfully in TF1.0.1 without any changes, see relevant code : 
https://github.com/tensorflow/tensorflow/blob/v1.0.1/tensorflow/python/kernel_tests/denormal_test.py#L31-L48

```
 def testPythonHasDenormals(self):
    """"""Non-tf numpy code should treat denormals correctly.""""""
    for dtype in np.float32, np.float64:
      tiny = np.finfo(dtype).tiny
      self.assertEqual(tiny, tiny / 16 * 16)

  def _flushDenormalsTest(self, use_gpu, dtypes):
    if control_imports.USE_OSS:
      # TODO(irving): Fix denormal flushing for open source.
      return
    with self.test_session(use_gpu=use_gpu):
      array_ops.identity(7).eval()
      for dtype in dtypes:
        tiny = np.finfo(dtype).tiny
        # Small shape to test main thread, large shape to test thread pool
        for shape in (), (1 << 20,):
          flush = 0.1 * constant_op.constant(tiny, shape=shape)
          self.assertAllEqual(flush.eval(), np.zeros(shape))
```
Looks like this test is disabled in TF1.0.1

### Source code / logs
```
$ bazel test --test_output=errors //tensorflow/python/kernel_tests:denormal_test

==================== Test output for //tensorflow/python/kernel_tests:denormal_test:
2017-07-31 08:45:48.796363: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-07-31 08:45:48.798909: I tensorflow/compiler/xla/service/service.cc:198] XLA service 0x100386c7120 executing computations on platform Host. Devices:
2017-07-31 08:45:48.798936: I tensorflow/compiler/xla/service/service.cc:206]   StreamExecutor device (0): <undefined>, <undefined>
FF..
======================================================================
FAIL: testFlushDenormalsCPU (__main__.DenormalTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/725e77151072daec43bc353cb6fcb26c/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/denormal_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/denormal_test.py"", line 50, in testFlushDenormalsCPU
    self._flushDenormalsTest(use_gpu=False, dtypes=(np.float32, np.float64))
  File ""/root/.cache/bazel/_bazel_root/725e77151072daec43bc353cb6fcb26c/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/denormal_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/denormal_test.py"", line 44, in _flushDenormalsTest
    self.assertAllEqual(flush.eval(), np.zeros(shape))
  File ""/root/.cache/bazel/_bazel_root/725e77151072daec43bc353cb6fcb26c/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/denormal_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 699, in assertAllEqual
    np.testing.assert_array_equal(a, b)
  File ""/usr/lib64/python2.7/site-packages/numpy/testing/utils.py"", line 871, in assert_array_equal
    verbose=verbose, header='Arrays are not equal')
  File ""/usr/lib64/python2.7/site-packages/numpy/testing/utils.py"", line 796, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Arrays are not equal

(mismatch 100.0%)
 x: array(1.1754946310819804e-39, dtype=float32)
 y: array(0.0)

======================================================================
FAIL: testFlushDenormalsGPU (__main__.DenormalTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/725e77151072daec43bc353cb6fcb26c/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/denormal_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/denormal_test.py"", line 54, in testFlushDenormalsGPU
    self._flushDenormalsTest(use_gpu=True, dtypes=(np.float32,))
  File ""/root/.cache/bazel/_bazel_root/725e77151072daec43bc353cb6fcb26c/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/denormal_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/denormal_test.py"", line 44, in _flushDenormalsTest
    self.assertAllEqual(flush.eval(), np.zeros(shape))
  File ""/root/.cache/bazel/_bazel_root/725e77151072daec43bc353cb6fcb26c/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/denormal_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 699, in assertAllEqual
    np.testing.assert_array_equal(a, b)
  File ""/usr/lib64/python2.7/site-packages/numpy/testing/utils.py"", line 871, in assert_array_equal
    verbose=verbose, header='Arrays are not equal')
  File ""/usr/lib64/python2.7/site-packages/numpy/testing/utils.py"", line 796, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Arrays are not equal

(mismatch 100.0%)
 x: array(1.1754946310819804e-39, dtype=float32)
 y: array(0.0)

----------------------------------------------------------------------
Ran 4 tests in 0.071s

FAILED (failures=2)
not equal lhs =  1.17549463108e-39
not equal rhs =  0.0
not equal lhs =  1.17549463108e-39
not equal rhs =  0.0

```"
11900,patch is not installed on some of Windows slaves,"http://ci.tensorflow.org/job/tf-master-win-bzl/1335/console
On win0-slave:
```
16:57:47 Analyzing: target //tensorflow/tools/pip_package:build_pip_package
16:58:28 ERROR: C:/tf_jenkins/home/workspace/tf-master-win-bzl/tensorflow/tools/pip_package/BUILD:70:1: error loading package 'tensorflow/contrib/session_bundle': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):
16:58:28 	File ""C:/tf_jenkins/home/workspace/tf-master-win-bzl/tensorflow/workspace.bzl"", line 119
16:58:28 		_apply_patch(repo_ctx, repo_ctx.attr.patch_file)
16:58:28 	File ""C:/tf_jenkins/home/workspace/tf-master-win-bzl/tensorflow/workspace.bzl"", line 110, in _apply_patch
16:58:28 		_execute_and_check_ret_code(repo_ctx, cmd)
16:58:28 	File ""C:/tf_jenkins/home/workspace/tf-master-win-bzl/tensorflow/workspace.bzl"", line 94, in _execute_and_check_ret_code
16:58:28 		fail(""Non-zero return code({1}) when ..., <2 more arguments>))
16:58:28 Non-zero return code(127) when executing 'C:/tools/msys64/usr/bin/bash -c patch -p1 -d C:/tmp/_bazel_system/424zmya1/external/protobuf_archive -i C:/tf_jenkins/home/workspace/tf-master-win-bzl/third_party/protobuf/add_noinlines.patch':
16:58:28 Stdout: 
16:58:28 Stderr: /usr/bin/bash: patch: command not found
16:58:28  and referenced by '//tensorflow/tools/pip_package:si
```"
11899,"Where can I find c++ head file ""tensorflow/cc/ops/sparse_ops.h""","I want to use the sparse tensor in C++ API(https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/add-sparse-to-tensors-map). 
But when I search in ""https://github.com/tensorflow/tensorflow"", there is no ""tensorflow/cc/ops/sparse_ops.h""
Does anyone know how to use the sparse tensor in C++?"
11897,Run mnist_with_summaries.py error：can not open shared object libcupti.so.8.o,"error info as below:
`2017-07-31 11:18:47.859639: I tensorflow/stream_executor/dso_loader.cc:129] Couldn't open CUDA library libcupti.so.8.0. LD_LIBRARY_PATH: 
2017-07-31 11:18:47.859724: F ./tensorflow/stream_executor/lib/statusor.h:205] Non-OK-status: status_ status: Failed precondition: could not dlopen DSO: libcupti.so.8.0; dlerror: libcupti.so.8.0: cannot open shared object file: No such file or directory
`

_____________________________________________
I have add the path to libcupti.so.8.0 to LD_LIBRARY_PATH, but no use.
`export LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH`

if you have any ideas to solve this, please leave your message."
11896,[Feature request] Dynamically add new machines in distributed TensorFlow,"I'm not sure this has been raised before. I did some search on Google and haven't found relevant stuff. If it do exist, please direct me there. Thank you.

I'm currently experimenting with distributed TensorFlow. When building a distributed cluster, all machines in the cluster should be fed into tf.train.Server as parameters. That is, the disitributed cluster configuration is defined when building the computation graph. Like the example provided in https://github.com/tensorflow/models/blob/master/inception/inception/imagenet_distributed_train.py.

But I have also read papers about robust distributed cluster that it would be nice if the framework support dynamically adding or removing machines if the cluster get larger or some machine goes down.

Is this doable in current version of TensorFlow. If so, is there an example to implement this? If not, is there plans for this?"
11894,Slow Hessian,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I have written custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 4.4.0-47-generic #68-Ubuntu and Also macOS Sierra 10.12.3 (CPU only) 
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: cuda/8.0 and cudnn/5.1
- **GPU model and memory**: 12GB, memory:100GB
- **Exact command to reproduce**: tf.hessians(ys,xs)

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
The command tf.hessians(ys, xs) is used to add nodes to the graph in order to compute hessians of ys with respect to xs where both ys and xs are list of tensors. Currently computing hessian with respect to a vector is not possible. The trick is to unstack the input tensor (x) into a list of one dimensional tensors (xs) and compute hessian with respect to each of them separately then stack them together. Tensorflow gets stuck in the phase of graph construction even when the dimension of x (length of list xs) is about one hundred. In the implementation of Hessian in gradients_impl.py the second derivative is implemented as the derivative of partial derivative with respect to each member of xs. I guess the slowness of graph construction is due to this line:

_hess = [gradients(_gradient, x, **kwargs)[0] for _gradient in _gradients]

which may add several unnecessary intermediate nodes with overlapping functionality to the graph due to the for loop. Is there any way other than looping over input dimensions that efficiently constructs the graph in a reasonable time?

### Source code / logs
This source code can simulate the problem:
import tensorflow as tf
import numpy as np
in_dimension = 256
x = tf.placeholder(tf.float32, shape=(1, in_dimension))
x_list = tf.unstack(x, axis=1)
xx= tf.stack(x_list, axis=1)
y = tf.pow(xx,3)
hess = tf.hessians(y, x_list)
sess = tf.Session()
print(sess.run(hess, feed_dict={x : np.random.normal(0, 1, size=(1, in_dimension))}))
"
11892,Softplus with parameters,"Should I go on implementing the parametric version of Softplus?

Current implementation: [softplus_impl.py#L35](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distributions/python/ops/bijectors/softplus_impl.py)

Reference to parametric version: [PyTorch docs](http://pytorch.org/docs/master/nn.html?highlight=softplus#torch.nn.Softplus)"
11891,'//tensorflow/contrib/verbs:rdma,"Build fails 
Thanks for help in advance

1. It must be a **bug** 

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:yes
- **TensorFlow version (use command below)**:Latest git
- **Python version**: 2.7.13
- **Bazel version (if compiling from source)**:Build label: 0.5.2
- **CUDA/cuDNN version**:
Built on Tue_Jan_10_13:22:03_CST_2017
Cuda compilation tools, release 8.0, V8.0.61
- **GPU model and memory**:  [GeForce GTX 860M] 4044MiB Driver Version: 375.66
- **Exact command to reproduce**:
bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=cuda //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
Build fails with carefully chosen configs:
Please specify the location of python. [Default is /home/op/anaconda2/bin/python]:
input the desired Python library path to use.  Default is /home/op/anaconda2/lib/python2.7/site-packages

y jemalloc as malloc support
n Google Cloud Platform
n Hadoop File System support
y XLA JIT support

y  VERBS support
n OpenCL support
y  CUDA support

CUDA SDK version: 8.0
CUDA 8.0 toolkit is installed /usr/local/cuda-8.0

cuDNN 6.0
 cuDNN 6 library is installed: /usr/local/cuda-8.0

Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 5.0]

clang as CUDA compiler? [y/N]: n
nvcc will be used as CUDA compiler
gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]
MPI support? [y/N]: n

flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]:
### Source code / logs
ERROR: /home/op/Downloads/tensorflow/tensorflow/contrib/verbs/BUILD:136:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 150 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
In file included from tensorflow/contrib/verbs/rdma.cc:18:0:
./tensorflow/contrib/verbs/rdma.h:21:30: fatal error: infiniband/verbs.h: No such file or directory
 #include <infiniband/verbs.h>
                              ^
compilation terminated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 3203.696s, Critical Path: 206.50s"
11890,No registered 'MirrorPad' OpKernel for XLA_CPU_JIT,"`tf.pad(..., mode='REFLECT')` doesn't work with tfcompile. Will this be supported and when? 

It's somewhat urgent on my part, and it seems like it should be a simple enough addition unlike control flow as in https://github.com/tensorflow/tensorflow/issues/11275"
11888,Can't import graph containing MutableHashTable,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 18
- **TensorFlow installed from (source or binary)**: Binary (pip)
- **TensorFlow version (use command below)**: v1.3.0.0rc0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
Import a meta graph containing `MutableHashTable` operations fails. The MutableHashTable in my use case is named and inside a scope. After some digging around I found that the error is a result of a collection `saveable_objects` containing names of `MutableHashTable`s but _without_ the proper scoping.

### Source code / logs
```
graph = create_eval_graph()
tf.train.export_meta_graph('eval_model.meta', graph=graph, as_text=True)

# ...
s = tf.train.import_meta_graph('eval_model.meta') # Fails 
```

This throws the following

```
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-238-7de9a7b0d1f2> in <module>()
----> 1 s = tf.train.import_meta_graph('eval_model.meta')

/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py in import_meta_graph(meta_graph_or_file, clear_devices, import_scope, **kwargs)
   1696                                       clear_devices=clear_devices,
   1697                                       import_scope=import_scope,
-> 1698                                       **kwargs)
   1699   if meta_graph_def.HasField(""saver_def""):
   1700     return Saver(saver_def=meta_graph_def.saver_def, name=import_scope)

/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py in import_scoped_meta_graph(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate)
    690           for value in field.value:
    691             col_op = graph.as_graph_element(
--> 692                 ops.prepend_name_scope(value, scope_to_prepend_to_names))
    693             graph.add_to_collection(key, col_op)
    694         elif kind == ""int64_list"":

/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in as_graph_element(self, obj, allow_tensor, allow_operation)
   2704 
   2705     with self._lock:
-> 2706       return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
   2707 
   2708   def _as_graph_element_locked(self, obj, allow_tensor, allow_operation):

/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _as_graph_element_locked(self, obj, allow_tensor, allow_operation)
   2764         if name not in self._nodes_by_name:
   2765           raise KeyError(""The name %s refers to an Operation not in the ""
-> 2766                          ""graph."" % repr(name))
   2767         return self._nodes_by_name[name]
   2768 

KeyError: ""The name 'lstm_c_table' refers to an Operation not in the graph.""
```

Where `lstm_c_table` is created as follows

```
tf.contrib.lookup.MutableHashTable(key_dtype=tf.string, value_dtype=tf.float32, default_value=value, name='lstm_c_table')
```

I looked at the generated proto and it contains the following node:

```
  node {
    name: ""input/lstm_c_table""
    op: ""MutableHashTableOfTensorsV2""
    attr {
      key: ""_output_shapes""
      value {
        list {
          shape {
          }
        }
      }
    }
    attr {
      key: ""container""
      value {
        s: """"
      }
    }
    attr {
      key: ""key_dtype""
      value {
        type: DT_STRING
      }
    }
    attr {
      key: ""shared_name""
      value {
        s: """"
      }
    }
    attr {
      key: ""use_node_name_sharing""
      value {
        b: true
      }
    }
    attr {
      key: ""value_dtype""
      value {
        type: DT_FLOAT
      }
    }
    attr {
      key: ""value_shape""
      value {
        shape {
          dim {
            size: 512
          }
        }
      }
    }
  }
```

```
collection_def {
  key: ""saveable_objects""
  value {
    node_list {
      value: ""lstm_c_table""
      value: ""lstm_h_table""
      value: ""history_table""
      value: ""first_table""
      value: ""encode_lstm_c_table""
      value: ""encode_lstm_h_table""
    }
  }
}
```

Either removing the above collection or prefixing all the values with `input/` results in the following error:

```
TypeError: Can't convert Operation 'input/lstm_c_table' to Tensor (target dtype=None, name=None, as_ref=True)
```

This solution is part of an imitation of `batch_sequences_with_states` so a graph can be exported that does not rely on queues but instead uses placeholders and the `feed_dict` mechanism, for use in interactive model evaluation."
11887,Plan for supporting tf.contrib.seq2seq.prepare_attention in r1.2,"Any plans to support 

https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/prepare_attention

in r1.2?"
11885,Command 'graph_transforms/transform_graph' not found.,"i ran the following script since i want to optimise the graph for using in android.

```
bazel graph_transforms/transform_graph \
--in_graph=stripped.pb \
--out_graph=optimized_stripped.pb \
--inputs='Mul' \
--outputs='final_result' \
--transforms='
  strip_unused_nodes(type=float, shape=""1,160,160,3"")
  fold_constants(ignore_errors=true)
  fold_batch_norms
  fold_old_batch_norms
  round_weights(num_steps=256)
```
Im getting the following error..

`Command 'graph_transforms/transform_graph' not found. Try 'bazel help'.
`"
11884,Bazel build failure with gpu version,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.2.0, I'm installing from tha master branch
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
0.5.3
- **CUDA/cuDNN version**:
5.1
- **GPU model and memory**:
GTX1060 6G
- **Exact command to reproduce**:
`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`

### Describe the problem
I followed every step from the installing & configuration page at tf official [website](https://www.tensorflow.org/install/install_sources#ConfigureInstallation). But when I came the bazel build step, I ran into the problem showed in the error logs below.

I should mention that:

1. I choosed cuda support in the ` ./configure ` step, you can find the configure details below as well.
2. I previously installed the cpu version of tensorflow on the same computer and it worked fine. Now I've uninstalled it of course.

### Source code / logs


My configuration:
```
Please specify the location of python. [Default is /usr/bin/python]: 
Found possible Python library paths:
/usr/local/lib/python2.7/dist-packages
/usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is /usr/local/lib/python2.7/dist-packages
Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: 
jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [y/N]: 
No Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [y/N]: 
No Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: 
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: 
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL support? [y/N]: 
No OpenCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
""Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 5
Please specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]
Do you want to use clang as CUDA compiler? [y/N]: 
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Do you wish to build TensorFlow with MPI support? [y/N]: 
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 
Add ""--config=mkl"" to your bazel command to build with MKL support.
Please note that MKL on MacOS or windows is still not supported.
If you would like to use a local MKL instead of downloading, please set the environment variable ""TF_MKL_ROOT"" every time before build.
Configuration finished

```
My command which introduced the error below:
`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`

My error message:

```
.......
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
	File ""/home/zzh/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1039
		_create_local_cuda_repository(repository_ctx)
	File ""/home/zzh/tensorflow/third_party/gpus/cuda_configure.bzl"", line 976, in _create_local_cuda_repository
		_host_compiler_includes(repository_ctx, cc)
	File ""/home/zzh/tensorflow/third_party/gpus/cuda_configure.bzl"", line 145, in _host_compiler_includes
		get_cxx_inc_directories(repository_ctx, cc)
	File ""/home/zzh/tensorflow/third_party/gpus/cuda_configure.bzl"", line 120, in get_cxx_inc_directories
		set(includes_cpp)
depsets cannot contain mutable items
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
	File ""/home/zzh/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1039
		_create_local_cuda_repository(repository_ctx)
	File ""/home/zzh/tensorflow/third_party/gpus/cuda_configure.bzl"", line 976, in _create_local_cuda_repository
		_host_compiler_includes(repository_ctx, cc)
	File ""/home/zzh/tensorflow/third_party/gpus/cuda_configure.bzl"", line 145, in _host_compiler_includes
		get_cxx_inc_directories(repository_ctx, cc)
	File ""/home/zzh/tensorflow/third_party/gpus/cuda_configure.bzl"", line 120, in get_cxx_inc_directories
		set(includes_cpp)
depsets cannot contain mutable items
INFO: Elapsed time: 5.610s
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/tools/pip_package

```
"
11883,Error while building android project from tensorflow/examples,"`Error:/Users/sagarsuri/tensorflow/tensorflow/core/BUILD:194:1 Failed to get cached inputs: Could not determine containing package for external/protobuf/src/google/protobuf/stubs/common.h.`

I am getting the above error while building Android project which is provided in the examples folder.

I executed this command to pull tensorflow project to my mac:

`git clone https://github.com/tensorflow/tensorflow.git --depth 1`"
11882,conv2d_transpose produce different results on GPU,"### System information
== cat /etc/issue ===============================================
Linux ST 4.2.0-42-generic #49~14.04.1-Ubuntu SMP Wed Jun 29 20:22:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""14.04.4 LTS, Trusty Tahr""
VERSION_ID=""14.04""

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 4.9.4-2ubuntu1~14.04.1) 4.9.4
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux ST 4.2.0-42-generic #49~14.04.1-Ubuntu SMP Wed Jun 29 20:22:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
msgpack-numpy (0.4.1)
numpy (1.13.1)
protobuf (3.2.0)
tensorflow (0.10.0)
tensorflow-gpu (1.0.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.0.0
tf.GIT_VERSION = v1.0.0-rc2-15-g47bba63-dirty
tf.COMPILER_VERSION = v1.0.0-rc2-15-g47bba63-dirty
Sanity check: array([1], dtype=int32)
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally

== env ==========================================================
LD_LIBRARY_PATH /home/abc/torch/install/lib:/usr/lib/x86_64-linux-gnu:/home/abc/torch/install/lib:/usr/local/cuda/lib64:/usr/local/cuda/lib64:/home/abc/torch/install/lib:/home/abc/code/torch/torch/install/lib:/usr/local/cuda/lib64::/usr/local/computecpp/lib:/data/software/gurobi652/linux64/lib
DYLD_LIBRARY_PATH /home/abc/torch/install/lib:/home/abc/torch/install/lib:/home/abc/code/torch/torch/install/lib:

== nvidia-smi ===================================================
Sun Jul 30 17:45:45 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1080    Off  | 0000:01:00.0      On |                  N/A |
|  0%   53C    P2    47W / 260W |   7909MiB /  8112MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1308    G   /usr/bin/X                                     357MiB |
|    0      2590    G   compiz                                         229MiB |
|    0      3254    G   ...el-token=CBAE43C38254E155E78826C3F38F0092    99MiB |
|    0      9480    C   python                                        1039MiB |
|    0     10432    C   /usr/bin/python                               5895MiB |
|    0     20408    C   /usr/bin/python                                283MiB |
|    0     28024    G   /usr/local/MATLAB/R2015a/bin/glnxa64/MATLAB      2MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/lib/python2.7/dist-packages/torch/lib/libcudart.so.8.0
/usr/local/lib/python2.7/dist-packages/torch/lib/libcudart.so
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/MATLAB/R2017a/bin/glnxa64/libcudart.so.8.0.44
/usr/local/MATLAB/R2015a/bin/glnxa64/libcudart.so.6.5.14
### Describe the problem
I am trying to use `tf.nn.conv2d_transpose` but it produces different results every time on GPU. However, the result would be the same when switching the device to CPU. It seems like a bug. Please check the toy model below for more details.

### Source code / logs
```python
import tensorflow as tf
import numpy as np

np.random.seed(1234)
conv_ = np.random.randn(10, 7, 7, 56)

with tf.device('/gpu:0'):
    bottom = tf.constant(conv_, dtype=tf.float32)
    weight = tf.get_variable(""weight"", [9, 9, 1, 56], initializer=tf.random_normal_initializer(0, 0.001))
    bias = tf.get_variable(""bias"", initializer=np.zeros(1, dtype=np.float32))	

    conv = tf.nn.conv2d_transpose(bottom, weight, [10, 19, 19, 1], [1, 3, 3, 1], padding='SAME')
    conv = tf.nn.bias_add(conv, bias)

sess = tf.Session()
sess.run(tf.global_variables_initializer())
np.array_equal(sess.run(conv), sess.run(conv))
```
`Out[2]: False`"
11881,python ImportError: No module named 'scipy',"In [1]:from sklearn import linear_model
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-1-a6ebbebad697> in <module>()
----> 1 from sklearn import linear_model
C:\ProgramData\Anaconda3\envs\tensorflow-gpu\lib\site-packages\sklearn\__init__.py in <module>()
     55 else:
     56     from . import __check_build
---> 57     from .base import clone
     58     __check_build  # avoid flakes unused variable error
     59 

C:\ProgramData\Anaconda3\envs\tensorflow-gpu\lib\site-packages\sklearn\base.py in <module>()
      8 
      9 import numpy as np
---> 10 from scipy import sparse
     11 from .externals import six
     12 from .utils.fixes import signature

ImportError: No module named 'scipy'"
11877,Why tf.FIFOQueue didn't removed when using tf.reset_default_graph?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Pip
- **TensorFlow version (use command below)**:  1.2.1
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**:

### Describe the problem
I want to empty the entire session and grapth while every loop . But the FIFOQueue seems didn't  removed when I  using tf.reset_default_graph.  

So how to make everything clean in currrent process(without create a subprocess \ or change FIFO's name)?

### Source code / logs
```
import tensorflow as tf

cluster = tf.train.ClusterSpec({""ps"": [""localhost:65062""], ""worker"": [""localhost:65063""]})
ps = tf.train.Server(cluster, job_name=""ps"", task_index=0)
worker = tf.train.Server(cluster, job_name=""worker"", task_index=0)

while True:
  print ""begin a new job""

  print(""PS: {0}"".format(ps.target))
  print(""Worker: {0}"".format(worker.target))

  with tf.Session(worker.target) as sess:
    with tf.device(""/job:ps/task:0""):
        W = tf.Variable(tf.zeros([784, 10]))
        b = tf.Variable(tf.zeros([10]))
        file_queue = tf.FIFOQueue(10,
                                     [tf.int32, tf.bool, tf.string, tf.string],
                                     shared_name = 'global_queue')
    init = tf.global_variables_initializer()
    sess.run([init, file_queue.close()])

  tf.reset_default_graph()
  time.sleep(2)
```

### ERROR
```
CancelledError (see above for traceback): Queue 'global_queue' is already closed.
	 [[Node: fifo_queue_Close = QueueCloseV2[cancel_pending_enqueues=false, _device=""/job:ps/replica:0/task:0/cpu:0""](fifo_queue)]]
```"
11876,All Windows GPU build links are broken.,"The link (bolded) is broken in the webpage of ""https://github.com/tensorflow/tensorflow"".

<li>Windows GPU: <a href=""**https://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows-gpu,PY=35/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tensorflow_gpu-1.3.0rc1-cp35-cp35m-win_amd64.whl**"">Python 3.5 64-bit</a> (<a href=""https://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows-gpu,PY=35/"">build history</a>) / <a href=""https://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows-gpu,PY=36/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tensorflow_gpu-1.3.0rc1-cp36-cp36m-win_amd64.whl"">Python 3.6 64-bit</a> (<a href=""https://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows-gpu,PY=36/"">build history</a>)</li>

Some other links in this webpage are also broken. 

But the file of the corresponding link in the webpage of the ""Build history"" can be downloaded. "
11872,How to shutdown a tf.train.Server's port?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Pip
- **TensorFlow version (use command below)**:  1.2.1
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**:

### Describe the problem
When I  createt a tf.train.Server object, It's open a port immediately, But I can not find any way to close the Server(I means the port tf.train.Server started).   

What can I do if I want to start-run-stop a tf.train.Server in a loop?

I need to start tf.train.Server in a while loop.

**one more thing: on my macbook, the code  can run without error, but on a linux server failed.**

### Source code / logs
```
import tensorflow as tf

while True:
  print ""begin a new job""
  cluster = tf.train.ClusterSpec({""ps"": [""localhost:65062""], ""worker"": [""localhost:65063""]})
  ps = tf.train.Server(cluster, job_name=""ps"", task_index=0)
  worker = tf.train.Server(cluster, job_name=""worker"", task_index=0)

  print(""PS: {0}"".format(ps.target))
  print(""Worker: {0}"".format(worker.target))

  with tf.Session(worker.target) as sess:
    with tf.device(""/job:ps/task:0""):
        W = tf.Variable(tf.zeros([784, 10]))
        b = tf.Variable(tf.zeros([10]))

    init = tf.global_variables_initializer()
    print(""RUNNING SESSION"")
    sess.run(init)
    print(""SESSION FINISHED"")
  tf.reset_default_graph()
  time.sleep(2)
```

Error
```
E0730 01:49:45.923951794   57304 server_chttp2.c:159]        {""created"":""@1501350585.923875462"",""description"":""No address added out of total 1 resolved"",""file"":""external/grpc/src/core/ext/transport/chttp2/server/insecure/server_chttp2.c"",""file_line"":125,""referenced_errors"":[{""created"":""@1501350585.923871295"",""description"":""Failed to add port to server"",""file"":""external/grpc/src/core/lib/iomgr/tcp_server_posix.c"",""file_line"":634,""referenced_errors"":[{""created"":""@1501350585.923863108"",""description"":""Unable to configure socket"",""fd"":18,""file"":""external/grpc/src/core/lib/iomgr/tcp_server_posix.c"",""file_line"":355,""referenced_errors"":[{""created"":""@1501350585.923845040"",""description"":""OS Error"",""errno"":98,""file"":""external/grpc/src/core/lib/iomgr/tcp_server_posix.c"",""file_line"":331,""os_error"":""Address already in use"",""syscall"":""bind""}]}],""target_address"":""ipv6:[::]:65062""}]}
Traceback (most recent call last):
  File ""test.py"", line 16, in <module>
    ps = tf.train.Server(cluster, job_name=""ps"", task_index=0)
  File ""/home/mk/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/server_lib.py"", line 145, in __init__
    self._server_def.SerializeToString(), status)
  File ""/home/mk/anaconda2/lib/python2.7/contextlib.py"", line 24, in __exit__
    self.gen.next()
  File ""/home/mk/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server
```
"
11871,Error during compilation of tensorflow-GPU using bazel 0.5.3,"Using Bazel 0.5.3 (from installer, both sh and deb) on Ubuntu 16.04 to compile tensorflow-gpu, I get the following error that fails compilation:

` Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
	File ""/home/nicola/Software/tensorflow/gpu/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1039
		_create_local_cuda_repository(repository_ctx)
	File ""/home/nicola/Software/tensorflow/gpu/tensorflow/third_party/gpus/cuda_configure.bzl"", line 976, in _create_local_cuda_repository
		_host_compiler_includes(repository_ctx, cc)
	File ""/home/nicola/Software/tensorflow/gpu/tensorflow/third_party/gpus/cuda_configure.bzl"", line 145, in _host_compiler_includes
		get_cxx_inc_directories(repository_ctx, cc)
	File ""/home/nicola/Software/tensorflow/gpu/tensorflow/third_party/gpus/cuda_configure.bzl"", line 120, in get_cxx_inc_directories
		set(includes_cpp)
depsets cannot contain mutable items
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
	File ""/home/nicola/Software/tensorflow/gpu/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1039
		_create_local_cuda_repository(repository_ctx)
	File ""/home/nicola/Software/tensorflow/gpu/tensorflow/third_party/gpus/cuda_configure.bzl"", line 976, in _create_local_cuda_repository
		_host_compiler_includes(repository_ctx, cc)
	File ""/home/nicola/Software/tensorflow/gpu/tensorflow/third_party/gpus/cuda_configure.bzl"", line 145, in _host_compiler_includes
		get_cxx_inc_directories(repository_ctx, cc)
	File ""/home/nicola/Software/tensorflow/gpu/tensorflow/third_party/gpus/cuda_configure.bzl"", line 120, in get_cxx_inc_directories
		set(includes_cpp)`

Compilation proceeds just fine using Bazel 0.5.2. Note: the error occurs when I try to compile ANY version of tensorflow (tested 1.2.1, 1.3rc0, 1.3rc1, git).

------------------------

### System information
- **Have I written custom code**: No
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from **: source
- **TensorFlow version**: v1.2.0-2210-g49961e5 1.2.1, v.1.3 (rc0, rc1), HEAD
- **Python version**: 3.5.2
- **Bazel version**: (Both 0.5.2 and 0.5.3 from deb installer, and sh installer)
- **CUDA/cuDNN version**: CUDA 8, CuDNN 6.1
- **GPU model and memory**: GeForce 1050Ti - 4Gb

### Describe the problem###
Make sure you clean the bazel cache. 
Install bazel 0.5.2 (from installer, deb or sh).
Try compilation of tensorflow-gpu. it should work fine. 
Remove bazel 0.5.2 and clean its cache. 
Instal bazel 0.5.3 (from installer, deb or sh)
Repeat compilation of tensorflow and the error appears, preventing the full compilation.
"
11868,keras resnet50 example yields different predictions than in stand-alone keras,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No (using example from Keras documentation here: https://keras.io/applications/#classify-imagenet-classes-with-resnet50
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.12.6
- **TensorFlow installed from (source or binary)**: Binary (CPU Version)
- **TensorFlow version (use command below)**: `('v1.2.0-5-g435cdfc', '1.2.1')`
- **Python version**: 2.7 (OS X system version)
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```python
from tensorflow.contrib.keras.python.keras.applications.resnet50 import ResNet50
from tensorflow.contrib.keras.python.keras.preprocessing import image
from tensorflow.contrib.keras.python.keras.applications.resnet50 import preprocess_input, decode_predictions

import numpy as np

model = ResNet50(weights='imagenet')

img_path = 'elephant.jpg'
img = image.load_img(img_path, target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)

preds = model.predict(x)

print('Predicted:', decode_predictions(preds, top=3)[0])
```

### Describe the problem

The above code yields the following output:

```python
('Predicted:', [(u'n02098286', u'West_Highland_white_terrier', 1.0), 
                (u'n15075141', u'toilet_tissue', 0.0), 
                (u'n02319095', u'sea_urchin', 0.0)])
```

However, the same code run using Stand-alone Keras yields this:

```python
('Predicted:', [(u'n02504013', u'Indian_elephant', 0.91937912), 
                (u'n01871265', u'tusker', 0.070962951), 
                (u'n02504458', u'African_elephant', 0.0095201703)])
```

Note: to reproduce under stand-alone Keras substitute this code for the imports at the top:

```python
from keras.applications.resnet50 import ResNet50
from keras.preprocessing import image
from keras.applications.resnet50 import preprocess_input, decode_predictions
```

Also note that you need the 'elephant.jpg' file in the working directory to reproduce. You can find that file here: https://github.com/rstudio/keras/blob/master/docs/articles/elephant.jpg

"
11867,Dense layer throwing error with inputs from Keras Concatenate layer,"I think this is the most weird error I have ever seen. I concatenated the outputs of `max_pool` and `average_pool` using two different methods and passed the output to another dense layer as :

```
1)   concatenated_layer = tf.concat([max_pool_output, avg_pool_output], axis=1)
      fc_layer = tf.layers.dense(concatenated_layer, 1024, tf.nn.relu)     # works fine

2) concatenated_layer = keras.layers.Concatenate([max_pool_output, avg_pool_output])
    fc_layer = tf.layers.dense(concatenated_layer, 1024, tf.nn.relu)     # throws error
```

The error thrown is this:

```
AttributeError                            Traceback (most recent call last)
<ipython-input-26-7320655fd554> in <module>()
----> 1 fc_layer = tf.layers.dense(concatenated_layer, 1024,tf.nn.relu,)

~/anaconda2/envs/Kaggle/lib/python3.5/site-packages/tensorflow/python/layers/core.py in dense(inputs, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, trainable, name, reuse)
    213                 trainable=trainable,
    214                 name=name,
--> 215                 dtype=inputs.dtype.base_dtype,
    216                 _scope=name,
    217                 _reuse=reuse)

AttributeError: 'Concatenate' object has no attribute 'dtype'
```

As the incoming inputs are all `float32`, shouldn't the dense layer infer that itself?"
11866,error in docs,"The docs [here]( https://www.tensorflow.org/versions/r0.12/api_docs/python/image/working_with_bounding_boxes)  for bounding boxes  have an error :
The paragraph reading 

`For example, if an image is 100 x 200 pixels and the bounding box is [0.1, 0.2, 0.5, 0.9], the bottom-left and upper-right coordinates of the bounding box will be (10, 40) to (50, 180).`

would better go something like:

`For example, if an image is 100 x 200 pixels and the bounding box is [0.1, 0.2, 0.5, 0.9], the upper-left and bottom-right coordinates of the bounding box will be (10, 40) to (50, 180), using a somewhat idiosyncratic (y,x) notation, or (40,10) to (180,50) using the rather more common (x,y) notation for points in a plane.  All this is with 'usual' coordinate axes (origin in the top left corner, increasing x to right and increasing y going down)`"
11865,"Batchnorm errors in ""successful"" Windows builds","* Current system configuration: 
* Windows 10 64 bit, intel i7-7700HQ latest microcode, Nvidia 1050 4GB.
* Driver: 384.94
* Python used: Anaconda 4.4.0 Python 3.6.2 and 3.5.3
* CUDA/cuDNN: 8.0.61/5.1 or 8.0.61.2/6.0
* swigwin 3.0.12
* Built 1.2.1 from source using VS 2015 Update 3, CMake 3.9.0 or 3.9.0 RC5, swigwin 3.0.12.
* Code modifications: in builds with both cuDNN and AVX enabled, the code was [modified accord to this comment](https://github.com/tensorflow/tensorflow/issues/11096#issuecomment-312049089)
* Issue description: In certain conditions ""successful"" builds of tensorflow with GPU support, results in broken batchnorm functionality. An example error:
 > InvalidArgumentError (see above for traceback): indices[1] is out of range
	 [[Node: gradients/batch_normalization/moments/Mean_1_grad/DynamicStitch = DynamicStitch[N=2, T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](gradients/batch_normalization/moments/Mean_1_grad/range, gradients/batch_normalization/moments/Mean_1_grad/mod, gradients/batch_normalization/moments/Mean_1_grad/Shape, gradients/batch_normalization/moments/Mean_1_grad/Fill)]]

This error was encountered in a variety of different builds. But it was most surprising when it occurred in an unmodified python 3.6 gpu build. Files and [configuration can be found here](https://github.com/aluo-x/tensorflow_windows).
"
11864,tf.nn.conv2d produces incorrect results,"conv2d doesn't seem to produce results that are correct when compared with a C++ implementation and MATLAB's conv2. These errors are most pronounced when importing parameters from an external model. The same network in tf produces an accuracy of 60% while the original network has an accuracy of 97%. To verify that the problem indeed is in tf's conv2d, I used the following simple example:
**MATLAB code**
`k2=reshape(1:9,[3,3])'`
`k2=repmat(k2,[1,1,3])`
`x2=reshape(1:16,[4,4])'`
`x2=repmat(x2,[1,1,3])`
`c=conv2(x2(:,:,1),k2(:,:,1),'same') + conv2(x2(:,:,2),k2(:,:,2),'same')+ conv2(x2(:,:,3),k2(:,:,3),'same')`

**output:**
`          87         186         249         225`
`         297         576         711         594`
`         621        1116        1251         990`
`         789        1338        1455        1095`

**Python TF code**
`import tensorflow as tf`
`import numpy as np`

`k=np.reshape(range(1,10),[3,3,1,1])`
`k=np.repeat(k,3,2).astype('float16')`

`x=np.reshape(range(1,17),[1,4,4,1])`
`x=np.repeat(x,3,3).astype('float16')`

`dev_X = tf.Variable(x,dtype=tf.float16)`
`dev_K = tf.Variable(k,dtype=tf.float16)`

`dev_C=tf.nn.conv2d(dev_X, dev_K, strides=[1, 1, 1, 1], padding='SAME')`

`session=tf.Session(); `
`session.run(tf.global_variables_initializer()); `
`print(session.run(dev_C)[0,:,:,0])`

**output:**
`[[  333.   534.   651.   435.]`
` [  693.  1044.  1179.   756.]`
` [ 1089.  1584.  1719.  1080.]`
` [  591.   822.   885.   525.]]`




What exactly is different in tf's implementation of convolution? Is this a bug?"
11863,"AbortionError(code=StatusCode.NOT_FOUND, details=""FeedInputs: unable to find feed output ToFloat:0"")","Hello
I am trying to host the *ssd_mobilenet_v1_coco* model from the Tensorflow Object Detection API model zoo with Tensorflow Serving. 

I was able to successfully export the model with the exporter script in `models/object_detection/exporter.py` as a SavedModel. The issue arises when I tried to run the modified client

```
from __future__ import print_function
from grpc.beta import implementations
import tensorflow as tf
from tensorflow_serving.apis import predict_pb2
from tensorflow_serving.apis import prediction_service_pb2


tf.app.flags.DEFINE_string('server', 'localhost:9000',
                           'PredictionService host:port')
tf.app.flags.DEFINE_string('image', '', 'path to image in JPEG format')
FLAGS = tf.app.flags.FLAGS


def main(_):
  host, port = FLAGS.server.split(':')
  channel = implementations.insecure_channel(host, int(port))
  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)
  with open(FLAGS.image, 'rb') as f:
    data = f.read()
    request = predict_pb2.PredictRequest()
    request.model_spec.name = 'mobilenet_v1'
    request.model_spec.signature_name = 'serving_default'
    request.inputs['inputs'].CopyFrom(
        tf.contrib.util.make_tensor_proto(data, shape=[1]))
    result = stub.Predict(request, 10.0)  # 10 secs timeout
    print(result)


if __name__ == '__main__':
  tf.app.run()
```

Here's the traceback for the gRPC request

```
Traceback (most recent call last):
  File ""/home/mcw-nn/Documents/SSP/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/tf_serving/tensorflow_serving/example/inception_client.py"", line 56, in <module>
    tf.app.run()
  File ""/home/mcw-nn/Documents/SSP/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/home/mcw-nn/Documents/SSP/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/tf_serving/tensorflow_serving/example/inception_client.py"", line 51, in main
    result = stub.Predict(request, 10.0)  # 10 secs timeout
  File ""/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py"", line 324, in __call__
    self._request_serializer, self._response_deserializer)
  File ""/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py"", line 210, in _blocking_unary_unary
    raise _abortion_error(rpc_error_call)
grpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.NOT_FOUND, details=""FeedInputs: unable to find feed output ToFloat:0"")
```

I am using this [commit](https://github.com/tensorflow/tensorflow/tree/16d39e94e3724417fcaed87035434e098e892842) of tensorflow to build the model server which is the default submodule of tensorflow serving"
11859,no such package '@local_config_cuda//cuda',"Please go to Stack Overflow for help and support:

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

No

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Linux — Ubuntu Server 16.04 LTS

Linux PowerEdge-R810 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

- **TensorFlow installed from (source or binary)**:

Source

- **TensorFlow version (use command below)**:

master, r1.3, r1.2.1

- **Python version**: 

2.7

- **Bazel version (if compiling from source)**:

Build label: 0.5.3
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jul 28 08:34:59 2017 (1501230899)
Build timestamp: 1501230899
Build timestamp as int: 1501230899


- **CUDA/cuDNN version**:

CUDA 8, cuDNN 6

- **GPU model and memory**:

GTX 1080, 8gb

- **Exact command to reproduce**:

`./configure;bazel build -c opt —config=cuda //tensorflow/tools/pip_package:build_pip_package`



-**Premade script**:
```
alex@PowerEdge-R810:~$ sh tf_env_collect.sh 
Collecting system information...
tf_env_collect.sh: 39: [: Linux: unexpected operator
tf_env_collect.sh: 41: [: Linux: unexpected operator
Traceback (most recent call last):
  File ""/tmp/check_tf.py"", line 1, in <module>
    import tensorflow as tf;
ImportError: No module named tensorflow
Wrote environment to tf_env.txt. You can review the contents of that file.
and use it to populate the fields in the github issue template.

cat tf_env.txt

alex@PowerEdge-R810:~$ cat tf_env.txt

== cat /etc/issue ===============================================
Linux PowerEdge-R810 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux PowerEdge-R810 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.11.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named tensorflow

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda-8.0/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri Jul 28 18:12:37 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.59                 Driver Version: 384.59                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1080    Off  | 00000000:0E:00.0 Off |                  N/A |
| 37%   32C    P0    32W / 180W |      0MiB /  8105MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
```

### Describe the problem

I am trying to compile tensorflow from source. When I run the above command, the bazel build fails. This is the configuration I used as well as the error. It seems like some other people had this issue but the threads were for much older versions and none of the solutions fixed it: 


```
Please specify the location of python. [Default is /usr/bin/python]: 
Found possible Python library paths:
/usr/local/lib/python2.7/dist-packages
/usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is /usr/local/lib/python2.7/dist-packages
Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: 
jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [y/N]: y
Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [y/N]: 
No Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: 
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: 
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL support? [y/N]: y
OpenCL support will be enabled for TensorFlow.

Please specify which C++ compiler should be used as the host C++ compiler. [Default is /usr/bin/g++]: 
Please specify which C compiler should be used as the host C compiler. [Default is /usr/bin/gcc]: 
Please specify the location where ComputeCpp for SYCL 1.2 is installed. [Default is /usr/local/computecpp]: 
Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
""Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 
Please specify the location where cuDNN 6 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]
Do you want to use clang as CUDA compiler? [y/N]: 
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Do you wish to build TensorFlow with MPI support? [y/N]: 
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 
Add ""--config=mkl"" to your bazel command to build with MKL support.
Please note that MKL on MacOS or windows is still not supported.
If you would like to use a local MKL instead of downloading, please set the environment variable ""TF_MKL_ROOT"" every time before build.
Configuration finished
...............
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
	File ""/home/alex/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1039
		_create_local_cuda_repository(repository_ctx)
	File ""/home/alex/tensorflow/third_party/gpus/cuda_configure.bzl"", line 976, in _create_local_cuda_repository
		_host_compiler_includes(repository_ctx, cc)
	File ""/home/alex/tensorflow/third_party/gpus/cuda_configure.bzl"", line 145, in _host_compiler_includes
		get_cxx_inc_directories(repository_ctx, cc)
	File ""/home/alex/tensorflow/third_party/gpus/cuda_configure.bzl"", line 120, in get_cxx_inc_directories
		set(includes_cpp)
depsets cannot contain mutable items
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
	File ""/home/alex/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1039
		_create_local_cuda_repository(repository_ctx)
	File ""/home/alex/tensorflow/third_party/gpus/cuda_configure.bzl"", line 976, in _create_local_cuda_repository
		_host_compiler_includes(repository_ctx, cc)
	File ""/home/alex/tensorflow/third_party/gpus/cuda_configure.bzl"", line 145, in _host_compiler_includes
		get_cxx_inc_directories(repository_ctx, cc)
	File ""/home/alex/tensorflow/third_party/gpus/cuda_configure.bzl"", line 120, in get_cxx_inc_directories
		set(includes_cpp)
depsets cannot contain mutable items
INFO: Elapsed time: 13.663s
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/tools/pip_package
```

"
11856,tf.variables_initializer seems broken.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: The nightly wheel.
- **TensorFlow version (use command below)**: The nightly build that went out at 2AM on 7/28/2017: https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/
- **Python version**: 2.7.13
- **Bazel version (if compiling from source)**: 0.5.3
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: GTX 1080
- **Exact command to reproduce**:

To reproduce, run this snippet in the python interpreter:
```
import tensorflow as tf
with tf.Session() as sess:
  v = tf.Variable(42, dtype=tf.int32)
  with tf.control_dependencies([tf.variables_initializer([v])]):
    result = tf.assign(v, v + 1)
  print(sess.run(result))
```

### Describe the problem
Running the snippet above yields an exception:

```
Caused by op u'Variable/read', defined at:
  File ""<stdin>"", line 2, in <module>
  File ""/usr/home/agent007/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 199, in __init__
    expected_shape=expected_shape)
  File ""/usr/home/agent007/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 330, in _init_from_args
    self._snapshot = array_ops.identity(self._variable, name=""read"")
  File ""/usr/home/agent007/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1408, in identity
    _result = _op_def_lib.apply_op(""Identity"", input=input, name=name)
  File ""/usr/home/agent007/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 328, in apply_op
    op_type_name, name, **keywords)
  File ""/usr/home/agent007/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/home/agent007/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2619, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/home/agent007/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1205, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

FailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable
	 [[Node: Variable/read = Identity[T=DT_INT32, _class=[""loc:@Variable""], _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable)]]
```

It seems like `tf.variables_initializer` is broken - it does not initialize the variables passed to it.

This has caused a TensorBoard test (:summary_test) to fail today (TensorBoard runs tests on nightly TensorFlow).
https://travis-ci.org/tensorflow/tensorboard/jobs/258655621
The test had been passing yesterday (on nightly built on 7/27).
"
11851,'No module named tensorflow'  error in jupyter notebook after installing tensorflow-gpu using pip in the same virtual environment. I am using python 2.7.12.," I am trying to use tensorflow-gpu on Ubuntu16.04. I installed tensorflow via pip inside virtual environment, and other required libraries. When I start jupyter notebook in the same environment and try to run a code that uses tensorflow, the line 'import tensorflow as tf' gives an error. 
It says no module named tensorflow. I had a similar problem with skimage(I initially installed scikit-image instead of python-skimage and I was getting an error. But it got resolved after I installed the right library. However I can't make sense of why it can't find the tensorflow module. "
11848,Can't import graph containing batch_sequences_with_states,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 18
- **TensorFlow installed from (source or binary)**: Binary (pip)
- **TensorFlow version (use command below)**: v1.3.0.0rc0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
As I'm working with sequences I make extensive use of `tf.contrib.training.batch_seq_with_states`. I need to be able to load my graph afterwards so I write out a meta graph file containing a graph definition using `Saver.save`. Upon loading using `tf.train.import_meta_graph` I get an error hinting the `batch_seq_with_states` operation isn't saved in the graph:

```
(Pdb) tf.train.import_meta_graph('model.ckpt-1.meta')
*** KeyError: ""The name 'input/batch_seq_with_states/InputQueueingStateSaver/' refers to an Operation not in the graph.""
```

Using Tensorboard I can inspect `input/batch_seq_with_states/InputQueueingStateSaver` just fine from the same files.

I will for now try to work around this by writing out a separate graph that relies on placeholders for data loading instead of `batch_seq_with_states` and then load the weights separate. "
11847,Bug: Op type not registered 'BlockLSTM' in binary,"### Describe the problem
I want to load and run a single tensorflow model within another C++ project. To do this, I defined the `tensorflow_all` library in [`tensorflow/BUILD`] which should include all the necessary dependencies. (Details below)

When loading the tensorflow model via the C++ API ([LoadSavedModel]), the following runtime error occurs:

[LoadSavedModel]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/loader.h#L50

```
2017-07-28 13:36:41.875493: I tensorflow/cc/saved_model/loader.cc:284] Loading SavedModel: fail. Took 43550 microseconds.
terminate called after throwing an instance of 'std::runtime_error'
  what():  Not found: Op type not registered 'BlockLSTM' in binary running on myMachine. 
  Make sure the Op and Kernel are registered in the binary running in this process.
```

I found that the BlockLSTM Op is registered in [`tensorflow/contrib/rnn/ops/lstm_ops.cc`].
However, I could not find a way to include the operation BlockLSTM in my C++ library `tensorflow_all`.

[`tensorflow/contrib/rnn/ops/lstm_ops.cc`]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/ops/lstm_ops.cc#L179

Bear with me if this is actually not a bug (or even a feature request). I am still getting started with tensorflow. 

Since my implementation is part of another larger project, it is currently difficult for me to give an easy to reproduce example. However, if this is necessary, I will do it as soon as I find the time.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

	Yes, a colleague of mine trained a tensorflow graph which uses the BlockLSTM Op from `tensorflow/contrib/rnn/` using the python API. 

	As mentioned before, I added code to the [`tensorflow/BUILD`] to create a library `tensorflow_all`. This library is used to load the graph externally.

	[`tensorflow/BUILD`]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/BUILD

	```bazel
	cc_binary(
		name = ""libtensorflow_all.so"",
		linkshared = 1,
		linkopts = [""-Wl,--version-script=tensorflow/tf_version_script.lds""],
		deps = [
			""//tensorflow/cc:cc_ops"",
			""//tensorflow/cc:client_session"",
			""//tensorflow/cc/saved_model:loader"",
			""//tensorflow/cc/saved_model:tag_constants"",
			""//tensorflow/core:all_kernels"",
			""//tensorflow/core:framework_internal"",
			""//tensorflow/core:tensorflow"",
			""//tensorflow/contrib:contrib_kernels"",
			""//tensorflow/contrib:contrib_ops_op_lib"",
		],
	)
	```
	I was hoping that `BlockLSTM` was included in `""//tensorflow/contrib:contrib_ops_op_lib""`, but that appears not to be the case.


- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2 LTS
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version**: Master branch (commit bb88ec7ecc4dc7ba72548a5115fb86e20b14de5b)
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: 5.1
- **GPU model and memory**: GeForce GTX 980, 4GB 
- **Exact command to reproduce**: Sorry. Not that easy to reproduce.
"
11846,Fractional 3d Max Pooling,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.2.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:8/5
- **GPU model and memory**:Nvidia 740M

### Describe the problem
There is a fractional maxpool op for 4D tensors. For medical imaging, data is available in the form of 3d images. Hence a fractional maxpool/avgpool  op for 5D tensors i.e., [batch_size, n_channels, depth, height, width] would be really useful.

"
11844,tf.contrib.rnn.static_rnn not found,"I am using tensorflow version 0.12.1 and trying to implement simple RNN using static_rnn.

    lstm_layer=tf.contrib.rnn.LayerNormBasicLSTMCell(n_hidden,forget_bias=1)
    outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32)

The IDE is not able to resolve `static_rnn`.

In tensorflow documentation `tf.contrib.rnn.static_rnn` is listed [here](https://www.tensorflow.org/api_guides/python/contrib.rnn).But on clicking on it it redirects to 404 page.Is there a change in `static_rnn` in tensorflow?If yes then whats the new alternative/implementation?"
11843,bazel build error: no such package '@protobuf//src/google/protobuf',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.2.x
- **Python version**:  3.5
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: sudo bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

### Describe the problem

When I use bazel to build, I met the follow error. 
PS: I had installed python3-protobuf using pip3.

> ERROR: no such package '@protobuf//src/google/protobuf': Could not find handler for bind rule //external:protobuf
"
11841,arg_max  error,"my code is

y=tf.Variable(tf.random_normal(shape=[2,5,3,10,12 ,5 ]))
a=tf.arg_max(y,0)
a.shape
sess=tf.Session()
sess.run(tf.global_variables_initializer())
sess.run(a)


error is :
InvalidArgumentError (see above for traceback): ArgOp : Unhandled input dimensions: 6
	 [[Node: ArgMax_7 = ArgMax[T=DT_FLOAT, Tidx=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable_9/read, ArgMax_7/dimension)]]
"
11840,Using a `tf.Tensor` as a Python `bool` is not allowed,"```
import tensorflow as tf

a = tf.constant([1,2,3])
b = tf.equal(a, 0)

with tf.Session() as ss:
    print(ss.run(a,b))
```
When I run the code above, I encounter the following errors:

> TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.

But when I change to the following code, it doesn't raise a exception
```
import tensorflow as tf

a = tf.constant([1,2,3])
b = tf.equal(a, 0)

with tf.Session() as ss:
    print(ss.run(a))
    print(ss.run(b))
```
with the output of 

> [1 2 3]
[False False False]

It seems to be a bug, or it's designed to be so?"
11839,tf.train.ExponentialMovingAverage.variables_to_restore,"When I view the API doc, [tf.train.ExponentialMovingAverage.variables_to_restore](https://www.tensorflow.org/versions/master/api_docs/python/tf/train/ExponentialMovingAverage#variables_to_restore). It said `If a variable has a moving average, use the moving average variable name as the restore name; otherwise, use the variable name.`, but I test it is wrong.
Look the code:

```
import tensorflow as tf

a = tf.Variable(tf.constant(1.0),name='a')
b = tf.Variable(tf.constant(3.0),name='b')
c = tf.Variable(tf.constant(5.0),name='c')
ema = tf.train.ExponentialMovingAverage(decay=0.9999)
ema.apply([a,b])

print(tf.get_default_graph().get_all_collection_keys())
print(tf.get_collection('moving_average_variables'))
print(tf.global_variables())
variables_to_restore = ema.variables_to_restore()
print(variables_to_restore)
```
And the output is:
```
['trainable_variables', 'moving_average_variables', 'cond_context', 'variables']
[<tf.Variable 'a:0' shape=() dtype=float32_ref>, <tf.Variable 'b:0' shape=() dtype=float32_ref>]
[<tf.Variable 'a:0' shape=() dtype=float32_ref>, <tf.Variable 'b:0' shape=() dtype=float32_ref>, <tf.Variable 'c:0' shape=() dtype=float32_ref>, <tf.Variable 'a/ExponentialMovingAverage:0' shape=() dtype=float32_ref>, <tf.Variable 'b/ExponentialMovingAverage:0' shape=() dtype=float32_ref>]
{'a/ExponentialMovingAverage': <tf.Variable 'a:0' shape=() dtype=float32_ref>, 'c/ExponentialMovingAverage': <tf.Variable 'c:0' shape=() dtype=float32_ref>, 'b/ExponentialMovingAverage': <tf.Variable 'b:0' shape=() dtype=float32_ref>}

```
Variable c don't has a moving average, but in `variables_to_restore`, it use key=shadow_name either.
Who can tell me what's wrong?"
11836,'module' object has no attribute 'BasicLSTMCell',"Hello guys,

I'm adopting a project of version 1.1.0 to version 1.2.1. Looks like some APIs have changed. I'm new to tensorflow, could someone tell what's the new APIs for those old ones below:

from tensorflow.contrib.rnn.python.ops import core_rnn_cell_impl as rnn_cell

core_rnn_cell_impl doesn't exist, but I need BasicLSTMCell is in rnn_cell.

Thank you very much.

Eric"
11831,No mention of how to use custom RunConfig with Estimator in the Estimator tutorial,"On the tutorial of creating estimators using tf.contrib.learn there doesn't seem to be any mention of how to create your own RunConfig object in order to specify the configurations for an Estimator run. The configuration in particular I wanted to find was on how to write summaries after custom sized steps. I eventually found it in the RunConfig description, but I think it would be worthwhile to mention it in the tutorial.

Link to the tutorial:
https://www.tensorflow.org/extend/estimators

Link to the RunConfig description:
https://www.tensorflow.org/api_docs/python/tf/contrib/learn/RunConfig

I was wondering if the tutorial could be updated to show how to create your own RunConfig object and use it with the Estimator. 
"
11829,Slow to import tensorflow.contrib with Python 3 because inspect.stack is slow,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra 10.12.5
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.2.0-2420-g2b4a0f9a4 1.3.0-rc0
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**: 0.5.2-homebrew
- **CUDA/cuDNN version**: CPU only build
- **GPU model and memory**: CPU only build
- **Exact command to reproduce**: time python3 -c ""import tensorflow.contrib""

### The problem
Doing `import tensorflow.contrib` take 7.5 seconds on my machine when doing it with Python 3.6.2. With Python 2.7.13 it takes 3.2 seconds.

Investigating this revealed that a lot of time is spent in `_inspect.stack()` in the function `make_decorator` in  `python/util/tf_decorator.py`. The stack is inspected to find the name of the caller of the function. With Python2 `inspect.stack()` is fast, but with Python 3 each call to `inspect.stack()` take approximately 0.2 seconds and there are 23 calls made, which account for the difference in time between Python 2 and 3.

### References
Keras by default imports tensorflow.contrib when the Tensorflow backend is used. Therefore Keras is slow to import when using Python 3: https://github.com/fchollet/keras/issues/7408

There is a stackoverflow question referencing this issue: https://stackoverflow.com/questions/45093653/import-tensorflow-contrib-module-is-slow-in-tensorflow-1-2-1"
11825,Why remove tolerate_dup_recv from LocaRendezvous.,"I did not find a PR for this [patch](https://github.com/tensorflow/tensorflow/commit/cbfd50ff0f01e1825922230a8bc6e5766da98dd7). Maybe this discussion belongs to that particular commit page. If so, I will move it there.

Can somebody explain the rationale behind this patch? Current contrib/verbs uses exactly this flag to transfer tensors. Note it is set true [here](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc#L34). Removing it totally breaks verbs. I am asking if this patch is absolutely needed since there is no simple workaround for contrib/verbs without this flag. "
11823,ERROR message when using tf.SyncReplicasOptimizer,"I'm running distributed tensorflow with estimators, in order to it in sync mode I'm using tf.SyncReplicasOptimizer, but casually (specially after evaluation) I see the following error on the master:
```
ERROR:tensorflow:==================================
Object was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):
<tf.Tensor 'report_uninitialized_variables/boolean_mask/Gather:0' shape=(?,) dtype=string>

['File ""cifar10_main.py"", line 538, in <module>\n    tf.app.run()', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))', 'File ""cifar10_main.py"", line 518, in main\n    hooks), run_config=config)', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/learn_runner.py"", line 210, in run\n    return _execute_schedule(experiment, schedule)', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/learn_runner.py"", line 47, in _execute_schedule\n    return task()', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 501, in train_and_evaluate\n    hooks=self._eval_hooks)', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 681, in _call_evaluate\n    hooks=hooks)', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 292, in evaluate\n    name=name)', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 638, in _evaluate_model\n    features, labels, model_fn_lib.ModeKeys.EVAL)', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 545, in _call_model_fn\n    features=features, labels=labels, **kwargs)', 'File ""cifar10_main.py"", line 331, in _resnet_model_fn\n    gradvars, global_step=tf.train.get_global_step())', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/sync_replicas_optimizer.py"", line 252, in apply_gradients\n    variables.global_variables())', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py"", line 170, in wrapped\n    return _add_should_use_warning(fn(*args, **kwargs))', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py"", line 139, in _add_should_use_warning\n    wrapped = TFShouldUseWarningWrapper(x)', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py"", line 96, in __init__\n    stack = [s.strip() for s in traceback.format_stack()]']
==================================

```
Code available at:https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10_estimator/cifar10_main.py
"
11822,tfcompile of tf.sin and tf.cos,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow version (use command below)**: ('v1.2.0-5-g435cdfc', '1.2.1')
- **Python version**: 2.7.12

### Describe the problem
I tried to use tfcompile for AOT compilation of a graph where are use tf.sin and tf.cos functions.
tfcompile raises following error:
```
E tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Not found: No registered 'Sin' OpKernel for XLA_CPU_JIT devices compatible with node Sin = Sin[T=DT_FLOAT](alpha/read)
	.  Registered:  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]

	 [[Node: Sin = Sin[T=DT_FLOAT](alpha/read)]]
F tensorflow/compiler/aot/tfcompile_main.cc:140] Check failed: ::tensorflow::Status::OK() == (tensorflow::tfcompile::Main(flags)) (OK vs. Not found: No registered 'Sin' OpKernel for XLA_CPU_JIT devices compatible with node Sin = Sin[T=DT_FLOAT](alpha/read)
	.  Registered:  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]

	 [[Node: Sin = Sin[T=DT_FLOAT](alpha/read)]])
```
"
11821,Gradient for self_adjoint_eigvals fails (self_adjoint_eig works fine),"As also noted in #1915 already (but that ticket had been closed), there is a bug in the implementation of `self_adjoint_eigvals` (whereas `self_adjoint_eig` works fine, but it is rather inefficient to compute eigenvectors and their gradients if I only require the eigenvalues!).

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no - stock usage of `tf.self_adjoint_eigvals` - minimal failing example below
- **OS Platform and Distribution**:  Linux Ubuntu 16.04
- **TensorFlow installed from**: source
- **TensorFlow version**: v1.2.1-2-gc996c7b
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 0.5.2 (binary install)
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: GeForce GTX 1070 with 8 GB memory
- **Exact command to reproduce**: see below

## Minimal failing example:
```python
a = tf.random_normal((10000,3))
covar = tf.matmul(tf.transpose(a), a)
eigvals, eigvect_holder = tf.self_adjoint_eig(covar)
eigvects = tf.transpose(eigvect_holder)
pure_eigvals = tf.self_adjoint_eigvals(covar)
tf.Session().run(tf.gradients(eigvals, a))  # works fine
tf.Session().run(tf.gradients(pure_eigvals, a))  # throws the error below
```

### Error message:
```python
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)
    340     try:
--> 341       xla_compile = op.get_attr(""_XlaCompile"")
    342       xla_separate_compiled_gradients = op.get_attr(

/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   1666       raise ValueError(""No attr named '"" + name + ""' in "" +
-> 1667                        str(self._node_def))
   1668     x = self._node_def.attr[name]

ValueError: No attr named '_XlaCompile' in name: ""SelfAdjointEigV2_16""
op: ""SelfAdjointEigV2""
input: ""MatMul_5""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""compute_v""
  value {
    b: false
  }
}


During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py in _call_cpp_shape_fn_impl(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)
    670           graph_def_version, node_def_str, input_shapes, input_tensors,
--> 671           input_tensors_as_shapes, status)
    672   except errors.InvalidArgumentError as err:

/usr/lib/python3.5/contextlib.py in __exit__(self, type, value, traceback)
     65             try:
---> 66                 next(self.gen)
     67             except StopIteration:

/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py in raise_exception_on_not_ok_status()
    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),
--> 466           pywrap_tensorflow.TF_GetCode(status))
    467   finally:

InvalidArgumentError: Shape must be rank 2 but is rank 1 for 'gradients_4/SelfAdjointEigV2_16_grad/MatMul' (op: 'MatMul') with input shapes: [0], [0].

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-53-c60afbcdc03b> in <module>()
      5 pure_eigvals = tf.self_adjoint_eigvals(covar)
      6 tf.Session().run(tf.gradients(eigvals, a))
----> 7 tf.Session().run(tf.gradients(pure_eigvals, a))

/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)
    538                 # functions.
    539                 in_grads = _MaybeCompile(
--> 540                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
    541               else:
    542                 # For function call ops, we add a 'SymbolicGradient'

/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)
    344       xla_scope = op.get_attr(""_XlaScope"").decode()
    345     except ValueError:
--> 346       return grad_fn()  # Exit early
    347 
    348   if not xla_compile:

/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py in <lambda>()
    538                 # functions.
    539                 in_grads = _MaybeCompile(
--> 540                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
    541               else:
    542                 # For function call ops, we add a 'SymbolicGradient'

/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/ops/linalg_grad.py in _SelfAdjointEigV2Grad(op, grad_e, grad_v)
    203           math_ops.matmul(
    204               array_ops.matrix_diag(grad_e) + f * math_ops.matmul(
--> 205                   v, grad_v, adjoint_a=True),
    206               v,
    207               adjoint_b=True))

/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py in matmul(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)
   1814     else:
   1815       return gen_math_ops._mat_mul(
-> 1816           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
   1817 
   1818 

/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py in _mat_mul(a, b, transpose_a, transpose_b, name)
   1215   """"""
   1216   result = _op_def_lib.apply_op(""MatMul"", a=a, b=b, transpose_a=transpose_a,
-> 1217                                 transpose_b=transpose_b, name=name)
   1218   return result
   1219 

/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py in apply_op(self, op_type_name, name, **keywords)
    765         op = g.create_op(op_type_name, inputs, output_types, name=scope,
    766                          input_types=input_types, attrs=attr_protos,
--> 767                          op_def=op_def)
    768         if output_structure:
    769           outputs = op.outputs

/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)
   2506                     original_op=self._default_original_op, op_def=op_def)
   2507     if compute_shapes:
-> 2508       set_shapes_for_outputs(ret)
   2509     self._add_op(ret)
   2510     self._record_op_seen_by_control_dependencies(ret)

/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in set_shapes_for_outputs(op)
   1871       shape_func = _call_cpp_shape_fn_and_require_op
   1872 
-> 1873   shapes = shape_func(op)
   1874   if shapes is None:
   1875     raise RuntimeError(

/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in call_with_requiring(op)
   1821 
   1822   def call_with_requiring(op):
-> 1823     return call_cpp_shape_fn(op, require_shape_fn=True)
   1824 
   1825   _call_cpp_shape_fn_and_require_op = call_with_requiring

/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py in call_cpp_shape_fn(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)
    608     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,
    609                                   input_tensors_as_shapes_needed,
--> 610                                   debug_python_shape_fn, require_shape_fn)
    611     if not isinstance(res, dict):
    612       # Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).

/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py in _call_cpp_shape_fn_impl(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)
    674       missing_shape_fn = True
    675     else:
--> 676       raise ValueError(err.message)
    677 
    678   if missing_shape_fn:

ValueError: Shape must be rank 2 but is rank 1 for 'gradients_4/SelfAdjointEigV2_16_grad/MatMul' (op: 'MatMul') with input shapes: [0], [0].
```

Thanks!"
11818,cxx,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
11817,Looks like a bug tensorflow 1.2.1 with gpu error during basic_gpu_test,"### System information
- **No custom code
- **OS Windows 10
- **TensorFlow installed from:https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.2.1-cp35-cp35m-win_amd64.whl
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 3.5 (Anaconda environment)
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**:
- **Exact command to reproduce**: python basic_gpu_test.py

The trace:

.....C:\Anaconda3\envs\py35\lib\site-packages\tensorflow\python\framework\test_util.py:591: RuntimeWarning: invalid value encountered in greater
  np.abs(a - b) > atol + rtol * np.abs(b), np.isnan(a) != np.isnan(b))
not close where =  (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
not close lhs =  []
not close rhs =  []
not close dif =  []
not close tol =  []
dtype = float32, shape = (1, 3, 5)
......E.
======================================================================
ERROR: testTypes (__main__.MathBuiltinUnaryTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""basic_gpu_test.py"", line 136, in testTypes
    self._testDtype(dtype, use_gpu=True)
  File ""basic_gpu_test.py"", line 114, in _testDtype
    self._compare(data, np.arcsinh, math_ops.asinh, use_gpu)
AttributeError: module 'tensorflow.python.ops.math_ops' has no attribute 'asinh'

----------------------------------------------------------------------
Ran 13 tests in 12.158s

FAILED (errors=1)

"
11816,Snappy related tests are failing,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 'v1.2.1-0-gb4957ff', '1.2.1'
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: No GPU
- **GPU model and memory**: No GPU
- **Exact command to reproduce**: bazel test //tensorflow/core:lib_io_snappy_snappy_buffers_test

### The problem:
While executing test `TEST(SnappyBuffers, MultipleWritesWithoutFlush)`. It fails when `Snappy_Uncompress()` method is called which internally calls `snappy::RawUncompress()`.
Compared the same on intel x86 where it works fine; However the data somehow gets lost on s390x.

I am aware that Snappy behaves differently on s390x as compared to others.

There is another test `//tensorflow/core:lib_io_table_test` which fails when snappy compress/uncompress is used.
Would like to know if the mentioned test-cases are used to test some complex functionality of TensorFlow? Can they be ignored?

### Source code / logs
Running main() from test_main.cc
[==========] Running 5 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 5 tests from SnappyBuffers
[ RUN      ] SnappyBuffers.MultipleWritesWithoutFlush
2017-07-27 12:25:34.898300: F tensorflow/core/lib/io/snappy/snappy_buffers_test.cc:148] Non-OK-status: TestMultipleWrites(10000, 10000, 10000, 10000, 2) status: Data loss: Snappy_Uncompress failed
external/bazel_tools/tools/test/test-setup.sh: line 159: 17664 Aborted                 (core dumped) ""${TEST_PATH}"" ""$@""
"
11815,tf.contrib.data.Iterator - Continue from dataset with reinitializable iterator.,"I'd like to request the following feature. In TF 1.3.0rc0 and probably before the reinitializable iterator can be used to switch between dataset sources as shown [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/datasets.md). This is great for switching between train and test data, but has a nasty side-effect: when reinitializing the iterator the iteration starts from the beginning of the dataset. Therefore, switching the dataset between epoch (i.e do a validation run every 100 iterations or so) causes the epoch to never finish. Moreover, training will only ever use the first 100 batches (except you have a huge shuffle buffer size) of the dataset. 

The following illustrates the problem

```python
import tensorflow as tf

a = tf.contrib.data.Dataset.range(50)
b = tf.contrib.data.Dataset.range(50)

iterator = tf.contrib.data.Iterator.from_structure(a.output_types, a.output_shapes)
next_element = iterator.get_next()

a_init_op = iterator.make_initializer(a)
b_init_op = iterator.make_initializer(b)

sess = tf.Session()
sess.run(a_init_op)
sess.run(next_element) 
# 0
sess.run(next_element) 
# 1

sess.run(b_init_op)
sess.run(next_element) 
sess.run(next_element) 

sess.run(a_init_op)
sess.run(next_element) 
# 0  <- rather expect 2
sess.run(next_element) 
# 1 <- rather expect 3
```

I'd like to request a reinitializable iterator that maintains the state of the dataset and continues from where it left off. Something along the lines

```python
# ... Same as above

a_init_op, a_continue_op = iterator.make_initializer(a)

sess = tf.Session()
sess.run(a_init_op)
sess.run(next_element) # 0

sess.run(b_init_op)
sess.run(next_element) 

sess.run(a_continue_op)
sess.run(next_element) # 1
```"
11812,InternalError: Blas GEMM launch failed ,"------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Distributor ID: Ubuntu
Description:    Ubuntu 16.04.2 LTS
Release:        16.04
- **TensorFlow installed from (source or binary)**:
pip3 install tensorflow-gpu

- **TensorFlow version (use command below)**:
v1.2.0-5-g435cdfc 1.2.1

- **Python version**: 
3.5

- **CUDA/cuDNN version**:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2016 NVIDIA Corporation
Built on Tue_Jan_10_13:22:03_CST_2017
Cuda compilation tools, release 8.0, V8.0.61

- **GPU model and memory**:
 description: 3D controller
          product: GK210GL [Tesla K80]
          vendor: NVIDIA Corporation
          physical id: 0
          bus info: pci@99ba:00:00.0
          version: a1
          width: 64 bits
          clock: 33MHz
          capabilities: bus_master cap_list
          configuration: driver=nvidia latency=0
          resources: iomemory:100-ff iomemory:140-13f irq:24 memory:21000000-21ffffff 
          memory:1000000000-13ffffffff memory:1400000000-1401ffffff

- **Code example**:
estimator = KerasRegressor(build_fn=self.create_model_function,
                                   input_dim=self.input_dim, output_dim=self.output_dim,
                                   **self.model_parameters)

param_grid = {'epochs': [5]
              ,'batch_size': [256]
              ,'neurons': [[10, 10, 10]]
              ,'dropout': [[0.0, 0.0]]}

grid = GridSearchCV(estimator=estimator, param_grid=param_grid, n_jobs=-1)


### Describe the problem
The InternalError occurred when I fit a sklearn.GridSearchCV object.
The error occurred only if I use GPU and I I use GridSearch object. It works fine on CPU and on single model fitting (using Keras wrapper).


### Error log
File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/model_selection/_search.py"", line 945, in fit
    return self._fit(X, y, groups, ParameterGrid(self.param_grid))
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/model_selection/_search.py"", line 564, in _fit
    for parameters in parameter_iterable
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 728, in __call__
    n_jobs = self._initialize_backend()
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 540, in _initialize_backend
    **self._backend_args)
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py"", line 311, in configure
    self._pool = MemmapingPool(n_jobs, **backend_args)
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py"", line 600, in __init__
    super(MemmapingPool, self).__init__(**poolargs)
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py"", line 420, in __init__
    super(PicklingPool, self).__init__(**poolargs)
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/multiprocessing/pool.py"", line 168, in __init__
    self._repopulate_pool()
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/multiprocessing/pool.py"", line 233, in _repopulate_pool
    w.start()
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/multiprocessing/process.py"", line 105, in start
    self._popen = self._Popen(self)
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/multiprocessing/context.py"", line 267, in _Popen
    return Popen(process_obj)
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/multiprocessing/popen_fork.py"", line 20, in __init__
    self._launch(process_obj)
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/multiprocessing/popen_fork.py"", line 74, in _launch
    code = process_obj._bootstrap()
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/multiprocessing/pool.py"", line 119, in worker
    result = (True, func(*args, **kwds))
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py"", line 344, in __call__
    return self.func(*args, **kwargs)
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 131, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 131, in <listcomp>
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/model_selection/_validation.py"", line 238, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/keras/wrappers/scikit_learn.py"", line 136, in fit
    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))
  File ""/home/aateam/Amplifon/amplifon-adv-planning/src/libs/amplifon_objects.py"", line 176, in create_test_model
    model.add(Dense(neurons[0], input_dim=input_dim, activation=last_activation))
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/keras/models.py"", line 436, in add
    layer(x)
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/keras/engine/topology.py"", line 596, in __call__
    output = self.call(inputs, **kwargs)
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/keras/layers/core.py"", line 838, in call
    output = K.dot(inputs, self.kernel)
  File ""/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py"", line 978, in dot
    out = tf.matmul(x, y)
  File ""/home/aateam/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py"", line 1816, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""/home/aateam/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 1217, in _mat_mul
    transpose_b=transpose_b, name=name)
  File ""/home/aateam/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/home/aateam/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/aateam/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(256, 32), b.shape=(32, 10), m=256, n=10, k=32
         [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](_arg_dense_1_input_0_0/_15, dense_1/kernel/read)]]
         [[Node: mul_1/_43 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_795_mul_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

"
11810,"Momentum, Adam, and other optimizers don't work for variable input/output sizes","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04.2
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.2.1, 1.3.0-rc0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.5.0
- **CUDA/cuDNN version**: 8.0/5.1.10
- **GPU model and memory**: GeForce GTX Titan X, 12GB
- **Exact command to reproduce**: N/A

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
None of the optimizers other than GradientDescentOptimizer seem to work when the network is working with inputs and outputs of variable sizes (in FCN, input size === output size, defined by [BATCH_SIZE, None, None, CHANNELS]). Below are the error I get when using various optimizers: 
Momentum Optimizer: `AttributeError: 'Tensor' object has no attribute 'is_fully_defined'`

RMSPropOptimizer: `ValueError: Shape of a new variable (expanding/step4/deconv/bias/RMSProp/) must be fully defined, but instead was <unknown>.`

AdamOptimizer: `AttributeError: 'Tensor' object has no attribute 'is_fully_defined'`

GradientDescentOptimizer: Works!

**Note that these optimizers had worked for the same code when I was using TF 1.0.1**

### Source code / logs
This is the stack trace for MomentumOptimizer

> /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)
    323 
    324     return self.apply_gradients(grads_and_vars, global_step=global_step,
--> 325                                 name=name)
    326 
    327   def compute_gradients(self, loss, var_list=None,/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc in apply_gradients(self, grads_and_vars, global_step, name)
    444                        ([str(v) for _, _, v in converted_grads_and_vars],))
    445     with ops.control_dependencies(None):
--> 446       self._create_slots([_get_variable_for(v) for v in var_list])
    447     update_ops = []
    448     with ops.name_scope(name, self._name) as name:/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/momentum.pyc in _create_slots(self, var_list)
     64   def _create_slots(self, var_list):
     65     for v in var_list:
---> 66       self._zeros_slot(v, ""momentum"", self._name)
     67 
     68   def _prepare(self):
/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc in _zeros_slot(self, var, slot_name, op_name)
    764     named_slots = self._slot_dict(slot_name)
    765     if _var_key(var) not in named_slots:
--> 766       named_slots[_var_key(var)] = slot_creator.create_zeros_slot(var, op_name)
    767     return named_slots[_var_key(var)]
/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/slot_creator.pyc in create_zeros_slot(primary, name, dtype, colocate_with_primary)
    168   slot_shape = (slot_shape if slot_shape.is_fully_defined()
    169                 else array_ops.shape(primary.initialized_value()))
--> 170   if slot_shape.is_fully_defined():
    171     initializer = init_ops.zeros_initializer(dtype)
    172     return create_slot_with_initializer(AttributeError: 'Tensor' object has no attribute 'is_fully_defined'"
11809,Setting weights for different layers in a CNN,I have a number of models which I have trained in `lasagne`. I have weights saved for all those models. As of now I want to switch to `tensorflow`. I can build the whole architecture in `tf` but I want to load the weights from my hard disk. How can I set the pre-trained weights for the network?
11808,tf.contrib.streaming_mean_squared_error returns incorrect result,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 18
- **TensorFlow installed from (source or binary)**: Binary (pip)
- **TensorFlow version (use command below)**: v1.3.0.0rc0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
`tf.contrib.streaming_mean_squared_error` returns wonky results. It returns the current mean square error tensor and an update op. According to the documentation their evaluated values should match. They don't, and the values of the latter tend to get weird. A minimal example should demonstrate.

### Source code / logs
Example: mean squared error between two arrays whose difference is an array of ones. The MSE should consistently be 1. It isn't though.

```python
sess = tf.InteractiveSession()
a = tf.constant(np.arange(3,7))
b = tf.constant(np.arange(2,6))
e = tf.contrib.metrics.streaming_mean_squared_error(a,b)
init_op = tf.local_variables_initializer(); sess.run(init_op)
sess.run(e) # (1.0, 1.0)
sess.run(e) # (1.0, 1.0)
sess.run(e) # (1.5, 1.0) !!!
sess.run(e) # (1.0, 1.0)
```
After having a brief look at the code, I don't see why the returned values don't match. The [docs](https://github.com/tensorflow/tensorflow/blob/b10f50ff15944badb7262a207f6628dfa52d6a9d/tensorflow/docs_src/api_guides/python/contrib.metrics.md) speak of _finalizing_ the value but it just looks like `total/count` is returned.
"
11807,"tensorflow build error in Illegal ambiguous match on configurable attribute ""copts"" in //tensorflow/python:gen_math_ops_py_wrappers_cc:","$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package 
ERROR: /root/tools/tensorflow-master/tensorflow/python/BUILD:1166:1: Illegal ambiguous match on configurable attribute ""copts"" in //tensorflow/python:gen_math_ops_py_wrappers_cc:
@local_config_cuda//cuda:using_clang
@local_config_cuda//cuda:using_nvcc
Multiple matches are not allowed unless one is unambiguously more specialized.
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.
INFO: Elapsed time: 0.190s"
11806,sparse ClusterSpec fails when using tf.cond,"Here's the minimal code to reproduce.
on machine 1 and machine 2
```python
import sys
import tensorflow as tf

cluster_spec = tf.train.ClusterSpec({
  ""a"": { 0: ""machine1:8000"" },
  ""b"": { 0: ""machine2:8001"" },
})
jobname = sys.argv[1]
taskid = int(sys.argv[2])
server = tf.train.Server(cluster_spec, jobname, taskid)

with tf.device(""/job:a/task:0/cpu:0""):
  queue = tf.FIFOQueue(
    capacity=100, dtypes=[tf.int64],
    shapes=[[]], shared_name=""a_queue"", name=""a_queue"")

if jobname == ""a"" and taskid == 0:
  enqueue_op = queue.enqueue(10)
  sess = tf.Session(server.target)
  while True:
    sess.run(enqueue_op)
else:
  dequeue_op = queue.dequeue()
  sess = tf.Session(server.target)
  while True:
    print(sess.run(dequeue_op))
```

on machine 3:

```python
import sys
import tensorflow as tf

cluster_spec = tf.train.ClusterSpec({
  ""a"": { 0: ""machine1:8000"" },
  ""b"": { 1: ""machine3:8001"" },
})
jobname = sys.argv[1]
taskid = int(sys.argv[2])
server = tf.train.Server(cluster_spec, jobname, taskid)

with tf.device(""/job:a/task:0/cpu:0""):
  queue = tf.FIFOQueue(
    capacity=100, dtypes=[tf.int64],
    shapes=[[]], shared_name=""a_queue"", name=""a_queue"")

if jobname == ""a"" and taskid == 0:
  enqueue_op = queue.enqueue(10)
  sess = tf.Session(server.target)
  while True:
    sess.run(enqueue_op)
else:
  with tf.device(""/job:b/task:1""):
    out = queue.dequeue()
    queue_b = tf.FIFOQueue(capacity=100, dtypes=[tf.int64], shapes=[[]], name=""b_queue"")
    # 1.
    # enq = queue_b.enqueue(out)
    # no_op = tf.no_op()
    # out = tf.cond(tf.equal(out, 10), lambda: enq, lambda: no_op)
    # 2.
    out = tf.cond(tf.equal(out, 10), lambda: queue_b.enqueue(out), lambda: tf.no_op())

  sess = tf.Session(server.target)
  while True:
    print(sess.run(out))
```

On machine3, it crashes complaining

```shell
tensorflow.python.framework.errors_impl.InternalError: No worker known as /job:b/replica:0/task:1
	 [[Node: cond/pred_id_S5 = _HostRecv[client_terminated=false, recv_device=""/job:a/replica:0/task:0/cpu:0"", send_device=""/job:b/replica:0/task:1/gpu:0"", send_device_incarnation=720279685140440577, tensor_name=""edge_8_cond/pred_id"", tensor_type=DT_BOOL, _device=""/job:a/replica:0/task:0/cpu:0""]()]]
```
There is no problem if the true_fn and false_fn just returns a already constructed op, like in the commented code."
11805,Cannot build Tensorflow: 'GLIBCXX_3.4.21' not found,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 3.6.1
- **Bazel version (if compiling from source)**: 0.5.2
- **CUDA/cuDNN version**: 8.0/5.0
- **GPU model and memory**: GTX1080, 8G
- **Exact command to reproduce**: bazel build --verbose_failures --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package


### Describe the problem

I cannot build Tensorflow from source. I have gcc libraries installed in ""/opt/ohpc/pub/compiler/gcc/5.4.0/lib64"". But the installer was not able to pick it up. It always tries to use the one in ""/usr/lib64"". I have set ""LDFLAGS"" ""LD_LIBRARY_PATH"", etc. Nothing works.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

$ bazel build --verbose_failures --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

WARNING: ignoring http_proxy in environment.
WARNING: Output base '/home/kai/.cache/bazel/_bazel_kai/e5757f5d9b24da3fc563b551b579ddc3' is on NFS. This may lead to surprising failures and undetermined behavior.
WARNING: /home/kai/test/tensorflow-1.2.1/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': Use SavedModel Builder instead.
WARNING: /home/kai/test/tensorflow-1.2.1/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': Use SavedModel instead.
INFO: Found 1 target...
ERROR: /home/kai/test/tensorflow-1.2.1/tensorflow/contrib/training/BUILD:339:1: null failed: protoc failed: error executing command
  (cd /home/kai/.cache/bazel/_bazel_kai/e5757f5d9b24da3fc563b551b579ddc3/execroot/org_tensorflow && \
  exec env - \
  bazel-out/host/bin/external/protobuf/protoc '--python_out=bazel-out/local_linux-py3-opt/genfiles/' -I. -Iexternal/protobuf/python -Ibazel-out/local_linux-py3-opt/genfiles/external/protobuf/python tensorflow/contrib/training/python/training/hparam.proto): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
/home/kai/.cache/bazel/_bazel_kai/e5757f5d9b24da3fc563b551b579ddc3/execroot/org_tensorflow/_bin/process-wrapper: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /home/kai/.cache/bazel/_bazel_kai/e5757f5d9b24da3fc563b551b579ddc3/execroot/org_tensorflow/_bin/process-wrapper)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 3.922s, Critical Path: 0.03s"
11804, No OpKernel was registered to support Op 'RFFT'  for CPU (running on android),"I'm running tensorflow on android. and got this error:
`Caused by: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'RFFT' with these attrs.  Registered devices: [CPU], Registered kernels:
                                                                       <no registered kernels>`

I'm using the master branch, where the RFFT CPU verison is already supported. So I'm wondering why this problem show up.

The TF version I used to build the graph(*.pb file) is also the latest master branch.
Here is to code I wrtie .pb graph:

        with tf.Graph().as_default(), tf.Session(config=tf.ConfigProto(
                allow_soft_placement=True)) as session:
            with tf.variable_scope(""model""):
                model = DeployModel(config=config)

            print('Graph build finished')
            # variable_names = [n.name for n in
            #                   tf.get_default_graph().as_graph_def().node]
            # for n in variable_names:
            #     print(n)

            saver = tf.train.Saver()
            saver.restore(session, save_path=path_join(self.config.model_path,
                                                       'latest.ckpt'))
            print(""model restored from %s"" % config.model_path)

            frozen_graph_def = graph_util.convert_variables_to_constants(
                session, session.graph.as_graph_def(),
                ['model/inputX', 'model/softmax', 'model/nn_outputs'])
            tf.train.write_graph(
                frozen_graph_def,
                os.path.dirname(graph_path),
                os.path.basename(graph_path),
                as_text=False,
            )

So maybe RFFT for CPU is still not supported on android in the latest branch?
"
11803,/gpu:0/stream doesn't appear on Timeline,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
('v1.2.0-2479-g88abddb', '1.3.0-rc0')
- **Python version**: 
2.7.12
- **Bazel version (if compiling from source)**:
bazel release 0.4.5
- **CUDA/cuDNN version**:
CUDA 8.0
cuDNN 5.1.5
- **GPU driver version**
375.66 
- **GPU model and memory**:
GeForce GTX 1080 
8GB
- **Exact command to reproduce**:
python convolution.py (attached)

### The Problem
Making a timeline.Timeline object in my environment, ""/gpu:0/stream:xx"" rows don't appear. 
![timeline_no_gpu](https://user-images.githubusercontent.com/1290076/28655300-d8748114-72d5-11e7-80a9-fd1dd8198baa.png)

I found the event collector function [BufferCompleted](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/gpu_tracer.cc#L149) were not called at all during measuring performance. 

Though BufferCompleted is registered as a callback function to CUPTI for processing GPU events, CUPTI doesn't call it in my environment. This comes from failing to flush GPU events.
GPU events are flushed by [ActivityFlushAll](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/gpu_tracer.cc#L206) with flag [CUPTI_ACTIVITY_FLAG_NONE](http://docs.nvidia.com/cuda/cupti/group__CUPTI__ACTIVITY__API.html#group__CUPTI__ACTIVITY__API) in tensorflow. But this flag doesn't seem to cover necessary GPU events, so GPU events were not flushed and collected.

I modified gpu_tracer.cc to call ActivityFlushAll with flag CUPTI_ACTIVITY_FLAG_FORCE_INT, then I got ""/gpu:0/stream:xx"" rows in Timeline. 
![timeline_with_gpu](https://user-images.githubusercontent.com/1290076/28655697-86a8cd7e-72d8-11e7-8d1f-03374c4ad788.png)

Is there any reason to use CUPTI_ACTIVITY_FLAG_NONE?


### Source code / logs
Attached source code comes from [tensorflow/models](https://github.com/tensorflow/models/blob/master/tutorials/image/mnist/convolutional.py) .
The following modification was added to measure performance.

        from tensorflow.python.client import timeline
        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
        run_metadata = tf.RunMetadata()
        l, lr, predictions = sess.run([loss, learning_rate, train_prediction],
                                       feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)

        tl = timeline.Timeline(run_metadata.step_stats)
        ctf = tl.generate_chrome_trace_format(show_memory=True,
                                            show_dataflow=True)
        with open(""timeline.json"", ""w"") as f:
          f.write(ctf)


[convolutional.py.zip](https://github.com/tensorflow/tensorflow/files/1178943/convolutional.py.zip)
"
11802,3D Convolutions not being forwarded to MKL,"This is a placeholder reminder for the Tensorflow/Intel team.  I'm in touch with Toby Boyd and Intel on this issue, I just want it to be in the databasase.

When compiling for MKL (--config=mkl) 3D convolutions remain in native Eigan. This strongly impacts a 3D medical application that needs to run inferencing on an edge device.



------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 14.04 and CentOS 7

- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.2.0rc0
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
0.5.2
- **CUDA/cuDNN version**:
n/a
- **GPU model and memory**:
n/a
- **Exact command to reproduce**:
Follow standard bazel build, say ""yes"" for MKL, build  wheel with --config=mkl. and install with pip

"
11798,bitwise_ops and gen_bitwise_ops cannot be imported,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.1.0
- **Python version**:  3.5.3
- **CUDA/cuDNN version**: 8.0.60
- **GPU model and memory**:  NVIDIA GeForce GT 730
- **Exact command to reproduce**:

In the bitwise_ops_test.py file, located in the ops folder in python, this code is there:

```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np
import six

from tensorflow.python.framework import constant_op
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import test_util
from tensorflow.python.ops import bitwise_ops
from tensorflow.python.ops import gen_bitwise_ops
from tensorflow.python.platform import googletest
```

These two lines are giving an error:

```
from tensorflow.python.ops import bitwise_ops
from tensorflow.python.ops import gen_bitwise_ops
```


I get an import error saying:


```
from tensorflow.python.ops import bitwise_ops
ImportError: cannot import name 'bitwise_ops'
```


Also theres no gen_bitwise_ops file in the ops folder, and the method isn't available in the init file.

"
11792,Mac build problem with local_config_python,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
No, this repros on master, r1.3, and r1.2
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
MacOS Sierra, 10.12.5
- **TensorFlow installed from (source or binary)**:
Source, repros on master, r1.3, r1.2
- **TensorFlow version (use command below)**:
See above.
- **Python version**: 2.7.10. Clean virtualenv
- **Bazel version (if compiling from source)**: 0.5.1-homebrew
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**:
`bazel build //tensorflow/tools/pip_package:build_pip_package`

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I'm trying to build the pip package generator so I can test changes to the pip package. The same command works on Linux.

### Source code / logs
❯ bazel build //tensorflow/tools/pip_package:build_pip_package
ERROR: /private/var/tmp/_bazel_dandelion/bd129d2cd1c27b48980a1c74fec9319a/external/local_config_python/BUILD:136:12: in outs attribute of genrule rule @local_config_python//:numpy_include: Genrules without outputs don't make sense.
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted."
11791,Feature request: equivalent of tf.nn.maxpooling_with_argmax for 3D maxpooling,"It would be great to have the equivalent of tf.nn.maxpooling_with_argmax in 3D, in order to allow for 3D unpooling layers.

I am implementing a 3Dversion of the originally 2D Deconvolution network [Noh et al. 2015]. It has been implemented in 2D in Tensorflow [here](https://github.com/fabianbormann/Tensorflow-DeconvNet-Segmentation/blob/master/DeconvNet.py) and in 3D in caffe [here](https://github.com/Microsoft/O-CNN/blob/master/caffe/examples/o-cnn/segmentation_5.prototxt). 

I need to use unpooling, and for that I need the indexes of the elements selected during pooling. This feature is implemented for 2D (tf.nn.maxpooling_with_argmax), but not for 3D.
"
11788,Add support for XLA cross products,"XLA currently supports basic trigonometric operations but does not natively support cross products. Cross products are fundamental to many geometric operations, and TF has a `tf.cross` function. Note relevant discussion in #8315 ([comment](https://github.com/tensorflow/tensorflow/issues/8315#issuecomment-317947166)), and preliminary work [here](https://gist.github.com/learyg/4019bae1cfc5bacbcdb318fd882b4d5d)."
11787,Unexpected behavior in tf.scatter_update,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.12.3
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.2.0
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: CPU
- **GPU model and memory**: CPU
- **Exact command to reproduce**:
```
>>> import tensorflow as tf
>>> sess = tf.Session()
>>> dim = 10
>>> probs = tf.Variable(tf.ones(dim), trainable=False)
>>> dist = tf.contrib.distributions.Categorical(probs=probs)
>>> mask = tf.Variable(tf.ones(dim), trainable=False)
>>> b = dist.sample([2])
>>> mask = tf.scatter_update(mask, b, [3,3])
>>> sess.run(tf.global_variables_initializer())
>>> print(mask.eval(session=sess))
[ 1.  1.  3.  3.  1.  1.  1.  1.  1.  1.]  # first call
>>> c = dist.sample([3])
>>> mask = tf.scatter_update(mask, c, [5,5,5])
>>> print(mask.eval(session=sess))
[ 3.  1.  3.  5.  3.  5.  5.  1.  1.  1.]  # second call
```

### Describe the problem
I am trying to update `mask` at indices sampled by `dist`, and I need past updates to persist as I add more updates to mask. That is, I would expect the second call to be `[ 1.  1.  3.  5.  1.  5.  5.  1.  1.  1.]`. Since `tf.scatter_update` mutates `mask`, I would expect the updates to persist, but it seems like the the updates are made anew every time I call `mask.eval()`. 

Furthermore, perhaps I'm missing something, but the output of the second call is further unexpected in the sense that I see three 3's in `mask`, even when I never assigned three 3's to mask, only two. Is there an explanation for this? 

If this is not a bug, then could we add a feature to allow for consistent and persistent updates?

### Source code / logs
Please see above.
"
11786,contrib.data.Dataset - doc issue with Dataset.map / tf.py_func in 1.3.0rc0,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: 1.3.0rc0
- **Python version**: 3.6
- **CUDA/cuDNN version**: 8/6
- **GPU model and memory**: GTX 1080
- **Exact command to reproduce**:

The following sample is taken from [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/datasets.md) and works in TF 1.2.1 

```python
import tensorflow as tf
import numpy as np

def _read_py_function(filename, label):
  return np.zeros((100,100,1)), label

def _resize_function(image_decoded, label):
  image_decoded.set_shape([None, None, None])
  image_resized = tf.image.resize_images(image_decoded, [28, 28])
  return image_resized, label

filenames = np.array([""/var/data/image1.jpg"", ""/var/data/image2.jpg""])
labels = np.array([0, 37])

dataset = tf.contrib.data.Dataset.from_tensor_slices((filenames, labels))
dataset = dataset.map(
    lambda filename, label: tf.py_func(
        _read_py_function, [filename, label], [tf.uint8, label.dtype]))
dataset = dataset.map(_resize_function)
```
In 1.3.0rc0 the following error is produced

```
Cannot convert a list containing a tensor of dtype <dtype: 'int32'> to <dtype: 'uint8'> (Tensor is: <tf.Tensor 'PyFunc:1' shape=<unknown> dtype=int32>)
```

This is due to the breaking change mentioned in [release notes](https://github.com/tensorflow/tensorflow/blob/r1.3/RELEASE.md). To fix, one now has to introduce an explicit `tuple()` like so

```python
dataset = dataset.map(
    lambda filename, label: tuple(tf.py_func(
        _read_py_function, [filename, label], [tf.uint8, label.dtype])))
```
This should at least be mentioned in the API docs / programmer guide.
"
11785,proposed new github for tensorflow notebooks,"I would like to propose that we make a separate repository in github for jupyter notebooks.

I think it should be separate from the main tensorflow github so that people can check it out without having to check out all of tensorflow.

I've created such a github and will work on it by myself for now but I think eventually there should be something more official or at least maybe what I have can be considered official enough to have pointers to it from the main tensorflow github.

https://github.com/reedkotler/tensorflow-notebooks
"
11784,[docs] Windows Instructions need updating.,"Any way to contribute to the installation instructions for Windows? I deal with a lot of people having installation instructions on Stack Overflow etc. and there are a couple of things on the Install pages that could do with being corrected, clarified or changed. Happy to do these myself if you'd like. I'll list them below:

- _""TensorFlow only supports version 3.5.x of Python on Windows.""_ is no longer true since 1.2 as 3.6 is supported.

- The r1.3 version of the install page introduces this new line: _""TensorFlow will not load if it cannot find cuDNN64_5.dll. To use a different version of cuDNN, you must build from source.""_ however I thought that 1.3 was built with cuDNN 6. Users may find this conflict confusing because, as I found with #11645, 1.3.0 will NOT load with cuDNN64_5.dll. Shouldn't this change to `cuDNN64_6.dll`?

- Anaconda Installation slightly misleading? I use anaconda to run our tensorflow environments without a problem. I download via `pip install tensorflow(-gpu)` instead of the unsupported `conda` version and, from a user's point of view, there's no reason for it be unsupported. is there a reason that a pypi installation put in an anaconda environment would act differently enough to virtual env/root install to be unsupported? I also don't see why the installation page can't recommend downloading straight from pypi instead of using the long `storage.googleapis...` url. Especially as 3.6 is now supported.

Happy to implement these changes if I knew where. Are the markdown files in docs_src linked to the website? If so i'll pop a PR into there.

Cheers"
11783,tf.estimate quickstart,"I am in the process of making jupyter notebooks from the tensorflow documentation.
( https://github.com/reedkotler/tensorflow-notebooks )

The tensorflow.org page on tf.esimator quickstart (https://www.tensorflow.org/get_started/tflearn)  had numerous problems. 

I've straightened out the coding issues and the fixes are in my notebook https://github.com/reedkotler/tensorflow-notebooks/blob/master/get_started/estimator/estimator.ipynb

I still have some more markup text to straighten out.

Aide from the python2/3, tensoflow 1.2, etc. issues, the actual test case was not predicting properly. I made my own simple one and it predicted just fine so I did not spend time trying to figure out whether the original was wrong or not.

I'm not volunteering to fix the main tensorflow markdown because I'm against the idea of having so much code there that can't be tested and is constantly breaking because tensorflow is changing and so I don't want to try and fix that with no way to test it and to give people something bulletproof that will work. There are lots of posts going unanswered in stackoverflow for 6 months of more on many of these pages, and for this scikit learn one especially because people know that already and it's an obvious starting place for newbies.

"
11782,CUDA_ERROR_LAUNCH_FAILED when using conv2d/max_pool on Tensorflow GPU (Windows 10),"When using conv2d and/or max_pool, the error below shows and stops the code. I used the code available here: [https://github.com/dennybritz/cnn-text-classification-tf](https://github.com/dennybritz/cnn-text-classification-tf)

```
2017-07-26 21:41:27.467585: E c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\stream_executor\cuda\cuda_driver.cc:1068] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED
2017-07-26 21:41:27.467797: E c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\stream_executor\cuda\cuda_timer.cc:54] Internal: error destroying CUDA event in context 000001178B37E810: CUDA_ERROR_LAUNCH_FAILED
2017-07-26 21:41:27.468679: E c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\stream_executor\cuda\cuda_timer.cc:59] Internal: error destroying CUDA event in context 000001178B37E810: CUDA_ERROR_LAUNCH_FAILED
2017-07-26 21:41:27.469846: F c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\stream_executor\cuda\cuda_dnn.cc:2479] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED
```

**What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?**

I have searched through all websites including GitHub and StackOverflow, and these two are the most relevant results and why they did not help:

1. [https://github.com/tensorflow/tensorflow/issues/6783](https://github.com/tensorflow/tensorflow/issues/6783) - The issue pertains to tf.one_hot. Is this function used in either one of the functions above? The issue seems to be not solved too. I tried using the code on a Linux/GPU and on a Windows/CPU setting as well and it worked.
2. [https://github.com/dennybritz/cnn-text-classification-tf/issues/90](https://github.com/dennybritz/cnn-text-classification-tf/issues/90) - They kind of 'resolved' the issue by reducing the batch size. I tried this and it worked, but it does not make sense. For one thing, I tried using the code on another Windows machine with an older GPU and it works well.

**Environment info**

- Tensorflow 1.2.1
- Python 3.5.3
- CUDA 8.0
- cuDNN 5.1
- OS: Windows 10
- GPU: GeForce GTX 1060

[EDITED] edited the links"
11781,run errors ,"Hi,when I run python for tensorflow ,my python file show errors ,and can't run success。errors likes:


017-07-26 19:57:14.274316: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on
 your machine and could speed up CPU computations.
2017-07-26 19:57:14.372830: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-26 19:57:14.372864: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
/home/ah0818lijhong/CNN-kereas/cnn-kereas.py:155: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
  merged = Merge([model_left, model_right,model_3], mode='concat')
Epoch 1/3
Traceback (most recent call last):
  File ""/home/ah0818lijhong/CNN-kereas/cnn-kereas.py"", line 167, in <module>
    model.fit(x_train, y_train,epochs=3)
  File ""/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/keras/models.py"", line 845, in fit
    initial_epoch=initial_epoch)
  File ""/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/keras/engine/training.py"", line 1485, in fit
    initial_epoch=initial_epoch)
  File ""/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/keras/engine/training.py"", line 1140, in _fit_loop
    outs = f(ins_batch)
  File ""/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py"", line 2073, in __call__
    feed_dict=feed_dict)
  File ""/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 778, in run
  File ""/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 778, in run
    run_metadata_ptr)
  File ""/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 982, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1032, in _do_run
    target_list, options, run_metadata)
  File ""/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1052, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[0,868] = 115873 is not in [0, 20001)
         [[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](embedding_1/embeddi
ngs/read, _recv_embedding_1_input_0)]]
Caused by op u'embedding_1/Gather', defined at:
  File ""/home/ah0818lijhong/CNN-kereas/cnn-kereas.py"", line 122, in <module>
    model_left.add(embedding_layer)
  File ""/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/keras/models.py"", line 422, in add
    layer(x)
  File ""/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py"", line 554, in __call__
    output = self.call(inputs, **kwargs)
  File ""/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/keras/layers/embeddings.py"", line 119, in call
    out = K.gather(self.embeddings, inputs)
  File ""/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py"", line 966, in gather
    return tf.gather(reference, indices)
  File ""/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1207, in gather
    validate_indices=validate_indices, name=name)
  File ""/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 768, in apply_op
    op_def=op_def)
  File ""/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2336, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1228, in __init__
    self._traceback = _extract_stack()
InvalidArgumentError (see above for traceback): indices[0,868] = 115873 is not in [0, 20001)
         [[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](embedding_1/embeddi
ngs/read, _recv_embedding_1_input_0)]]



I hope someone can fix it .thank you."
11780," Check failed: sss_[idx]->Get(key, &value) Failed to seek to the record for tensor","hi, 
 I am so appreciated with your great job. 
here is my question:
when i finetune the resnet_v1_50 with pretrained model.   I have exclude the logists layer, when comes to 
the code 'saver.restore(sess, checkpoint_path)', it gives me the error like this:
![image](https://user-images.githubusercontent.com/9246739/28619251-144e03be-723a-11e7-8612-26c2cde56b01.png)
can you help me?"
11778,Bug in ctc_ops.py 's documentation,"see in 
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/ctc_ops.py#L201
it says the inputs is logits, but from the op test https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/ctc_decoder_ops_test.py#L111 
 we can see the input should be log probability .
Although seems this does not affect the decoder's output, but it affects the decoder's return of path's log_prob, and the discribe of neg_sum_logits is confusing.
I think the document should be changed, the inputs should be log of probabilities and the output should be -log(p), maybe we can transform the output to p with tf.exp(-p), i think probability output is more comfortable

and confusing people from another issue #6034"
11777,No OpKernel was registered to support Op 'Ceil' on Android,"I'm running tensorflow on android. And it reports a exception of one of my op 'Ceil'. The exception info is as below:
`Caused by: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Ceil' with these attrs.  
Registered devices: [CPU], Registered kernels:  <no registered kernels>
[[Node: model/frame/Ceil = Ceil[T=DT_DOUBLE](model/frame/truediv)]]
   at org.tensorflow.Session.run(Native Method)`

I believe Ceil is a basic op and it should has CPU implement. So maybe I miss sth. ?   ( I clone tensorflow master branch and build the lib and java interface on that)"
11776,//py_test_dir/tensorflow/python:bitwise_ops_test failing on Windows,"http://ci.tensorflow.org/job/tensorflow-pr-win-bazel/32/console
```
ERROR: testPopulationCountOp (__main__.BitwiseOpTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""\\?\C:\tmp\Bazel.runfiles_5m16karn\runfiles\org_tensorflow\py_test_dir\tensorflow\python\ops\bitwise_ops_test.py"", line 66, in testPopulationCountOp
    inputs = np.array(raw_inputs, dtype=dtype.as_numpy_dtype)
OverflowError: Python int too large to convert to C long
```
Culprit: cl/163090921 "
11774,"app crashed on android device of api level = 19 when load library ""libtensorflow_inference.so""","### System information
- Os version 4.4.4
-sdk level 19

### logcat:
07-26 15:01:59.429 6521-6521/? E/AndroidRuntime: FATAL EXCEPTION: main
                                                 Process: al.omid.tfdroid, PID: 6521
                                                 java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol ""rand"" referenced by ""libtensorflow_inference.so""...
                                                     at java.lang.Runtime.loadLibrary(Runtime.java:364)
                                                     at java.lang.System.loadLibrary(System.java:526)
                                                     at al.omid.tfdroid.MainActivity.<clinit>(MainActivity.java:28)
                                                     at java.lang.Class.newInstanceImpl(Native Method)
                                                     at java.lang.Class.newInstance(Class.java:1208)
                                                     at android.app.Instrumentation.newActivity(Instrumentation.java:1061)
                                                     at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2114)
                                                     at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2257)
                                                     at android.app.ActivityThread.access$800(ActivityThread.java:143)
                                                     at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1209)
                                                     at android.os.Handler.dispatchMessage(Handler.java:102)
                                                     at android.os.Looper.loop(Looper.java:136)
                                                     at android.app.ActivityThread.main(ActivityThread.java:5120)
                                                     at java.lang.reflect.Method.invokeNative(Native Method)
                                                     at java.lang.reflect.Method.invoke(Method.java:515)
                                                     at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:818)
                                                     at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:634)
                                                     at dalvik.system.NativeStart.main(Native Method)
07-26 15:02:01.179 6521-6521/al.omid.tfdroid W/System.err:     at com.android.internal.os.RuntimeInit$UncaughtHandler.uncaughtException(RuntimeInit.java:94)


### Describe the problem
Could  you pleas tell me how can I solve this ?  Is ""libtensorflow_inference.so"" usable for android devices of api level < 20 ?"
11772,Feature request: have optimizers handle complex tensors,"When a tensor that contains complex values reaches an optimizer, it is cast to real.
This means that the net will not learn features that depend on the imaginary part of the tensor.

While we wait for the possible feature, is there a workaround that you recommend? I'm thinking of separating the real and imaginary parts before feeding the optimizer (like, turning a complex tensor [C] into [R,I]), would that work?)."
11771,Occur fatal error 1002 when compiling tensorflow 1.3 with vs2015 DEBUG in windows10,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
      no custom code(original code cloned from github )
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
      Windows 10
- **TensorFlow installed from (source or binary)**:
      source(cmake from tensorflow/contribe/cmake)
- **TensorFlow version (use command below)**:
      r1.3
- **Visual studio version**: 
      vs2015
- **CUDA/cuDNN version**:
      no GPU/CUDA 8.0+cuDNN5.1
- **GPU model and memory**:
      NVIDIA Titan X(12GB)
- **Exact command to reproduce**:
      tf_cc.vcxproj -> A:\C++\tensorflow-1.3.0\Source_GPU\tf_cc.dir\Debug\tf_cc.lib
      65>a:\c++\tensorflow-1.3.0\source_gpu\external\eigen_archive\eigen\src\core\products\generalblockpanelkernel.h(1977): fatal error C1002:

### Describe the problem
Hello, I get error 1002 when compiling tensorflow r1.3 with vs2015 in tf_core_kernels , use DEBUG mode without GPU, which occured during compilation of GPU version too. I belive I have enough memory for compilation. However, I compiled successfully through RELEASE mode without error.

Same issue in tensorflow-r1.2."
11770,tf.contrib.data.Dataset filter documentation broken,"With the old input pipeline functions, in tf.train.batch() you could specify the ""allow_smaller_final_batch"" parameter, which would allow or disallow a smaller final batch. With the new input pipeline functions in tf.contrib.data, the batch function allows a smaller final batch by default, and (to the best of my knowledge) there is no way to skip this last half-batch to ensure all batch sizes are equal.

Is there a possibility that a ""allow_smaller_final_batch"" flag could be added to the new batch function?"
11769,Issue for tensorboard --logdir=save/,"### System information
`tensorboard --logdir=save/` this command occurred some issue.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X with AMD.
- **TensorFlow installed from (source or binary)**: pip3.6
- **TensorFlow version (use command below)**: The latest version
- **Python version**: 3.6
- **Memory**: 16G

```
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/tensorboard/tensorboard.py"", line 32, in <module>
    from tensorflow.python.summary import event_file_inspector as efi
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/summary/event_file_inspector.py"", line 122, in <module>
    from tensorflow.python.platform import gfile
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/platform/gfile.py"", line 22, in <module>
    from tensorflow.python.lib.io.file_io import copy as Copy
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io.py"", line 27, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
ImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow.so, 2): Symbol not found: _PyBytes_AsString
  Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow.so
  Expected in: flat namespace
 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow.so
```

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"" = `v1.2.0-5-g435cdfc 1.2.1`"
11766,should tf.contrib.data.FixedLengthRecordDataset work with GS?,"Trying to open a GS file (gs://path/), with tf.contrib.data.FixedLengthRecordDataset, I get: tensorflow/core/framework/op_kernel.cc:1158] Out of range: EOF reached, 0 bytes were read out of 262144 bytes requested.


"
11765,cond with gradients of map_fn hangs,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.1
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: `('v1.2.0-5-g435cdfc', '1.2.1')`
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0 / 5.1.5
- **GPU model and memory**: GeForce GTX 1080, 8113MiB
- **Exact command to reproduce**: `python test.py` (see below)

### Describe the problem
Running a `cond` in which a lambda (even the one that isn't used) includes the `gradients` of a tensor made by `map_fn` makes TensorFlow hang. See test.py below for reproduction.

Doing the `gradients` outside of the lambda makes it work. See test-workaround.py for example.

### Source code / logs
test.py:
```py
import tensorflow as tf

print 'start'
numbers = tf.zeros([2], dtype=tf.float32)

result = tf.map_fn(lambda image: image, numbers)

def get_next_step():
  objective = result[0]
  grads, = tf.gradients(objective, numbers)
  return numbers + grads

def current_or_next(use_next):
  return tf.cond(use_next,
                 get_next_step,
                 lambda: numbers)

always_old = current_or_next(use_next=tf.constant(False))

print 'creating session'
sess = tf.Session()
print 'before run'
sess.run(always_old)
print 'after run'
```

test-workaround.py:
```py
import tensorflow as tf

print 'start'
numbers = tf.zeros([2], dtype=tf.float32)

result = tf.map_fn(lambda image: image, numbers)

def get_next_step():
  objective = result[0]
  grads, = tf.gradients(objective, numbers)
  return numbers + grads
next_step = get_next_step() # <--

def current_or_next(use_next):
  return tf.cond(use_next,
                 lambda: next_step, # <--
                 lambda: numbers)

always_old = current_or_next(use_next=tf.constant(False))

print 'creating session'
sess = tf.Session()
print 'before run'
sess.run(always_old)
print 'after run'
```

I haven't been able to get a traceback after it hangs."
11761,404 for mac GPU,"Installation section in readme for Mac GPU: 404 error

https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.3.0rc0-py2-none-any.whl "
11758,Unexpected behavior in tf.contrib.distributions.Categorical,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.12.3
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.2.0
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: CPU
- **GPU model and memory**: CPU
- **Exact command to reproduce**:
```
>>> import tensorflow as tf
>>> sess = tf.Session()
>>> t = tf.contrib.distributions.Categorical(probs=[0.0,0.0,0.0])
>>> print(sess.run(t.sample([1])))
>>> [3]  # outside the support of the distribution!
```

### Describe the problem
I wonder if this is a bug or if this is a design choice to have `tf.contrib.distributions.Categorical` output a number outside the support of the distribution if all of the probabilities are 0. In any case, it would be helpful to throw an error when all probabilities are 0, rather than output a number that doesn't work.

### Source code / logs
See above.
"
11756,Infinity mask breaks gradient,"I'm trying to do softmax over selected indices, using infinity mask to silent out the unwanted ones. However, the gradient of those unwanted entires become nan as opposed to 0.

The reason I didn't use boolean mask is that the mask indices are different in my batch, which can't end up with a nice matrix form. If there's workaround here I'll be more than happy to adopt.

The code I tested the infinity mask is

```
import numpy as np
import tensorflow as tf

a = tf.placeholder(tf.float32, [5])
inf_mask = tf.placeholder(tf.float32, [5])

b = tf.multiply(a, inf_mask)
sf = tf.nn.softmax(b)

loss = (sf[2] - 0)
grad = tf.gradients(loss, a)

sess = tf.Session()

a_np = np.ones([5])
np_mask = np.ones([5]) * 4
np_mask[1] = -np.inf

print sess.run([sf, grad], feed_dict={
    a: a_np,
    inf_mask: np_mask
})

sess.close()
```

The output is `[array([ 0.25,  0.  ,  0.25,  0.25,  0.25], dtype=float32), [array([-0.25,   nan,  0.75, -0.25, -0.25], dtype=float32)]]`

The mask is working but the gradient has a nan, which should have been 0 I think."
11755,how to assign a tensor to a sub part of another tensor,"Hi, dear all,

I am new to tensorflow. Previously I use theano. Now I want to assign a tensor to a sub part of another tensor. e.g.:

a = 
[[0,0,0,0],
[0,0,0,0],
[0,0,0,0],
[0,0,0,0]]
b = 
[[1],
[3],
[4].
[5]]
I would assign to a[0,:], like this
a = 
[[1,0,0,0],
[3,0,0,0],
[4,0,0,0],
[5,0,0,0]]
in theano we use set_subtensor, what should I do with tensorflow? Thank you very much!"
11753,tf.train.SyncReplicasOptimizer no synchronization among workers,"
### System information
- Have I written custom code : Yes
- OS Platform and Distribution : Linux
- TensorFlow installed from (source or binary)**: Binary
- TensorFlow version (use command below)**: 1.2.1
- Python version**: 3.5.2

### Problem Description
I'm trying to train an rnn model with distributed synchronized training and between graph replication. I'm using tf.train.replica_device_setter. Asynchronous Training works perfectly fine. As written in the [documentation](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/training/sync_replicas_optimizer.py) I'm wrapping my optimizer and creating the hook:

```
def training(loss,learning_rate,global_step,num_workers,is_chief):
    optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)
    optimizer = tf.train.SyncReplicasOptimizer(optimizer, replicas_to_aggregate=num_workers,
                                       total_num_replicas=num_workers)
    gvs = optimizer.compute_gradients(loss)
    capped_gvs = [(tf.clip_by_value(grad, -CLIPPING_THRESHOLD, CLIPPING_THRESHOLD), var) for       grad, var in gvs]
    train_op = optimizer.apply_gradients(capped_gvs,global_step=global_step)
    print('Is Chief?: ' + str(is_chief))
    hook=optimizer.make_session_run_hook(is_chief)
    return train_op,hook
```
For creating and running the Session I'm using exactly as told in the documentation:
```
sess = tf.train.MonitoredTrainingSession(master=server.target, is_chief=(task_index == 0),hooks=[hook])
sess.run([train_op],feed_dict=...)
```

However as already noticed in #9596 and several other issues[[1](https://stackoverflow.com/questions/41293576/distributed-tensorflow-good-example-for-synchronous-training-on-cpus),[2](https://github.com/tensorflow/tensorflow/issues/8978)] the training does not seem to synchronize among workers. So is there a bug in SyncReplicasOptimizer? I'm seeing several hints for this hypothesis: 

1. One worker is constantly ahead by several steps in my logs.
2. When stopping one worker the other just continues with the training as if nothing happened. In a synchronized setting training should stop or crash.
3. The training steps take approximately the same time as asynchronous training. Synchronous Training should be slower because of the synchronization.

Questions:
1. Is there any test with which one can confirm, that sync_replicas_optimizer.py really does synchronize?
2. Is the API-Documentation regarding sync_replicas_optimizer.py up to date?
2. Is this somehow related to tf.train.replica_device_setter as mentioned by @jmchen-g in #9596?
3. Are there any workarounds for this?
"
11750,summary.FileWriter should support with statement,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  v1.2.0-5-g435cdfc 1.2.1
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: GTX1050 4GB
- **Exact command to reproduce**:



### Describe the problem

Python users would expect a file writer to support the `with` statement, but it doesn't:



### Source code / logs

```py
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

with tf.Graph().as_default():
    with tf.Session() as sess:
        my_var = tf.Variable(tf.truncated_normal([3, 4]), name=""myvar"", dtype=tf.float32)
        my_var_plus = my_var.assign_add(np.ones([3, 4], dtype=np.float32) * 10.0)
        a = tf.placeholder(tf.float32, shape=[None], name=""input_a"")
        b = tf.reduce_sum(a, name=""sum_b"")
        c = tf.reduce_max(a, name=""max_c"")
        d = tf.add(b, c)
        with tf.summary.FileWriter(""./tf_summary"", sess.graph):
            res = sess.run(d, feed_dict={a: np.array([5, 6, 7, 8], dtype=np.float32)})
            print(res)
            init = tf.variables_initializer([my_var])
            sess.run(init)
            for _ in range(10):
                res = sess.run(my_var_plus)
            print(res)
```


```
Traceback (most recent call last):
  File ""/home/kaiyin/miniconda3/envs/tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-19-39981f92616d>"", line 13, in <module>
    with tf.summary.FileWriter(""./tf_summary"", sess.graph):
AttributeError: __enter__
```"
11746,ctc,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
11745,Using AOT compilation on network with bidirectionnal layer fails because of missing Exit on Switch node,"I have been reproducing that for a while, ranging from 1.0.1 builds to current master. Running `tfcompile` fails like this:

```
tensorflow/bazel-bin/tensorflow/compiler/aot/tfcompile --graph=test.pb  --config=native_client/tfcompile.config.pbtxt
2017-07-13 13:29:55.989141: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 4 visible devices
INVALID ARGUMENTS: Missing Exit successor to bidirectional_rnn/fw/fw/while/Switch
```

I have been able to find that this was 100% repro when we have the bidirectionnal layer in the network:
```
tf.nn.bidirectional_dynamic_rnn(...)
```

As long as there are no more bidirectionnal layers, then it is fine. I searched and I have not been able to find any documentation (but maybe I missed it?) that would state properly the status of the `Exit` node wrt `Switch` node. Further investigation would show that some of the `Switch` nodes in the network would be followed by one `Exit` but that this one was being followed by `Identity`.

I assumed that the checks performed by `tfcompile` were too tight, and took a chance:
```
diff --git a/tensorflow/compiler/tf2xla/functionalize_control_flow.cc b/tensorflow/compiler/tf2xla/functionalize_control_flow.cc
index faa88ecfe..fda3cbd00 100644
--- a/tensorflow/compiler/tf2xla/functionalize_control_flow.cc
+++ b/tensorflow/compiler/tf2xla/functionalize_control_flow.cc
@@ -383,8 +383,8 @@ Status FunctionalizeLoop(Graph* graph, Frame* frame,
         }
       }
       if (arg.exit == nullptr) {
-        return errors::InvalidArgument(""Missing Exit successor to "",
-                                       arg.switch_node->name());
+        // return errors::InvalidArgument(""Missing Exit successor to "",
+        //                                arg.switch_node->name());
       }
     }
   }
@@ -454,7 +454,7 @@ Status FunctionalizeLoop(Graph* graph, Frame* frame,
       graph->AddEdge(in_edge->src(), in_edge->src_output(), while_node, i);
     }
 
-    if (!arg.is_loop_invariant) {
+    if (arg.exit != nullptr && !arg.is_loop_invariant) {
       std::vector<const Edge*> edges(arg.exit->out_edges().begin(),
                                      arg.exit->out_edges().end());
       for (const Edge* edge : edges) {
```

This indeed worked and I have been able to build (even cross-build for ARM/RPi3) a RNN-based network (using `BasicRNN` cells or `BasicLSTM` cells).

So I guess that the questions is really: am I just lucky that it works because one `Switch` MUST really have an `Exit` node (and thus, is there something wrong in the current model), or is it just being picky ?"
11744,can't understand tf.contrib.training.bucket_by_sequence_length,"```
seqLen = 10
inputs = []
inputs.append(tf.convert_to_tensor(np.array([2,3,3,3,3,3,5,4,3])))
inputs.append(tf.convert_to_tensor(np.array([2, 3, 4])))
inputs.append(tf.convert_to_tensor(np.array([3,4,3])))
inputs.append(tf.convert_to_tensor(np.array([3,4,2])))
inputs.append(tf.convert_to_tensor(np.array([3,4,5])))
inputs.append(tf.convert_to_tensor(np.array([3,4,4])))
inputs.append(tf.convert_to_tensor(np.array([3,4,1])))

sequences, output = tf.contrib.training.bucket_by_sequence_length(input_length=seqLen, tensors= inputs, batch_size=[4,2], 
                                                                  bucket_boundaries =[4], allow_smaller_final_batch=True,
                                              dynamic_pad=True, capacity=32)

init_op = tf.global_variables_initializer()

sess = tf.Session()

sess.run(init_op)

coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess=sess, coord=coord)

try:
    while not coord.should_stop():
        s, o= sess.run([sequences, output])
        print s
        print '****'
        print o
        print '----'
        coord.request_stop()

except tf.errors.OutOfRangeError:
    print('Done training -- epoch limit reached')
finally:
    coord.request_stop()

coord.join(threads)
sess.close()
```
It seems that the documentation of those bucketing function is quite not clear
I can't figure out why the result the this code is 
```
[10 10]
****
[array([[2, 3, 3, 3, 3, 3, 5, 4, 3],
       [2, 3, 3, 3, 3, 3, 5, 4, 3]]), array([[2, 3, 4],
       [2, 3, 4]]), array([[3, 4, 3],
       [3, 4, 3]]), array([[3, 4, 2],
       [3, 4, 2]]), array([[3, 4, 5],
       [3, 4, 5]]), array([[3, 4, 4],
       [3, 4, 4]]), array([[3, 4, 1],
       [3, 4, 1]])]
----
```

what I intended was :  batch size 4 for those input length are smaller than 4,  batch size 2 for those input length is bigger or the same as 4(because the bucket_boundaries = [4])

There was a similar post before #5609 but I still think that there lacks a  proper example for it.

which point do i misunderstand about?"
11743,ImportError: cannot import name contrib,How to downgrade tensorflow to 1.0.0  ?
11742,Docker :: Restoring from a model outside the container returns FailedPreconditionError,"### System information
- **OS Platform and Distribution**: Docker Tensorflow CPU Image
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: Python 2.7.12

### Source code / logs
```python
saver = tf.train.Saver()
with tf.Session(config=config) as sess:
    saver.restore(sess, checkpoint_file)
```

### The Issue
I'm binding a host `models` directory to `/data/models` inside the container.
When trying to restore, I got a `Failed precondition`
Everything's okay when I'm not using Docker
Everything's okay when I'm building a Docker Image with the models already copied into it (but not flexible)

```
INFO:tensorflow:Restoring parameters from /data/models/OSVOS_parent.ckpt-50000
2017-07-25 08:53:42.434679: W tensorflow/core/framework/op_kernel.cc:1158] Failed precondition: /data/models/OSVOS_parent.ckpt-50000.index
///
tensorflow.python.framework.errors_impl.FailedPreconditionError: /data/models/OSVOS_parent.ckpt-50000.index
	 [[Node: save/RestoreV2_20 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_20/tensor_names, save/RestoreV2_20/shape_and_slices)]]

FailedPreconditionError (see above for traceback): /data/models/OSVOS_parent.ckpt-50000.index
	 [[Node: save/RestoreV2_20 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_20/tensor_names, save/RestoreV2_20/shape_and_slices)]]
```"
11740,"several Session , Models in one file ","so I am using CNN for fave detection and also I have CNN for age classification . but it seems two network doesn't work together.

my code is :

```
def main():

    minsize = 20
    threshold = [0.6, 0.7, 0.7]
    factor = 0.709


    print('Creating networks and loading parameters')
   


    sess= tf.Session()
    with tf.variable_scope(""MTCNN""):

            pnet, rnet, onet = FD2.create_mtcnn(sess, None,""MTCNN"")
    writer = tf.summary.FileWriter(""./graphs"", sess.graph)
      
    img = cv2.imread('./test1.jpg')
    img_matlab = img.copy()

    # BGR -> RGB
    tmp = img_matlab[:, :, 2].copy()
    img_matlab[:, :, 2] = img_matlab[:, :, 0]
    img_matlab[:, :, 0] = tmp

    boundingboxes, points = FD2.detect_face(img_matlab, minsize, pnet, rnet, onet, threshold, factor)
    points=np.reshape(points.T, (-1,10))

    boxes=boundingboxes    
    x1 = boxes[:, 0]
    y1 = boxes[:, 1]
    x2 = boxes[:, 2]
    y2 = boxes[:, 3]


    with sess.as_default():
    
    
        label_list_age = AGE_LIST
        nlabels_age = len(label_list_age)

        model_fn_age = select_model(FLAGS.model_type_age)
        images = tf.placeholder(tf.float32, [None, RESIZE_FINAL, RESIZE_FINAL, 3])
        soft = tf.placeholder(tf.float32, [None, RESIZE_FINAL, RESIZE_FINAL, 3])

        logits_age = model_fn_age(nlabels_age, images, 1, False)
        init = tf.global_variables_initializer()
        requested_step = FLAGS.requested_step if FLAGS.requested_step else None
        checkpoint_path_age = '%s' % (FLAGS.model_dir_age)
        model_checkpoint_path_age, global_step_age = get_checkpoint(checkpoint_path_age, requested_step, FLAGS.checkpoint_age)
       
        saver = tf.train.Saver()
        saver.restore(sess, model_checkpoint_path_age)
        softmax_output_age = tf.nn.softmax(logits_age)
        coder= ImageCoder()

    #with tf.Session as sess:

       
        
        for i in range(x1.shape[0]):

       
            upper_cut = [min(img.shape[0], int(y2[i]) ), min(img.shape[1], int(x2[i]))]
            lower_cut = [max(int(y1[i]), 0), max(int(x1[i]) , 0)]
            age_image = img[lower_cut[0]:upper_cut[0], lower_cut[1]:upper_cut[1]]
            best_choice_age= classify_age(sess, label_list_age, softmax_output_age, coder, boxes,age_image )
            age.append(best_choice_age)
   
        img = markLandmark(img,points)
        img = drawBoxes(img, boundingboxes)

        cv2.imshow('window',img)
        cv2.waitKey(0)

        sess.close()

if __name__ == '__main__':
    main()

```




and the  error is : 
```
  File ""/home/sepid/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2336, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/sepid/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1228, in __init__
    self._traceback = _extract_stack()

NotFoundError (see above for traceback): Key MTCNN/rnet/conv5-2/weights not found in checkpoint
	 [[Node: save/RestoreV2_45 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_45/tensor_names, save/RestoreV2_45/shape_and_slices)]]
	 [[Node: save/RestoreV2_18/_169 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_28_save/RestoreV2_18"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
```
"
11738,Using VSCode to import tensorflow failed,I've installed tensorflow on Windows 10 successfully.But I can only run tensorflow under terminal command .What I want to do is to write python script on VSCode. It turns out some configuration may be done for VSCode.So Is there anyone who can solve this?Thanks a lot.
11737,"syntaxnet: parser_eval --graph_builder=greedy ,Assign requires shapes of both tensors to match. lhs shape= [32,69] rhs shape= [400,69]","Error: 
![image](https://user-images.githubusercontent.com/21216639/28559200-238f40a6-7148-11e7-90c6-0f426bfe8426.png)

I use GPU to train;
when greedy training, I can train model and parser eval with --graph_builder=structured successful, but parser_eval with --graph_builder=greedy failed;
here is my configuration: 
""bazel-bin/syntaxnet/parser_trainer"" \
  --arg_prefix=brain_parser \
  --batch_size=400 \
  --compute_lexicon \
  --decay_steps=4400 \
  --graph_builder=greedy \
  --hidden_layer_sizes=1024,1024 \
  --learning_rate=0.08 \
  --momentum=0.9 \
  --seed=4 \
  --output_path=$TMP_DIR \
  --task_context=$TMP_DIR/context \
  --training_corpus=training-corpus \
  --tuning_corpus=tuning-corpus \
  --params=$PARAMS \
  --num_epochs=25 \
  --report_every=1000 \
  --checkpoint_every=10000 \
  --logtostderr

""bazel-bin/syntaxnet/parser_eval"" \
  --task_context=$TMP_DIR/brain_parser/greedy/$PARAMS/context \
  --hidden_layer_sizes=1024,1024 \
  --input=dev-corpus \
  --output=stdout \
  --arg_prefix=brain_parser \
  --graph_builder=greedy \
  --model_path=$TMP_DIR/brain_parser/greedy/$PARAMS/model \
  --logtostderr \
  > $TMP_DIR/greedy-out

""bazel-bin/syntaxnet/parser_eval"" \
  --task_context=$TMP_DIR/context \
  --hidden_layer_sizes=1024,1024 \
  --beam_size=1 \
  --input=dev-corpus \
  --output=stdout \
  --arg_prefix=brain_parser \
  --graph_builder=structured \
  --model_path=$TMP_DIR/brain_parser/greedy/$PARAMS/model \
  --logtostderr \
  > $TMP_DIR/struct-beam1-out


"
11736,tf1.2.1 can't build on Ubuntu 16.04 pyenv,"I can't build tf1.2.1 from source. on ubuntu 16.04 and pyenv.
tf1.0, tf1.1 are ok to be build normally.

### System information
- Ubuntu 16.04
- tf from git checkout r1.2
- python 3.5.2
- bazel 0.5.2 (from deb)
- CUDA 8.0, cudnn5
- GTX 1080ti x2

### python environment

```
pyenv install 3.5.2
pyenv virtualenv 3.5.2 tf1.2
pyenv local tf1.2
```
required packages on https://www.tensorflow.org/install/install_sources are all installed.

### my config

`cat cat .tf_configure.bazelrc`

```
build --action_env PYTHON_BIN_PATH=""/home/yusuke/.pyenv/shims/python""                                                                                                                                        
build --action_env PYTHON_LIB_PATH=""/home/yusuke/.pyenv/versions/tf1.2/lib/python3.5/site-packages""
build --define PYTHON_BIN_PATH=""/home/yusuke/.pyenv/shims/python""
build --define PYTHON_LIB_PATH=""/home/yusuke/.pyenv/versions/tf1.2/lib/python3.5/site-packages""
build --force_python=py3
build --host_force_python=py3
build --python3_path=""/home/yusuke/.pyenv/shims/python""
test --force_python=py3
test --host_force_python=py3
test --define PYTHON_BIN_PATH=""/home/yusuke/.pyenv/shims/python""
test --define PYTHON_LIB_PATH=""/home/yusuke/.pyenv/versions/tf1.2/lib/python3.5/site-packages""
run --define PYTHON_BIN_PATH=""/home/yusuke/.pyenv/shims/python""
run --define PYTHON_LIB_PATH=""/home/yusuke/.pyenv/versions/tf1.2/lib/python3.5/site-packages""
build --define with_jemalloc=true
build --define with_xla_support=true
build:opt --cxxopt=-march=native --copt=-march=native
build --action_env TF_NEED_CUDA=""1""
build --action_env TF_NEED_OPENCL=""0""
build --action_env TF_CUDA_CLANG=""0""
build --action_env CUDA_TOOLKIT_PATH=""/usr/local/cuda""
build --action_env TF_CUDA_VERSION=""8.0""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
build --action_env TF_CUDNN_VERSION=""5""
build --action_env CUDNN_INSTALL_PATH=""/usr/local/cuda-8.0""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1""
```

### fail logs

command

```
$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures --action_env=PYENV_VERSION=tf1.2
 --action_env=PYENV_ROOT=""$HOME/.pyenv"" --action_env=PATH=""$PYENV_ROOT/bin:$PATH"" --action_env=PYENV_VIRTUAL_ENV=/home/yusuke/.pyenv/versions/3.5.2/envs/tf1.2 --action_env=VIRTUAL_ENV=/home/yusuke/.pyenv/v
ersions/3.5.2/envs/tf1.2 --action_env=PYENV_ROOT=/home/yusuke/.pyenv --action_env=PYENV_SHELL=bash --action_env=DYLD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/lib --action_env=LD_LIBRARY_PATH=/usr/l
ocal/cuda/lib:/usr/local/cuda/lib: --action_env=PYENV_VIRTUALENV_INIT=1 -s 
```
error log

```
ERROR: /home/yusuke/.cache/bazel/_bazel_yusuke/072dea721cfe39744fc124596230e888/external/farmhash_archive/BUILD.bazel:12:1: C++ compilation of rule '@farmhash_archive//:farmhash' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/yusuke/.cache/bazel/_bazel_yusuke/072dea721cfe39744fc124596230e888/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/lib: \
    PATH=/home/yusuke/bin:/home/yusuke/.local/bin:/home/yusuke/.local/bin:/home/yusuke/.pyenv/plugins/pyenv-virtualenv/shims:/home/yusuke/.pyenv/shims:/home/yusuke/.pyenv/bin:/home/yusuke/bin:/home/yusuke/.local/bin:/home/yusuke/.local/bin:/home/yusuke/.pyenv/plugins/pyenv-virtualenv/shims:/home/yusuke/.pyenv/shims:/home/yusuke/.pyenv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin:/snap/bin:/usr/local/cuda/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -g0 -MD -MF bazel-out/host/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.d '-frandom-seed=bazel-out/host/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.o' -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c external/farmhash_archive/src/farmhash.cc -o bazel-out/host/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 2.
python: can't open file 'external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc': [Errno 2] No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 0.345s, Critical Path: 0.07s
```

other command

```
$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures  -s
```

error log

```
ERROR: /home/yusuke/.cache/bazel/_bazel_yusuke/072dea721cfe39744fc124596230e888/external/protobuf/BUILD:241:1: C++ compilation of rule '@protobuf//:js_embed' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/yusuke/.cache/bazel/_bazel_yusuke/072dea721cfe39744fc124596230e888/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/lib: \
    PATH=/home/yusuke/bin:/home/yusuke/.local/bin:/home/yusuke/.local/bin:/home/yusuke/.pyenv/plugins/pyenv-virtualenv/shims:/home/yusuke/.pyenv/shims:/home/yusuke/.pyenv/bin:/home/yusuke/bin:/home/yusuke/.local/bin:/home/yusuke/.local/bin:/home/yusuke/.pyenv/plugins/pyenv-virtualenv/shims:/home/yusuke/.pyenv/shims:/home/yusuke/.pyenv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin:/snap/bin:/usr/local/cuda/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -g0 -MD -MF bazel-out/host/bin/external/protobuf/_objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.d '-frandom-seed=bazel-out/host/bin/external/protobuf/_objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.o' -iquote external/protobuf -iquote bazel-out/host/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c external/protobuf/src/google/protobuf/compiler/js/embed.cc -o bazel-out/host/bin/external/protobuf/_objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 2.
python: can't open file 'external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc': [Errno 2] No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 0.417s, Critical Path: 0.08s
```
"
11735,"python configure fails on Windows if ""bash on Ubuntu on Windows"" installed","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
    No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10,  with ""bash on Ubuntu on Windows"" installed
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
ec808c9b75af5dcabcb7233b10b72cfd1366fcde
- **Python version**: 
3.5.3
- **Bazel version (if compiling from source)**:
9e62187df84ae425a5d7226b6baf1bef576f0a10
- **CUDA/cuDNN version**:
None
- **GPU model and memory**:
None
- **Exact command to reproduce**:
bazel --output_base C:\t  clean --expunge
bazel --output_base C:\t build --features generate_pdb_file --action_env=USE_DYNAMIC_CRT=1 --action_env=NO_MSVC_WRAPPER=1 --color=no --compilation_mode fastbuild --verbose_failures --experimental_ui --copt=/Z7 --linkopt=/DEBUG:FASTLINK --copt=/DNDEBUG --host_copt=/DNDEBUG //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
Build tensorflow on windows with bazel may fail because  ""bash on Ubuntu on Windows"" is installed
It may invoke the wrong ""bash"" for executing these commands.

Related: https://github.com/bazelbuild/bazel/issues/3445

### Source code / logs
```
Analyzing: target //tensorflow/tools/pip_package:build_pip_package
ERROR: C:/os/tensorflow/third_party/py/numpy/BUILD:11:1: no such package '@local_config_python//': Traceback (most recent call last):
        File ""C:/os/tensorflow/third_party/py/python_configure.bzl"", line 310
                _create_local_python_repository(repository_ctx)
        File ""C:/os/tensorflow/third_party/py/python_configure.bzl"", line 269, in _create_local_python_repository
                _check_python_bin(repository_ctx, python_bin)
        File ""C:/os/tensorflow/third_party/py/python_configure.bzl"", line 225, in _check_python_bin
                _python_configure_fail(""PYTHON_BIN_PATH is not executab..."")
        File ""C:/os/tensorflow/third_party/py/python_configure.bzl"", line 37, in _python_configure_fail
                fail((""%sPython Configuration Error:%...)))
Python Configuration Error: PYTHON_BIN_PATH is not executable.  Is it the python binary?
```"
11730,Using XLA JIT Compilation results in bad_alloc error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.0.1-0-ge895d5c-dirty 1.0.1
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: Build label: 0.4.5- (@non-git)
- **CUDA/cuDNN version**: 8.0/5.1
- **GPU model and memory**: NVIDIA TX2
- **Exact command to reproduce**: `python3 script.py`
```
# Config to turn on XLA JIT compilation
config = tf.ConfigProto()
config.graph_options.optimizer_options.global_jit_level = \
    tf.OptimizerOptions.ON_1
with tf.Session(config=config) as sess:
```

### Describe the problem
Bug in XLA JIT compilation. Script works as expected without assigning config with JIT optimizer option. 

### Source code / logs
Changing the following code: 
```
with tf.Session() as sess:
```
To:
```
# Config to turn on XLA JIT compilation
config = tf.ConfigProto()
config.graph_options.optimizer_options.global_jit_level = \
    tf.OptimizerOptions.ON_1
with tf.Session(config=config) as sess:
```
Results in the following error at runtime: 
```
nvidia@tegra-ubuntu:~/dev$ python3 script.py
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:873] ARM has no NUMA node, hardcoding to return zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GP10B
major: 6 minor: 2 memoryClockRate (GHz) 1.3005
pciBusID 0000:00:00.0
Total memory: 7.67GiB
Free memory: 4.32GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GP10B, pci bus id: 0000:00:00.0)
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 4.03G (4323303424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 6 visible devices
I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:
I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>
I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 6 visible devices
I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform CUDA. Devices:
I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): GP10B, Compute Capability 6.2
terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc
Aborted (core dumped)
```"
11727,tf.one_hot indices out of bound,"Hi.

I have ran into some nasty indexing bugs, which weren't caught by TF. I investigated and found the following behaviour:

```python
>>> sess=tf.Session()
>>> sess.run(tf.one_hot(tf.constant(3), 4))
array([ 0.,  0.,  0.,  1.], dtype=float32)
>>> sess.run(tf.one_hot(tf.constant(0), 4))
array([ 1.,  0.,  0.,  0.], dtype=float32)
>>> sess.run(tf.one_hot(tf.constant(4), 4))
array([ 0.,  0.,  0.,  0.], dtype=float32)
>>> sess.run(tf.one_hot(tf.constant(-1), 4))
array([ 0.,  0.,  0.,  0.], dtype=float32)
```

Both overflow and underflow result in a zeroed array. Should this be caught, and errors be thrown? If not, why and when would this zeroed behaviour make sense? Or is it costly to check and throw assertions?
"
11726,Error while generating executable from obj file created with XLA aot for CPU,"Hi,
I m trying to generate executable (currently x86 for testing) using xla aot. I built the test file and generated .o using bazel build. Now I am trying to generate executable using clang (gcc also gives the similar error). I get the following error as undefined reference to **tensorflow::tfcompile::runtime::MallocContiguousBuffers** and
**tensorflow::tfcompile::runtime::FreeContiguous**
```
**clang  ../aot_build/9fb1b9df97c5e4f9c88d55c740dbcaad/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/compiler/aot/tests/_objs/test_graph_binary/tensorflow/compiler/aot/tests/test_graph.o -lstdc++ -v**
clang version 3.8.0 (tags/RELEASE_380/final)
Target: x86_64-unknown-linux-gnu
Thread model: posix
InstalledDir: /pkg/qct/software/llvm/build_tools/llvm38_160329/bin
Found candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/4.8
Found candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/4.8.4
Found candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/4.9
Found candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/4.9.3
Selected GCC installation: /usr/lib/gcc/x86_64-linux-gnu/4.8
Candidate multilib: .;@m64
Selected multilib: .;@m64
 ""/usr/bin/ld"" -z relro --hash-style=gnu --build-id --eh-frame-hdr -m elf_x86_64 -dynamic-linker /lib64/ld-linux-x86-64.so.2 -o a.out /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crt1.o /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/4.8/crtbegin.o -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/lib/../lib64 -L/usr/lib/x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../.. -L/afs/localcell/cm/gv2.6/sysname/pkg.@sys/qct/software/llvm/build_tools/llvm38_160329/bin/../lib -L/lib -L/usr/lib ../aot_build/9fb1b9df97c5e4f9c88d55c740dbcaad/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/compiler/aot/tests/_objs/test_graph_binary/tensorflow/compiler/aot/tests/test_graph.o -lstdc++ -lgcc --as-needed -lgcc_s --no-as-needed -lc -lgcc --as-needed -lgcc_s --no-as-needed /usr/lib/gcc/x86_64-linux-gnu/4.8/crtend.o /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crtn.o
../aot_build/9fb1b9df97c5e4f9c88d55c740dbcaad/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/compiler/aot/tests/_objs/test_graph_binary/tensorflow/compiler/aot/tests/test_graph.o: In function `main':
test_graph.cc:(.text.startup.main+0xb4): undefined reference to `tensorflow::tfcompile::runtime::MallocContiguousBuffers(long const*, unsigned long, void**, bool)'
test_graph.cc:(.text.startup.main+0xda): undefined reference to `tensorflow::tfcompile::runtime::MallocContiguousBuffers(long const*, unsigned long, void**, bool)'
test_graph.cc:(.text.startup.main+0xf8): undefined reference to `xla::ExecutableRunOptions::set_intra_op_thread_pool(Eigen::ThreadPoolDevice const*)'
test_graph.cc:(.text.startup.main+0x1dc): undefined reference to `__tensorflow_compiler_aot_tests__test_graph_tfmatmul'
test_graph.cc:(.text.startup.main+0x22e): undefined reference to `tensorflow::tfcompile::runtime::FreeContiguous(void*)'
test_graph.cc:(.text.startup.main+0x23a): undefined reference to `tensorflow::tfcompile::runtime::FreeContiguous(void*)'
test_graph.cc:(.text.startup.main+0x289): undefined reference to `tensorflow::tfcompile::runtime::FreeContiguous(void*)'
test_graph.cc:(.text.startup.main+0x295): undefined reference to `tensorflow::tfcompile::runtime::FreeContiguous(void*)'
clang-3.8: error: linker command failed with exit code 1 (use -v to see invocation)

I am running it on  Ubuntu 14.04.5
Tensorflow version: 1.2.1
Installed tensorflow from source with XLA enabled and everything else disabled.
bazel: release 0.5.1
GPU: N/A
Python: 2.7.6 (although I thik it's not applicable here)
clang version 3.8.0

I am following this: https://www.tensorflow.org/performance/xla/tfcompile
Initially I was getting error in build

ERROR: /local/mnt/workspace/ankitac/virtual/tensorflow/tensorflow/compiler/aot/tests/aot_project/BUILD:12:1: undeclared inclusion(s) in rule '//tensorflow/compiler/aot/tests/aot_project:test_graph_binary':
this rule is missing dependency declarations for the following files included by 'tensorflow/compiler/aot/tests/aot_project/test_graph.cc':
  '/local/mnt/workspace/ankitac/virtual/tensorflow/tensorflow/compiler/aot/tests/aot_project/test_graph_tfmatmul.h'.
Target //tensorflow/compiler/aot/tests/aot_project:test_graph_binary failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 5.092s, Critical Path: 1.34s
```
I have BUILD file as:
```
load(""//tensorflow/compiler/aot:tfcompile.bzl"", ""tf_library"")

tf_library(
    name = ""test_graph_tfmatmul"",
    cpp_class = ""foo::MatMulComp"",
    graph = ""test_graph_tfmatmul.pb"",
    config = ""test_graph_tfmatmul.config.pbtxt"",
)

cc_binary(
    name = ""test_graph_binary"",
    srcs = [
        ""test_graph.cc"",  # include test_graph_tfmatmul.h to access the generated header
    ],
    deps = [
        "":test_graph_tfmatmul"",  # link in the generated object file
        ""//third_party/eigen3"",
    ],
    linkopts = [
          ""-lpthread"",
    ]
)
```
Building that using
```
bazel --output_user_root=/local/mnt/workspace/ankitac/virtual/aot_build/ build  //tensorflow/compiler/aot/tests/aot_project:test_graph_binary
```
**For now I resolved this issue by adding my cc_binary to aot/test BUILD and building it from there works. But crating executable is still the issue.**

Any help is highly appreciated!

Thanks & regards


"
11725,tf_cnn_benchmarks.py stuck when running with multiple GPUs and ImageNet data with protocol grpc+verbs,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, running tf_cnn_benchmarks.py from benchmarks repo
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2 LTS
- **TensorFlow installed from (source or binary)**: Unmodified source with RDMA Verbs enabled
- **TensorFlow version (use command below)**: 1.3.0-rc0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.5.1
- **CUDA/cuDNN version**: 8.0/6
- **GPU model and memory**: NVIDIA Tesla P100 PCIe 16GB (8 per node)
- **Exact command to reproduce**: 

PS: CUDA_VISIBLE_DEVICES='' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=ps --task_index=0 --server_protocol grpc+verbs

Worker0: CUDA_VISIBLE_DEVICES='0,1,2,3,4,5,6,7' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=worker --task_index=0 --num_gpus=8 --data_dir=/data/imagenet_data/ --train_dir=/data/imagenet_train/ --server_protocol grpc+verbs

Worker1: CUDA_VISIBLE_DEVICES='0,1,2,3,4,5,6,7' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=worker --task_index=1 --num_gpus=8 --data_dir=/data/imagenet_data/ --train_dir=/data/imagenet_train/ --server_protocol grpc+verbs

- **RDMA driver version**: MLNX_OFED_LINUX-4.1-1.0.2.0

### Describe the problem
When running the above commands (Inception V3 synchronized data parallelism training with 2 workers and 1 external ps), the tf_cnn_benchmarks application hangs forever after some iterations (usually in warm up).

It happens only when real data is involved (ImageNet), and with >4 GPUs. (More GPUs, less iterations before it hangs). Doesn't happen with grpc protocol, or when running with ""synthetic"" data.

The master_service in the workers is stuck [here](
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/master_session.cc#L608), which I guess means some operations in the computation have not been completed.

The RDMA protocol looks valid and clean, all messages corresponds to the protocol (see below logs). 
There some tensors requested by the workers which they don't receive, but they are passed by the RDMA Verbs transport to the BaseRendezvoudMgr with RecvLocalAsync in a valid way, and for some reason the higher level worker service doesn't trigger the Send kernel on those tensors.

Any help is much appreciated!
If there are some debug mechanisms I can use to understand which tensors/operations have not been completed it can greatly help. I was mostly debugging this from the RDMA Verbs layer till now, without much success, and I feel I don't have enough information there to understand what's missing.
Also I feel we don't have enough knowledge on how the step_id acts (diving into this in the code now, but there's some higher level documentation it can greatly help).

My initial guess was an occurrence of a racy condition when loading the data, since it creates a gap in execution time (worker0 starts the first training step 30-60 seconds after worker1, since it does the preprocessing of the data twice for a reason I couldn't understand yet), but after the first iteration (which usually passes successfully) the time is synchronized between workers.

### Source code / logs
Those are the logs of the runtime after moving the logging in [rdma.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc) to VLOG(0) (also adding Tensor name and step id for all cases, in some cases the step_id doesn't mean anything like BUFFER_REQUEST/RESPONSE for example), and also some VLOG in [master_session.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/master_session.cc)

[worker0](https://gist.github.com/shamoya/15a42f421e088473b8f02bf00c16d0fc)
[worker1](https://gist.github.com/shamoya/dd3126c02c73990a6e28b534d9a9ddf6)
[ps](https://gist.github.com/shamoya/0c856365802ae4d42b38baf988149574)

Unfortunately they are fairly large, but it's better then to cut the log files IMO.
Example for analysis I did in the verbs layer, comparing the Sent Tensor requests to the actual received tensors writes in both workers:

worker 0:
 -  /job:ps/replica:0/task:0/cpu:0;f3c10d28b54074c0;/job:worker/replica:0/task:0/gpu:0;edge_116943_group_deps_2/NoOp_1;0:0 80661058974090965
-  /job:worker/replica:0/task:1/cpu:0;1a50d5c51cd9c5d1;/job:worker/replica:0/task:0/gpu:0;edge_116947_group_deps_3/NoOp_1;0:0 80661058974090965
- /job:worker/replica:0/task:1/gpu:2;7f00fadabfe781f5;/job:worker/replica:0/task:0/gpu:0;edge_111078_group_deps_1/NoOp_2;0:0 80661058974090965
- /job:worker/replica:0/task:1/gpu:4;b07185dd19f62088;/job:worker/replica:0/task:0/gpu:0;edge_111080_group_deps_1/NoOp_4;0:0 80661058974090965

worker 1:
- /job:ps/replica:0/task:0/cpu:0;f3c10d28b54074c0;/job:worker/replica:0/task:1/cpu:0;edge_155113_AssignAdd;0:0 80661058974090965
- /job:worker/replica:0/task:0/gpu:0;f3df8abf03739fe8;/job:worker/replica:0/task:1/cpu:0;edge_116948_group_deps_3;0:0 80661058974090965

The tensors requests received well by the other side and passed to RecvLocalAsync, but are not called later.

Thanks a lot."
11724,Importing TensorFlow breaks numpy.linalg.matrix_power(),"#10771 
## System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 3.6.1
- **Bazel version (if compiling from source)**: bazel release 0.5.1
- **CUDA/cuDNN version**: CUDA V8.0.61 / 
- **GPU model and memory**: NVIDIA Titan X Fall 2016
- **Exact command to reproduce**:
```python
import numpy as np
import tensorflow as tf
np.random.seed(seed=0)
X = np.random.randn(2000, 2000)
Y = np.linalg.matrix_power(X, 30)
```

### Describe the problem
This seems to be a bug where importing tensorflow breaks numpy.linalg.matrix_power()

The two following code snippets are producing different results on my system when entered directly into the python interpreter. Note, I am restarting the python kernel between running each block.

**This works:**
```python
import numpy as np
np.random.seed(seed=0)
X = np.random.randn(2000, 2000)
Y = np.linalg.matrix_power(X, 30)
>>> X 
array([[ 1.76405235,  0.40015721,  0.97873798, ...,  0.15843385,
        -1.14190142, -1.31097037],
       [-1.53292105, -1.71197016,  0.04613506, ..., -0.03057244,
         1.57708821, -0.8128021 ],
       [ 0.61334917,  1.84369998,  0.27109098, ..., -0.53788475,
         0.39344443,  0.28651827],
       ..., 
       [-0.17117027,  0.57332063, -0.89516715, ..., -0.01409412,
         1.28756456, -0.6953778 ],
       [-1.53627571,  0.57441228, -0.20564476, ...,  0.90499929,
         0.51428298,  0.72148202],
       [ 0.51262101, -0.90758583,  1.78121159, ..., -1.12554283,
         0.95170926, -1.15237806]])
>>> Y
array([[ -1.04752205e+48,   2.10841282e+47,  -4.54826843e+47, ...,
         -7.84526353e+46,  -4.45185369e+47,  -1.96340973e+47],
       [ -5.40802471e+46,   1.12546832e+48,  -1.88764494e+47, ...,
         -3.72182046e+47,   7.97461852e+47,  -5.04546546e+46],
       [ -3.59835691e+47,  -6.90559050e+46,  -8.78538707e+47, ...,
          7.67940928e+47,   2.10052546e+47,   1.75193723e+47],
       ...,
       [  2.20970288e+46,   3.60679821e+47,   5.76631889e+47, ...,
          1.21938369e+48,  -8.61048462e+47,  -3.93610572e+47],
       [ -1.19116636e+48,   2.47318954e+48,   2.65693291e+46, ...,
          9.18513286e+47,   3.91490216e+47,  -7.08113716e+47],
       [ -2.25527724e+47,  -4.94088618e+46,  -2.69359430e+47, ...,
         -4.07174632e+47,   7.38250907e+47,   5.86758288e+46]])
```
**This produces the wrong result:**
```python
import numpy as np
import tensorflow as tf
np.random.seed(seed=0)
X = np.random.randn(2000, 2000)
Y = np.linalg.matrix_power(X, 30)
>>> X
array([[ 1.76405235,  0.40015721,  0.97873798, ...,  0.15843385,
        -1.14190142, -1.31097037],
       [-1.53292105, -1.71197016,  0.04613506, ..., -0.03057244,
         1.57708821, -0.8128021 ],
       [ 0.61334917,  1.84369998,  0.27109098, ..., -0.53788475,
         0.39344443,  0.28651827],
       ..., 
       [-0.17117027,  0.57332063, -0.89516715, ..., -0.01409412,
         1.28756456, -0.6953778 ],
       [-1.53627571,  0.57441228, -0.20564476, ...,  0.90499929,
         0.51428298,  0.72148202],
       [ 0.51262101, -0.90758583,  1.78121159, ..., -1.12554283,
         0.95170926, -1.15237806]])
>>> Y
array([[ -3.40382764e+91,   2.85027458e+91,   1.14039870e+91, ...,
          3.32682992e+91,   6.00166234e+91,   2.33233825e+91],
       [  3.86088264e+91,   1.15500453e+92,   2.83821815e+91, ...,
         -6.16959058e+91,  -1.91501705e+91,  -4.67672849e+91],
       [  5.05026067e+91,   7.72796711e+91,  -4.70112473e+91, ...,
          8.41553063e+90,  -2.66176140e+91,   8.50899233e+90],
       ...,
       [ -2.08592878e+91,   2.28435173e+91,   9.15188619e+90, ...,
         -1.25550051e+91,   1.85247259e+91,  -8.73231986e+90],
       [ -4.73923534e+91,   1.61385540e+92,   1.26364668e+92, ...,
          2.83667716e+91,   5.06236372e+90,  -5.18395025e+91],
       [ -1.52984791e+91,  -5.57421948e+90,   3.27657918e+91, ...,
         -7.08972359e+91,  -1.58912068e+91,  -1.22216698e+91]])
```

The values for X are exactly the same. Interestingly, if you run the first code block, then import tensorflow without restarting the kernel and recompute the matrix power, the correct result is returned.

I haven't been able to find a reference to this anywhere online, and it's a big problem not being able to use tensorflow and numpy in the same script.
"
11723,tf.nn.sparse_softmax_cross_entropy_with_logits raise Segmentation fault,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
code from tensorflow/nmt
- **OS Platform and Distribution**:centos 7
- **TensorFlow installed from**:pip install
- **TensorFlow version**:('v1.2.0-5-g435cdfc', '1.2.1')
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:without gpu
- **GPU model and memory**:None
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

### Describe the problem
As the code pointed, tf.nn.sparse_softmax_cross_entropy_with_logits seems supporting less size tensor than tf.nn.softmax_cross_entropy_with_logits. At least, do shape check rather than Segmenation fault error.

### Source code / logs
```python
import tensorflow as tf
import numpy as np
labels=tf.placeholder(tf.int32, [None,128])
logits=tf.placeholder(tf.float32, [None,128,600000])
crossent1=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)
labels_one_hot=tf.one_hot(labels, 600000, dtype=tf.int32)
crossent2 = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=logits)
logits_data=np.random.rand(39,128,600000)
labels_data=np.random.randint(600000,size=(39,128))
fd={logits:logits_data, labels:labels_data}
with tf.Session() as sess:
    sess.run([crossent2], feed_dict=fd)
    print 'OK2' # OK2
with tf.Session() as sess:
    sess.run([crossent1], feed_dict=fd)
    print 'OK1' #Segmentation fault
```
"
11721,Memory Leak from Training Step,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes, although my code is somewhat based on the MNIST deep learning tutorial.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04 VERSION=""14.04.5 LTS, Trusty Tahr""
- **TensorFlow installed from (source or binary)**:
Installed via the VirtualEnv method
- **TensorFlow version (use command below)**:
1.1 and 1.2
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
CUDA Version 8.0.44
- **GPU model and memory**:
GeForce GTX 780M 4GB
- **Exact command to reproduce**:
`self.sess.run(self.train_step, feed_dict={self.x: trainingdata, self.y_true: traininglabels, self.keepratio: self.training_keep_rate})`

### Describe the problem

This is very similar to [the bug report I submitted here](https://github.com/tensorflow/tensorflow/issues/9590), but is a bit of a slower leak and is present in both TF 1.1 and 1.2.  I have finalized my graph.  Using the architecture described by [Zeiler et al., 2013 (ZF Net)](https://arxiv.org/pdf/1311.2901v3.pdf), batch sizes of 64, and 224x224 grayscale (1 channel) input, it leaks approximately 4GB after approximately 3000 batches.  This makes it unworkable, for say, 80 epochs of ImageNet training.  I have confirmed that the leak either does not occur or is much less severe (hard to tell which) if I comment out the training line (i.e. still do all of my preprocessing and loading).


As directed in that last linked issue, I tried to call sess.run with 
options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata
and start the program with 
env TF_CPP_MIN_VLOG_LEVEL=1 python deep_learning_main.py

but the amount of spew was enormous, and it won't respond to keyboard interrupts (I have to kill the job).  If that info would be helpful, how do I go about recording/saving this information properly to upload and help you all debug?

[1_08.zip](https://github.com/tensorflow/tensorflow/files/1170372/1_08.zip)"
11720,how to use the trained model(build in OOP schema) to predict new samples?,"Hi, there
   I write a OOP schema cnn model, but there is some error when predict. I know that if define Variable and write the code like `c` style in one file, it is easy to restore the value, but in OOP schema, it is something wrong. Here is my class:
```python
class TextCNN(object):
    """"""
    A CNN for text classification.
    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.
    """"""

    def __init__(self,
                 is_training,
                 sequence_length,
                 num_classes,
                 vocab_size,
                 embedding_size,
                 filter_sizes,
                 num_filters,
                 l2_reg_lambda=0.0,
                 drop_out=1.0):
        self.is_training = is_training
        self.sequence_length = sequence_length
        self.num_classes = num_classes
        self.vocab_size = vocab_size
        self.embedding_size = embedding_size
        self.filter_sizes = filter_sizes
        self.num_filters = num_filters
        self.l2_reg_lambda = l2_reg_lambda
        self.input_x_test = tf.placeholder(tf.int32, [None, self.sequence_length], name='input_x_text')
        self.input_y_test = tf.placeholder(tf.int32, [None, self.num_classes], name='input_y_text')
        if self.is_training:
            self.dropout_keep_prob = drop_out
        else:
            self.dropout_keep_prob = 1.0

        # Embedding layer
        with tf.device('/cpu:0'), tf.name_scope(""embedding""):
            self.W = tf.Variable(
                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),
                name=""W"")
        self.l2_loss = tf.constant(0.0)

    def inference(self, input_x):
        if not self.is_training:
            input_x = self.input_x_test
        embedded_chars = tf.nn.embedding_lookup(self.W, input_x)
        embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)

        # Create a convolution + maxpool layer for each filter size
        pooled_outputs = []
        for i, filter_size in enumerate(self.filter_sizes):
            with tf.name_scope(""conv-maxpool-%s"" % filter_size):
                # Convolution Layer
                filter_shape = [filter_size, self.embedding_size, 1, self.num_filters]
                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=""W"")
                b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name=""b"")
                conv = tf.nn.conv2d(
                    embedded_chars_expanded,
                    W,
                    strides=[1, 1, 1, 1],
                    padding=""VALID"",
                    name=""conv"")
                # Apply nonlinearity
                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=""relu"")
                # Maxpooling over the outputs
                pooled = tf.nn.max_pool(
                    h,
                    ksize=[1, self.sequence_length - filter_size + 1, 1, 1],
                    strides=[1, 1, 1, 1],
                    padding='VALID',
                    name=""pool"")
                pooled_outputs.append(pooled)

        # Combine all the pooled features
        num_filters_total = self.num_filters * len(self.filter_sizes)
        h_pool = tf.concat(pooled_outputs, 3)
        h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])

        # Add dropout
        with tf.name_scope(""dropout""):
            h_drop = tf.nn.dropout(h_pool_flat, self.dropout_keep_prob)

        # Final (unnormalized) scores and predictions
        with tf.name_scope(""output""):
            W = tf.get_variable(
                ""W"",
                shape=[num_filters_total, self.num_classes],
                initializer=tf.contrib.layers.xavier_initializer())
            b = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name=""b"")
            self.l2_loss += tf.nn.l2_loss(W)
            self.l2_loss += tf.nn.l2_loss(b)
            logits = tf.nn.xw_plus_b(h_drop, W, b, name=""scores"")
            return logits

    def loss(self, logits, input_y):
        # CalculateMean cross-entropy loss
        with tf.name_scope(""loss""):
            if not self.is_training:
                input_y = self.input_y_test
            losses = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=input_y)
            loss = tf.reduce_mean(losses) + self.l2_reg_lambda * self.l2_loss
            return loss

    def training(self, loss, learning_rate):
        tf.summary.scalar('loss', loss)
        optimizer = tf.train.AdamOptimizer(learning_rate)
        global_step = tf.Variable(0, name='global_step', trainable=False)
        train_op = optimizer.minimize(loss, global_step=global_step)
        return train_op

    def evaluation(self, logits, input_y):
        if not self.is_training:
            input_y = self.input_y_test
        predictions = tf.argmax(logits, 1, name=""predictions"")
        correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))
        accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=""accuracy"")
        acc_summary = tf.summary.scalar(""accuracy"", accuracy)
        return accuracy
```

## and this is my train file:
```python
cnn = TextCNN(...)
logits = cnn.inference(x_batch)
loss = cnn.loss(logits, y_batch)
train_op = cnn.training(loss, FLAGS.learning_rate)
eval_correct = cnn.evaluation(logits, y_batch)
_, loss_value, accuracy = sess.run([train_op, loss, eval_correct])
if step % FLAGS.checkpoint_every == 0 and step != 0:
    path = saver.save(sess, checkpoint_prefix, global_step=step)
```


## and this is my predict file:
```python
with graph.as_default():
    session_conf = tf.ConfigProto(
        allow_soft_placement=FLAGS.allow_soft_placement,
        log_device_placement=FLAGS.log_device_placement)
    sess = tf.Session(config=session_conf)

    data_set_test = DataSet(...)

    with sess.as_default():
        cnn = TextCNN(
            is_training=False,
            sequence_length=data_set_test.max_seq_len,
            num_classes=len(data_set_test.label_dict),
            vocab_size=len(data_set_test.vocab),
            embedding_size=FLAGS.embedding_dim,
            filter_sizes=list(map(int, FLAGS.filter_sizes.split("",""))),
            num_filters=FLAGS.num_filters,
            l2_reg_lambda=FLAGS.l2_reg_lambda,
            drop_out=FLAGS.dropout_keep_prob
        )

        saver = tf.train.Saver()
        saver.restore(sess, checkpoint_file)
        x_batch, y_batch = data_set_test.next_batch(-1, True, True)
        logits = cnn.inference(x_batch)
        results = sess.run(logits, feed_dict={cnn.input_x_test: x_batch})
        pass
```


## error is:
```
tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value output/b_1
	 [[Node: output/b_1/read = Identity[T=DT_FLOAT, _class=[""loc:@output/b_1""], _device=""/job:localhost/replica:0/task:0/cpu:0""](output/b_1)]]
```

Do you know how to load the Variable like OOP schema, is there a demo projects?
Thanks!

"
11718,gather_nd InvalidArgumentError,"2017-07-24 10:35:25.247357: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: flat indices[12287, :] = [127, 42] does not index into param (shape: [32,512,100,1]).
	 [[Node: conv-maxpool-3/GatherNd = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](ExpandDims, conv-maxpool-3/stack)]]
2017-07-24 10:35:25.247422: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: flat indices[12287, :] = [127, 42] does not index into param (shape: [32,512,100,1]).
	 [[Node: conv-maxpool-3/GatherNd = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](ExpandDims, conv-maxpool-3/stack)]]
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1039, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1021, in _run_fn
    status, run_metadata)
  File ""/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py"", line 89, in __exit__
    next(self.gen)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: flat indices[12287, :] = [127, 42] does not index into param (shape: [32,512,100,1]).
	 [[Node: conv-maxpool-3/GatherNd = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](ExpandDims, conv-maxpool-3/stack)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""mix.py"", line 66, in <module>
    autoen_nn.clf_train_epoch(x_train,y_train,batch_size,dropout_rate = 0.5)
  File ""/Users/MilesZhao/Desktop/ORNL/data/epath_data/yong/cnn.py"", line 248, in clf_train_epoch
    feed_dict = feed_dict)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 778, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 982, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1032, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1052, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: flat indices[12287, :] = [127, 42] does not index into param (shape: [32,512,100,1]).
	 [[Node: conv-maxpool-3/GatherNd = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](ExpandDims, conv-maxpool-3/stack)]]

Caused by op 'conv-maxpool-3/GatherNd', defined at:
  File ""mix.py"", line 64, in <module>
    num_classes)
  File ""/Users/MilesZhao/Desktop/ORNL/data/epath_data/yong/cnn.py"", line 125, in __init__
    filter_size)
  File ""/Users/MilesZhao/Desktop/ORNL/data/epath_data/yong/cnn.py"", line 217, in conv_layer
    local_in_doc_mat = tf.gather_nd(in_doc_mat,temp)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1321, in gather_nd
    name=name)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 768, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2336, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1228, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): flat indices[12287, :] = [127, 42] does not index into param (shape: [32,512,100,1]).
	 [[Node: conv-maxpool-3/GatherNd = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](ExpandDims, conv-maxpool-3/stack)]]
"
11715,Using higher order operators within tf.contrib.data.Dataset.map raises a segmentation fault,"### System information

- **Have I written custom code**: Yes
- **OS Platform and Distribution**: Ubuntu 16.04
- **TensorFlow installed from**: binary
- **TensorFlow version**: `('v1.2.0-2323-geaa7a8e', '1.3.0-rc0')`
- **Python version**: 2.7.6
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: GTX 1080 (8GB)
- **Exact command to reproduce**: see the code snippet below

### Describe the problem

When using a higher order operator (such as `tf.map_fn`) within the `tf.contrib.data.Dataset.map` method, the program exits with a `Segmentation fault`.

### Source code / logs

```python
import tensorflow as tf

dataset = tf.contrib.data.Dataset.from_tensor_slices([
  tf.constant([1, 2, 3, 4, 5, 6])])

dataset = dataset.map(lambda elems: tf.map_fn(lambda x: x * x, elems))

iterator = dataset.make_one_shot_iterator()
next_element = iterator.get_next()

with tf.Session() as sess:
  print(sess.run(next_element))
```

---

My particular use case is to split a sentence into tokens and then map a function on each token to build tensors of characters."
11714,run silent /.configure,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04

- **TensorFlow installed from (source or binary)**:
source

- **TensorFlow version (use command below)**:
1.3

- **Python version**: 
intel python

- **Bazel version (if compiling from source)**:
0.5

- **CUDA/cuDNN version**:
none
- **GPU model and memory**:
none
- **Exact command to reproduce**:

### Describe the problem
I am building a docker container, compiling tensorflow from source, would like to use MKL, and need to run ```configure``` script silently in order to set up the build correctly. Cannot figure out where, when and how the MKL libraries are being installed.

### Source code / logs
none

Is there a way to run the ```configure``` script silently? alternatively,
Does someone have a Docker script that will compile tensorflow USING the MKL?  "
11713,decode_csv is extremely slow,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
I tested on both MacOS 10, Centos 7, https://pastebin.com/LsADQ89s

- **TensorFlow installed from (source or binary)**:
Tensorflow 1.2.1, also tried with tensorflow compiled from sources

- **TensorFlow version (use command below)**:
('v1.2.0-5-g435cdfc', '1.2.1'), also compiled and tested from master

- **Python version**: 
2.7.13

- **CUDA/cuDNN version**:
no

- **GPU model and memory**:
no

- **Exact command to reproduce**:
see above

### The problem
I noticed that a huge amount of training time CPU is idle. I isolated the problem to the code snippet bellow, where I create a string tensor using tf.constant with N rows each of them contains comma separated list of randomly generated numberical values, then I decode these string tensor to rank2 tensor of floats using 3 different approaches. 


I discovered following:
decoding tfrecords with *tf.parse_example* is quite fast
decoding csv with *decode_csv* is ~20 (4 cores) 60 (16 cores) times slower than parse_example and it utilises only 1 core on my machine
decoding csv with *tf.string_split* + *tf.string_to_number()* is ~20-60 times slower than parse_example. 
It seems *tf.string_to_number()* makes it slow, if I comment out the line with *tf.string_to_number* it is 2-3 times slower than parse example




The code I used for that:
https://pastebin.com/ZR152y5M



### Source code / logs
Running parse_example
Total time: 2.01925897598

Running string_split
Total time: 5.52524709702

Running decode_csv
Total time: 22.6595089436

"
11712,nn,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
11711,First time of inference very slow on iOS,"### Describe the problem
I tried to run the inference graph on iOS. However, the first run was very slow. It gets faster at the subsequent runs. My test inference graph is very simple, and just with two features as input, and run a logistic regression on it. The first run on `session.run(...)` will take about 15ms to run. The subsequent runs (starting from the second run) can go downs as low as 0.2ms.

I have seen a similar issue raise here: https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/PDIBnp1ftxk

Is it a common problem on the c++ runtime? what cause this problem? Is it the lib level initialization issue or the graph level? I was not able to find any support about this on stackoverflow. 

Can we have an option to prepare the graph in optimized state in order to eliminate  problem with slow first run?"
11709,"Most of embedding_lookup/embedding_lookup_sparse computations are on CPU,  Most of the GPU time is transferring data.","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Centos 7
- **TenorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
r.12
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
0.5.2
- **CUDA/cuDNN version**:
8.0
- **GPU model and memory**:
P40


### Describe the problem
I have done some profiling about *embedding_lookup_sparse*, i have use `tf.device('\gpu:0')` to specify where the ops run. But the results show:

- Most of the operations are performed on the CPU.
- GPU was transferring data at most of time.

After I looked into the codes, there are some Ops without GPU version, including `dynamic_partition`,  `segment_sum`, `strided_slice`. So I'm a little confused.

Is this intended behavior? Or are these Ops not implemented yet? Thanks for the explanations.

Here is some profiling result:
### tfprof
```
I tensorflow/core/common_runtime/simple_placer.cc:841] hidden/embed/embedding_lookup_sparse/embedding_lookup/DynamicPartition:(DynamicPartition)/job:localhost/replica:0/task:0/cpu:0
hidden/embed/embedding_lookup_sparse: (SegmentSum): /job:localhost/replica:0/task:0/cpu:0
hidden/embed/embedding_lookup_sparse/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/cpu:0
```
### nvprof
```
==4762== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 44.07%  130.029s    240276  541.17us     768ns  8.1151ms  [CUDA memcpy DtoH]
 25.20%  74.3504s    480619  154.70us     992ns  8.9016ms  [CUDA memcpy HtoD]
  9.84%  29.0339s    330000  87.981us  61.922us  610.68us  void tensorflow::GatherOpKernel<float, int>(float const *, int const *, tensorflow::GatherOpKernel<float, int>*, __int64, __int64, __int64)
```
"
11708,error in installation instructions,"In instructions specified at: https://www.tensorflow.org/install/install_linux#ValidateYourInstallation

it states:

 For example, the following command installs the CPU-only version of TensorFlow for Python 2.7:

 (tensorflow)$ pip install --ignore-installed --upgrade \
 https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.1-cp34-cp34m-linux_x86_64.whl

However it is for Python **3.4** (not 2.7)"
11707,error in tensorflow tutorial,"In tensorflow tutorial at: https://www.tensorflow.org/get_started/get_started

under 'A custom model' section it is missing an assignment for eval_input_fn as:

eval_input_fn = tf.contrib.learn.io.numpy_input_fn({""x"": x_eval}, y_eval, 4, num_epochs=1000)
"
11706,no input tensor in mobilenet_v1_1.0_224.ckpt,"I am newbie in TF and slim. Sorry if this a basic question. I downloaded mobilenet_v1_1.0_224.ckpt, built latest TF and TF-slim tree and ran tensorflow/tensorflow/python/tools/inspect_checkpoint.py on it.

I dont find input tensors, that are usually 224,224,3 or 112,112,3 etc size. Am I missing something here?

[mobilenet.txt](https://github.com/tensorflow/tensorflow/files/1169140/mobilenet.txt)
"
11704,ERROR: Failed to load Skylark extension '//tensorflow:workspace.bzl'.,"Environment info

Operating System: Ubuntu 16.04
Bazel version : 0.5.2
python version : 3.5.2
tensorflow version : 1.2.0
Hello,

When I try this command :

bazel build tensorflow/examples/label_image:label_image && bazel-bin/tensorflow/examples/label_image/label_image --graph=/tmp/output_graph.pb --labels=/tmp/output_labels.txt --output_layer=final_result --image=$HOME/piscine/nvxpool/cad_image_182022_259650_1987_3303.jpg --input_layer=Mul --output_layer='final_result'
I have this error :

ERROR: Failed to load Skylark extension '//tensorflow:workspace.bzl'. It usually happens when the repository is not defined prior to being used. Maybe repository '' was defined later in your WORKSPACE file? ERROR: cycles detected during target parsing.

I do not understand because before restarting my computer this command worked. Do you have any ideas ?

(For information before restarting my computer I try this tutorial https://github.com/DigitalGlobe/mltools/tree/master/examples/polygon_classify_cnn [The section Getting Started]. Do you think the problem comes from there?)

Thanks
"
11702,syntaxnet: Global training is worse than greedy trainning when the training data size is 1 millon data,"As you can see the following is my experimental configuration parameters and experiment results,  I'd like to know on a fairly large amount of data， how to adjust the super parameters can get a result that global better than greedy;
when the trainning data size is 20,000， We got the global trainning better than greedy trainning using default configuration parameters, But when the training data increased to 1 million， We can't reproduce the results;
This is our experimental result:
![image](https://user-images.githubusercontent.com/21216639/28505738-14a4b908-7059-11e7-8288-ff7ac89abc95.png)
This is our experimental configuration: 
greedy trainging parameters: 
![image](https://user-images.githubusercontent.com/21216639/28505764-4063d40c-7059-11e7-99d1-417cef636d1c.png)
global trainging parameters: 
![image](https://user-images.githubusercontent.com/21216639/28505797-5ba4f318-7059-11e7-8acc-fe101383b909.png)

I guess that's the parameter setting problem， Anyone who has experience can help me with this problem? Thanks

"
11701,tf.contrib.learn.KMeansClustering squeeze problem when using predict(),"This is a problem with every version of tensorflow I've tried (1.1, 1.2, 1.3).

After using the fit() method train a KMeansClustering model, if the predict() method is used to predict a *single* data point's cluster then there is an internal error (predicting two data points works fine)...

In ./tensorflow/contrib/learn/python/learn/estimators/estimator.py in _predict_generator() the batch_length is rightly determined by asking for the first dimension of the shape, but the problem is (at least for KMeansClustering) ./tensorflow/contrib/factorization/python/ops/clustering_ops.py _infer_graph() uses tf.squeeze to remove all dimensions with size of 1 for the distances and indices.  An error occurs when computing batch_size because the value will have rank 0 after the squeeze.  This appears to have been there since gardener@ originally imported that code for computing kmeans.

A) This certainly does not need to squeeze the first dimension as, in my case, if predict() is called for a single value, then the distances and indices are both just one element.
B) I cannot really see why a squeeze should be done there in the first place.  Perhaps flatten? (that doesn't make sense either)

I have a pull request ready which just removes the squeeze.  But I wanted to file an issue to which the original developer might be able to comment.

Or I can try to make it squeeze all but the first dimension, though the shape of those tensors are unknown when the graph is built. I'll have to figure out how to do that. I'd prefer to just not squeeze if it's not necessary.

Here is a script that demonstrates the problem:
```python
#/usr/bin/python3

import tensorflow as tf

K = 4    # K classes
km = tf.contrib.learn.KMeansClustering(K, '/tmp/test-kmeans-tf-model')

X = [[1.1, 1.2],
     [2.2, 2.5],
     [3.3, 3.6],
     [2.4, 2.7],
     [14.1, 3.2],
     [4.12, 15.2],
     [3.14, 3.6],
     [2.4, 13.7],
     [3.1, 3.2],
     [4.1, 4.2],
     [13.1, 13.2],
     [4.11, 14.2],
     [5.1, 15.2]]

def train_input_fn():
    data = tf.constant(X, tf.float32)
    return (data, None)


# train
km.fit(input_fn=train_input_fn, steps=1000)

def predict_input_fn():
    ### WORKS FINE
    #data = tf.constant([[2.1, 3.1], [1.1, 1.2]])

    ### FAILS TO PREDICT
    data = tf.constant([[1.1, 1.2]])
    return data

gen = km.predict(input_fn=predict_input_fn, as_iterable=True)
for t in gen:
    print(""predict returned: %s"" % t['cluster_idx'])


```
Removing the squeeze in clustering_ops.py fixes this script's problem (or give it two data points to predict).
"
11700, tf.import_graph_def  load quantized graph error,"[root@A01-R04-I221-14 export_dir]# python predict.py 
Traceback (most recent call last):
  File ""predict.py"", line 70, in <module>
    load_graph('quantized_graph.pb')
  File ""predict.py"", line 33, in load_graph
    tf.import_graph_def(graph_def)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 365, in import_graph_def
    % (input_name,)))
ValueError: graph_def is invalid at node u'while/add_6/y': More inputs specified ('while/Switch:1') than the op expects.."
11696,tf.Print return type,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 17.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.2.1
- **Python version**:
2.7
- **Exact command to reproduce**: 
```
a = tf.constant([[1,2],[3,4],[5,6],[7,8]])
partitions = tf.dynamic_partition(a,range(a.shape[0]),a.shape[0])
print(type(partitions)) # <type 'list'>
partitions = tf.Print(partitions,[partitions],""partition is:"") # prints the list
print(type(partitions)) # <class 'tensorflow.python.framework.ops.Tensor'>
```
### Describe the problem
`tf.Print` is supposed to be an identity operation so I assume it should not change type from `list` to `Tensor`. If we are not supposed to pass a list, it should raise an error instead of changing the type."
11695,Makefile build fails on OSX: -lprotobuf not found,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No. Using the provided Makefile build system. 

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Mac OS X 10.12.6

- **TensorFlow installed from (source or binary)**:
Source - r 1.3

- **TensorFlow version (use command below)**:
r1.3

- **Bazel version (if compiling from source)**:
Build label: 0.5.2-homebrew

- **Exact command to reproduce**:
`sh /tensorflow/contrib/makefile/build_all_linux.sh 
`
### Describe the problem
I think there is a bug in the make files when building for OSX. Actually, the build finishes ok but the scripts give a linked error when compiling the provided benchmark program. 
The error given is:

```
ld: library not found for -lprotobuf
clang: error: linker command failed with exit code 1 (use -v to see invocation)

```

### Source code / logs
This is the line where error occurs:
```
gcc --std=c++11 -DIS_SLIM_BUILD -fno-exceptions -DNDEBUG -O3 -march=native -I. -Itf_build/tensorflow/tensorflow/contrib/makefile/downloads/ -Itf_build/tensorflow/tensorflow/contrib/makefile/downloads/eigen -Itf_build/tensorflow/tensorflow/contrib/makefile/downloads/gemmlowp -Itf_build/tensorflow/tensorflow/contrib/makefile/gen/proto/ -Itf_build/tensorflow/tensorflow/contrib/makefile/gen/proto_text/ -Itf_build/tensorflow/tensorflow/contrib/makefile/gen/protobuf-host/include -I/usr/local/include \
	-o tf_build/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark tf_build/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/core/util/reporter.o tf_build/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/tools/benchmark/benchmark_model.o tf_build/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/tools/benchmark/benchmark_model_main.o \
	 tf_build/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a -L/usr/local/lib -all_load -lstdc++ -lprotobuf -lz -lm
```

As you can see towards the end it links against -lprotobuf, however, it does not give the path to the library (It's missing `-L/some-path/tensorflow/contrib/makefile/gen/protobuf-host/lib`). When I manually compile the program, instead of using the makefile, and pass the library path, it works fine. 

It is easy to fix by modifying the lines 186 onwards in the Makefile from:

```
ifeq ($(TARGET),OSX)
	LDFLAGS += -all_load
endif
```
to
```
ifeq ($(TARGET),OSX)
	LDFLAGS += -all_load
ifeq ($(HAS_GEN_HOST_PROTOC),true)
	LIBFLAGS += -L$(MAKEFILE_DIR)/gen/protobuf-host/lib
	export LD_LIBRARY_PATH=$(MAKEFILE_DIR)/gen/protobuf-host/lib
endif
```
Another related issue is that I get a whole bunch of warnings from ranlib about some of the built libraries having no symbol. Any comments on that will also be appreciated:

```
/Library/Developer/CommandLineTools/usr/bin/ranlib: file: /fullpath/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(mkl_layout_pass.o) has no symbols
/Library/Developer/CommandLineTools/usr/bin/ranlib: file: /fullpath/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(mkl_tfconversion_pass.o) has no symbols
/Library/Developer/CommandLineTools/usr/bin/ranlib: file: /fullpath/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(cupti_wrapper.o) has no symbols
/Library/Developer/CommandLineTools/usr/bin/ranlib: file: /fullpath/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(android_armv7a_cpu_utils_helper.o) has no symbols

```

Thanks."
11693,Tensorflow error while building using gpu,"ERROR: /home/overlord/.cache/bazel/_bazel_overlord/38c05a5232666f398e07b83e8b030232/external/lmdb/BUILD.bazel:8:1: C++ compilation of rule '@lmdb//:lmdb' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 33 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
external/lmdb/midl.c: In function 'MDB_ID* mdb_midl_alloc(int)':
external/lmdb/midl.c:105:47: error: invalid conversion from 'void*' to 'MDB_IDL {aka long unsigned int*}' [-fpermissive]
  MDB_IDL ids = malloc((num+2) * sizeof(MDB_ID));
                                               ^
external/lmdb/midl.c: In function 'void mdb_midl_shrink(MDB_ID**)':
external/lmdb/midl.c:123:58: error: invalid conversion from 'void*' to 'MDB_IDL {aka long unsigned int*}' [-fpermissive]
   (ids = realloc(ids, (MDB_IDL_UM_MAX+2) * sizeof(MDB_ID))))
                                                          ^
external/lmdb/midl.c: In function 'int mdb_midl_grow(MDB_ID**, int)':
external/lmdb/midl.c:134:54: error: invalid conversion from 'void*' to 'MDB_IDL {aka long unsigned int*}' [-fpermissive]
  idn = realloc(idn, (*idn + num + 2) * sizeof(MDB_ID));
                                                      ^
external/lmdb/midl.c: In function 'int mdb_midl_need(MDB_ID**, unsigned int)':
external/lmdb/midl.c:148:50: error: invalid conversion from 'void*' to 'MDB_IDL {aka long unsigned int*}' [-fpermissive]
   if (!(ids = realloc(ids-1, num * sizeof(MDB_ID))))
                                                  ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps."
11692,A bug of tf.reduce_logsumexp with `-inf`,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux CentOS 7
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.2.0
- **Python version**: 2.7.13
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/5.1.3
- **GPU model and memory**: Tesla K40m, 11439MiB
- **Exact command to reproduce**:  `python -c ""import tensorflow as tf; print tf.Session().run(tf.reduce_logsumexp(float('-inf')))""`

### Describe the problem
The doc of tf.reduce_logsumexp says it

> Computes log(sum(exp(elements across dimensions of a tensor))).

However, it does not when the tensor is `-inf`.

### Source code / logs
```
python -c ""import tensorflow as tf; print tf.Session().run(tf.reduce_logsumexp(float('-inf')))""
```
prints
```
nan
```
--------------------

```
python -c ""import tensorflow as tf; print tf.Session().run(tf.log(tf.reduce_sum(tf.exp(float('-inf')))))""
```
prints
```
-inf
```
"
11690,[Feature request] Instance Normalization,"[Instance Normalization](https://arxiv.org/abs/1607.08022) is recently widely used in style transfer and GAN, since it avoid the drawback of batch normalization which brings in-batch correlations. Tensorflow only has quantized version instance norm right now, but a full version is also easy to implement, I am interested in it. 
Also I wonder if it is better to implemented in C++ using eigen or simply add some lines in `nn_impls.py` with the benefit of being able to use `fused_batch_norm`. [Mxnet](https://github.com/dmlc/mxnet/blob/03b7d1402ab8d1142391c601fddbc4081632aa3c/src/operator/instance_norm-inl.h) does the first way while [Pytorch](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/instancenorm.py) use the latter method."
11689,Distributed Tensorflow Authorization,"Could somebody comment on the security design of distributed tensorflow? 
Is there some kind of authorization in place for the grpc calls in distributed tensorflow?
E.g., are the clients validated based on the IP or anything else?

If not, how difficult would it be to add authorization support and what would be the best place in the code to get started?
"
11688,Tensorflow v1.2.1 compile error,"I'm, trying to build tensorflow from source. I've followed the tutorial in https://www.tensorflow.org/install/install_sources without success.

The only difference is that I'm trying to use OpenCL with SYCL.

### System information
- **OS Platform and Distribution**: Ubuntu 14.04 64 bits
- **TensorFlow version to be compiled**: v1.2.1
- **Python version**: 2.7.6
- **Bazel version (from repository)**: Build label: 0.5.2
- **GPU model and memory**: Radeon HD 7850

### Problem
I'm trying to compile tensorflow v1.2.1 but I'm having an error about numpy missing dependecies declaration.
It seems there is some workaround because headers are there, but I have no clue about how to do it. My bazel knowledge is low.

**Configuration:**

```
flener@flener-desktop:~/Downloads/tensorflow-src$ uname -a
Linux flener-desktop 3.11.0-14-generic #21-Ubuntu SMP Tue Nov 12 17:04:55 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux
flener@flener-desktop:~/Downloads/tensorflow-src$ git checkout v1.2.1
HEAD is now at b4957ff... Merge pull request #11156 from av8ramit/1.2.1
flener@flener-desktop:~/Downloads/tensorflow-src$ git clean -fdx
Removing .bazelrc
Removing .tf_configure.bazelrc
Removing bazel-bin
Removing bazel-genfiles
Removing bazel-out
Removing bazel-tensorflow-src
Removing bazel-testlogs
Removing tensorflow/tools/git/gen/
Removing third_party/eigen3/mkl_include
Removing third_party/mkl/include
Removing third_party/mkl/libdl.so.2
Removing third_party/mkl/libiomp5.so
Removing third_party/mkl/libmklml_intel.so
Removing third_party/mkl/mkl.config
Removing third_party/mkl/mklml_lnx_2018.0.20170425.tgz
Removing third_party/mkl/mklml_lnx_2018.0.20170425/
Removing tools/python_bin_path.sh
flener@flener-desktop:~/Downloads/tensorflow-src$ ./configure
Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python2.7
Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]
	
Do you wish to build TensorFlow with MKL support? [y/N] y
MKL support will be enabled for TensorFlow
Do you wish to download MKL LIB from the web? [Y/n] y
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 
Do you wish to use jemalloc as the malloc implementation? [Y/n] y
jemalloc enabled
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] n
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] n
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] n
No XLA JIT support will be enabled for TensorFlow
Do you wish to build TensorFlow with VERBS support? [y/N] y
VERBS support will be enabled for TensorFlow
Do you wish to build TensorFlow with OpenCL support? [y/N] y
OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] n
No CUDA support will be enabled for TensorFlow
Please specify which C++ compiler should be used as the host C++ compiler. [Default is ]: /usr/bin/g++-4.9
Please specify which C compiler should be used as the host C compiler. [Default is ]: /usr/bin/gcc-4.9
Please specify the location where ComputeCpp for SYCL 1.2 is installed. [Default is /usr/local/computecpp]: 
INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.
Configuration finished

```

**Build command:**
`flener@flener-desktop:~/Downloads/tensorflow-src$ bazel build --local_resources 4096,4.0,1.0 --verbose_failures -c opt --config=sycl //tensorflow/tools/pip_package:build_pip_package`

**Error:**
```
ERROR: /home/flener/Downloads/tensorflow-src/tensorflow/python/BUILD:158:1: undeclared inclusion(s) in rule '//tensorflow/python:numpy_lib':
this rule is missing dependency declarations for the following files included by 'tensorflow/python/lib/core/numpy.cc':
  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/arrayobject.h'
  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/ndarrayobject.h'
  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/ndarraytypes.h'
  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/npy_common.h'
  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/numpyconfig.h'
  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/_numpyconfig.h'
  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/npy_endian.h'
  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/npy_cpu.h'
  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/utils.h'
  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/_neighborhood_iterator_imp.h'
  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/__multiarray_api.h'
  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/npy_interrupt.h'.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```

"
11685,retrain inception-v3 error,"
"
11684,"Can anybody please let me know an error free code of basic CNN tensorflow code.  I am having hard time to resolve  tensorflow coding working, even  the tensorflow tutorial code from tensorflow.org","Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
11682,Missing numpy dependency at installation,"I installed PyTorch on a fresh Python 3.6 installation (`brew install python3`), got an error and found out that numpy wasn't installed. I installed numpy and then things work. Shouldn't it be included among dependencies of the PyTorch package? (with the right version requirement, see #559?)

```
Minhs-MacBook-Pro:~ cumeo$ pip3 install --user http://download.pytorch.org/whl/torch-0.1.12.post2-cp36-cp36m-macosx_10_7_x86_64.whl
Collecting torch==0.1.12.post2 from http://download.pytorch.org/whl/torch-0.1.12.post2-cp36-cp36m-macosx_10_7_x86_64.whl
  Downloading http://download.pytorch.org/whl/torch-0.1.12.post2-cp36-cp36m-macosx_10_7_x86_64.whl (3.7MB)
    100% |████████████████████████████████| 3.7MB 1.6MB/s
Collecting pyyaml (from torch==0.1.12.post2)
Installing collected packages: pyyaml, torch
Successfully installed pyyaml-3.12 torch-0.1.12.post2
Minhs-MacBook-Pro:~ cumeo$ python3
Python 3.6.2 (default, Jul 17 2017, 16:44:45)
[GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import torch
ImportError: numpy.core.multiarray failed to import
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/cumeo/Library/Python/3.6/lib/python/site-packages/torch/__init__.py"", line 53, in <module>
    from torch._C import *
ImportError: numpy.core.multiarray failed to import
>>> import numpy
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'numpy'
>>>
Minhs-MacBook-Pro:~ cumeo$ pip3 install --user numpy
Collecting numpy
  Using cached numpy-1.13.1-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl
Installing collected packages: numpy
Successfully installed numpy-1.13.1
Minhs-MacBook-Pro:~ cumeo$ python3
Python 3.6.2 (default, Jul 17 2017, 16:44:45)
[GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import torch
>>>
```"
11681,Different GPU memory not all allocated,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
win10
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.1
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
cuda8.0,cudnn5.5
- **GPU model and memory**:
GTX Titan 6GB, GTX 1080Ti 11GB

### Describe the problem
Note that tensorflow will allocate tensor/op to all available gpus, but i only got 1080Ti occupied, I wonder if it's a bug or tensorflow doesn't support different gpu models."
11679,Error when running imported/restored model that uses feedable iterator (tf.contrib.data),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: Build #242 (Jul 17, 2017 2:25:00 AM)
- **Python version**: Python 3.6.2
- **Bazel version (if compiling from source)**: na
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: 670 gtx 2gb
- **Exact command to reproduce**: na

### Describe the problem
I can't restore and run checkpoints of models that use feedable iterators, but I can restore and run checkpoints of models that directly use `make_one_shot_iterator()`. Below is code for the feedable iterator version, and below that code for the `make_one_shot_iterator()` version:

```python
import tensorflow as tf
from tensorflow.contrib.data import Dataset

BATCH_SIZE = 4
ITERATION_COUNT = 2
dataset = Dataset.from_tensor_slices(tf.constant([[0, 0],
                                                  [0, 1],
                                                  [1, 0],
                                                  [1, 1]], dtype=tf.float32))
batched_dataset = dataset.batch(BATCH_SIZE)
iterator_handle_placeholder = tf.placeholder(tf.string, shape=[])
tf.add_to_collection('placeholders', iterator_handle_placeholder)
iterator = tf.contrib.data.Iterator.from_string_handle(iterator_handle_placeholder, batched_dataset.output_types, batched_dataset.output_shapes)

# create some graph
inputs = iterator.get_next()
sum_placeholder = tf.placeholder_with_default(tf.reduce_sum(inputs), shape=[])
tf.add_to_collection('placeholders', sum_placeholder)
sum_variable = tf.get_variable('sum', initializer=tf.zeros(shape=[]))
assign_sum = tf.assign_add(sum_variable, sum_placeholder)
tf.add_to_collection('assigns', assign_sum)
saver = tf.train.Saver()

# run graph and save it at the end
with tf.Session() as session:
    session.run(tf.global_variables_initializer())
    batched_dataset_iterator = batched_dataset.make_one_shot_iterator()
    batched_dataset_iterator_handle = session.run(batched_dataset_iterator.string_handle())
    tf.add_to_collection('handles', batched_dataset_iterator_handle)
    inputs_feed_dict = {iterator_handle_placeholder: batched_dataset_iterator_handle}
    for i in range(ITERATION_COUNT):
        inputs_sum = session.run(assign_sum, feed_dict=inputs_feed_dict)
        inputs_feed_dict[sum_placeholder] = inputs_sum
        print(inputs_sum)
    saver.save(session, 'checkpoints/haha')

# restore saved graph and run it
with tf.Session() as session:
    saver = tf.train.import_meta_graph('checkpoints/haha.meta')
    saver.restore(session, 'checkpoints/haha')
    assign_sum = tf.get_collection('assigns')[0]
    iterator_handle_placeholder = tf.get_collection('placeholders')[0]
    batched_dataset_iterator_handle = tf.get_collection('handles')[0]
    sum_placeholder = tf.get_collection('placeholders')[1]
    inputs_feed_dict = {iterator_handle_placeholder: batched_dataset_iterator_handle}
    for i in range(ITERATION_COUNT):
        inputs_sum = session.run(assign_sum, feed_dict=inputs_feed_dict)
        print(inputs_sum)
        inputs_feed_dict[sum_placeholder] = inputs_sum
```
gives me
```
C:\Users\Jonathan\Miniconda3\envs\ai\python.exe ""C:/Software Projects/ai/tensorflow/LayerTests.py""
2017-07-21 20:18:54.013753: W C:\tf_jenkins\home\workspace\nightly-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-07-21 20:18:54.257354: I C:\tf_jenkins\home\workspace\nightly-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:955] Found device 0 with properties: 
name: GeForce GTX 670
major: 3 minor: 0 memoryClockRate (GHz) 0.98
pciBusID 0000:02:00.0
Total memory: 2.00GiB
Free memory: 1.64GiB
2017-07-21 20:18:54.257634: I C:\tf_jenkins\home\workspace\nightly-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:976] DMA: 0 
2017-07-21 20:18:54.257773: I C:\tf_jenkins\home\workspace\nightly-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:986] 0:   Y 
2017-07-21 20:18:54.257935: I C:\tf_jenkins\home\workspace\nightly-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 670, pci bus id: 0000:02:00.0)
4.0
8.0
2017-07-21 20:18:54.705095: I C:\tf_jenkins\home\workspace\nightly-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 670, pci bus id: 0000:02:00.0)
2017-07-21 20:18:54.774833: W C:\tf_jenkins\home\workspace\nightly-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Not found: Container localhost does not exist.
Traceback (most recent call last):
  File ""C:\Users\Jonathan\Miniconda3\envs\ai\lib\site-packages\tensorflow\python\client\session.py"", line 1327, in _do_call
    return fn(*args)
  File ""C:\Users\Jonathan\Miniconda3\envs\ai\lib\site-packages\tensorflow\python\client\session.py"", line 1306, in _run_fn
    status, run_metadata)
  File ""C:\Users\Jonathan\Miniconda3\envs\ai\lib\contextlib.py"", line 88, in __exit__
    next(self.gen)
  File ""C:\Users\Jonathan\Miniconda3\envs\ai\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.NotFoundError: Container localhost does not exist.
	 [[Node: IteratorFromStringHandle = IteratorFromStringHandle[_device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_Placeholder_0_0)]]
	 [[Node: PlaceholderWithDefault/_7 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_7_PlaceholderWithDefault"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Software Projects/ai/tensorflow/LayerTests.py"", line 47, in <module>
    inputs_sum = session.run(assign_sum, feed_dict=inputs_feed_dict)
  File ""C:\Users\Jonathan\Miniconda3\envs\ai\lib\site-packages\tensorflow\python\client\session.py"", line 895, in run
    run_metadata_ptr)
  File ""C:\Users\Jonathan\Miniconda3\envs\ai\lib\site-packages\tensorflow\python\client\session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\Jonathan\Miniconda3\envs\ai\lib\site-packages\tensorflow\python\client\session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""C:\Users\Jonathan\Miniconda3\envs\ai\lib\site-packages\tensorflow\python\client\session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Container localhost does not exist.
	 [[Node: IteratorFromStringHandle = IteratorFromStringHandle[_device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_Placeholder_0_0)]]
	 [[Node: PlaceholderWithDefault/_7 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_7_PlaceholderWithDefault"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

Caused by op 'IteratorFromStringHandle', defined at:
  File ""C:/Software Projects/ai/tensorflow/LayerTests.py"", line 13, in <module>
    iterator = tf.contrib.data.Iterator.from_string_handle(iterator_handle_placeholder, batched_dataset.output_types, batched_dataset.output_shapes)
  File ""C:\Users\Jonathan\Miniconda3\envs\ai\lib\site-packages\tensorflow\contrib\data\python\ops\dataset_ops.py"", line 238, in from_string_handle
    string_handle)
  File ""C:\Users\Jonathan\Miniconda3\envs\ai\lib\site-packages\tensorflow\python\ops\gen_dataset_ops.py"", line 362, in iterator_from_string_handle
    string_handle=string_handle, name=name)
  File ""C:\Users\Jonathan\Miniconda3\envs\ai\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""C:\Users\Jonathan\Miniconda3\envs\ai\lib\site-packages\tensorflow\python\framework\ops.py"", line 2628, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Users\Jonathan\Miniconda3\envs\ai\lib\site-packages\tensorflow\python\framework\ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Container localhost does not exist.
	 [[Node: IteratorFromStringHandle = IteratorFromStringHandle[_device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_Placeholder_0_0)]]
	 [[Node: PlaceholderWithDefault/_7 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_7_PlaceholderWithDefault"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]


Process finished with exit code 1
```
Here's the version that uses just a `make_one_shot_iterator()`:
```python
import tensorflow as tf
from tensorflow.contrib.data import Dataset

BATCH_SIZE = 4
ITERATION_COUNT = 2
dataset = Dataset.from_tensor_slices(tf.constant([[0, 0],
                                                  [0, 1],
                                                  [1, 0],
                                                  [1, 1]], dtype=tf.float32))
batched_dataset = dataset.batch(BATCH_SIZE)

# create some graph
batched_dataset_iterator = batched_dataset.make_one_shot_iterator()
inputs = batched_dataset_iterator.get_next()
sum_placeholder = tf.placeholder_with_default(tf.reduce_sum(inputs), shape=[])
tf.add_to_collection('placeholders', sum_placeholder)
sum_variable = tf.get_variable('sum', initializer=tf.zeros(shape=[]))
assign_sum = tf.assign_add(sum_variable, sum_placeholder)
tf.add_to_collection('assigns', assign_sum)
saver = tf.train.Saver()

# run graph and save it at the end
with tf.Session() as session:
    session.run(tf.global_variables_initializer())
    inputs_feed_dict = {}
    for i in range(ITERATION_COUNT):
        inputs_sum = session.run(assign_sum, feed_dict=inputs_feed_dict)
        inputs_feed_dict[sum_placeholder] = inputs_sum
        print(inputs_sum)
    saver.save(session, 'checkpoints/haha')

# restore saved graph and run it
with tf.Session() as session:
    saver = tf.train.import_meta_graph('checkpoints/haha.meta')
    saver.restore(session, 'checkpoints/haha')
    assign_sum = tf.get_collection('assigns')[0]
    sum_placeholder = tf.get_collection('placeholders')[1]
    inputs_feed_dict = {}
    for i in range(ITERATION_COUNT):
        inputs_sum = session.run(assign_sum, feed_dict=inputs_feed_dict)
        print(inputs_sum)
        inputs_feed_dict[sum_placeholder] = inputs_sum
```
Result:
```
C:\Users\Jonathan\Miniconda3\envs\ai\python.exe ""C:/Software Projects/ai/tensorflow/LayerTests.py""
2017-07-21 20:12:26.273776: W C:\tf_jenkins\home\workspace\nightly-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-07-21 20:12:26.516570: I C:\tf_jenkins\home\workspace\nightly-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:955] Found device 0 with properties: 
name: GeForce GTX 670
major: 3 minor: 0 memoryClockRate (GHz) 0.98
pciBusID 0000:02:00.0
Total memory: 2.00GiB
Free memory: 1.64GiB
2017-07-21 20:12:26.516852: I C:\tf_jenkins\home\workspace\nightly-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:976] DMA: 0 
2017-07-21 20:12:26.516992: I C:\tf_jenkins\home\workspace\nightly-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:986] 0:   Y 
2017-07-21 20:12:26.517145: I C:\tf_jenkins\home\workspace\nightly-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 670, pci bus id: 0000:02:00.0)
4.0
8.0
2017-07-21 20:12:26.926502: I C:\tf_jenkins\home\workspace\nightly-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 670, pci bus id: 0000:02:00.0)
12.0
24.0

Process finished with exit code 0
```"
11678,BUILD Failed: missing input file '@mkl//:LICENSE'.,"

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 7
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: latest
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:0.4.5
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: bazel build --config=mkl  -c opt //tensorflow/tools/pip_package:build_pip_package



I was trying to build Tensorflow. and hitting this error missing input file '@mkl//:LICENSE'.
i did it a week back, no problem faced. everything worked fine with same command line

"
11677,Does orthogonal initialization have GPU implementation (fails when explicitly assigned on a gpu)?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:  1.2.1
- **Python version**:  3.4.3
- **Bazel version (if compiling from source)**: 0.5.2
- **CUDA/cuDNN version**:  8.0/6.0
- **GPU model and memory**: Tesla P100 (16 gb)

### Describe the problem
I get an error when trying to use orthogonal initialization which is explicitly assigned to be run on GPU. Looking at the log it looks like some parts of it (e.g. QR decomposition) are available on CPU only. Is this the case or is there some bug that is blocking it from being run on GPU only?

### Source code / logs
The following code could be used to reproduce the issue:
```python
import tensorflow as tf
with tf.device('/gpu:0'):
    a = tf.get_variable('a', (10, 3, 3, 20), tf.float32, tf.orthogonal_initializer())
sess = tf.Session()
sess.run(tf.global_variables_initializer())
```
The error log:

```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1284     try:
-> 1285       return fn(*args)
   1286     except errors.OpError as e:

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1254       # Ensure any changes to the graph are reflected in the runtime.
-> 1255       self._extend_graph()
   1256       with errors.raise_exception_on_not_ok_status() as status:

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _extend_graph(self)
   1315           tf_session.TF_ExtendGraph(
-> 1316               self._session, graph_def.SerializeToString(), status)
   1317         self._opened = True

/usr/lib/python3.4/contextlib.py in __exit__(self, type, value, traceback)
     65             try:
---> 66                 next(self.gen)
     67             except StopIteration:

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/errors_impl.py in raise_exception_on_not_ok_status()
    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),
--> 466           pywrap_tensorflow.TF_GetCode(status))
    467   finally:

InvalidArgumentError: Cannot assign a device for operation 'a': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Colocation Debug Info:
Colocation group had the following types and devices: 
Assign: GPU CPU 
Reshape: GPU CPU 
RealDiv: GPU CPU 
Abs: GPU CPU 
DiagPart: CPU 
StridedSlice: GPU CPU 
Qr: CPU 
Add: GPU CPU 
Mul: GPU CPU 
Identity: CPU 
VariableV2: GPU CPU 
RandomStandardNormal: GPU CPU 
Minimum: GPU CPU 
Const: GPU CPU 
Pack: GPU CPU 
	 [[Node: a = VariableV2[_class=[""loc:@a""], container="""", dtype=DT_FLOAT, shape=[10,3,3,20], shared_name="""", _device=""/device:GPU:0""]()]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-1-f5bccc7031ec> in <module>()
      5 
      6 sess = tf.Session()
----> 7 sess.run(tf.global_variables_initializer())

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    894     try:
    895       result = self._run(None, fetches, feed_dict, options_ptr,
--> 896                          run_metadata_ptr)
    897       if run_metadata:
    898         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1122     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1123       results = self._do_run(handle, final_targets, final_fetches,
-> 1124                              feed_dict_tensor, options, run_metadata)
   1125     else:
   1126       results = []

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1277     if handle is None:
   1278       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-> 1279                            options, run_metadata)
   1280     else:
   1281       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1296         except KeyError:
   1297           pass
-> 1298       raise type(e)(node_def, op, message)
   1299 
   1300   def _extend_graph(self):

InvalidArgumentError: Cannot assign a device for operation 'a': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Colocation Debug Info:
Colocation group had the following types and devices: 
Assign: GPU CPU 
Reshape: GPU CPU 
RealDiv: GPU CPU 
Abs: GPU CPU 
DiagPart: CPU 
StridedSlice: GPU CPU 
Qr: CPU 
Add: GPU CPU 
Mul: GPU CPU 
Identity: CPU 
VariableV2: GPU CPU 
RandomStandardNormal: GPU CPU 
Minimum: GPU CPU 
Const: GPU CPU 
Pack: GPU CPU 
	 [[Node: a = VariableV2[_class=[""loc:@a""], container="""", dtype=DT_FLOAT, shape=[10,3,3,20], shared_name="""", _device=""/device:GPU:0""]()]]

Caused by op 'a', defined at:
  File ""/usr/lib/python3.4/runpy.py"", line 170, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.4/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/IPython/core/interactiveshell.py"", line 2698, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/IPython/core/interactiveshell.py"", line 2802, in run_ast_nodes
    if self.run_code(code, result):
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/IPython/core/interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-f5bccc7031ec>"", line 4, in <module>
    a = tf.get_variable('a', (10, 3, 3, 20), tf.float32, tf.orthogonal_initializer())
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/variable_scope.py"", line 1065, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/variable_scope.py"", line 962, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/variable_scope.py"", line 367, in get_variable
    validate_shape=validate_shape, use_resource=use_resource)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/variable_scope.py"", line 352, in _true_getter
    use_resource=use_resource)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/variable_scope.py"", line 725, in _get_single_variable
    validate_shape=validate_shape)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/variables.py"", line 199, in __init__
    expected_shape=expected_shape)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/variables.py"", line 283, in _init_from_args
    name=name)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/state_ops.py"", line 131, in variable_op_v2
    shared_name=shared_name)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/gen_state_ops.py"", line 684, in _variable_v2
    name=name)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 2576, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'a': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Colocation Debug Info:
Colocation group had the following types and devices: 
Assign: GPU CPU 
Reshape: GPU CPU 
RealDiv: GPU CPU 
Abs: GPU CPU 
DiagPart: CPU 
StridedSlice: GPU CPU 
Qr: CPU 
Add: GPU CPU 
Mul: GPU CPU 
Identity: CPU 
VariableV2: GPU CPU 
RandomStandardNormal: GPU CPU 
Minimum: GPU CPU 
Const: GPU CPU 
Pack: GPU CPU 
	 [[Node: a = VariableV2[_class=[""loc:@a""], container="""", dtype=DT_FLOAT, shape=[10,3,3,20], shared_name="""", _device=""/device:GPU:0""]()]]
```"
11676,Error in tf.contrib.layers.batch_norm when explicitly assigned on gpu,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:  1.2.1
- **Python version**:  3.4.3
- **Bazel version (if compiling from source)**: 0.5.2
- **CUDA/cuDNN version**:  8.0/6.0
- **GPU model and memory**: Tesla P100 (16 gb)

### Describe the problem
Batch norm layer fails with an error when explicitly assigned to be run on gpu and zero_debias_moving_mean is False. Interesting that I'm getting this error only when is_training is a placeholder (passing just True doesn't reproduce the error). If commented line is used instead (zero_debias_moving_mean=True) the code also works without any error.

### Source code / logs
The following code could be used to reproduce the issue:
```python
import tensorflow as tf
is_training = tf.placeholder(tf.bool, name='is_training')
a = tf.placeholder(tf.float32, shape=(10, 10, 10, 10))
with tf.device('/gpu:0'):
    b = tf.contrib.layers.batch_norm(a, is_training=is_training)
#     b = tf.contrib.layers.batch_norm(a, is_training=is_training, zero_debias_moving_mean=True)
sess = tf.Session()
sess.run(tf.global_variables_initializer())
```
The error log:

```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1284     try:
-> 1285       return fn(*args)
   1286     except errors.OpError as e:

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1254       # Ensure any changes to the graph are reflected in the runtime.
-> 1255       self._extend_graph()
   1256       with errors.raise_exception_on_not_ok_status() as status:

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _extend_graph(self)
   1315           tf_session.TF_ExtendGraph(
-> 1316               self._session, graph_def.SerializeToString(), status)
   1317         self._opened = True

/usr/lib/python3.4/contextlib.py in __exit__(self, type, value, traceback)
     65             try:
---> 66                 next(self.gen)
     67             except StopIteration:

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/errors_impl.py in raise_exception_on_not_ok_status()
    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),
--> 466           pywrap_tensorflow.TF_GetCode(status))
    467   finally:

InvalidArgumentError: Cannot assign a device for operation 'BatchNorm/Reshape_2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
	 [[Node: BatchNorm/Reshape_2 = Reshape[T=DT_BOOL, Tshape=DT_INT32, _device=""/device:GPU:0""](is_training, BatchNorm/Reshape_2/shape)]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-1-285b7d56d919> in <module>()
      8 #     b = tf.contrib.layers.batch_norm(a, is_training=is_training, zero_debias_moving_mean=True)
      9 sess = tf.Session()
---> 10 sess.run(tf.global_variables_initializer())

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    894     try:
    895       result = self._run(None, fetches, feed_dict, options_ptr,
--> 896                          run_metadata_ptr)
    897       if run_metadata:
    898         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1122     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1123       results = self._do_run(handle, final_targets, final_fetches,
-> 1124                              feed_dict_tensor, options, run_metadata)
   1125     else:
   1126       results = []

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1277     if handle is None:
   1278       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-> 1279                            options, run_metadata)
   1280     else:
   1281       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1296         except KeyError:
   1297           pass
-> 1298       raise type(e)(node_def, op, message)
   1299 
   1300   def _extend_graph(self):

InvalidArgumentError: Cannot assign a device for operation 'BatchNorm/Reshape_2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
	 [[Node: BatchNorm/Reshape_2 = Reshape[T=DT_BOOL, Tshape=DT_INT32, _device=""/device:GPU:0""](is_training, BatchNorm/Reshape_2/shape)]]

Caused by op 'BatchNorm/Reshape_2', defined at:
  File ""/usr/lib/python3.4/runpy.py"", line 170, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.4/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/IPython/core/interactiveshell.py"", line 2698, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/IPython/core/interactiveshell.py"", line 2802, in run_ast_nodes
    if self.run_code(code, result):
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/IPython/core/interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-285b7d56d919>"", line 7, in <module>
    b = tf.contrib.layers.batch_norm(a, is_training=is_training)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args
    return func(*args, **current_args)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 634, in batch_norm
    outputs = layer.apply(inputs, training=is_training)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/layers/base.py"", line 492, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/layers/base.py"", line 441, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/layers/normalization.py"", line 395, in call
    decay = _smart_select(training, lambda: self.momentum, lambda: 1.)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/layers/normalization.py"", line 568, in _smart_select
    pred = array_ops.reshape(pred, [1])
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 2502, in reshape
    name=name)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 2576, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'BatchNorm/Reshape_2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
	 [[Node: BatchNorm/Reshape_2 = Reshape[T=DT_BOOL, Tshape=DT_INT32, _device=""/device:GPU:0""](is_training, BatchNorm/Reshape_2/shape)]]
```
"
11674,"ServingInputReceiver passes Estimator model_fn only dictionary of features, but model_fn is allowed to take single tensor feature","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes, I have written custom code. I have more discussion about the bug in this Jupyter notebook: https://gist.github.com/nkashy1/fc1ec4ee218963216dea3ab5242bf611

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Goobuntu

- **TensorFlow installed from (source or binary)**:
PyPI

- **TensorFlow version (use command below)**:
1.2.1

- **Python version**: 
2.7.6

- **Bazel version (if compiling from source)**:
Not relevant

- **CUDA/cuDNN version**:
Not relevant

- **GPU model and memory**:
Not relevant

- **Exact command to reproduce**:
Check this notebook: https://gist.github.com/nkashy1/fc1ec4ee218963216dea3ab5242bf611

### Describe the problem
The [tf.estimator.Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator) interface allows users to provide a `model_fn` which accepts features either within a single tensor or within a dictionary mapping strings to tensors.

The Estimator `export_savedmodel` method requires a `serving_input_receiver_fn` argument, which is a function of no arguments that produces a [ServingInputReceiver](https://www.tensorflow.org/api_docs/python/tf/estimator/export/ServingInputReceiver). The features tensors from this `ServingInputReceiver` are passed to the `model_fn` for serving.

Upon instantiation, the `ServingInputReceiver` wraps single tensor features into a dictionary. This raises an error for estimators whose `model_fn` expects a single tensor as its `features` argument.

### Source code / logs
Gist: https://gist.github.com/nkashy1/fc1ec4ee218963216dea3ab5242bf611

You can run that notebook to see log messages, etc.

#### Misc
Possibly related to this stackoverflow thread: https://stackoverflow.com/questions/42835809/how-to-export-estimator-model-with-export-savedmodel-function"
11673,"Error in tf.contrib.layers.batch_norm when center=False, data_format='NCHW' and zero_debias_moving_mean=True","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:  1.2.1
- **Python version**:  3.4.3
- **Bazel version (if compiling from source)**: 0.5.2
- **CUDA/cuDNN version**:  8.0/6.0
- **GPU model and memory**: Tesla P100 (16 gb)

### Describe the problem
Batch norm layer fails with an error when both center=False, data_format='NCHW' and zero_debias_moving_mean=True arguments are used. It looks like the solution would be just adding additional if to check if beta is None in the same way it is done for gamma, but maybe there are some more dependencies.

### Source code / logs
The following code could be used to reproduce the issue:
```python
import tensorflow as tf
a = tf.placeholder(tf.float32, shape=(10, 10, 10, 10))
b = tf.contrib.layers.batch_norm(a, center=False, data_format='NCHW',
                                 zero_debias_moving_mean=True)
sess = tf.Session()
sess.run(tf.global_variables_initializer())
```
The error log:

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py in apply_op(self, op_type_name, name, **keywords)
    489                 as_ref=input_arg.is_ref,
--> 490                 preferred_dtype=default_dtype)
    491           except TypeError as err:

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)
    675         if ret is None:
--> 676           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    677 

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    120   _ = as_ref
--> 121   return constant(v, dtype=dtype, name=name)
    122 

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)
    101   tensor_value.tensor.CopyFrom(
--> 102       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
    103   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)
    363     if values is None:
--> 364       raise ValueError(""None values not supported."")
    365     # if dtype is provided, forces numpy array to be the type

ValueError: None values not supported.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py in apply_op(self, op_type_name, name, **keywords)
    503               observed = ops.internal_convert_to_tensor(
--> 504                   values, as_ref=input_arg.is_ref).dtype.name
    505             except ValueError as err:

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)
    675         if ret is None:
--> 676           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    677 

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    120   _ = as_ref
--> 121   return constant(v, dtype=dtype, name=name)
    122 

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)
    101   tensor_value.tensor.CopyFrom(
--> 102       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
    103   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)
    363     if values is None:
--> 364       raise ValueError(""None values not supported."")
    365     # if dtype is provided, forces numpy array to be the type

ValueError: None values not supported.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-1-c9cf0f67668a> in <module>()
      3 
      4 a = tf.placeholder(tf.float32, shape=(10, 10, 10, 10))
----> 5 b = tf.contrib.layers.batch_norm(a, center=False, data_format='NCHW', zero_debias_moving_mean=True)
      6 sess = tf.Session()
      7 sess.run(tf.global_variables_initializer())

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py in func_with_args(*args, **kwargs)
    179       current_args = current_scope[key_func].copy()
    180       current_args.update(kwargs)
--> 181     return func(*args, **current_args)
    182   _add_op(func)
    183   setattr(func_with_args, '_key_op', _key_op(func))

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/contrib/layers/python/layers/layers.py in batch_norm(inputs, decay, center, scale, epsilon, activation_fn, param_initializers, param_regularizers, updates_collections, is_training, reuse, variables_collections, outputs_collections, trainable, batch_weights, fused, data_format, zero_debias_moving_mean, scope, renorm, renorm_clipping, renorm_decay)
    806       mean = array_ops.reshape(mean, params_shape_broadcast)
    807       variance = array_ops.reshape(variance, params_shape_broadcast)
--> 808       beta = array_ops.reshape(beta, params_shape_broadcast)
    809       if gamma is not None:
    810         gamma = array_ops.reshape(gamma, params_shape_broadcast)

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape(tensor, shape, name)
   2500   """"""
   2501   result = _op_def_lib.apply_op(""Reshape"", tensor=tensor, shape=shape,
-> 2502                                 name=name)
   2503   return result
   2504 

~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py in apply_op(self, op_type_name, name, **keywords)
    506               raise ValueError(
    507                   ""Tried to convert '%s' to a tensor and failed. Error: %s"" %
--> 508                   (input_name, err))
    509             prefix = (""Input '%s' of '%s' Op has type %s that does not match"" %
    510                       (input_name, op_type_name, observed))

ValueError: Tried to convert 'tensor' to a tensor and failed. Error: None values not supported.
```"
11670,Feature Request: Quantized DepthwiseConv2dNative ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: (master as of post) d3edb8c60ed4fd831d62833ed22f5c23486c561c
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
Models, such as MobileNet use _DepthwiseConv2dNative_. I couldn't find a quantized kernel and the quantization pass in _GraphTransform_ doesn't currently support DepthwiseConv2dNative: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/quantize_nodes.cc#L57

Are there plans to provide a quantized version of DepthwiseConv2dNative?

"
11666,Different behavior tf.contrib.data.Dataset.unbatch() TF ver. 1.2.1 and ver 1.3.0-rc0,"Code for reproduce problem:
```python
import tensorflow as tf
print(tf.__version__)
dataset = (tf.contrib.data.Dataset.range(10)
           .batch(5)
           .unbatch())
iterator = dataset.make_initializable_iterator()
print(iterator.get_next())
```

results in TF ver. 1.2.1:
```
1.2.1                                                  
Tensor(""IteratorGetNext:0"", shape=(), dtype=int64)     
```

results in TF ver. 1.3.0-rc0:
```
1.3.0-rc0
(<tf.Tensor 'IteratorGetNext:0' shape=() dtype=int64>,)
```
"
11665,Feature request: add a `local_init_feed_dict` to `tf.train.Scaffold`,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.2.0
- **Python version**: 3.6.1
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See below.

### Describe the problem
Feature request: add a `local_init_feed_dict` to `tf.train.Scaffold`. It would be useful to be able to create local variables (which are not saved or restored) and have them initialized by a `tf.train.MonitoredTrainingSession` with a `feed_dict`. In the example below, the variable `X_var` is forced to be part of the `GLOBAL_VARIABLES` collection in order to be able to initialize the variable with a `feed_dict`. This has the undesirable consequence that the variable will be saved to disk.

### Source code / logs
```python
import tensorflow as tf
import numpy as np

# Data that we wish to sample, but not save to disk.
X = np.eye(15, dtype=np.float32)

# Create a graph that samples rows from X randomly.
graph = tf.Graph()
with graph.as_default():
    X_init = tf.placeholder(tf.float32, shape=X.shape)
    # Here, we want to use tf.GraphKeys.LOCAL_VARIABLES,
    # but can't because there is no feed_dict for that collection in tf.train.Scaffold.
    X_var = tf.Variable(X_init, trainable=False, collections=[tf.GraphKeys.GLOBAL_VARIABLES])
    queue = tf.RandomShuffleQueue(
        capacity=X.shape[0],
        min_after_dequeue=1,
        dtypes=[tf.float32],
        shapes=[X.shape[1]])
    enqueue_op = queue.enqueue_many([X_var])
    row = queue.dequeue()

# Sample a few rows from X.
with graph.as_default():
    sess_params = {
        'scaffold': tf.train.Scaffold(
            init_feed_dict={X_init: X},
            init_fn=lambda scaffold, sess: sess.run(enqueue_op))
    }
    with tf.train.MonitoredTrainingSession(**sess_params) as sess:
        print(sess.run(row))
        print(sess.run(row))
```
"
11662,Group Convolutions Support Request,"Group convolutions recently become popular, will this be supported in tf.nn module?"
11660,Gradients be dropped in tf.train.SyncReplicaOptimizer,"### Problem Description
tf.train.SyncReplicaOptimizer is very helpful for backup workers, but the gradients computed by slow workers are just dropped. Below is the documentation I see

> Once the gradients have been computed, push them into gradient_queue only if local_step equals global_step, otherwise the gradients are just dropped

The problem is, I want to use backup workers in online training, it means every sample can be just consumed by TensorFlow once and I don't want some samples to be just ignored, is it possible to make backup servers behave like the one in Hadoop which is guaranteed to utilize all data partitions?
Or can the workers know whether its gradient is dropped or not by the aggregator?
I think this feature will be useful."
11658,Build error in Windows 10,"My environment is 
 - Windows 10 64bit
 - Lenovo P50 workstation notebook
 - NVIDIA Quadro M2000M (4GB) / Intel HD Graphics P530

Build environment is
 -  Microsoft Visual Studio Enterprise 2015 with Visual C++ 2015
 -  Anaconda 4.1.1 (Python 3.5 64-bit)
 -  Git for Windows version 2.9.2.windows.1
 -  swigwin-3.0.10
 -  NVidia CUDA Toolkit 8.0
 -  NVidia CUDNN 5.1
 -  CMake 3.6

In building process, two errors were fixed by myself.
[1]
 - C2001 error is caused by 
   * D:\workspace\tensorflow\tensorflow\contrib\cmake\build\re2\src\re2\re2\testing\re2_test.cc
   * D:\workspace\tensorflow\tensorflow\contrib\cmake\build\re2\src\re2\re2\testing\search_test.cc
 - I do 'save as' two files with unicode and LS options (I am in Korea, it might be language diffence)

[2]
 - C1071 error is caused by 
   * D:\workspace\tensorflow\tensorflow\contrib\cmake\build\re2\src\re2\re2\testing\search_test.cc
 - I eliminated an empy line and '// namespace re2"" at the end of the code

However, I haven't clear this error after I put command ""**MSBuild /p:Configuration=Release tf_tutorials_example_trainer.vcxproj**""

I think it is a kind of link errors. There is a full log from command window. What is the problem?
please forgive me some instructions are written in Korean.
>  
D:\workspace\tensorflow\tensorflow\contrib\cmake\build>MSBuild /p:Configuration=Release tf_tutorials_example_trainer.vcxproj
Microsoft (R) Build Engine 버전 14.0.25420.1]
Copyright (C) Microsoft Corporation. All rights reserved.

빌드 시작: 2017-07-21 오전 10:27:34
1 노드의 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_tutorials_example_trainer.v
cxproj"" 프로젝트(기본 대상)입니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_tutorials_example_trainer.vcxproj
""(1) 프로젝트가 1 노드에서 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\ZERO_CHECK.vcxproj
""(2)을(를) 빌드하고 있습니다(기본 대상).
InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\ZERO_CHECK\ZERO_CHECK.tlog\unsuccessfulbuild""을(를
  ) 만들고 있습니다.
CustomBuild:
  모든 출력이 최신 상태입니다.
FinalizeBuildStatus:
  ""x64\Release\ZERO_CHECK\ZERO_CHECK.tlog\unsuccessfulbuild"" 파일을 삭제하고 있습니다.
  ""x64\Release\ZERO_CHECK\ZERO_CHECK.tlog\ZERO_CHECK.lastbuildstate""에 연결(touching)하고 있 습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\ZERO_CHECK.vcxproj"" 프로젝트를 빌드했습니다(기본
대상).

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_tutorials_example_trainer.vcxproj
""(1) 프로젝트가 1 노드에서 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_cc_framework.vc
xproj""(3)을(를) 빌드하고 있습니다(기본 대상).
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_cc_framework.vcxproj""(3) 프로젝트 가 1
노드에서 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_framework.vcxproj""(4)을(
를) 빌드하고 있습니다(기본 대상).
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_framework.vcxproj""(4) 프로젝 트가
1 노드에서 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\proto_text.vcxproj""(5)을( 를) 빌드
하고 있습니다(기본 대상).
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\proto_text.vcxproj""(5) 프로젝트가 1  노드에서
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\grpc.vcxproj""(6)을(를) 빌드하고 있습 니다(기본 대상
).
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\grpc.vcxproj""(6) 프로젝트가 1 노드에 서 ""D:\wo
rkspace\tensorflow\tensorflow\contrib\cmake\build\protobuf.vcxproj""(7)을(를) 빌드하고 있습니 다(기본 대상).
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\protobuf.vcxproj""(7) 프로젝트가 1 노 드에서 ""D
:\workspace\tensorflow\tensorflow\contrib\cmake\build\zlib.vcxproj""(8)을(를) 빌드하고 있습니 다(기본 대상).
InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\zlib\zlib.tlog\unsuccessfulbuild""을(를) 만들고 있습니다.
CustomBuild:
  Performing update step for 'zlib'
FinalizeBuildStatus:
  ""x64\Release\zlib\zlib.tlog\unsuccessfulbuild"" 파일을 삭제하고 있습니다.
  ""x64\Release\zlib\zlib.tlog\zlib.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\zlib.vcxproj"" 프로젝트를 빌드했습니다(기본 대상).

InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\protobuf\protobuf.tlog\unsuccessfulbuild""을(를) 만들
  고 있습니다.
CustomBuild:
  Performing update step for 'protobuf'
FinalizeBuildStatus:
  ""x64\Release\protobuf\protobuf.tlog\unsuccessfulbuild"" 파일을 삭제하고 있습니다.
  ""x64\Release\protobuf\protobuf.tlog\protobuf.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\protobuf.vcxproj"" 프로젝트를 빌드했습니다(기본 대상
).

InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\grpc\grpc.tlog\unsuccessfulbuild""을(를) 만들고 있습니다.
CustomBuild:
  Performing update step for 'grpc'
FinalizeBuildStatus:
  ""x64\Release\grpc\grpc.tlog\unsuccessfulbuild"" 파일을 삭제하고 있습니다.
  ""x64\Release\grpc\grpc.tlog\grpc.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\grpc.vcxproj"" 프로젝트를 빌드했습니다(기본 대상).

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\proto_text.vcxproj""(5) 프로젝트가 1  노드에서
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj""(9)을(를) 빌드하 고 있습니
다(기본 대상).
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj""(9) 프로젝트가 1 노드에서
 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\cub.vcxproj""(10)을(를) 빌드하고 있습니다(기본 대
상).
InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\cub\cub.tlog\unsuccessfulbuild""을(를)  만들고 있습니다.
FinalizeBuildStatus:
  ""x64\Release\cub\cub.tlog\unsuccessfulbuild"" 파일을 삭제하고 있습니다.
  ""x64\Release\cub\cub.tlog\cub.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\cub.vcxproj"" 프로젝트를 빌드했습니다(기본 대상).

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj""(9) 프로젝트가 1 노드에서
 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\eigen.vcxproj""(11)을(를) 빌드하고 있습니다(기본
 대상).
InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\eigen\eigen.tlog\unsuccessfulbuild""을( 를) 만들고 있습니다
  .
FinalizeBuildStatus:
  ""x64\Release\eigen\eigen.tlog\unsuccessfulbuild"" 파일을 삭제하고 있습니다.
  ""x64\Release\eigen\eigen.tlog\eigen.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\eigen.vcxproj"" 프로젝트를 빌드했습니 다(기본 대상).

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj""(9) 프로젝트가 1 노드에서
 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\farmhash_copy_headers_to_destinatio
n.vcxproj""(12)을(를) 빌드하고 있습니다(기본 대상).
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\farmhash_copy_headers_to_destination
.vcxproj""(12) 프로젝트가 1 노드에서 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\farmhash_
create_destination_dir.vcxproj""(13)을(를) 빌드하고 있습니다(기본 대상).
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\farmhash_create_destination_dir.vcxp
roj""(13) 프로젝트가 1 노드에서 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\farmhash.vcxpr
oj""(14)을(를) 빌드하고 있습니다(기본 대상).
InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\farmhash\farmhash.tlog\unsuccessfulbuild""을(를) 만들
  고 있습니다.
FinalizeBuildStatus:
  ""x64\Release\farmhash\farmhash.tlog\unsuccessfulbuild"" 파일을 삭제하고 있습니다.
  ""x64\Release\farmhash\farmhash.tlog\farmhash.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\farmhash.vcxproj"" 프로젝트를 빌드했습니다(기본 대상
).

InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\farmhash_create_destination_dir\farmhash.8BF8255
  3.tlog\unsuccessfulbuild""을(를) 만들고 있습니다.
FinalizeBuildStatus:
  ""x64\Release\farmhash_create_destination_dir\farmhash.8BF82553.tlog\unsuccessfulbuild"" 파일을
   삭제하고 있습니다.
  ""x64\Release\farmhash_create_destination_dir\farmhash.8BF82553.tlog\farmhash_create_destin
  ation_dir.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\farmhash_create_destination_dir.vcxp
roj"" 프로젝트를 빌드했습니다(기본 대상).

InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\farmhash_copy_headers_to_destination\farmhash.6D
  ED34F1.tlog\unsuccessfulbuild""을(를) 만들고 있습니다.
PreBuildEvent:
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_if_different D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/farmhash/src/farmhash/src/farmhash.h D:/workspace/tensorflow/tens
  orflow/contrib/cmake/build/external/farmhash_archive D:/workspace/tensorflow/tensorflow/co
  ntrib/cmake/build/external/farmhash_archive/util/
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  :VCEnd
FinalizeBuildStatus:
  ""x64\Release\farmhash_copy_headers_to_destination\farmhash.6DED34F1.tlog\unsuccessfulbuild
  "" 파일을 삭제하고 있습니다.
  ""x64\Release\farmhash_copy_headers_to_destination\farmhash.6DED34F1.tlog\farmhash_copy_hea
  ders_to_destination.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\farmhash_copy_headers_to_destination
.vcxproj"" 프로젝트를 빌드했습니다(기본 대상).

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj""(9) 프로젝트가 1 노드에서
 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\fft2d.vcxproj""(15)을(를) 빌드하고 있습니다(기본
 대상).
InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\fft2d\fft2d.tlog\unsuccessfulbuild""을( 를) 만들고 있습니다
  .
FinalizeBuildStatus:
  ""x64\Release\fft2d\fft2d.tlog\unsuccessfulbuild"" 파일을 삭제하고 있습니다.
  ""x64\Release\fft2d\fft2d.tlog\fft2d.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\fft2d.vcxproj"" 프로젝트를 빌드했습니 다(기본 대상).

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj""(9) 프로젝트가 1 노드에서
 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\gemmlowp.vcxproj""(16)을(를) 빌드하고 있습니다
(기본 대상).
InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\gemmlowp\gemmlowp.tlog\unsuccessfulbuild""을(를) 만들
  고 있습니다.
FinalizeBuildStatus:
  ""x64\Release\gemmlowp\gemmlowp.tlog\unsuccessfulbuild"" 파일을 삭제하고 있습니다.
  ""x64\Release\gemmlowp\gemmlowp.tlog\gemmlowp.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\gemmlowp.vcxproj"" 프로젝트를 빌드했습니다(기본 대상
).

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj""(9) 프로젝트가 1 노드에서
 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\gif_copy_headers_to_destination.vcx
proj""(17)을(를) 빌드하고 있습니다(기본 대상).
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\gif_copy_headers_to_destination.vcxp
roj""(17) 프로젝트가 1 노드에서 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\gif_create_des
tination_dir.vcxproj""(18)을(를) 빌드하고 있습니다(기본 대상).
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\gif_create_destination_dir.vcxproj""(
18) 프로젝트가 1 노드에서 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\gif.vcxproj""(19)을(를
) 빌드하고 있습니다(기본 대상).
InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\gif\gif.tlog\unsuccessfulbuild""을(를)  만들고 있습니다.
FinalizeBuildStatus:
  ""x64\Release\gif\gif.tlog\unsuccessfulbuild"" 파일을 삭제하고 있습니다.
  ""x64\Release\gif\gif.tlog\gif.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\gif.vcxproj"" 프로젝트를 빌드했습니다(기본 대상).

InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\gif_create_destination_dir\gif_crea.BBA1858C.tlo
  g\unsuccessfulbuild""을(를) 만들고 있습니다.
FinalizeBuildStatus:
  ""x64\Release\gif_create_destination_dir\gif_crea.BBA1858C.tlog\unsuccessfulbuild"" 파일을 삭제하고
   있습니다.
  ""x64\Release\gif_create_destination_dir\gif_crea.BBA1858C.tlog\gif_create_destination_dir.
  lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\gif_create_destination_dir.vcxproj""
프로젝트를 빌드했습니다(기본 대상).

InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\gif_copy_headers_to_destination\gif_copy.B8CE1EC
  7.tlog\unsuccessfulbuild""을(를) 만들고 있습니다.
PreBuildEvent:
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_if_different D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/gif/install/include/gif_lib.h D:/workspace/tensorflow/tensorflow/
  contrib/cmake/build/external/gif_archive/giflib-5.1.4//
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  :VCEnd
FinalizeBuildStatus:
  ""x64\Release\gif_copy_headers_to_destination\gif_copy.B8CE1EC7.tlog\unsuccessfulbuild"" 파일을
   삭제하고 있습니다.
  ""x64\Release\gif_copy_headers_to_destination\gif_copy.B8CE1EC7.tlog\gif_copy_headers_to_de
  stination.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\gif_copy_headers_to_destination.vcxp
roj"" 프로젝트를 빌드했습니다(기본 대상).

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj""(9) 프로젝트가 1 노드에서
 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\highwayhash_copy_headers_to_destina
tion.vcxproj""(20)을(를) 빌드하고 있습니다(기본 대상).
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\highwayhash_copy_headers_to_destinat
ion.vcxproj""(20) 프로젝트가 1 노드에서 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\highwa
yhash_create_destination_dir.vcxproj""(21)을(를) 빌드하고 있습니다(기본 대상).
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\highwayhash_create_destination_dir.v
cxproj""(21) 프로젝트가 1 노드에서 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\highwayhash
.vcxproj""(22)을(를) 빌드하고 있습니다(기본 대상).
InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\highwayhash\highwayhash.tlog\unsuccessfulbuild""을
  (를) 만들고 있습니다.
CustomBuild:
  Performing update step for 'highwayhash'
FinalizeBuildStatus:
  ""x64\Release\highwayhash\highwayhash.tlog\unsuccessfulbuild"" 파일을 삭제하고 있습니다.
  ""x64\Release\highwayhash\highwayhash.tlog\highwayhash.lastbuildstate""에 연결(touching)하고 있습니다
  .
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\highwayhash.vcxproj"" 프로젝트를 빌드 했습니다(기본
 대상).

InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\highwayhash_create_destination_dir\highwayh.B196
  919F.tlog\unsuccessfulbuild""을(를) 만들고 있습니다.
FinalizeBuildStatus:
  ""x64\Release\highwayhash_create_destination_dir\highwayh.B196919F.tlog\unsuccessfulbuild""
  파일을 삭제하고 있습니다.
  ""x64\Release\highwayhash_create_destination_dir\highwayh.B196919F.tlog\highwayhash_create_
  destination_dir.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\highwayhash_create_destination_dir.v
cxproj"" 프로젝트를 빌드했습니다(기본 대상).

InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\highwayhash_copy_headers_to_destination\highwayh
  .E3D19F9A.tlog\unsuccessfulbuild""을(를) 만들고 있습니다.
PreBuildEvent:
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_directory D:/workspace/tensorflow/tensorflo
  w/contrib/cmake/build/highwayhash/install/include/ D:/workspace/tensorflow/tensorflow/cont
  rib/cmake/build/external/highwayhash/highwayhash
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  :VCEnd
FinalizeBuildStatus:
  ""x64\Release\highwayhash_copy_headers_to_destination\highwayh.E3D19F9A.tlog\unsuccessfulbu
  ild"" 파일을 삭제하고 있습니다.
  ""x64\Release\highwayhash_copy_headers_to_destination\highwayh.E3D19F9A.tlog\highwayhash_co
  py_headers_to_destination.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\highwayhash_copy_headers_to_destinat
ion.vcxproj"" 프로젝트를 빌드했습니다(기본 대상).

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj""(9) 프로젝트가 1 노드에서
 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\jpeg_copy_headers_to_destination.vc
xproj""(23)을(를) 빌드하고 있습니다(기본 대상).
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\jpeg_copy_headers_to_destination.vcx
proj""(23) 프로젝트가 1 노드에서 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\jpeg_create_d
estination_dir.vcxproj""(24)을(를) 빌드하고 있습니다(기본 대상).
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\jpeg_create_destination_dir.vcxproj""
(24) 프로젝트가 1 노드에서 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\jpeg.vcxproj""(25)을
(를) 빌드하고 있습니다(기본 대상).
InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\jpeg\jpeg.tlog\unsuccessfulbuild""을(를) 만들고 있습니다.
FinalizeBuildStatus:
  ""x64\Release\jpeg\jpeg.tlog\unsuccessfulbuild"" 파일을 삭제하고 있습니다.
  ""x64\Release\jpeg\jpeg.tlog\jpeg.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\jpeg.vcxproj"" 프로젝트를 빌드했습니다(기본 대상).

InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\jpeg_create_destination_dir\jpeg_cre.A5B93FBD.tl
  og\unsuccessfulbuild""을(를) 만들고 있습니다.
FinalizeBuildStatus:
  ""x64\Release\jpeg_create_destination_dir\jpeg_cre.A5B93FBD.tlog\unsuccessfulbuild"" 파일을  삭제하
  고 있습니다.
  ""x64\Release\jpeg_create_destination_dir\jpeg_cre.A5B93FBD.tlog\jpeg_create_destination_di
  r.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\jpeg_create_destination_dir.vcxproj""
 프로젝트를 빌드했습니다(기본 대상).

InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\jpeg_copy_headers_to_destination\jpeg_cop.01FA55
  1C.tlog\unsuccessfulbuild""을(를) 만들고 있습니다.
PreBuildEvent:
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_if_different D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/jpeg/install/include/jconfig.h D:/workspace/tensorflow/tensorflow
  /contrib/cmake/build/external/jpeg_archive
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_if_different D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/jpeg/install/include/jerror.h D:/workspace/tensorflow/tensorflow/
  contrib/cmake/build/external/jpeg_archive
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_if_different D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/jpeg/install/include/jmorecfg.h D:/workspace/tensorflow/tensorflo
  w/contrib/cmake/build/external/jpeg_archive
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_if_different D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/jpeg/install/include/jpeglib.h D:/workspace/tensorflow/tensorflow
  /contrib/cmake/build/external/jpeg_archive
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_if_different D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/jpeg/src/jpeg/cderror.h D:/workspace/tensorflow/tensorflow/contri
  b/cmake/build/external/jpeg_archive
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_if_different D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/jpeg/src/jpeg/cdjpeg.h D:/workspace/tensorflow/tensorflow/contrib
  /cmake/build/external/jpeg_archive
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_if_different D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/jpeg/src/jpeg/jdct.h D:/workspace/tensorflow/tensorflow/contrib/c
  make/build/external/jpeg_archive
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_if_different D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/jpeg/src/jpeg/jinclude.h D:/workspace/tensorflow/tensorflow/contr
  ib/cmake/build/external/jpeg_archive
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_if_different D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/jpeg/src/jpeg/jmemsys.h D:/workspace/tensorflow/tensorflow/contri
  b/cmake/build/external/jpeg_archive
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_if_different D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/jpeg/src/jpeg/jpegint.h D:/workspace/tensorflow/tensorflow/contri
  b/cmake/build/external/jpeg_archive
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_if_different D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/jpeg/src/jpeg/jversion.h D:/workspace/tensorflow/tensorflow/contr
  ib/cmake/build/external/jpeg_archive
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_if_different D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/jpeg/src/jpeg/transupp.h D:/workspace/tensorflow/tensorflow/contr
  ib/cmake/build/external/jpeg_archive
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  :VCEnd
FinalizeBuildStatus:
  ""x64\Release\jpeg_copy_headers_to_destination\jpeg_cop.01FA551C.tlog\unsuccessfulbuild"" 파 일
  을 삭제하고 있습니다.
  ""x64\Release\jpeg_copy_headers_to_destination\jpeg_cop.01FA551C.tlog\jpeg_copy_headers_to_
  destination.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\jpeg_copy_headers_to_destination.vcx
proj"" 프로젝트를 빌드했습니다(기본 대상).

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj""(9) 프로젝트가 1 노드에서
 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\jsoncpp.vcxproj""(26)을(를) 빌드하고 있습니다(
기본 대상).
InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\jsoncpp\jsoncpp.tlog\unsuccessfulbuild""을(를) 만들고
  있습니다.
CustomBuild:
  Performing update step for 'jsoncpp'
FinalizeBuildStatus:
  ""x64\Release\jsoncpp\jsoncpp.tlog\unsuccessfulbuild"" 파일을 삭제하고 있습니다.
  ""x64\Release\jsoncpp\jsoncpp.tlog\jsoncpp.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\jsoncpp.vcxproj"" 프로젝트를 빌드했습 니다(기본 대상)
.

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj""(9) 프로젝트가 1 노드에서
 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\lmdb_copy_headers_to_destination.vc
xproj""(27)을(를) 빌드하고 있습니다(기본 대상).
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\lmdb_copy_headers_to_destination.vcx
proj""(27) 프로젝트가 1 노드에서 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\lmdb_create_d
estination_dir.vcxproj""(28)을(를) 빌드하고 있습니다(기본 대상).
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\lmdb_create_destination_dir.vcxproj""
(28) 프로젝트가 1 노드에서 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\lmdb.vcxproj""(29)을
(를) 빌드하고 있습니다(기본 대상).
InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\lmdb\lmdb.tlog\unsuccessfulbuild""을(를) 만들고 있습니다.
FinalizeBuildStatus:
  ""x64\Release\lmdb\lmdb.tlog\unsuccessfulbuild"" 파일을 삭제하고 있습니다.
  ""x64\Release\lmdb\lmdb.tlog\lmdb.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\lmdb.vcxproj"" 프로젝트를 빌드했습니다(기본 대상).

InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\lmdb_create_destination_dir\lmdb_cre.EF427DD5.tl
  og\unsuccessfulbuild""을(를) 만들고 있습니다.
FinalizeBuildStatus:
  ""x64\Release\lmdb_create_destination_dir\lmdb_cre.EF427DD5.tlog\unsuccessfulbuild"" 파일을  삭제하
  고 있습니다.
  ""x64\Release\lmdb_create_destination_dir\lmdb_cre.EF427DD5.tlog\lmdb_create_destination_di
  r.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\lmdb_create_destination_dir.vcxproj""
 프로젝트를 빌드했습니다(기본 대상).

InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\lmdb_copy_headers_to_destination\lmdb_cop.18AF5D
  C1.tlog\unsuccessfulbuild""을(를) 만들고 있습니다.
PreBuildEvent:
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_if_different D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/lmdb/install/include/lmdb.h D:/workspace/tensorflow/tensorflow/co
  ntrib/cmake/build/external/lmdb/
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_if_different D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/lmdb/install/include/midl.h D:/workspace/tensorflow/tensorflow/co
  ntrib/cmake/build/external/lmdb/
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  :VCEnd
FinalizeBuildStatus:
  ""x64\Release\lmdb_copy_headers_to_destination\lmdb_cop.18AF5DC1.tlog\unsuccessfulbuild"" 파 일
  을 삭제하고 있습니다.
  ""x64\Release\lmdb_copy_headers_to_destination\lmdb_cop.18AF5DC1.tlog\lmdb_copy_headers_to_
  destination.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\lmdb_copy_headers_to_destination.vcx
proj"" 프로젝트를 빌드했습니다(기본 대상).

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj""(9) 프로젝트가 1 노드에서
 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\png_copy_headers_to_destination.vcx
proj""(30)을(를) 빌드하고 있습니다(기본 대상).
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\png_copy_headers_to_destination.vcxp
roj""(30) 프로젝트가 1 노드에서 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\png_create_des
tination_dir.vcxproj""(31)을(를) 빌드하고 있습니다(기본 대상).
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\png_create_destination_dir.vcxproj""(
31) 프로젝트가 1 노드에서 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\png.vcxproj""(32)을(를
) 빌드하고 있습니다(기본 대상).
InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\png\png.tlog\unsuccessfulbuild""을(를)  만들고 있습니다.
FinalizeBuildStatus:
  ""x64\Release\png\png.tlog\unsuccessfulbuild"" 파일을 삭제하고 있습니다.
  ""x64\Release\png\png.tlog\png.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\png.vcxproj"" 프로젝트를 빌드했습니다(기본 대상).

InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\png_create_destination_dir\png_crea.DCB84CF1.tlo
  g\unsuccessfulbuild""을(를) 만들고 있습니다.
FinalizeBuildStatus:
  ""x64\Release\png_create_destination_dir\png_crea.DCB84CF1.tlog\unsuccessfulbuild"" 파일을 삭제하고
   있습니다.
  ""x64\Release\png_create_destination_dir\png_crea.DCB84CF1.tlog\png_create_destination_dir.
  lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\png_create_destination_dir.vcxproj""
프로젝트를 빌드했습니다(기본 대상).

InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\png_copy_headers_to_destination\png_copy.153F572
  B.tlog\unsuccessfulbuild""을(를) 만들고 있습니다.
PreBuildEvent:
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_if_different D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/png/install/include/libpng12/png.h D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/external/png_archive/
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_if_different D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/png/install/include/libpng12/pngconf.h D:/workspace/tensorflow/te
  nsorflow/contrib/cmake/build/external/png_archive/
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  :VCEnd
FinalizeBuildStatus:
  ""x64\Release\png_copy_headers_to_destination\png_copy.153F572B.tlog\unsuccessfulbuild"" 파일을
   삭제하고 있습니다.
  ""x64\Release\png_copy_headers_to_destination\png_copy.153F572B.tlog\png_copy_headers_to_de
  stination.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\png_copy_headers_to_destination.vcxp
roj"" 프로젝트를 빌드했습니다(기본 대상).

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj""(9) 프로젝트가 1 노드에서
 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\re2.vcxproj""(33)을(를) 빌드하고 있습니다(기본 대
상).
InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\re2\re2.tlog\unsuccessfulbuild""을(를)  만들고 있습니다.
CustomBuild:
  Performing update step for 're2'
FinalizeBuildStatus:
  ""x64\Release\re2\re2.tlog\unsuccessfulbuild"" 파일을 삭제하고 있습니다.
  ""x64\Release\re2\re2.tlog\re2.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\re2.vcxproj"" 프로젝트를 빌드했습니다(기본 대상).

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj""(9) 프로젝트가 1 노드에서
 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_protos_cc.vcxproj""(34)을(를) 빌드하고
있습니다(기본 대상).
InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""tf_protos_cc.dir\Release\tf_protos_cc.tlog\unsuccessfulbuild
  ""을(를) 만들고 있습니다.
CustomBuild:
  모든 출력이 최신 상태입니다.
ClCompile:
  모든 출력이 최신 상태입니다.
Lib:
  모든 출력이 최신 상태입니다.
  tf_protos_cc.vcxproj -> D:\workspace\tensorflow\tensorflow\contrib\cmake\build\Release\tf_
  protos_cc.lib
FinalizeBuildStatus:
  ""tf_protos_cc.dir\Release\tf_protos_cc.tlog\unsuccessfulbuild"" 파일을 삭제하고 있습니다.
  ""tf_protos_cc.dir\Release\tf_protos_cc.tlog\tf_protos_cc.lastbuildstate""에 연결(touching)하고 있
  습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_protos_cc.vcxproj"" 프로젝트를 빌드했습니다(기
본 대상).

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj""(9) 프로젝트가 1 노드에서
 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\zlib_copy_headers_to_destination.vc
xproj""(35)을(를) 빌드하고 있습니다(기본 대상).
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\zlib_copy_headers_to_destination.vcx
proj""(35) 프로젝트가 1 노드에서 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\zlib_create_d
estination_dir.vcxproj""(36)을(를) 빌드하고 있습니다(기본 대상).
InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\zlib_create_destination_dir\zlib_cre.A3320549.tl
  og\unsuccessfulbuild""을(를) 만들고 있습니다.
FinalizeBuildStatus:
  ""x64\Release\zlib_create_destination_dir\zlib_cre.A3320549.tlog\unsuccessfulbuild"" 파일을  삭제하
  고 있습니다.
  ""x64\Release\zlib_create_destination_dir\zlib_cre.A3320549.tlog\zlib_create_destination_di
  r.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\zlib_create_destination_dir.vcxproj""
 프로젝트를 빌드했습니다(기본 대상).

InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\zlib_copy_headers_to_destination\zlib_cop.ED29D1
  64.tlog\unsuccessfulbuild""을(를) 만들고 있습니다.
PreBuildEvent:
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_if_different D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/zlib/install/include/zconf.h D:/workspace/tensorflow/tensorflow/c
  ontrib/cmake/build/external/zlib_archive
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  setlocal
  ""C:\Program Files\CMake\bin\cmake.exe"" -E copy_if_different D:/workspace/tensorflow/tensor
  flow/contrib/cmake/build/zlib/install/include/zlib.h D:/workspace/tensorflow/tensorflow/co
  ntrib/cmake/build/external/zlib_archive
  if %errorlevel% neq 0 goto :cmEnd
  :cmEnd
  endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
  :cmErrorLevel
  exit /b %1
  :cmDone
  if %errorlevel% neq 0 goto :VCEnd
  :VCEnd
FinalizeBuildStatus:
  ""x64\Release\zlib_copy_headers_to_destination\zlib_cop.ED29D164.tlog\unsuccessfulbuild"" 파 일
  을 삭제하고 있습니다.
  ""x64\Release\zlib_copy_headers_to_destination\zlib_cop.ED29D164.tlog\zlib_copy_headers_to_
  destination.lastbuildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\zlib_copy_headers_to_destination.vcx
proj"" 프로젝트를 빌드했습니다(기본 대상).

InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""tf_core_lib.dir\Release\tf_core_lib.tlog\unsuccessfulbuild""을
  (를) 만들고 있습니다.
CustomBuild:
  모든 출력이 최신 상태입니다.
ClCompile:
  모든 출력이 최신 상태입니다.
  모든 출력이 최신 상태입니다.
  모든 출력이 최신 상태입니다.
  모든 출력이 최신 상태입니다.
  모든 출력이 최신 상태입니다.
  모든 출력이 최신 상태입니다.
  모든 출력이 최신 상태입니다.
  모든 출력이 최신 상태입니다.
  모든 출력이 최신 상태입니다.
  모든 출력이 최신 상태입니다.
  모든 출력이 최신 상태입니다.
Lib:
  모든 출력이 최신 상태입니다.
  tf_core_lib.vcxproj -> D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.
  dir\Release\tf_core_lib.lib
FinalizeBuildStatus:
  ""tf_core_lib.dir\Release\tf_core_lib.tlog\unsuccessfulbuild"" 파일을 삭제하고 있습니다.
  ""tf_core_lib.dir\Release\tf_core_lib.tlog\tf_core_lib.lastbuildstate""에 연결(touching)하고 있습니다
  .
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj"" 프로젝트를 빌드 했습니다(기본
 대상).

InitializeBuildStatus:
  ""proto_text.dir\Release\proto_text.tlog\unsuccessfulbuild""에 연결(touching)하고 있습니다.
CustomBuild:
  모든 출력이 최신 상태입니다.
ClCompile:
  모든 출력이 최신 상태입니다.
Link:
  C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\bin\amd64\link.exe /ERRORREPORT:QUE
  UE /OUT:""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\Release\proto_text.exe"" /I
  NCREMENTAL:NO /NOLOGO kernel32.lib user32.lib gdi32.lib winspool.lib shell32.lib ole32.lib
   oleaut32.lib uuid.lib comdlg32.lib advapi32.lib zlib\install\lib\zlibstatic.lib gif\insta
  ll\lib\giflib.lib png\install\lib\libpng12_static.lib jpeg\install\lib\libjpeg.lib lmdb\in
  stall\lib\lmdb.lib jsoncpp\src\jsoncpp\src\lib_json\Release\jsoncpp.lib farmhash\install\l
  ib\farmhash.lib fft2d\\src\lib\fft2d.lib highwayhash\install\lib\highwayhash.lib protobuf\
  src\protobuf\Release\libprotobuf.lib re2\src\re2\Release\re2.lib grpc\src\grpc\Release\grp
  c++_unsecure.lib grpc\src\grpc\Release\grpc_unsecure.lib grpc\src\grpc\Release\gpr.lib wso
  ck32.lib ws2_32.lib shlwapi.lib ""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\l
  ib\x64\cudart_static.lib"" ""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\lib\x64
  \cuda.lib"" ""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\lib\x64\cublas.lib"" ""C
  :\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\lib\x64\cublas_device.lib"" ""C:\Prog
  ram Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\lib\x64\cufft.lib"" ""C:\Program Files\NVID
  IA GPU Computing Toolkit\CUDA\v8.0\lib\x64\curand.lib"" ""C:\Program Files\NVIDIA GPU Comput
  ing Toolkit\CUDA\v8.0\extras\CUPTI\libx64\cupti.lib"" ""C:\Program Files\NVIDIA GPU Computin
  g Toolkit\CUDA\v8.0\lib\x64\cusolver.lib"" ""C:\Program Files\NVIDIA GPU Computing Toolkit\C
  UDA\v8.0\lib\x64\cudnn.lib"" Release\tf_protos_cc.lib /MANIFEST /MANIFESTUAC:""level='asInvo
  ker' uiAccess='false'"" /manifest:embed /PDB:""D:/workspace/tensorflow/tensorflow/contrib/cm
  ake/build/Release/proto_text.pdb"" /SUBSYSTEM:CONSOLE /TLBID:1 /DYNAMICBASE /NXCOMPAT /IMPL
  IB:""D:/workspace/tensorflow/tensorflow/contrib/cmake/build/Release/proto_text.lib"" /MACHIN
  E:X64  /machine:x64 /ignore:4049 /ignore:4197 /ignore:4217 /ignore:4221 proto_text.dir\Rel
  ease\gen_proto_text_functions.obj
  proto_text.dir\Release\gen_proto_text_functions_lib.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\arena.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\bitmap.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\coding.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\status.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\stringpiece
  .obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\threadpool.
  obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\gif_io.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\crc32c.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\crc32c_acce
  lerate.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\hash.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\histogram.o
  bj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\block.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\block_build
  er.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\buffered_in
  putstream.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\compression
  .obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\format.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\inputbuffer
  .obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\inputstream
  _interface.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\iterator.ob
  j
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\path.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\random_inpu
  tstream.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\record_read
  er.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\record_writ
  er.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\snappy_inpu
  tbuffer.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\snappy_outp
  utbuffer.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\table.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\table_build
  er.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\two_level_i
  terator.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\zlib_inputs
  tream.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\zlib_output
  buffer.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\jpeg_handle
  .obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\jpeg_mem.ob
  j
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\collection_
  registry.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\png_io.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\distributio
  n_sampler.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\random.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\simple_phil
  ox.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\weighted_pi
  cker.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\base64.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\numbers.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\ordered_cod
  e.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\proto_text_
  util.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\scanner.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\str_util.ob
  j
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\strcat.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\stringprint
  f.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\wav_io.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\cpu_feature
  _guard.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\cpu_info.ob
  j
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\D_\workspac
  e\tensorflow\tensorflow\core\platform\cuda_libdevice_path.cc.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\denormal.ob
  j
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\D_\workspac
  e\tensorflow\tensorflow\core\platform\env.cc.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\D_\workspac
  e\tensorflow\tensorflow\core\platform\env_time.cc.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\file_system
  .obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\protobuf_ut
  il.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\setround.ob
  j
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\tensor_codi
  ng.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\D_\workspac
  e\tensorflow\tensorflow\core\platform\tracing.cc.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\D_\workspac
  e\tensorflow\tensorflow\core\platform\default\cuda_libdevice_path.cc.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\gpu_tracer.
  obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\logging.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\protobuf.ob
  j
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\D_\workspac
  e\tensorflow\tensorflow\core\platform\default\tracing.cc.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\resource_ha
  ndle.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\D_\workspac
  e\tensorflow\tensorflow\core\platform\windows\env.cc.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\D_\workspac
  e\tensorflow\tensorflow\core\platform\windows\env_time.cc.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\D_\workspac
  e\tensorflow\tensorflow\core\platform\windows\error.cc.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\net.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\port.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\windows_fil
  e_system.obj
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\D_\workspac
  e\tensorflow\tensorflow\core\platform\posix\error.cc.obj
gpu_tracer.obj : error LNK2019: ""public: void __cdecl tensorflow::StepStatsCollector::Save(c
lass std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const
 &,class tensorflow::NodeExecStats *)"" (?Save@StepStatsCollector@tensorflow@@QEAAXAEBV?$basi
c_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeExecStats@2@@Z) 외부 기호(참조 위치:
""public: virtual class tensorflow::Status __cdecl tensorflow::gputracer::GPUTracerImpl::Coll
ect(class tensorflow::StepStatsCollector *)"" (?Collect@GPUTracerImpl@gputracer@tensorflow@@U
EAA?AVStatus@3@PEAVStepStatsCollector@3@@Z) 함수)에서 확인하지 못했습니다. [D:\workspace\tensorflow\tenso
rflow\contrib\cmake\build\proto_text.vcxproj]
gpu_tracer.obj : error LNK2019: ""public: enum CUptiResult __cdecl perftools::gputools::profi
ler::CuptiWrapper::ActivityDisable(enum CUpti_ActivityKind)"" (?ActivityDisable@CuptiWrapper@
profiler@gputools@perftools@@QEAA?AW4CUptiResult@@W4CUpti_ActivityKind@@@Z) 외부 기호(참조 위치: ""pu
blic: class tensorflow::Status __cdecl tensorflow::gputracer::CUPTIManager::DisableTrace(voi
d)"" (?DisableTrace@CUPTIManager@gputracer@tensorflow@@QEAA?AVStatus@3@XZ) 함수)에서 확인하지 못했습니다.
[D:\workspace\tensorflow\tensorflow\contrib\cmake\build\proto_text.vcxproj]
gpu_tracer.obj : error LNK2019: ""public: enum CUptiResult __cdecl perftools::gputools::profi
ler::CuptiWrapper::ActivityEnable(enum CUpti_ActivityKind)"" (?ActivityEnable@CuptiWrapper@pr
ofiler@gputools@perftools@@QEAA?AW4CUptiResult@@W4CUpti_ActivityKind@@@Z) 외부 기호(참조 위치: ""publ
ic: class tensorflow::Status __cdecl tensorflow::gputracer::CUPTIManager::EnableTrace(class
tensorflow::gputracer::CUPTIClient *)"" (?EnableTrace@CUPTIManager@gputracer@tensorflow@@QEAA
?AVStatus@3@PEAVCUPTIClient@23@@Z) 함수)에서 확인하지 못했습니다. [D:\workspace\tensorflow\tensorflow\con
trib\cmake\build\proto_text.vcxproj]
gpu_tracer.obj : error LNK2019: ""public: enum CUptiResult __cdecl perftools::gputools::profi
ler::CuptiWrapper::ActivityFlushAll(unsigned int)"" (?ActivityFlushAll@CuptiWrapper@profiler@
gputools@perftools@@QEAA?AW4CUptiResult@@I@Z) 외부 기호(참조 위치: ""public: class tensorflow::Status
 __cdecl tensorflow::gputracer::CUPTIManager::DisableTrace(void)"" (?DisableTrace@CUPTIManage
r@gputracer@tensorflow@@QEAA?AVStatus@3@XZ) 함수)에서 확인하지 못했습니다. [D:\workspace\tensorflow\tenso
rflow\contrib\cmake\build\proto_text.vcxproj]
gpu_tracer.obj : error LNK2019: ""public: enum CUptiResult __cdecl perftools::gputools::profi
ler::CuptiWrapper::ActivityGetNextRecord(unsigned char *,unsigned __int64,struct CUpti_Activ
ity * *)"" (?ActivityGetNextRecord@CuptiWrapper@profiler@gputools@perftools@@QEAA?AW4CUptiRes
ult@@PEAE_KPEAPEAUCUpti_Activity@@@Z) 외부 기호(참조 위치: ""private: void __cdecl tensorflow::gputra
cer::CUPTIManager::InternalBufferCompleted(struct CUctx_st *,unsigned int,unsigned char *,un
signed __int64,unsigned __int64)"" (?InternalBufferCompleted@CUPTIManager@gputracer@tensorflo
w@@AEAAXPEAUCUctx_st@@IPEAE_K2@Z) 함수)에서 확인하지 못했습니다. [D:\workspace\tensorflow\tensorflow\cont
rib\cmake\build\proto_text.vcxproj]
gpu_tracer.obj : error LNK2019: ""public: enum CUptiResult __cdecl perftools::gputools::profi
ler::CuptiWrapper::ActivityGetNumDroppedRecords(struct CUctx_st *,unsigned int,unsigned __in
t64 *)"" (?ActivityGetNumDroppedRecords@CuptiWrapper@profiler@gputools@perftools@@QEAA?AW4CUp
tiResult@@PEAUCUctx_st@@IPEA_K@Z) 외부 기호(참조 위치: ""private: void __cdecl tensorflow::gputracer:
:CUPTIManager::InternalBufferCompleted(struct CUctx_st *,unsigned int,unsigned char *,unsign
ed __int64,unsigned __int64)"" (?InternalBufferCompleted@CUPTIManager@gputracer@tensorflow@@A
EAAXPEAUCUctx_st@@IPEAE_K2@Z) 함수)에서 확인하지 못했습니다. [D:\workspace\tensorflow\tensorflow\contrib\
cmake\build\proto_text.vcxproj]
gpu_tracer.obj : error LNK2019: ""public: enum CUptiResult __cdecl perftools::gputools::profi
ler::CuptiWrapper::ActivityRegisterCallbacks(void (__cdecl*)(unsigned char * *,unsigned __in
t64 *,unsigned __int64 *),void (__cdecl*)(struct CUctx_st *,unsigned int,unsigned char *,uns
igned __int64,unsigned __int64))"" (?ActivityRegisterCallbacks@CuptiWrapper@profiler@gputools
@perftools@@QEAA?AW4CUptiResult@@P6AXPEAPEAEPEA_K1@ZP6AXPEAUCUctx_st@@IPEAE_K5@Z@Z) 외부 기호(참조
 위치: ""public: __cdecl tensorflow::gputracer::CUPTIManager::CUPTIManager(void)"" (??0CUPTIMana
ger@gputracer@tensorflow@@QEAA@XZ) 함수)에서 확인하지 못했습니다. [D:\workspace\tensorflow\tensorflow\con
trib\cmake\build\proto_text.vcxproj]
gpu_tracer.obj : error LNK2019: ""public: enum CUptiResult __cdecl perftools::gputools::profi
ler::CuptiWrapper::GetTimestamp(unsigned __int64 *)"" (?GetTimestamp@CuptiWrapper@profiler@gp
utools@perftools@@QEAA?AW4CUptiResult@@PEA_K@Z) 외부 기호(참조 위치: ""public: virtual class tensorfl
ow::Status __cdecl tensorflow::gputracer::GPUTracerImpl::Start(void)"" (?Start@GPUTracerImpl@
gputracer@tensorflow@@UEAA?AVStatus@3@XZ) 함수)에서 확인하지 못했습니다. [D:\workspace\tensorflow\tensorf
low\contrib\cmake\build\proto_text.vcxproj]
gpu_tracer.obj : error LNK2019: ""public: enum CUptiResult __cdecl perftools::gputools::profi
ler::CuptiWrapper::EnableCallback(unsigned int,struct CUpti_Subscriber_st *,enum CUpti_Callb
ackDomain,unsigned int)"" (?EnableCallback@CuptiWrapper@profiler@gputools@perftools@@QEAA?AW4
CUptiResult@@IPEAUCUpti_Subscriber_st@@W4CUpti_CallbackDomain@@I@Z) 외부 기호(참조 위치: ""public: vi
rtual class tensorflow::Status __cdecl tensorflow::gputracer::GPUTracerImpl::Start(void)"" (?
Start@GPUTracerImpl@gputracer@tensorflow@@UEAA?AVStatus@3@XZ) 함수)에서 확인하지 못했습니다. [D:\workspac
e\tensorflow\tensorflow\contrib\cmake\build\proto_text.vcxproj]
gpu_tracer.obj : error LNK2019: ""public: enum CUptiResult __cdecl perftools::gputools::profi
ler::CuptiWrapper::Subscribe(struct CUpti_Subscriber_st * *,void (__cdecl*)(void *,enum CUpt
i_CallbackDomain,unsigned int,void const *),void *)"" (?Subscribe@CuptiWrapper@profiler@gputo
ols@perftools@@QEAA?AW4CUptiResult@@PEAPEAUCUpti_Subscriber_st@@P6AXPEAXW4CUpti_CallbackDoma
in@@IPEBX@Z1@Z) 외부 기호(참조 위치: ""public: virtual class tensorflow::Status __cdecl tensorflow::g
putracer::GPUTracerImpl::Start(void)"" (?Start@GPUTracerImpl@gputracer@tensorflow@@UEAA?AVSta
tus@3@XZ) 함수)에서 확인하지 못했습니다. [D:\workspace\tensorflow\tensorflow\contrib\cmake\build\proto_te
xt.vcxproj]
gpu_tracer.obj : error LNK2019: ""public: enum CUptiResult __cdecl perftools::gputools::profi
ler::CuptiWrapper::Unsubscribe(struct CUpti_Subscriber_st *)"" (?Unsubscribe@CuptiWrapper@pro
filer@gputools@perftools@@QEAA?AW4CUptiResult@@PEAUCUpti_Subscriber_st@@@Z) 외부 기호(참조 위치: ""pu
blic: virtual class tensorflow::Status __cdecl tensorflow::gputracer::GPUTracerImpl::Stop(vo
id)"" (?Stop@GPUTracerImpl@gputracer@tensorflow@@UEAA?AVStatus@3@XZ) 함수)에서 확인하지 못했습니다. [D:\wo
rkspace\tensorflow\tensorflow\contrib\cmake\build\proto_text.vcxproj]
D:\workspace\tensorflow\tensorflow\contrib\cmake\build\Release\proto_text.exe : fatal error
LNK1120: 11개의 확인할 수 없는 외부 참조입니다. [D:\workspace\tensorflow\tensorflow\contrib\cmake\build\pro
to_text.vcxproj]
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\proto_text.vcxproj"" 프로젝트를 빌드했습니다(기본
대상). - 실패

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_framework.vcxproj"" 프로젝트를 빌드했
습니다(기본 대상). - 실패

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_cc_framework.vcxproj"" 프로젝트를  빌드했습니
다(기본 대상). - 실패

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_tutorials_example_trainer.vcxproj
""(1) 프로젝트가 1 노드에서 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_cc_ops.vcxproj""
(37)을(를) 빌드하고 있습니다(기본 대상).
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_cc_ops.vcxproj""(37) 프로젝트가 1  노드에서
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\create_cc_ops_header_dir.vcxproj""(47
)을(를) 빌드하고 있습니다(기본 대상).
InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""x64\Release\create_cc_ops_header_dir\create_c.0DE93F51.tlog\
  unsuccessfulbuild""을(를) 만들고 있습니다.
FinalizeBuildStatus:
  ""x64\Release\create_cc_ops_header_dir\create_c.0DE93F51.tlog\unsuccessfulbuild"" 파일을 삭제하고 있
  습니다.
  ""x64\Release\create_cc_ops_header_dir\create_c.0DE93F51.tlog\create_cc_ops_header_dir.last
  buildstate""에 연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\create_cc_ops_header_dir.vcxproj"" 프 로
젝트를 빌드했습니다(기본 대상).

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_cc_ops.vcxproj"" 프로젝트를 빌드했 습니다(기본 대
상). - 실패

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_tutorials_example_trainer.vcxproj
""(1) 프로젝트가 1 노드에서 ""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_stream_executor
.vcxproj""(107)을(를) 빌드하고 있습니다(기본 대상).
InitializeBuildStatus:
  ""AlwaysCreate""이(가) 지정되었기 때문에 ""tf_stream_executor.dir\Release\tf_strea.A6006ACF.tlog\unsucc
  essfulbuild""을(를) 만들고 있습니다.
CustomBuild:
  모든 출력이 최신 상태입니다.
ClCompile:
  모든 출력이 최신 상태입니다.
Lib:
  모든 출력이 최신 상태입니다.
  tf_stream_executor.vcxproj -> D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_st
  ream_executor.dir\Release\tf_stream_executor.lib
FinalizeBuildStatus:
  ""tf_stream_executor.dir\Release\tf_strea.A6006ACF.tlog\unsuccessfulbuild"" 파일을 삭제하고  있습니다.
  ""tf_stream_executor.dir\Release\tf_strea.A6006ACF.tlog\tf_stream_executor.lastbuildstate""에
   연결(touching)하고 있습니다.
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_stream_executor.vcxproj"" 프로젝트 를 빌드
했습니다(기본 대상).

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_tutorials_example_trainer.vcxproj
"" 프로젝트를 빌드했습니다(기본 대상). - 실패


빌드하지 못했습니다.

""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_tutorials_example_trainer.vcxproj
""(기본 대상)(1)->
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_cc_framework.vcxproj""(기본 대상)(3)->
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\tf_core_framework.vcxproj""(기본 대상)(4)
->
""D:\workspace\tensorflow\tensorflow\contrib\cmake\build\proto_text.vcxproj""(기본 대상)(5)->
(Link 대상) ->
  gpu_tracer.obj : error LNK2019: ""public: void __cdecl tensorflow::StepStatsCollector::Save
(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > con
st &,class tensorflow::NodeExecStats *)"" (?Save@StepStatsCollector@tensorflow@@QEAAXAEBV?$ba
sic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeExecStats@2@@Z) 외부 기호(참조 위치
: ""public: virtual class tensorflow::Status __cdecl tensorflow::gputracer::GPUTracerImpl::Co
llect(class tensorflow::StepStatsCollector *)"" (?Collect@GPUTracerImpl@gputracer@tensorflow@
@UEAA?AVStatus@3@PEAVStepStatsCollector@3@@Z) 함수)에서 확인하지 못했습니다. [D:\workspace\tensorflow\ten
sorflow\contrib\cmake\build\proto_text.vcxproj]
  gpu_tracer.obj : error LNK2019: ""public: enum CUptiResult __cdecl perftools::gputools::pro
filer::CuptiWrapper::ActivityDisable(enum CUpti_ActivityKind)"" (?ActivityDisable@CuptiWrappe
r@profiler@gputools@perftools@@QEAA?AW4CUptiResult@@W4CUpti_ActivityKind@@@Z) 외부 기호(참조 위치: ""
public: class tensorflow::Status __cdecl tensorflow::gputracer::CUPTIManager::DisableTrace(v
oid)"" (?DisableTrace@CUPTIManager@gputracer@tensorflow@@QEAA?AVStatus@3@XZ) 함수)에서 확인하 지 못했습니다
. [D:\workspace\tensorflow\tensorflow\contrib\cmake\build\proto_text.vcxproj]
  gpu_tracer.obj : error LNK2019: ""public: enum CUptiResult __cdecl perftools::gputools::pro
filer::CuptiWrapper::ActivityEnable(enum CUpti_ActivityKind)"" (?ActivityEnable@CuptiWrapper@
profiler@gputools@perftools@@QEAA?AW4CUptiResult@@W4CUpti_ActivityKind@@@Z) 외부 기호(참조 위치: ""pu
blic: class tensorflow::Status __cdecl tensorflow::gputracer::CUPTIManager::EnableTrace(clas
s tensorflow::gputracer::CUPTIClient *)"" (?EnableTrace@CUPTIManager@gputracer@tensorflow@@QE
AA?AVStatus@3@PEAVCUPTIClient@23@@Z) 함수)에서 확인하지 못했습니다. [D:\workspace\tensorflow\tensorflow\c
ontrib\cmake\build\proto_text.vcxproj]
  gpu_tracer.obj : error LNK2019: ""public: enum CUptiResult __cdecl perftools::gputools::pro
filer::CuptiWrapper::ActivityFlushAll(unsigned int)"" (?ActivityFlushAll@CuptiWrapper@profile
r@gputools@perftools@@QEAA?AW4CUptiResult@@I@Z) 외부 기호(참조 위치: ""public: class tensorflow::Stat
us __cdecl tensorflow::gputracer::CUPTIManager::DisableTrace(void)"" (?DisableTrace@CUPTIMana
ger@gputracer@tensorflow@@QEAA?AVStatus@3@XZ) 함수)에서 확인하지 못했습니다. [D:\workspace\tensorflow\ten
sorflow\contrib\cmake\build\proto_text.vcxproj]
  gpu_tracer.obj : error LNK2019: ""public: enum CUptiResult __cdecl perftools::gputools::pro
filer::CuptiWrapper::ActivityGetNextRecord(unsigned char *,unsigned __int64,struct CUpti_Act
ivity * *)"" (?ActivityGetNextRecord@CuptiWrapper@profiler@gputools@perftools@@QEAA?AW4CUptiR
esult@@PEAE_KPEAPEAUCUpti_Activity@@@Z) 외부 기호(참조 위치: ""private: void __cdecl tensorflow::gput
racer::CUPTIManager::InternalBufferCompleted(struct CUctx_st *,unsigned int,unsigned char *,
unsigned __int64,unsigned __int64)"" (?InternalBufferCompleted@CUPTIManager@gputracer@tensorf
low@@AEAAXPEAUCUctx_st@@IPEAE_K2@Z) 함수)에서 확인하지 못했습니다. [D:\workspace\tensorflow\tensorflow\co
ntrib\cmake\build\proto_text.vcxproj]
  gpu_tracer.obj : error LNK2019: ""public: enum CUptiResult __cdecl perftools::gputools::pro
filer::CuptiWrapper::ActivityGetNumDroppedRecords(struct CUctx_st *,unsigned int,unsigned __
int64 *)"" (?ActivityGetNumDroppedRecords@CuptiWrapper@profiler@gputools@perftools@@QEAA?AW4C
UptiResult@@PEAUCUctx_st@@IPEA_K@Z) 외부 기호(참조 위치: ""private: void __cdecl tensorflow::gputrace
r::CUPTIManager::InternalBufferCompleted(struct CUctx_st *,unsigned int,unsigned char *,unsi
gned __int64,unsigned __int64)"" (?InternalBufferCompleted@CUPTIManager@gputracer@tensorflow@
@AEAAXPEAUCUctx_st@@IPEAE_K2@Z) 함수)에서 확인하지 못했습니다. [D:\workspace\tensorflow\tensorflow\contri
b\cmake\build\proto_text.vcxproj]
  gpu_tracer.obj : error LNK2019: ""public: enum CUptiResult __cdecl perftools::gputools::pro
filer::CuptiWrapper::ActivityRegisterCallbacks(void (__cdecl*)(unsigned char * *,unsigned __
int64 *,unsigned __int64 *),void (__cdecl*)(struct CUctx_st *,unsigned int,unsigned char *,u
nsigned __int64,unsigned __int64))"" (?ActivityRegisterCallbacks@CuptiWrapper@profiler@gputoo
ls@perftools@@QEAA?AW4CUptiResult@@P6AXPEAPEAEPEA_K1@ZP6AXPEAUCUctx_st@@IPEAE_K5@Z@Z) 외부 기호(
참조 위치: ""public: __cdecl tensorflow::gputracer::CUPTIManager::CUPTIManager(void)"" (??0CUPTIMa
nager@gputracer@tensorflow@@QEAA@XZ) 함수)에서 확인하지 못했습니다. [D:\workspace\tensorflow\tensorflow\c
ontrib\cmake\build\proto_text.vcxproj]
  gpu_tracer.obj : error LNK2019: ""public: enum CUptiResult __cdecl perftools::gputools::pro
filer::CuptiWrapper::GetTimestamp(unsigned __int64 *)"" (?GetTimestamp@CuptiWrapper@profiler@
gputools@perftools@@QEAA?AW4CUptiResult@@PEA_K@Z) 외부 기호(참조 위치: ""public: virtual class tensor
flow::Status __cdecl tensorflow::gputracer::GPUTracerImpl::Start(void)"" (?Start@GPUTracerImp
l@gputracer@tensorflow@@UEAA?AVStatus@3@XZ) 함수)에서 확인하지 못했습니다. [D:\workspace\tensorflow\tenso
rflow\contrib\cmake\build\proto_text.vcxproj]
  gpu_tracer.obj : error LNK2019: ""public: enum CUptiResult __cdecl perftools::gputools::pro
filer::CuptiWrapper::EnableCallback(unsigned int,struct CUpti_Subscriber_st *,enum CUpti_Cal
lbackDomain,unsigned int)"" (?EnableCallback@CuptiWrapper@profiler@gputools@perftools@@QEAA?A
W4CUptiResult@@IPEAUCUpti_Subscriber_st@@W4CUpti_CallbackDomain@@I@Z) 외부 기호(참조 위치: ""public:
virtual class tensorflow::Status __cdecl tensorflow::gputracer::GPUTracerImpl::Start(void)""
(?Start@GPUTracerImpl@gputracer@tensorflow@@UEAA?AVStatus@3@XZ) 함수)에서 확인하지 못했습니다. [D:\worksp
ace\tensorflow\tensorflow\contrib\cmake\build\proto_text.vcxproj]
  gpu_tracer.obj : error LNK2019: ""public: enum CUptiResult __cdecl perftools::gputools::pro
filer::CuptiWrapper::Subscribe(struct CUpti_Subscriber_st * *,void (__cdecl*)(void *,enum CU
pti_CallbackDomain,unsigned int,void const *),void *)"" (?Subscribe@CuptiWrapper@profiler@gpu
tools@perftools@@QEAA?AW4CUptiResult@@PEAPEAUCUpti_Subscriber_st@@P6AXPEAXW4CUpti_CallbackDo
main@@IPEBX@Z1@Z) 외부 기호(참조 위치: ""public: virtual class tensorflow::Status __cdecl tensorflow:
:gputracer::GPUTracerImpl::Start(void)"" (?Start@GPUTracerImpl@gputracer@tensorflow@@UEAA?AVS
tatus@3@XZ) 함수)에서 확인하지 못했습니다. [D:\workspace\tensorflow\tensorflow\contrib\cmake\build\proto_
text.vcxproj]
  gpu_tracer.obj : error LNK2019: ""public: enum CUptiResult __cdecl perftools::gputools::pro
filer::CuptiWrapper::Unsubscribe(struct CUpti_Subscriber_st *)"" (?Unsubscribe@CuptiWrapper@p
rofiler@gputools@perftools@@QEAA?AW4CUptiResult@@PEAUCUpti_Subscriber_st@@@Z) 외부 기호(참조 위치: ""
public: virtual class tensorflow::Status __cdecl tensorflow::gputracer::GPUTracerImpl::Stop(
void)"" (?Stop@GPUTracerImpl@gputracer@tensorflow@@UEAA?AVStatus@3@XZ) 함수)에서 확인하지 못했습니다. [D:\
workspace\tensorflow\tensorflow\contrib\cmake\build\proto_text.vcxproj]
  D:\workspace\tensorflow\tensorflow\contrib\cmake\build\Release\proto_text.exe : fatal erro
r LNK1120: 11개의 확인할 수 없는 외부 참조입니다. [D:\workspace\tensorflow\tensorflow\contrib\cmake\build\p
roto_text.vcxproj]

    경고 0개
    오류 12개

경과 시간: 00:00:06.72

D:\workspace\tensorflow\tensorflow\contrib\cmake\build>
"
11657,BUG: When restore model from check point,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: win 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:1.10
- **Python version**: 3.52
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:8.0/5.1
- **GPU model and memory**:2Gb
- **Exact command to reproduce**:


### Describe the problem
I trained my model in a GPU version tensorflow(1.10), but when I restore the model in another computer without GPU it throw the error at the fallow command:
`result = mnist_classifier.predict(x = feature, batch_size = 1,as_iterable = False)`

I had tested the whole project on the computer which trained the model, so I think the code is right.



### Source code / logs
> Traceback (most recent call last):
>   File ""D:\xieminzhao\tf-test\main_func\main_func.py"", line 71, in <module>
>     print(rf.cooper_recog(img_test_2)) # had been tested.
>   File ""D:\xieminzhao\tf-test\main_func\recognition_func.py"", line 37, in cooper_recog
>     return recogni_is_cooper.is_cooper(image);
>   File ""D:\xieminzhao\tf-test\main_func\recogni_is_cooper.py"", line 110, in is_cooper
>     result = mnist_classifier.predict(x = feature, batch_size = 1,as_iterable = False)
> AssertionError
"
11652,can't import data_utils from tensorflow.models.rnn.translate(No module name tensorflow.models),"i encountered this while trying to import data_utils from tensorflow.models.rnn.translate.(in the end - NO module named tensorfow.models was my output)

"
11651,Feature Request: tf.extract_image_patches gradients operation for variable size inputs,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Scientific Linux 7.3
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.1.0-rc0-61-g1ec6ed5 1.1.0 GPU
- **Python version**: 3.6.1
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: 8.0.44
- **GPU model and memory**: n/a

### Describe the problem
I'm using the `tf.extract_image_patches` operation to extract a number of overlapping frames from an image with variable size. The current gradient operation fails unless the input image dimensions are fixed.
Specifically, in my case my code fails here 
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_grad.py#L690
where `rows_in`, `rows_out` and `cols_out` are all `None`.

For more background: my application is in speech processing and my ""image"" is a spectrogram-like array of features which has a fixed height but can vary in length with the duration of the input audio. I'm trying to split it into overlapping windows dynamically to pass to a classifier downstream.
In my case the input  has shape `(1 x None x 128 x  1)`, where the unknown dimension is typically ~15,000, my image patches are `400x128` and the result has shape `(1 x None x None x 51200)`.

I feel it should be possible to modify the gradient op to accommodate dynamic input dimensions, would this be a reasonable feature addition?
The current gradient implementation was added here: https://github.com/tensorflow/tensorflow/issues/2921
Perhaps this feature request would tie in with https://github.com/tensorflow/tensorflow/issues/6847 ?

### Source code/Logs
I think the information provided explains my problem well enough, let me know if I should provide a more detailed problem description.
An expanded code snippet can also be found here: https://github.com/PaddyT/waveform-asr/blob/master/waveasr/models/wavenet.py#L455#L495
``
extracted = tf.extract_image_patches(images=input_image,
                                                           ksizes=[1, 400, 128, 1],
                                                           strides=[1, 160, 1, 1],
                                                           rates=[1, 1, 1, 1],
                                                           padding='VALID')``


```
 File ""~/diss/waveform-asr/tests/test_graphs/test_wavenet.py"", line 35, in <module>
    alpha=0.5)
  File ""~/diss/waveform-asr/waveasr/models/wavenet.py"", line 321, in __init__
    self.train_op = self.optimizer.minimize(loss=self.objective, global_step=self.global_step)
  File ""~/miniconda2/envs/diss/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 315, in minimize
    grad_loss=grad_loss)
  File ""~/miniconda2/envs/diss/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 386, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""~/miniconda2/envs/diss/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 560, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""~/miniconda2/envs/diss/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 368, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""~/miniconda2/envs/diss/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 560, in <lambda>
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""~/miniconda2/envs/diss/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py"", line 610, in _ExtractImagePatchesGrad
    rows_out = int(ceil((rows_in - ksize_r_eff + 1) / stride_r))
TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'```"
11650,BUG: DropoutWrapper incorrectly updates memory state,"### Describe the problem
GRU units (wikipedia):
![image](https://user-images.githubusercontent.com/26974149/28431669-fb89275a-6d39-11e7-929a-43be917d2602.png)

tf.contrib.rnn.DropoutWrapper documentation:
state_keep_prob: unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added. State dropout is performed on the output states of the cell.

In GRU units, the output state is the memory state. When variational_recurrent=True, the same temporal dropout mask is applied to the output state, in each time step, with the remaining outputs divided by the dropout probability. This leads to exponential growth of the memory state and exploding outputs (given long enough time series):
![image](https://user-images.githubusercontent.com/26974149/28432360-7644d7b2-6d3c-11e7-8999-80ff55344f61.png)

The correct way is probably to divide U_z, U_r and U_h and not the output state.

### Source code / logs
```
################################### 
def length(sequence):
	used = tf.sign(tf.reduce_max(tf.abs(sequence), axis=2))
	length = tf.reduce_sum(used, axis=1)
	return length
################################### 
def GRU(x, units, act, in_dp, out_dp, tmp_dp):
	gru_cell = tf.contrib.rnn.GRUCell(units, activation=act, kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=True))
	wrapped_gru_cell = tf.contrib.rnn.DropoutWrapper(gru_cell, input_keep_prob=in_dp, output_keep_prob=out_dp, state_keep_prob=tmp_dp, variational_recurrent=True, dtype=x.dtype, input_size=x.get_shape()[2])
	outputs, state = tf.nn.dynamic_rnn(wrapped_gru_cell, x, dtype=x.dtype, sequence_length=length(x))
	return [outputs, state]
################################### 
Running with parameters (all exploded):
act=tf.nn.tanh, in_dp=1., out_dp=1., tmp_dp=0.5/0.9, n_hidden = 32/64/128
```

### System information
== cat /etc/issue ===============================================
Linux mvdslab 4.4.0-83-generic #106-Ubuntu SMP Mon Jun 26 17:54:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.2 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux mvdslab 4.4.0-83-generic #106-Ubuntu SMP Mon Jun 26 17:54:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.12.1)
numpydoc (0.6.0)
protobuf (3.3.0)
tensorflow (1.2.0)
tensorflow-gpu (1.2.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.2.0
tf.GIT_VERSION = v1.2.0-1131-gbc691dd
tf.COMPILER_VERSION = v1.2.0-1131-gbc691dd
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /home/anton/torch/install/lib:/usr/local/cuda-8.0/lib64
DYLD_LIBRARY_PATH /home/anton/torch/install/lib:/home/anton/torch/install/lib:

== nvidia-smi ===================================================
Thu Jul 20 10:36:06 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro K1200        Off  | 0000:01:00.0      On |                  N/A |
| 39%   38C    P0     1W /  35W |    459MiB /  4034MiB |      3%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1183    G   /usr/lib/xorg/Xorg                             110MiB |
|    0      6218    G   ...el-token=544D2517E333A364F4E0C0630D2C9DD1   347MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7"
11645,1.3.0rc0 GPU Windows Binaries are broken.,"UPDATE: This is most likely due to the new cuDNN requirement not being specified. Users stumbling across this issue should check what cuDNN version they have installed as 1.3 binaries are built with cuDNN v6.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win10-64
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.3.0rc0 GPU
- **Python version**: 3.6
- **CUDA/cuDNN version**: 8/5.1
- **GPU model and memory**: k40 12gb
- **Exact command to reproduce**: `import tensorflow as tf`

### Describe the problem
_Only tried GPU versions_
Despite what the release config matrix says here: https://ci.tensorflow.org/view/Release/job/release-win/lastBuild/ it appears that the binaries (pip and from ci) for 1.3.0rc0 do not work on windows. Each time will return the pywrap/DLL error etc. Checked with @mrry check script and everything is ok. Tried with python 3.5 and 3.6. 1.2.1 works on both those python versions just fine. I noticed that the 3.5 build for win-gpu was failing but jenkins reckons 3.6 build is ok but i'm unable to replicate a successful install.

Note that I have been running tf-gpu on this machine for a while now so not a dll problem etc. Epecially as a) mrry check script is ok and b) 1.2.1 works just 

### Source code / logs
*insert pywrap/DLL error here*"
11644,Is there a way to visualise the hidden layer during the Seq2Seq translation in translate.py ,"Is there any way I could visualise the hidden layer process during the translation process in Sequence to Sequence Modelling using Tensorflow.

Tensorflow version: 1.0 CPU version Python: 2.7 Windows"
11642,how to load my pretrain model in ckpt file v2 format？,"Hi,when I use ckpt v1 format,the command line as follows,
`python ./faster_rcnn/test_net.py --gpu 0 --weights ./models/VGGnet_fast_rcnn_iter_150000.ckpt  --imdb voc_2007_test --cfg ./experiments/cfgs/faster_rcnn_end2end.yml  --network VGGnet_test`
it works.
but I train my model in voc 2007 dataset ,the output like this.
![2017-07-20 21-27-37](https://user-images.githubusercontent.com/10908813/28419655-48ddb6d0-6d92-11e7-8e8f-8a5b0a2b14ee.png)

the test_net.py code as follows,
```
'# start a session
    saver = tf.train.Saver()
    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))
    saver.restore(sess, args.model)
    print ('Loading model weights from {:s}').format(args.model)
    test_net(sess, network, imdb, weights_filename)'
```
the output is:
`Waiting for ./output/faster_rcnn_end2end_pva_voc/voc_2007_trainval/PVAnet_iter_10000.ckpt to exist...`
I  have read some issues. but can't solve it . 
I need some help about this question.
how to load my pretrain model in ckpt file v2 format ?  modify my test_net.py?
thanks!


"
11640,mnist_with_summaries.py kills gpu,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Antergos GNU/Linux x86_64
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 3.6.1
- **CUDA/cuDNN version**: 8.0.61 / 6.0.21
- **GPU model and memory**: Nvidia GeForce 840m (Optimus), 2GB memory
- **Exact command to reproduce**: python mnist_with_summaries.py

### Description
Running the following tutorial code produces an error(see below), and renders the gpu unusable until the system is rebooted:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py

If we comment out line 167:
#run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
and line 171:
#options=run_options,
then the code runs fine without error.

### Source code / logs
2017-07-20 11:55:43.203763: I tensorflow/stream_executor/dso_loader.cc:129] Couldn't open CUDA library libcupti.so.8.0. LD_LIBRARY_PATH: /usr/lib/nvidia:/usr/lib32/nvidia:/usr/lib:/usr/lib32:/usr/lib:
2017-07-20 11:55:43.203810: F ./tensorflow/stream_executor/lib/statusor.h:205] Non-OK-status: status_ status: Failed precondition: could not dlopen DSO: libcupti.so.8.0; dlerror: libcupti.so.8.0: cannot open shared object file: No such file or directory
"
11639,Windows 64-bit GPU Build Failing,"The nightly windows 64-bit GPU build has been failing for over 3 days.
I could reproduce the error on different machines:

https://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows-gpu,PY=36/245/consoleText

Maybe the health monitor on the main site could include this build as well? Otherwise breaking errors might not be noticed for some longer time.
"
11638,Probably wrong implementation for tf.layers.max_pooling1d when data_format='channels_first',"In function ``call`` of class ``_Pooling1D``,  when the input ``data_format='channels_first'``, it should transform input tensor from '**N,C,H**' to '**N,C,H,W**' (batch_size, channels, height, width), meaning that we should expand dimension on the last dimension. 

However, in the code we use ``inputs = array_ops.expand_dims(inputs, 1)``, expanding on the second dimension and transforming from '**N,C,H**' to '**N,1,C,H**'. Then the ``pool_shape`` and ``strides`` are looking at the third dimension, which is not consistant with our ``expand_dims(inputs, 1)`` used before.

I think the code should be changed to ``inputs = array_ops.expand_dims(inputs, -1)`` and ``return array_ops.squeeze(outputs, -1)``. Using **-1** will expand and squeeze on the last dimension, transforming from '**N,C,H**' to **'**N,C,H,1**', and then doing ``pool_shape`` and ``strides`` on the third dimension.

### Source Code
------------------------
````
  def call(self, inputs):
    # There is no TF op for 1D pooling, hence we make the inputs 4D.
    if self.data_format == 'channels_last':
      inputs = array_ops.expand_dims(inputs, 2)
      pool_shape = (1,) + self.pool_size + (1, 1)
      strides = (1,) + self.strides + (1, 1)
      data_format = 'NHWC'
    else:
      inputs = array_ops.expand_dims(inputs, 1)
      pool_shape = (1, 1) + self.pool_size + (1,)
      strides = (1, 1) + self.strides + (1,)
      data_format = 'NCHW'
````

"
11637,when i train a GAN model AdamOptimizer get ValueError ,"    with tf.variable_scope(tf.get_variable_scope(),reuse=None):
        d_optim = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5).minimize(d_loss, var_list=d_vars, global_step=global_step)
        g_optim = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5).minimize(g_loss, var_list=g_vars, global_step=global_step)

i set reuse=None but it still has ValueError: Variable d_bn2/beta/Adam/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?
i even tried set it to False ,but it still has the error
is that a bug in version r1.2?"
11636,stack bidirectional RNNs: swap_memory and time_major args,"### Feature Request
adding `swap_memory` and `time_major` arguments to `stack_bidirectional_rnn` and `stack_bidirectional_dynamic_rnn`

from the source i see that both stack_bidirectional function just wraps bidirectional_rnn so it use concatenated output as next bidirectional layer's input. is it intentional that `swap_memory` and `time_major` argument not added?"
11635,Why are tensorboard removed after r1.2?,"Previously it was located in tensorflow/tensorboard. Is it removed or moved to somewhere else? What if I want to add new features in tensorboard?

Any suggestion is appreciated! 
"
11633,"when connect mnist,download mnist data,show network connection error.","![image](https://user-images.githubusercontent.com/22673941/28404110-88b436e0-6d5a-11e7-9344-596a22f5d64b.png)
I know if the error show ,just our company can not connect to mnist, can i manual download mnist data, and use it? how can i do this?"
11629,missing line of code in get_started tutorial on website,"as mentioned by an issue closed 19 days ago [https://github.com/tensorflow/tensorflow/issues/11042](https://github.com/tensorflow/tensorflow/issues/11042) there is a line missing in the tutorial code at [https://www.tensorflow.org/get_started/get_started#a_custom_model](https://www.tensorflow.org/get_started/get_started#a_custom_model).
this earlier issue was closed, but on the current version of the site that i see, the issue still persists.

the missing line is:
 `eval_input_fn = tf.contrib.learn.io.numpy_input_fn({""x"": x_eval}, y_eval,4, num_epochs=1000)`
 which should appear after: 
`input_fn = tf.contrib.learn.io.numpy_input_fn({""x"": x_train}, y_train, 4, num_epochs=1000)`

please note that these same lines appear in the code above the mentioned block in the ""basic usage"" section.

also, i hope opening a new issue is the correct action in this case. if i should have done something else, please let me know
"
11628,"A very weird bug of tf_debug: Non-Ok-status env->NewWritableFile(file_path, &f) status: Resource exaustated","### System information
- **Have I written custom code**:  Yes
- **OS Platform and Distribution**: CentOS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version**: 1.2
- **Python version**:  3.4

When I run the seq2seq model, I got the `nan` loss. Thus I use `tf_debug` to find out where the problem occurs. I use `tf_debug` by 
 
    sv = tf.train.Supervisor(logdir=FLAGS.log_root,
                             is_chief=True,
                             saver=saver,
                             summary_op=None,
                             save_summaries_secs=60,
                             save_model_secs=FLAGS.checkpoint_secs)
    sess = sv.prepare_or_wait_for_session(config=tf.ConfigProto(
        allow_soft_placement=True))
    sess = tf_debug.LocalCLIDebugWrapperSession(sess)
    sess.add_tensor_filter(""has_inf_or_nan"", tf_debug.has_inf_or_nan)

But it got such logs and exit:

    tfdebug Non-OK-status: env->NewWritableFile(file_path, &f) status: Resource exaustated: /tmp/tfdbg_9_gcc3sc/gradients/output/Reshape_1_grad/Reshape_0_DebugIdentity_150023213
    Aborted (core dumped)

I think it is kind of issue related to tf_debug and it may be new, since I can not find anything when I google the error. "
11626,SparseSoftmaxCrossEntropyWithLogits error while calculating gradient in c++ mode,"I'm trying to use the C++ API to train a CNN model. My last layer is using SparseSoftmaxCrossEntropyWithLogits. Here is the python code which generates the prototxt of the model:

```
import tensorflow as tf 
from tensorflow.python.framework import ops 
from tensorflow.python.framework import dtypes

import random  import numpy as np

NUM_CLASSES = 102  IMAGE_HEIGHT = 224  IMAGE_WIDTH = 224  BATCH_SIZE = 25  NUM_CHANNELS = 3  LEARNING_RATE = 0.0001

with tf.Session() as sess:



images_placeholder = tf.placeholder (tf.float32,
                                                shape=(BATCH_SIZE, IMAGE_HEIGHT,
                                                IMAGE_WIDTH, NUM_CHANNELS), name=""input"")   
labels_placeholder = tf.placeholder (tf.float32,
                                                shape=(BATCH_SIZE), name=""label"")

    with tf.name_scope(""conv1_1"") as scope:         
                 kernel = tf.Variable (tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32, stddev=1e-2),
                                                  name=""weights"")         
                 conv = tf.nn.conv2d (images_placeholder, kernel, [1, 1, 1, 1], padding='SAME')      
                 biases = tf.Variable (tf.constant(0.0, shape=[64], dtype=tf.float32),
                                                  trainable=True, name='biases')      
                 out = tf.nn.bias_add (conv, biases)         
                 conv1_1 = tf.nn.relu (out, name=scope)

    pool1 = tf.nn.max_pool (conv1_1,
                            ksize=[1, 2, 2, 1],
                            strides=[1, 2, 2, 1],
                            padding='SAME',
                            name='pool1')

    with tf.name_scope('conv2_1') as scope:         
                 kernel = tf.Variable (tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32, stddev=1e-2),
                                                  name='weights')       
                 conv = tf.nn.conv2d (pool1, kernel, [1, 1, 1, 1], padding='SAME')       
                 biases = tf.Variable (tf.constant(0.0, shape=[128], dtype=tf.float32),
                                                  trainable=True, name='biases')      
                 out = tf.nn.bias_add (conv, biases)         
                 conv2_1 = tf.nn.relu (out, name=scope)

    pool2 = tf.nn.max_pool (conv2_1,
                            ksize=[1, 2, 2, 1],
                            strides=[1, 2, 2, 1],
                            padding='SAME',
                            name='pool2')

    with tf.name_scope('conv3_1') as scope:         
                 kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32, stddev=1e-2),
                                                name='weights')       
                 conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')        
                 biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),
                                                 trainable=True, name='biases')      
                 out = tf.nn.bias_add(conv, biases)      
                 conv3_1 = tf.nn.relu(out, name=scope)

    pool3 = tf.nn.max_pool (conv3_1,
                            ksize=[1, 2, 2, 1],
                            strides=[1, 2, 2, 1],
                            padding='SAME',
                            name='pool3')

    with tf.name_scope('conv4_1') as scope:         
                 kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32, stddev=1e-2),
                                                name='weights')       
                conv = tf.nn.conv2d(pool3, kernel, [1, 1, 1, 1], padding='SAME')        
                biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),
                                                trainable=True, name='biases')      
                out = tf.nn.bias_add(conv, biases)      
                conv4_1 = tf.nn.relu(out, name=scope)

    pool4 = tf.nn.max_pool (conv4_1,
                            ksize=[1, 2, 2, 1],
                            strides=[1, 2, 2, 1],
                            padding='SAME',
                            name='pool4')   

    with tf.name_scope('mentee_conv5_1') as scope:      
                 kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32, stddev=1e-2),
                                                name='weights')       
                 conv = tf.nn.conv2d(pool4, kernel, [1, 1, 1, 1], padding='SAME')        
                 biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32), trainable=True,
                                                name='biases')         
                 out = tf.nn.bias_add(conv, biases)      
                 conv5_1 = tf.nn.relu(out, name=scope)

    pool5 = tf.nn.max_pool (conv5_1,
                            ksize=[1, 2, 2, 1],
                            strides=[1, 2, 2, 1],
                            padding='SAME',
                            name='pool5')

    with tf.name_scope('fc1') as scope:         
                 shape = int(np.prod(pool5.get_shape()[1:]))         
                 fc1w = tf.Variable(tf.truncated_normal([shape, 4096], dtype=tf.float32, 
                                             stddev=1e-2), name='weights')       
                 fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),
                                             trainable=True, name='biases')      
                 pool5_flat = tf.reshape(pool5, [-1, shape])         
                 fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)        
                 fc1 = tf.nn.relu(fc1l)
                 fc1 = tf.nn.dropout(fc1, 0.5)


    labels = tf.to_int64(labels_placeholder)    
    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits (labels=labels,
                        logits=fc1, name=""xentropy"")    
    loss = tf.reduce_mean (cross_entropy, name='loss')

    optimizer = tf.train.AdamOptimizer (LEARNING_RATE)  
    global_step = tf.Variable (0, name='global_step', trainable=False)  
    train_op = optimizer.minimize (loss, global_step=global_step, name=""train"")

    init = tf.initialize_variables (tf.all_variables(), name='init_all_vars_op')    
    tf.train.write_graph (sess.graph_def, ""models/"", ""graph.pb"", as_text=False)
```

Unfortunately when I call the c++ code to run the train_op node, it'll throw below error:

`E tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'message' not in Op<name=PreventGradient; signature=input:T -> output:T; attr=T:type>; NodeDef: gradients/xentropy/xentropy_grad/PreventGradient = PreventGradient[T=DT_FLOAT, message=""Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\'s interaction with tf.gradients()"", _device=""/job:localhost/replica:0/task:0/cpu:0""](xentropy/xentropy:1)
`
I'm still confused whether the error comes from a bug inside the TF code or not."
11624,Missing file,The file named node_def_pb2 is missing from tensorflow/core/framework. The file is required when I try to load the MNIST data as part of the tensorflow tutorial. 
11623,Different results from scipy imread and tensorflow decode_jpeg,"### Problem Description
Image decoded using decode_jpeg from tensor flow is visually similar, but numerically different from one returned by scipy imread.

**Minimal Example**:
```
import numpy as np
import scipy
import tensorflow as tf
def minimal_example():
    #image_source = 'https://upload.wikimedia.org/wikipedia/commons/8/88/Astronaut-EVA.jpg'
    image_path = 'astronaut.jpg'
    image_file = open(image_path,'rb')
    image_raw = image_file.read()
    image_scipy = scipy.misc.imread(image_path)
    image_tf = tf.image.decode_jpeg(image_raw).eval(session=tf.Session())
    print('Error: ', np.sum(np.abs(image_tf - image_scipy)))
    #Error:  3420883624
```

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.2.0-rc2-21-g12f033d 1.2.0
- **Python version**: 3.5

"
11621,WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.,"I have built a Custom Estimator, and I created an input function using the [Datasets API](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/datasets.md) when I call:

``` estimator.predict(input_fn) ```

I see the following Warning:
WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.

The input function looks like this:
```
def get_input_fn(review, word_to_id):
  """"""Create a Input function for classify_sentiment_analysis.py.

   Args:
      review (str): review sentence
      word_to_id (list): list with all words represented in the embedding.
                         The index is the word index in the embedding.

   Return: sequence of indexes that map that words to the embedding.

  """"""
  def _word_to_index(sequence):
    """"""Convert a sequence of words into a sequence of integers.""""""
    id_sequence = []
    UNK = 399999 # vector for unkown words
    for word in sequence:
      if word in word_to_id:
        id_sequence.append(word_to_id.index(word))
      else:
        id_sequence.append(UNK)
    return np.array(id_sequence)

  def input_fn():
    """"""Input function.""""""
    # make review a sequence of words
    review_split = review.split(' ')
    # converting words to indexes
    review_id = _word_to_index(review_split)
    # calculates the length of the sequence
    x_len = len(review_split)
    # creates the dataset from in memory data
    ds = tf.contrib.data.Dataset.from_tensors(review_id)
    # the model expects a batch
    ds = ds.batch(1)

    # creates iterator
    x = ds.make_one_shot_iterator().get_next()

    dict_x = {'x': x, rnn_common.RNNKeys.SEQUENCE_LENGTH_KEY: [x_len]}
    # no label needed since we're only using this input function for prediction
    # if training make sure to return a label
    return dict_x, None

  return input_fn
```

Thank you!"
11620,Improve exception safety with smart pointers,"Would you like to [wrap any pointer data members](https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/core/lib/io/recordio_test.cc#L105 ""Update candidate: RecordioTest class"") with the template class “[std::unique_ptr](https://en.wikipedia.org/wiki/Smart_pointer#unique_ptr ""Description for usage of smart pointers"")”?"
11619,Feature request: tensordot GPU kernel,"Hi,

I have two placeholder tensors with some matching axes that I would like to multiply element-wise and sum across. Implementation with tf.tensordot works perfectly on cpu, but it looks like there is no gpu kernel available for this op. Would it be possible to have one added? Or is there a workaround using simpler operations such as multiply and reduce_sum that are currently gpu-supported?

Specifically I would like to be able to run something equivalent to the following on gpu:
`a=tf.placeholder(shape=[None,16,17,18]) #e.g.`
`b=tf.placeholder(shape=[5,17,18,20]) `
`c=tf.tensordot(a,b,axes=[[2,3],[1,2]])`
`#shape of c: [None,16,5,20]`

When I run c in a session using tf.device('/gpu:0') I get the following error:
`InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Tensordot/ListDiff': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
	 [[Node: Tensordot/ListDiff = ListDiff[T=DT_INT32, out_idx=DT_INT32, _device=""/device:GPU:0""](Tensordot/range, Tensordot/add_1)]]`
"
11618,Unable to optimise the graph,"I have frozen the tensorflow graph and I have noticed that the using the graph in realtime for prediction is too slow. so I would like to do graph optimization.

Within my project file, I have got a folder called model which contains the model file (pb file).

Now I tried running the following command inside the model dir
```
 bazel build tensorflow/python/tools:strip_unused
```
But it is firing the following error,


>  bazel build tensorflow/python/tools:strip_unused
> ERROR: no such package 'tensorflow/python/tools': BUILD file not found on package path.
> 
> "
11617,Error Building from source with ARM processor gif_io/png_io errors,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I'm following [this](https://github.com/samjabrahams/tensorflow-on-raspberry-pi/blob/master/GUIDE.md) guide for installing tensorflow on a raspberry pi, because it most closely resembles the environment that I am building on.
I noticed that there were a couple of things that had changed since those instructions were written, so if things had changed in the code when I checked them, I didn't follow those instructions for changing the code. (I don't think the error is because of these instructions, which is why I am posting here, instead of there).
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
I am building in a Debian Jessie environment, that I have built exclusively for the purposes of building tensorflow on ARM, with the latest versions of gcc/g++ that I could find.
- **TensorFlow installed from (source or binary)**:
Source

- **TensorFlow version (use command below)**:
I'm assuming the most recent, because I just cloned it yesterday.
The script errors because tf hasn't actually made it to the installation step.
== cat /etc/issue ===============================================
Linux 3.14.38-yocto-00005-ge466b18 #1 SMP PREEMPT Tue May 30 10:41:58 MDT 2017 armv7l GNU/Linux
VERSION_ID=""8""
VERSION=""8 (jessie)""
== compiler =====================================================
c++ (Debian 4.8.4-1) 4.8.4
gcc version 4.8.4 (Debian 4.8.4-1) 
Copyright (C) 2013 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
== uname -a =====================================================
Linux 3.14.38-yocto-00005-ge466b18 #1 SMP PREEMPT Tue May 30 10:41:58 MDT 2017 armv7l GNU/Linux
== check pips ===================================================
numpy (1.8.2)

- **Python version**: 
Python 2.7.9

- **Bazel version (if compiling from source)**:
Build label: 0.4.5- (@non-git)

- **CUDA/cuDNN version**:""
N/A

- **GPU model and memory**:
N/A

- **Exact command to reproduce**:
`bazel build -c opt //tensorflow/tools/pip_package:build_pip_package`
Though, I've also gotten errors with 
```
bazel build -c opt --copt=""-mfpu=neon-vfpv4"" --copt=""-funsafe-math-optimizations"" --copt=""-ftree-vectorize"" --copt=""-fomit-frame-pointer"" --local_resources 1024,1.0,1.0 --verbose_failures tensorflow/tools/pip_package:build_pip_package
```
and 
```
bazel build -c opt --copt=""-mfpu=neon-vfpv4"" --copt=""-funsafe-math-optimizations"" --copt=""-ftree-vectorize"" --copt=""-fomit-frame-pointer"" --local_resources 1024,1.0,1.0 --verbose_failures tensorflow/tools/pip_package:build_pip_package
```

### Describe the problem
I am getting the included error when I build tensorflow. I've gotten the same error with the png_io.cc file as well. I installed the libpng-dev library as was recommended [here](https://github.com/tensorflow/tensorflow/issues/3631). Unlike in that example, that library did not exist on the computer I was installing it on (neither did libgif-dev, which was my guess for the gif library error). Is there a reason these libraries should be installed at the point that I run that command? Will I also need a libjpg-dev or something of the sort?

### Source code / logs
```
ERROR: /root/tf/tensorflow/tensorflow/core/BUILD:1357:1: C++ compilation of rule '//tensorflow/core:gif_internal' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 106 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
In file included from tensorflow/core/lib/gif/gif_io.cc:20:0:
./tensorflow/core/platform/gif.h:26:2: error: #error Define the appropriate PLATFORM_<foo> macro for this platform
 #error Define the appropriate PLATFORM_<foo> macro for this platform
  ^
tensorflow/core/lib/gif/gif_io.cc:33:20: error: 'GifFileType' was not declared in this scope
 int input_callback(GifFileType* gif_file, GifByteType* buf, int size) {
                    ^
tensorflow/core/lib/gif/gif_io.cc:33:33: error: 'gif_file' was not declared in this scope
 int input_callback(GifFileType* gif_file, GifByteType* buf, int size) {
                                 ^
tensorflow/core/lib/gif/gif_io.cc:33:43: error: 'GifByteType' was not declared in this scope
 int input_callback(GifFileType* gif_file, GifByteType* buf, int size) {
                                           ^
tensorflow/core/lib/gif/gif_io.cc:33:56: error: 'buf' was not declared in this scope
 int input_callback(GifFileType* gif_file, GifByteType* buf, int size) {
                                                        ^
tensorflow/core/lib/gif/gif_io.cc:33:61: error: expected primary-expression before 'int'
 int input_callback(GifFileType* gif_file, GifByteType* buf, int size) {
                                                             ^
tensorflow/core/lib/gif/gif_io.cc:33:69: error: expression list treated as compound expression in initializer [-fpermissive]
 int input_callback(GifFileType* gif_file, GifByteType* buf, int size) {
                                                                     ^
tensorflow/core/lib/gif/gif_io.cc:33:71: error: expected ',' or ';' before '{' token
 int input_callback(GifFileType* gif_file, GifByteType* buf, int size) {
                                                                       ^
tensorflow/core/lib/gif/gif_io.cc:111:1: error: expected '}' at end of input
 }  // namespace tensorflow
 ^
tensorflow/core/lib/gif/gif_io.cc:111:1: error: expected '}' at end of input
Target //tensorflow/tools/pip_package:build_pip_package failed to build

```
"
11614,TypeError: __init__() got an unexpected keyword argument 'shape',"I read these codes from a book, but it can't work, and after I searched for a couple of days haven't  similar 'shape' problem how can I fix it? Is anybody have ideas? plz, thanks!
----------------------------------------------------------------

\# -*- coding:utf-8 -*-
import tensorflow as tf
g1 = tf.Graph()
with g1.as_default():
    v = tf.get_variable(""v"",initializer=tf.zeros_initializer(shape=[1]))
"
11613,Error in InputQueueingStateSaver/barrier,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 18
- **TensorFlow installed from (source or binary)**: Binary (pip)
- **TensorFlow version (use command below)**: v1.2.0-rc2-21-g12f033d 1.2.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
I encounter an error related to barriers when using `tf.contrib.training.batch_sequences_with_states`. I use this function to batch sequences. For now, just batches of 4. I do not understand the error it but I tried empirically poking at it and it only occurs with some capacity values. For example, it does not occur within 15 minutes with a `capacity` value of 16 (good for 4 batches) and cycles through the input sequences as expected. It does occur within a minute with `capacity` values of 32 and 64.

I know this is incredibly vague but I can explore more if you can help me understand the underlying issue. I do believe this question does not fit the StackOverflow format as this is buggy behaviour. 

### Source code / logs
Below the error log. One thing I do not understand for example is that it always complains about components 0-8. I have verified that this occurs for multiple keys. My best guess is that this has to do with `save_state` trying to save over state that has already been saved?

```
2017-07-19 17:16:52.465021: W tensorflow/core/framework/op_kernel.cc:1158] Invalid argument: Key 00030_of_00112:20120305_seq1 already has a value for component 8 in barrier _3_input/batch_seq_with_states/InputQueueingStateSaver/barrier
2017-07-19 17:16:52.465073: W tensorflow/core/kernels/queue_base.cc:294] _2_input/batch_seq_with_states/fifo_queue: Skipping cancelled enqueue attempt with queue not closed
2017-07-19 17:16:52.465117: W tensorflow/core/framework/op_kernel.cc:1158] Invalid argument: Key 00030_of_00112:20120305_seq1 already has a value for component 7 in barrier _3_input/batch_seq_with_states/InputQueueingStateSaver/barrier
2017-07-19 17:16:52.465168: W tensorflow/core/framework/op_kernel.cc:1158] Invalid argument: Key 00030_of_00112:20120305_seq1 already has a value for component 5 in barrier _3_input/batch_seq_with_states/InputQueueingStateSaver/barrier
2017-07-19 17:16:52.465226: W tensorflow/core/framework/op_kernel.cc:1158] Invalid argument: Key 00030_of_00112:20120305_seq1 already has a value for component 3 in barrier _3_input/batch_seq_with_states/InputQueueingStateSaver/barrier
2017-07-19 17:16:52.465261: W tensorflow/core/framework/op_kernel.cc:1158] Invalid argument: Key 00030_of_00112:20120305_seq1 already has a value for component 4 in barrier _3_input/batch_seq_with_states/InputQueueingStateSaver/barrier
2017-07-19 17:16:52.465292: W tensorflow/core/framework/op_kernel.cc:1158] Invalid argument: Key 00030_of_00112:20120305_seq1 already has a value for component 0 in barrier _3_input/batch_seq_with_states/InputQueueingStateSaver/barrier
2017-07-19 17:16:52.465324: W tensorflow/core/framework/op_kernel.cc:1158] Invalid argument: Key 00030_of_00112:20120305_seq1 already has a value for component 1 in barrier _3_input/batch_seq_with_states/InputQueueingStateSaver/barrier
2017-07-19 17:16:52.465798: W tensorflow/core/framework/op_kernel.cc:1158] Invalid argument: Key 00030_of_00112:20120305_seq1 already has a value for component 2 in barrier _3_input/batch_seq_with_states/InputQueueingStateSaver/barrier
2017-07-19 17:16:52.469856: W tensorflow/core/framework/op_kernel.cc:1158] Invalid argument: Key 00030_of_00112:20120305_seq1 already has a value for component 6 in barrier _3_input/batch_seq_with_states/InputQueueingStateSaver/barrier
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, Key 00030_of_00112:20120305_seq1 already has a value for component 8 in barrier _3_input/batch_seq_with_states/InputQueueingStateSaver/barrier
         [[Node: input/batch_seq_with_states/InputQueueingStateSaver/BarrierInsertContext_num_frames = BarrierInsertMany[T=DT_INT64, _class=[""loc:@input/batch_seq_with_states/fifo_queue""], component_index=8, _device=""/job:localhost/replica:0/task:0/cpu:0""](input/batch_seq_with_states/InputQueueingStateSaver/barrier, input/batch_seq_with_states/InputQueueingStateSaver/StringJoinCurrentKeys, input/batch_seq_with_states/InputQueueingStateSaver/dims_checked_expanded_context_num_frames)]]
17:16:52 [tensorflow          ] [INFO    ] : Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, Key 00030_of_00112:20120305_seq1 already has a value for component 8 in barrier _3_input/batch_seq_with_states/InputQueueingStateSaver/barrier
         [[Node: input/batch_seq_with_states/InputQueueingStateSaver/BarrierInsertContext_num_frames = BarrierInsertMany[T=DT_INT64, _class=[""loc:@input/batch_seq_with_states/fifo_queue""], component_index=8, _device=""/job:localhost/replica:0/task:0/cpu:0""](input/batch_seq_with_states/InputQueueingStateSaver/barrier, input/batch_seq_with_states/InputQueueingStateSaver/StringJoinCurrentKeys, input/batch_seq_with_states/InputQueueingStateSaver/dims_checked_expanded_context_num_frames)]]
Traceback (most recent call last):
  File ""./train.py"", line 108, in <module>
    solver.train(**pick(all_args, list(inspect.signature(solver.train).parameters)))
  File ""/home/ruben/attention/attend/attend/solver.py"", line 122, in train
    coord.join(threads + input_threads)
  File ""/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/home/ruben/anaconda3/lib/python3.6/site-packages/six.py"", line 686, in reraise
    raise value
  File ""/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 238, in _run
    enqueue_callable()
  File ""/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1063, in _single_operation_run
    target_list_as_strings, status, None)
  File ""/home/ruben/anaconda3/lib/python3.6/contextlib.py"", line 89, in __exit__
    next(self.gen)
  File ""/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Key 00030_of_00112:20120305_seq1 already has a value for component 8 in barrier _3_input/batch_seq_with_states/InputQueueingStateSaver/barrier
         [[Node: input/batch_seq_with_states/InputQueueingStateSaver/BarrierInsertContext_num_frames = BarrierInsertMany[T=DT_INT64, _class=[""loc:@input/batch_seq_with_states/fifo_queue""], component_index=8, _device=""/job:localhost/replica:0/task:0/cpu:0""](input/batch_seq_with_states/InputQueueingStateSaver/barrier, input/batch_seq_with_states/InputQueueingStateSaver/StringJoinCurrentKeys, input/batch_seq_with_states/InputQueueingStateSaver/dims_checked_expanded_context_nu

```

Relevant source code in `provider.py`:

```python
batch = tf.contrib.training.batch_sequences_with_states(
        input_sequences={
            'images': example,
            self.feat_name: target,
        },
        input_key      = context['key'],
        input_context  = context,
        input_length   = tf.cast(context['num_frames'], tf.int32),
        initial_states = initial_states,
        num_unroll     = self.time_steps,
        batch_size     = self.batch_size,
        num_threads    = 2, # TODO change
        capacity       = self.batch_size * 4,
        name           = 'batch_seq_with_states'
        )


```
"
11612,pre-trained weights,can i use the tensorflow pre-trained weight model in keras and Vice versa?
11611,estimators/SVM_test.py included in test suite?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.2 LTS (Xenial Xerus)""
- **TensorFlow installed from (source or binary)**:
Installed from source, master branch
- **TensorFlow version (use command below)**:
>>> print(tensorflow.VERSION)
1.2.1
>>> print(tensorflow.GIT_VERSION)
b'v1.2.0-2149-gf092326'
- **Python version**: 
Python 2.7.12 (default, Nov 19 2016, 06:48:10) 
[GCC 5.4.0 20160609] on linux2
- **Bazel version (if compiling from source)**:
[bazel release 0.5.2]
- **CUDA/cuDNN version**:
n/a
- **GPU model and memory**:
n/a
- **Exact command to reproduce**:
bazel test //tensorflow/python/... --test_verbose_timeout_warnings

### Describe the problem
I am troubleshooting my usage of estimators/SVM (https://stackoverflow.com/questions/45194171/tensorflow-svm-code-update-to-v1-2), and along the way it seems like svm_test.py isn't executed when running the bazel test command above. I'm a tensorflow newb, so it would be super helpful for me to understand what I'm missing here.

### Source code / logs
`[...]
//tensorflow/python:decorator_utils_test                        (cached) PASSED in 1.2s
  WARNING: //tensorflow/python:decorator_utils_test: Test execution time (1.2s excluding execution overhead) outside of range for MODERATE tests. Consider setting timeout=""short"" or size=""small"".
//tensorflow/python:deprecation_test                            (cached) PASSED in 1.1s
  WARNING: //tensorflow/python:deprecation_test: Test execution time (1.1s excluding execution overhead) outside of range for MODERATE tests. Consider setting timeout=""short"" or size=""small"".
//tensorflow/python:dequantize_op_test                          (cached) PASSED in 1.1s
//tensorflow/python:device_lib_test                             (cached) PASSED in 1.1s
//tensorflow/python:device_setter_test                          (cached) PASSED in 1.2s
//tensorflow/python/estimator:dnn_linear_combined_test          (cached) PASSED in 19.0s
  Stats over 4 runs: max = 19.0s, min = 11.8s, avg = 15.4s, dev = 2.5s
//tensorflow/python/estimator:dnn_test                          (cached) PASSED in 22.8s
//tensorflow/python/estimator:estimator_test                    (cached) PASSED in 9.5s
//tensorflow/python/estimator:export_output_test                (cached) PASSED in 1.2s
//tensorflow/python/estimator:export_test                       (cached) PASSED in 5.3s
//tensorflow/python/estimator:feeding_functions_test            (cached) PASSED in 1.3s
//tensorflow/python/estimator:feeding_queue_runner_test         (cached) PASSED in 2.5s
//tensorflow/python/estimator:head_test                         (cached) PASSED in 7.2s
//tensorflow/python/estimator:linear_test                       (cached) PASSED in 35.3s
//tensorflow/python/estimator:model_fn_test                     (cached) PASSED in 1.2s
//tensorflow/python/estimator:numpy_io_test                     (cached) PASSED in 1.3s
//tensorflow/python/estimator:optimizers_test                   (cached) PASSED in 1.0s
//tensorflow/python/estimator:pandas_io_test                    (cached) PASSED in 1.5s
//tensorflow/python/estimator:parsing_utils_test                (cached) PASSED in 1.5s
//tensorflow/python/estimator:run_config_test                   (cached) PASSED in 1.2s
//tensorflow/python/estimator:util_test                         (cached) PASSED in 1.0s
  WARNING: //tensorflow/python/estimator:util_test: Test execution time (1.0s excluding execution overhead) outside of range for MODERATE tests. Consider setting timeout=""short"" or size=""small"".
//tensorflow/python:events_writer_test                          (cached) PASSED in 0.9s
//tensorflow/python/feature_column:feature_column_test          (cached) PASSED in 5.2s
  WARNING: //tensorflow/python/feature_column:feature_column_test: Test execution time (5.2s excluding execution overhead) outside of range for MODERATE tests. Consider setting timeout=""short"" or size=""small"".
//tensorflow/python:file_io_test                                (cached) PASSED in 1.9s
[...]
`
"
11610,Changing batch size changes output for float32 matmuls elementwise by ~1e-8 (at least on CPU),"While developing a Hierarchical Attention Network, we have discovered that changing the batch size of the input effects the output of dynamic RNNs (while keeping everything else constant). In other words, feeding in [[1,2,3,4,5]] and [[6,7,8,9,10]] individually with batch size 1 will give a different result than feeding in [[1,2,3,4,5],[6,7,8,9,10]] together with batch size 2. We are currently running Bidirectional Dynamic RNNs with GRUs on the CPU-version of Tensorflow 1.2.

While the change in output is small, when a network has many layers of RNNs, the differences become amplified. In our case, changing the batch size from 1 to 10 changes the network accuracy on our test set from 50% to 46%.

System information and shortened sample code below.

### System information
== cat /etc/issue ===============================================
Linux pc93071.ornl.gov 3.10.0-514.26.1.el7.x86_64 #1 SMP Tue Jun 20 01:16:02 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""7.3 (Maipo)""
VERSION_ID=""7.3""
REDHAT_BUGZILLA_PRODUCT_VERSION=7.3
REDHAT_SUPPORT_PRODUCT_VERSION=""7.3""

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux pc93071.ornl.gov 3.10.0-514.26.1.el7.x86_64 #1 SMP Tue Jun 20 01:16:02 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.1)
numpydoc (0.6.0)
protobuf (3.3.0)
tensorflow (1.2.1)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.2.1
tf.GIT_VERSION = v1.2.0-5-g435cdfc
tf.COMPILER_VERSION = v1.2.0-5-g435cdfc
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================

### Source code / logs

```
import numpy as np
import tensorflow as tf
from tensorflow.contrib.rnn import LSTMCell, GRUCell

embeddings = np.random.rand(8000,350).astype(np.float32)
embeddings -= embeddings.mean()
embeddings /= (embeddings.std()*2.5)

#doc input and line count
line = tf.placeholder(tf.int32, shape=[None,10])
num_words = tf.reduce_sum(tf.cast(tf.greater(line,0),tf.int32),1)
word_embeds = tf.nn.embedding_lookup(tf.get_variable('embeddings',
              initializer=embeddings,dtype=tf.float32),line)

[word_outputs_fw,word_outputs_bw],_ = \
        tf.nn.bidirectional_dynamic_rnn(GRUCell(5),GRUCell(5),
        word_embeds,sequence_length=num_words,
        dtype=tf.float32)

word_outputs = tf.concat((word_outputs_fw, word_outputs_bw),2)

init_op = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init_op)

a = np.array([[1,2,3,4,5,0,0,0,0,0]])
b = np.array([[6,7,8,9,10,11,12,13,14,15]])
ab = np.array([[1,2,3,4,5,0,0,0,0,0],[6,7,8,9,10,11,12,13,14,15]])

feed_dict = {line:a}
print sess.run(word_outputs,feed_dict=feed_dict)
feed_dict = {line:b}
print sess.run(word_outputs,feed_dict=feed_dict)
feed_dict = {line:ab}
print sess.run(word_outputs,feed_dict=feed_dict)
```

### Sample Output

Below, the first two matrices are the results of feeding in two inputs one at a time with batch size 1, while the second two matrices are the results of feeding in two inputs together with batch size 2. You can see that the outputs are not exactly the same. While the differences between the two are small, this becomes a major issue when there are multiple layers of RNNs as the differences become more pronounced after each layer.

```
[[[ 0.07946277 -0.09917585  0.01027258 -0.03145921 -0.06281948  0.27924815
   -0.32083094 -0.18930595  0.17904316 -0.09718883]
  [ 0.09456758 -0.06845391 -0.02745478 -0.10440759 -0.07491632  0.27888948
   -0.27896836  0.03206063  0.09979809 -0.00771215]
  [ 0.01643243  0.12345143  0.12964873  0.01598591 -0.18927756  0.37746075
   -0.03456679 -0.01384296  0.03874877  0.06282371]
  [ 0.04219431  0.02407469 -0.1588002   0.1497623  -0.17770161  0.3960323
    0.16187154 -0.04393335 -0.02065297  0.10994863]
  [-0.13827246 -0.07322901 -0.012384    0.12282669  0.07407188 -0.14240782
    0.140168    0.02362901  0.06010906  0.05862212]
  [ 0.          0.          0.          0.          0.          0.          0.
    0.          0.          0.        ]
  [ 0.          0.          0.          0.          0.          0.          0.
    0.          0.          0.        ]
  [ 0.          0.          0.          0.          0.          0.          0.
    0.          0.          0.        ]
  [ 0.          0.          0.          0.          0.          0.          0.
    0.          0.          0.        ]
  [ 0.          0.          0.          0.          0.          0.          0.
    0.          0.          0.        ]]]
[[[ 0.16048311 -0.02057735  0.18933752 -0.12690066 -0.04377137  0.32376432
    0.13263705 -0.07457904  0.14895026 -0.18088266]
  [ 0.16808736 -0.11579071 -0.11836589 -0.31363881  0.1567639  -0.08804542
    0.1359456  -0.03568897  0.12253968 -0.08998561]
  [ 0.19741508 -0.01034784 -0.03235145 -0.27677989  0.1338885  -0.14571345
    0.08804264 -0.02352159  0.04717591 -0.37237346]
  [ 0.24377933 -0.27160296 -0.11816068 -0.45893419 -0.09967859 -0.04910848
    0.03985181 -0.01856269  0.04410465 -0.21198548]
  [ 0.16746353 -0.20125373 -0.2098352  -0.36264825  0.02557869 -0.06599348
   -0.11331714 -0.17118242 -0.08420456 -0.22979215]
  [-0.09969822 -0.14207448  0.12536064 -0.22236535  0.11328859 -0.09342889
   -0.02536193 -0.28028104 -0.11790876 -0.10144062]
  [-0.09796695 -0.14415297 -0.19729097 -0.25542045 -0.15568495 -0.12689842
   -0.14712927 -0.35488427 -0.06447952 -0.19063833]
  [-0.12240371 -0.07732555 -0.2645728   0.11042064 -0.19387801  0.07324903
   -0.03920996  0.05104404 -0.09357925 -0.13582835]
  [-0.07295815 -0.02809375 -0.24317381  0.04480781 -0.06040902  0.03428879
    0.10196722 -0.06142509 -0.36903486 -0.16991363]
  [-0.01382132 -0.09746805  0.13226555  0.19477166  0.02158988  0.09287433
    0.01845972 -0.16030487 -0.2186746  -0.07543172]]]
[[[ 0.07946277 -0.09917583  0.01027257 -0.03145922 -0.06281949  0.27924818
   -0.32083094 -0.1893059   0.17904317 -0.09718874]
  [ 0.09456757 -0.06845395 -0.02745485 -0.10440758 -0.07491633  0.27888948
   -0.27896842  0.03206065  0.09979802 -0.00771213]
  [ 0.01643244  0.12345135  0.1296487   0.01598606 -0.18927751  0.37746072
   -0.0345668  -0.01384297  0.03874873  0.06282371]
  [ 0.0421943   0.02407462 -0.15880026  0.14976241 -0.17770153  0.39603227
    0.16187152 -0.04393341 -0.02065307  0.10994864]
  [-0.13827249 -0.07322903 -0.01238406  0.12282679  0.07407197 -0.14240779
    0.14016804  0.02362898  0.06010904  0.05862212]
  [ 0.          0.          0.          0.          0.          0.          0.
    0.          0.          0.        ]
  [ 0.          0.          0.          0.          0.          0.          0.
    0.          0.          0.        ]
  [ 0.          0.          0.          0.          0.          0.          0.
    0.          0.          0.        ]
  [ 0.          0.          0.          0.          0.          0.          0.
    0.          0.          0.        ]
  [ 0.          0.          0.          0.          0.          0.          0.
    0.          0.          0.        ]]

 [[ 0.16048311 -0.02057736  0.18933751 -0.12690073 -0.04377136  0.32376438
    0.13263711 -0.07457898  0.14895013 -0.18088272]
  [ 0.16808733 -0.11579067 -0.11836579 -0.31363881  0.15676391 -0.0880454
    0.13594568 -0.03568894  0.12253958 -0.08998567]
  [ 0.19741504 -0.01034782 -0.0323514  -0.27677989  0.13388851 -0.14571348
    0.08804271 -0.02352155  0.0471758  -0.37237346]
  [ 0.2437793  -0.27160296 -0.11816064 -0.45893413 -0.09967858 -0.04910852
    0.03985184 -0.01856264  0.04410452 -0.21198554]
  [ 0.16746351 -0.20125373 -0.20983508 -0.36264819  0.02557869 -0.06599346
   -0.11331721 -0.17118247 -0.08420463 -0.22979221]
  [-0.09969822 -0.14207451  0.12536073 -0.22236532  0.11328855 -0.09342889
   -0.02536207 -0.28028107 -0.11790879 -0.10144066]
  [-0.09796697 -0.14415301 -0.19729097 -0.25542039 -0.15568499 -0.12689844
   -0.14712939 -0.35488427 -0.06447949 -0.19063836]
  [-0.12240377 -0.07732558 -0.2645728   0.11042073 -0.19387813  0.07324899
   -0.03921008  0.05104404 -0.0935792  -0.13582836]
  [-0.07295817 -0.02809371 -0.24317381  0.04480787 -0.06040911  0.03428881
    0.10196713 -0.06142508 -0.3690348  -0.16991359]
  [-0.0138213  -0.09746802  0.13226555  0.19477174  0.02158987  0.09287442
    0.0184597  -0.16030489 -0.21867451 -0.07543168]]]

```"
11608,Distributed Tensorflow : Cannot assign a device for operation...,"I am trying to use a distributed version of Self-Normalizing Networks on MNIST 

The full code is provided here:
`
class DNN_forward_MC(object):

        def __init__(self,num_data,num_test,dim_input,dim_output,num_hidden_layers,num_hidden_dims,keep_prob,num_minibatch,X_train,Y_train,X_test,Y_test):


                # cluster specification
                parameter_servers = [""localhost:2222""]
                workers = [     ""localhost:2223"", 
                        ""localhost:2224"",
                        ""localhost:2225""]

                cluster = tf.train.ClusterSpec({""ps"":parameter_servers, ""worker"":workers})

                # input flags
                tf.app.flags.DEFINE_string(""job_name"", """", ""Either 'ps' or 'worker'"")
                tf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")
                FLAGS = tf.app.flags.FLAGS

                # start a server for a specific task
                server = tf.train.Server(
                                        cluster,
                                        job_name=FLAGS.job_name,
                                        task_index=FLAGS.task_index)


                is_chief=(FLAGS.task_index == 0)
                if FLAGS.job_name == ""ps"":
                        server.join()

                elif FLAGS.job_name == ""worker"":
                        with tf.device(tf.train.replica_device_setter(
                worker_device=""/job:worker/task:%d/cpu:0"" % FLAGS.task_index,
                cluster=cluster,ps_device=""/job:ps/cpu:0"")):

                                global_step = tf.Variable(0, name=""global_step"", trainable=False)
                                self.initialize_model(num_data,num_test,dim_input,dim_output,num_hidden_layers,num_hidden_dims,keep_prob,num_minibatch)

                                predictions = self.predict_training(tip='redus')
                                predictions_training_full = self.predict_training(tip='full')

                                predictions_testing_full = self.predict_testing()

                                cost =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.Y_train, logits=predictions))
                                opt = tf.train.GradientDescentOptimizer(1e-3)
                                opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=3,total_num_replicas=3)
                                train_op = opt.minimize(cost,global_step=global_step)
                                init_op = tf.global_variables_initializer()

                                #grads_and_vars = optimizer.compute_gradients(cost, tf.trainable_variables())

                                local_init_op = opt.local_step_init_op
                                if is_chief:
                                        local_init_op = opt.chief_init_op

                                ready_for_local_init_op = opt.ready_for_local_init_op

                                # Initial token and chief queue runners required by the sync_replicas mode
                                chief_queue_runner = opt.get_chief_queue_runner()
                                sync_init_op = opt.get_init_tokens_op()

                        train_dir = tempfile.mkdtemp()
                        sv = tf.train.Supervisor(logdir=train_dir,is_chief=(FLAGS.task_index == 0),init_op=init_op,local_init_op=local_init_op,recovery_wait_secs=1,ready_for_local_init_op=ready_for_local_init_op)

                        if is_chief:
                                print(""Worker %d: Initializing session..."" % FLAGS.task_index)
                        else:
                                print(""Worker %d: Waiting for session to be initialized..."" % FLAGS.task_index)


                        sess_config = tf.ConfigProto(allow_soft_placement=True,log_device_placement=False,device_filters=[""/job:ps"", ""/job:worker/task:%d"" % FLAGS.task_index])

                        with sv.prepare_or_wait_for_session(server.target,config=sess_config) as sess:
                        #with tf.train.MonitoredTrainingSession(master=server.target,is_chief=is_chief,checkpoint_dir='./logs/',save_checkpoint_secs=60,save_summaries_steps=1) as sess:        
                                while True:

                                        lista = np.arange(self.num_data)
                                        np.random.shuffle(lista)
                                        current_index = lista[:self.num_minibatch]

                                        likelihood_now,_,step_now = self.sess.run([cost,train_op,global_step],feed_dict={self.X_train:X_train[current_index],self.Y_train:Y_train[current_index]})
                                        printare = 'nll for minibatch at iteration '+str(i)+' is '+str(likelihood_now)
                                        print(printare)
                                        predictions_now = self.sess.run(predictions_training_full,feed_dict={self.X_train_full:X_train})
                                        print 'accuracy is : '+str(self.sess.run(tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_train,1),tf.argmax(tf.nn.softmax(predictions_now),1)),tf.float32))))
                                        if step_now>10:
                                                break

                                print '*******training last iteration**********'
                                
                                if is_chief:

                                        predictions_now = self.sess.run(predictions_testing_full,feed_dict={self.X_test:X_test})
                                        print '***********testing*************'

                                        print 'accuracy at testing time is :' +str(self.sess.run(tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_test,1),tf.argmax(tf.nn.softmax(predictions_now),1)),tf.float32))))

                        sv.stop()
        def initialize_model(self,num_data,num_test,dim_input,dim_output,num_hidden_layers,num_hidden_dims,keep_prob,num_minibatch):

                ### model using just Hinton's Dropout
                self.sess = tf.Session()
                self.num_minibatch = num_minibatch
                self.num_data = num_data
                self.num_test = num_test
                self.dim_input = dim_input
                self.dim_output = dim_output
                self.num_hidden_layers = num_hidden_layers
                self.num_hidden_dims = num_hidden_dims

                self.keep_prob = keep_prob
                self.alpha_p = -1.7580993408473766
                q = 1.0 - keep_prob
                prod  = q + np.power(self.alpha_p,2)*q*(1-q)
                self.a_affine = np.power(prod,-0.5)
                self.b_affine = -self.a_affine * (self.alpha_p*(1-q))

                self.X_train_full = tf.placeholder(shape=(self.num_data,dim_input),dtype=tf.float32)
        
                self.X_train = tf.placeholder(tf.float32,shape=(num_minibatch,dim_input))
                self.Y_train = tf.placeholder(tf.float32,shape=(num_minibatch,dim_output))
                self.X_test = tf.placeholder(tf.float32,shape=(num_test,dim_input))
                self.Y_test = tf.placeholder(tf.float32,shape=(num_test,dim_output))

                self.weights = []
                self.biases = []

                ### create parameters for Global DNN
                for j in range(self.num_hidden_layers+1):                                                                                                              
                        self.weights.append(tf.Variable(tf.random_normal(shape=(num_hidden_dims[j-1],self.num_hidden_dims[j]),stddev=1.0/num_hidden_dims[j-1]),dtype=tf.float32,name='weights_'+str(j)))
                        self.biases.append(tf.Variable(tf.random_normal(shape=(self.num_hidden_dims[j],)),dtype=tf.float32,name='biases'+str(j)))          
                
                self.weights.append(tf.Variable(tf.random_normal(shape=(self.num_hidden_dims[self.num_hidden_layers],self.num_hidden_dims[self.num_hidden_layers+1])
                ,stddev=1.0/self.num_hidden_dims[self.num_hidden_layers]),dtype=tf.float32,name='weights_'+str(self.num_hidden_layers+1)))
                self.biases.append(tf.Variable(tf.random_normal(shape=(self.num_hidden_dims[self.num_hidden_layers+1],)),dtype=tf.float32,name='biases_'+str(self.num_hidden_layers+1)))

               
        def selu(self,x):

                alpha= 1.6732632423543772848170429916717
                lamb = 1.0507009873554804934193349852946
                return lamb * tf.where(x>0.0,x,alpha * tf.nn.elu(x))

        def alpha_dropout(self,x):

                mask = tf.cast(tf.random_uniform(shape=x.get_shape()) < ( 1.0 - self.keep_prob ),dtype=tf.float32)
                x_masked = tf.multiply(x,mask)
                out = x_masked * self.a_affine + self.b_affine
                return out      

        def model(self,input,apply_dropout=True):

                for j in range(1,self.num_hidden_layers +1):

                        input = self.selu(tf.add(tf.matmul(input,self.weights[j]),self.biases[j]))
                        if apply_dropout:

                                input = self.alpha_dropout(input)

                final_output = tf.add(tf.matmul(input,self.weights[self.num_hidden_layers+1]),self.biases[self.num_hidden_layers+1])

                return final_output


        def predict_training(self,tip):

                if tip == 'full':
                        prediction_DNN_global = self.model(input=self.X_train_full,apply_dropout=False)
                else:
                        prediction_DNN_global = self.model(input = self.X_train,apply_dropout=True)

                return prediction_DNN_global

                
        def predict_testing(self):

                prediction_DNN_global = self.model(input = self.X_test,apply_dropout=False)

                return prediction_DNN_global

if __name__ == '__main__':

        training_data = np.genfromtxt('/home/spopescu/mnist_train.csv',dtype=np.float64,delimiter=',')
        testing_data = np.genfromtxt('/home/spopescu/mnist_test.csv',dtype=np.float64,delimiter=',')

        X_training = training_data[:,1:]
        X_testing = testing_data[:,1:]
        Y_training = one_hot_encoder(training_data[:,0].reshape(X_training.shape[0],1))
        Y_testing = one_hot_encoder(testing_data[:,0].reshape(X_testing.shape[0],1))

        obiect = DNN_forward_MC(num_data=X_training.shape[0],num_test=X_testing.shape[0],dim_input=X_training.shape[1],dim_output=Y_training.shape[1],num_hidden_layers=3,num_hidden_dims=[X_training.shape[1],200,50,20,Y_training.shape[1]],keep_prob=0.80,num_minibatch=20,X_train=X_training,Y_train=Y_training,X_test=X_testing,Y_test=Y_testing)

`

I am using the following Slurm bash script to send an array job:
`
#!/bin/sh
#SBATCH --mem=20G
#SBATCH -J SNN
#SBATCH --array=0-3
#SBATCH -o slurm-%A_%a.out
#SBATCH -e slurm-%A_%a.err
#SBATCH --nodelist=compute-2-3

task_type=(""ps"" ""worker"" ""worker"" ""worker"")
task_index_array=(0 0 1 2)
srun --exclusive -N1 -n1 /home/spopescu/anaconda/bin/python2.7 DNN_forward_MC.py --job_name=${task_type[$SLURM_ARRAY_TASK_ID]} --task_index=${task_index_array[$SLURM_ARRAY_TASK_ID]}
`

I am using the latest version of Tensorflow as of today.

I am getting the following error on my chief worker:

`
2017-07-19 14:49:14.087646: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}
2017-07-19 14:49:14.087702: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> localhost:2224, 2 -> localhost:2225}
2017-07-19 14:49:14.091606: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2223
2017-07-19 14:49:15.137109: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 4b5afb77d3aa2b77 with config: 
device_filters: ""/job:ps""
device_filters: ""/job:worker/task:0""
allow_soft_placement: true

Traceback (most recent call last):
  File ""DNN_forward_MC.py"", line 221, in <module>
    obiect = DNN_forward_MC(num_data=X_training.shape[0],num_test=X_testing.shape[0],dim_input=X_training.shape[1],dim_output=Y_training.shape[1],num_hidden_layers=3,num_hidden_dims=[X_training.shape[1],200,50,20,Y_training.shape[1]],keep_prob=0.80,num_minibatch=20,X_train=X_training,Y_train=Y_training,X_test=X_testing,Y_test=Y_testing)
  File ""DNN_forward_MC.py"", line 108, in __init__
    likelihood_now,_,step_now = self.sess.run([cost,train_op,global_step],feed_dict={self.X_train:X_train[current_index],self.Y_train:Y_train[current_index]})
  File ""/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 789, in run
    run_metadata_ptr)
  File ""/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 997, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1132, in _do_run
    target_list, options, run_metadata)
  File ""/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1152, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'save/RestoreV2_10': Operation was explicitly assigned to /job:ps/task:0/device:CPU:0 but available devices are [ /job:localhost/replica:0/task:0/cpu:0 ]. Make sure the device specification refers to a valid device.
         [[Node: save/RestoreV2_10 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:ps/task:0/device:CPU:0""](save/Const, save/RestoreV2_10/tensor_names, save/RestoreV2_10/shape_and_slices)]]

Caused by op u'save/RestoreV2_10', defined at:
  File ""DNN_forward_MC.py"", line 221, in <module>
    obiect = DNN_forward_MC(num_data=X_training.shape[0],num_test=X_testing.shape[0],dim_input=X_training.shape[1],dim_output=Y_training.shape[1],num_hidden_layers=3,num_hidden_dims=[X_training.shape[1],200,50,20,Y_training.shape[1]],keep_prob=0.80,num_minibatch=20,X_train=X_training,Y_train=Y_training,X_test=X_testing,Y_test=Y_testing)
  File ""DNN_forward_MC.py"", line 90, in __init__
    sv = tf.train.Supervisor(logdir=train_dir,is_chief=(FLAGS.task_index == 0),init_op=init_op,local_init_op=local_init_op,recovery_wait_secs=1,ready_for_local_init_op=ready_for_local_init_op)
  File ""/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 300, in __init__
    self._init_saver(saver=saver)
  File ""/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 448, in _init_saver
    saver = saver_mod.Saver()
  File ""/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1139, in __init__
    self.build()
  File ""/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1170, in build
    restore_sequentially=self._restore_sequentially)
  File ""/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 691, in build
    restore_sequentially, reshape)
  File ""/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 407, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 247, in restore_op
    [spec.tensor.dtype])[0])
  File ""/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 640, in restore_v2
    dtypes=dtypes, name=name)
  File ""/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)

  File ""/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'save/RestoreV2_10': Operation was explicitly assigned to /job:ps/task:0/device:CPU:0 but available devices are [ /job:localhost/replica:0/task:0/cpu:0 ]. Make sure the device specification refers to a valid device.
         [[Node: save/RestoreV2_10 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:ps/task:0/device:CPU:0""](save/Const, save/RestoreV2_10/tensor_names, save/RestoreV2_10/shape_and_slices)]]
`



I have mostly seen the ""cannot assign a device for operation ..."" error in the case of people using GPU without specifying allow_soft_placement=True but in my case I am using just CPUs.

I have seen another instance of this error when users were not specifying server.target as the device of a session,thereby creating a local session but this is not the case in my code.

Any help would be much appreciated.


"
11605,[solved] Reducing the binary size,"Hi , all ~
    i am now succeed in using a cross compiled lib project call tensorflow. 
    now i got tensorflow run on ios , android and linux , using one same app code . 
    any one has any questions can ask me for help.

    so my question now is turned to ""Reducing the binary size"". 
    many posts told that to config  tf_op_files.txt, but my questions are:
    1. need we also config other files such as  tf_pb_text_files.txt ,  proto_text_cc_files.txt  ... for reducing?
    2. is there any other thinking for reducing than configuring the txt files Exhaustive Attackly ?"
11604,Unable to compile a quantized graph using XLA AOT?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: Master
- **Python version**: 2.7.13
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: 8.0/5.1
- **GPU model and memory**: GTX 860M
- **Exact command to reproduce**: 

`bazel build -c opt --cxxopt='-std=c++11' --linkopt='-lm'    --cpu=armeabi-v7a    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --crosstool_top=//external:android/crosstool    //tensorflow/compiler/aot:inception_v3 --verbose_failures`

### Describe the problem
I am currently trying to use `tfcompile` to compile a quantized inception_v3 model for android, following the instructions given in the documentation [here](https://www.tensorflow.org/performance/xla/tfcompile), but I have gotten this error below:

```
INFO: Found 1 target...
ERROR: /home/kwotsin/Android/tensorflow/tensorflow/compiler/aot/BUILD:11:1: Executing genrule //tensorflow/compiler/aot:gen_inception_v3 failed: bash failed: error executing command 
  (cd /home/kwotsin/.cache/bazel/_bazel_kwotsin/655cf5567faa2deb9e3725ec794eb35d/execroot/tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=:/usr/local/cuda/lib64:/usr/local/cuda/lib64 \
    PATH=/usr/local/cuda/bin:/usr/local/cuda/bin:/home/kwotsin/bin:/home/kwotsin/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/kwotsin/Android/Sdk/tools:/home/kwotsin/Android/Sdk/platform-tools \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL=0 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/compiler/aot/tfcompile --graph=tensorflow/compiler/aot/inception_v3.pb --config=tensorflow/compiler/aot/inception_v3.config.pbtxt --entry_point=__tensorflow_compiler_aot__inception_v3 --cpp_class=inception_v3_cpp --target_triple=armv7-none-android --out_header=bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/tensorflow/compiler/aot/inception_v3.h --out_object=bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/tensorflow/compiler/aot/inception_v3.o '): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 6.
2017-07-19 19:53:26.407268: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-19 19:53:26.407310: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-19 19:53:26.407326: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-07-19 19:53:26.407330: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-19 19:53:26.407333: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-07-19 19:53:26.424064: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-07-19 19:53:26.426662: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Not found: No registered 'Const' OpKernel for XLA_CPU_JIT devices compatible with node InceptionV3/Conv2d_1a_3x3/weights/read/_224__cf__224_quantized_const = Const[dtype=DT_QUINT8, value=Tensor<type: quint8 shape: [3,3,3,32] values: [[[97 115 118]]]...>]()
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='XLA_CPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BOOL]
  device='XLA_GPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BOOL]
  device='CPU'

	 [[Node: InceptionV3/Conv2d_1a_3x3/weights/read/_224__cf__224_quantized_const = Const[dtype=DT_QUINT8, value=Tensor<type: quint8 shape: [3,3,3,32] values: [[[97 115 118]]]...>]()]]
2017-07-19 19:53:26.429093: F tensorflow/compiler/aot/tfcompile_main.cc:154] Non-OK-status: status status: Not found: No registered 'Const' OpKernel for XLA_CPU_JIT devices compatible with node InceptionV3/Conv2d_1a_3x3/weights/read/_224__cf__224_quantized_const = Const[dtype=DT_QUINT8, value=Tensor<type: quint8 shape: [3,3,3,32] values: [[[97 115 118]]]...>]()
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='XLA_CPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BOOL]
  device='XLA_GPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BOOL]
  device='CPU'

	 [[Node: InceptionV3/Conv2d_1a_3x3/weights/read/_224__cf__224_quantized_const = Const[dtype=DT_QUINT8, value=Tensor<type: quint8 shape: [3,3,3,32] values: [[[97 115 118]]]...>]()]]
/bin/bash: line 1:  6904 Aborted                 (core dumped) bazel-out/host/bin/tensorflow/compiler/aot/tfcompile --graph=tensorflow/compiler/aot/inception_v3.pb --config=tensorflow/compiler/aot/inception_v3.config.pbtxt --entry_point=__tensorflow_compiler_aot__inception_v3 --cpp_class=inception_v3_cpp --target_triple=armv7-none-android --out_header=bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/tensorflow/compiler/aot/inception_v3.h --out_object=bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/tensorflow/compiler/aot/inception_v3.o
Target //tensorflow/compiler/aot:inception_v3 failed to build
INFO: Elapsed time: 0.339s, Critical Path: 0.18s
```

Despite the ""SSE4.1 etc."" instructions that popped up, I made sure that I configured the tensorflow installation with XLA enabled, so it shouldn't have popped up. 

Also, my quantized graph was created using the Graph Transform Tool with the following command, producing a graph that worked exactly as expected:

```
/home/kwotsin/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=./frozen_model.pb \
--out_graph=./quantized_model.pb \
--inputs='Placeholder_only' \
--outputs='InceptionV3/Predictions/Softmax' \
--transforms='
  add_default_attributes
  strip_unused_nodes(type=float, shape=""1,299,299,3"")
  fold_constants(ignore_errors=true)
  fold_batch_norms
  fold_old_batch_norms
  quantize_weights
  strip_unused_nodes
  sort_by_execution_order'
```

Is XLA AOT compilation of quantized models currently supported? Because when I tried to build with a frozen, non-quantized graph, I got the correct output - a cpp object file and a header file. I thought it would be nice if XLA AOT could be used concurrently with a quantized model to obtain the maximum level of mobile optimization."
11603,//tensorflow/python:nn_test is failing on ppc64le with AssertionError: False is not true ,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
     Ubuntu 16.04 (ppc64le)
- **TensorFlow installed from (source or binary)**:
      Installed from source (v1.2.1)
- **TensorFlow version (use command below)**:
      ('v1.2.1-0-gb4957ff', '1.2.1')
- **Python version**: 
     Python 2.7.5
- **Bazel version (if compiling from source)**:
       0.4.5-2017-07-13 (@037b9b9)
- **CUDA/cuDNN version**:
     NA
- **GPU model and memory**:
      NA
- **Exact command to reproduce**:
      bazel test //tensorflow/python:nn_test

### Describe the problem
Here  testNaNs is failing , see relevant code-
https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/python/ops/nn_test.py#L835-L841

```
def testNaNs(self):
    # Test that relu(nan) = nan for various sizes.
    for i in range(18):
      x = np.zeros(i) + np.nan
      with self.test_session():
        z = nn_ops.relu(constant_op.constant(x)).eval()
        print(""\n current i value is "", i)
        print(""\n z value is "", z)
        self.assertTrue(np.isnan(z).all())
```
This test is failing ,because nn_ops.relu function returning incorrect results on ppc64le (0 vs expected nan).......(for i = 2 to 18 )

for` i = 0`  :   z value =   []    ............... (ok)
for `i = 1`  :   z value =  [ nan]  ........... (ok)
for `i = 2`  :   z value =  [ 0.  0.]  ...........(not ok on ppc64le, bcz `0. != nan`)

It looks like this is a bug in the nn_ops.relu function for ppc64le, currently I am trying to understand the reason. Please provide comments/suggestions if any. Thanks!
### Source code / logs

```
$ bazel test --test_output=errors //tensorflow/python:nn_test

......................../root/.cache/bazel/_bazel_root/725e77151072daec43bc353cb6fcb26c/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/nn_test.runfiles/org_tensorflow/tensorflow/python/ops/nn_test.py:108: RuntimeWarning: divide by zero encountered in log
  stirling_approx = z * np.log(z) - z + 0.5 * np.log(2. * np.pi * z)
/root/.cache/bazel/_bazel_root/725e77151072daec43bc353cb6fcb26c/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/nn_test.runfiles/org_tensorflow/tensorflow/python/ops/nn_test.py:108: RuntimeWarning: invalid value encountered in multiply
  stirling_approx = z * np.log(z) - z + 0.5 * np.log(2. * np.pi * z)
................F.......
======================================================================
FAIL: testNaNs (__main__.ReluTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/725e77151072daec43bc353cb6fcb26c/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/nn_test.runfiles/org_tensorflow/tensorflow/python/ops/nn_test.py"", line 841, in testNaNs
    self.assertTrue(np.isnan(z).all())
AssertionError: False is not true

----------------------------------------------------------------------
Ran 48 tests in 7.253s

FAILED (failures=1)
0.0208333333333
0.00566666666667
0.0075
0.0208333333333
0.00566666666667
0.0075
0.0208333333333
0.00566666666667
0.0075
L2Loss gradient err = 9.6958e-12
L2Normalize gradient err = 4.2424e-08
L2Normalize gradient err = 5.45829e-07
L2Normalize gradient err = 7.61142e-05
================================================================================

```"
11602,"There is a input sequence, how to calculate probability for a special output sequence?","
"
11601,"who can tell me ,where is gru backward code?i want modify it","who can tell me ,where is gru backward code?i want modify it

i want implete https://arxiv.org/pdf/1606.03401.pdf

reduce gru size"
11599,Would you mind not calling the protobuf repository protobuf?,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

Yes

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Ubuntu 16.04

- **TensorFlow installed from (source or binary)**:

Source

- **TensorFlow version (use command below)**:

1.1.0

- **Python version**: 

2.7.12

- **Bazel version (if compiling from source)**:

0.5.2
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

Currently the protobuf repo in workspace.bzl is called @protobuf. Unfortunately, in github.com/grpc/grpc, the protobuf is bind to @com_github_google_protobuf//:protobuf....So if I have anything that uses the GRPC repository it cannot be built together with Bazel.

I tried rename all @protobuf// to @com_github_google_protobuf//, but it does not work, presumably because the patch file for protobuf get in the way.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
11598,Unclear about how to make BeamSearchDecoder work,"## UPDATE: In the latest tensorflow 1.2.1, this is no longer a problem.
##  Please ignore this problem and install the latest tensorflow.

Hello, I am trying to understand the way to use BeamSearchDecoder in a seq2seq model by following the tutorial of [nmt](https://github.com/tensorflow/nmt#beam-search). However, both the documentation and error message seem to be very unclear for starters. I have wrote the minimal code for a common seq2seq purpose with beam search:
``` python
import tensorflow as tf
from tensorflow.python.layers.core import Dense

# INPUTS
X = tf.placeholder(tf.int32, [None, None])
Y = tf.placeholder(tf.int32, [None, None])
X_seq_len = tf.placeholder(tf.int32, [None])
Y_seq_len = tf.placeholder(tf.int32, [None])

# ENCODER         
encoder_out, encoder_state = tf.nn.dynamic_rnn(
    cell = tf.nn.rnn_cell.BasicLSTMCell(128), 
    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),
    sequence_length = X_seq_len,
    dtype = tf.float32)

# DECODER COMPONENTS
Y_vocab_size = 10000
decoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))
projection_layer = Dense(Y_vocab_size)
decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(128)

# TRAINING DECODER
training_helper = tf.contrib.seq2seq.TrainingHelper(
    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),
    sequence_length = Y_seq_len,
    time_major = False)
training_decoder = tf.contrib.seq2seq.BasicDecoder(
    cell = decoder_cell,
    helper = training_helper,
    initial_state = encoder_state,
    output_layer = projection_layer)
training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(
    decoder = training_decoder,
    impute_finished = True,
    maximum_iterations = tf.reduce_max(Y_seq_len))
training_logits = training_decoder_output.rnn_output

# PREDICTING_DECODER
predicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(
    cell = decoder_cell,
    embedding = decoder_embedding,
    start_tokens = tf.tile(tf.constant([1], dtype=tf.int32), [128]),
    end_token = 2,
    initial_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=10),
    beam_width = 10,
    output_layer = projection_layer,
    length_penalty_weight = 0.0)
predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(
    decoder = predicting_decoder,
    impute_finished = True,
    maximum_iterations = 2 * tf.reduce_max(Y_seq_len))
predicting_logits = predicting_decoder_output.sample_id

# LOSS
masks = tf.sequence_mask(Y_seq_len, tf.reduce_max(Y_seq_len), dtype=tf.float32)
loss = tf.contrib.seq2seq.sequence_loss(logits = training_logits, targets = Y, weights = masks)

# BACKWARD
params = tf.trainable_variables()
gradients = tf.gradients(loss, params)
clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)
train_op = tf.train.AdamOptimizer().apply_gradients(zip(clipped_gradients, params))
```
The error occurs at BeamSearchDecoder:
```
Traceback (most recent call last):
  File ""test.py"", line 48, in <module>
    length_penalty_weight = 0.0)
  File ""/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py"", line 175, in __init__
    initial_state, self._cell.state_size)
  File ""/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/nest.py"", line 319, in map_structure
    assert_same_structure(structure[0], other, check_types=check_types)
  File ""/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/nest.py"", line 146, in assert_same_structure
    % (nest1, nest2))
ValueError: The two structures don't have the same number of elements. First structure: Tensor(""tile_batch/Reshape:0"", shape=(20, ?, 128), dtype=float32), second structure: LSTMStateTuple(c=128, h=128).
```"
11597, Not found: Key <variable_name> not found in checkpoint even though it exists in meta graph,"### System information
Python version: 3.6
Tensorflow version: 1.1

Here are the commands needed to reproduce the error:

```
import tensorflow as tf
import numpy as np

    
tf.reset_default_graph()

# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3
x_data = np.random.rand(100).astype(np.float32)
y_data = x_data * 0.1 + 0.3

# Try to find values for W and b that compute y_data = W * x_data + b
# (We know that W should be 0.1 and b 0.3, but TensorFlow will
# figure that out for us.)
W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name='W')
b = tf.Variable(tf.zeros([1]), name='b')
y = W * x_data + b

# Minimize the mean squared errors.
loss = tf.reduce_mean(tf.square(y - y_data))
optimizer = tf.train.GradientDescentOptimizer(0.5)
opt_op = optimizer.minimize(loss)

# Track the moving averages of all trainable variables.
ema = tf.train.ExponentialMovingAverage(decay=0.9999)
variables = tf.trainable_variables()
print(variables)
averages_op = ema.apply(tf.trainable_variables())
with tf.control_dependencies([opt_op]):
    train_op = tf.group(averages_op)

# Before starting, initialize the variables.  We will 'run' this first.
init = tf.global_variables_initializer()

saver = tf.train.Saver(tf.trainable_variables())

# Launch the graph.
sess = tf.Session()
sess.run(init)

# Fit the line.
for _ in range(201):
    sess.run(train_op)

w_reference = sess.run('W/ExponentialMovingAverage:0')
b_reference = sess.run('b/ExponentialMovingAverage:0')

saver.save(sess, os.path.join(""model_ex1""))

tf.reset_default_graph()

tf.train.import_meta_graph(""model_ex1.meta"")
sess = tf.Session()

print('------------------------------------------------------')
for var in tf.global_variables():
    print('all variables: ' + var.op.name)
for var in tf.trainable_variables():
    print('normal variable: ' + var.op.name)
for var in tf.moving_average_variables():
    print('ema variable: ' + var.op.name)
print('------------------------------------------------------')


restore_vars = {}

ema = tf.train.ExponentialMovingAverage(1.0)
for var in tf.trainable_variables():
    print('%s: %s' % (ema.average_name(var), var.op.name))
    restore_vars[ema.average_name(var)] = var

saver = tf.train.Saver(restore_vars, name='ema_restore')

saver.restore(sess, os.path.join(""model_ex1""))

w_restored = sess.run('W:0')
b_restored = sess.run('b:0')

print(w_reference)
print(w_restored)
print(b_reference)
print(b_restored)

```"
11592,Cross compile TensorFlow for armeabi-v7a failed,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.2.1
- **Python version**:
 2.7.12
- **Bazel version (if compiling from source)**:
0.5.2
- **CUDA/cuDNN version**:
n/a
- **GPU model and memory**:
n/a
- **Exact command to reproduce**:
bazel build -c opt tensorflow/core:android_tensorflow_lib --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I was able to cross-compile TensorFlow to armeabi-v7a using TensorFlow 1.0. But when I used the same command using TensorFlow 1.2.1, the compilation failed. Please find the detailed error message in the Source code/logs section. 

The command I used is as follows:
bazel build -c opt tensorflow/core:android_tensorflow_lib --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

ERROR: /home/yitao/.cache/bazel/_bazel_yitao/c0c4f6e5ab173d18db201c4c55c4dc60/external/protobuf/BUILD:133:1: C++ compilation of rule '@protobuf//:protobuf' failed: false failed: error executing command /bin/false -MD -MF bazel-out/stub_armeabi-v7a-opt/bin/external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/stubs/substitute.pic.d ... (remaining 26 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
Target //tensorflow/core:android_tensorflow_lib failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 42.154s, Critical Path: 6.27s


"
11591,slice_input_producer bug (slows down exponentially for larger datasets),"My input pipeline involves sampling a filename and label with slice_input_producer, then data augmentation, then batching with tf.train.shuffle_batch. For smaller datasets (<1M), this works fine. For larger datasets, slice_input_producer slows down, even though all it's doing is sampling from two lists. It's bad enough that it's several orders of magnitude slower than all the rest of my input pipeline and training combined. I wrote a quick test to measure the time per call to slice_input_producer for different lengths of input lists:
```
import time

import tensorflow as tf

rept = 1000
for datasize in [1e3,1e6,1e7,1e8]:
    sym_x = tf.train.slice_input_producer(
        [list(range(int(datasize)))], shuffle=True,
        capacity=10)
    with tf.Session() as sess:
        sess.run(tf.local_variables_initializer())
        sess.run(tf.global_variables_initializer())
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess=sess, coord=coord)
        start_time = time.time()
        for r in range(rept):
            x = sess.run(sym_x)
        print('Datalength: %i  ---  time = %.3f ms/sample'
              % (datasize,(time.time()-start_time)/rept*1000))
        coord.request_stop()
        coord.join(threads)
        sess.close()
```

Which gives:

> Datalength: 1000  ---  time = 0.172 ms/sample
> Datalength: 1000000  ---  time = 0.207 ms/sample
> Datalength: 10000000  ---  time = 0.537 ms/sample
> Datalength: 100000000  ---  time = 13.991 ms/sample


python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
('v1.2.0-0-g12f033d', '1.2.0')"
11589,[beginner][bug] TensorFlow doesn't seem to be decomposing GradientDescent into XLA ops,"### System information
- **Tensorflow compiled on a branch of a fork**: https://github.com/singam-sanjay/tensorflow/tree/trace_learning_XLA_ops. Contains extra LOG(INFO) and std::cout statements to notify the names of the functions being called.
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.2.0-1878-ga5066f6', '1.2.1')
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: release 0.5.2
- **CUDA/cuDNN version**: CUDA 8
- **GPU model and memory**: NVIDIA Quadro K1200,  4 GB
- **Exact command to reproduce**: python <tflow_parent>/tensorflow/tensorflow/example/tutorials/mnist/mnist/mnist_softmax_xla.py
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/1157069/tf_env.txt)

### Context
My work involves securing Tensorflow's computations. The approach I've planned to take is to add XLA ops during the conversion of TF ops to XLA ops,  which are lowered into functions calls (in LLVM-IR) that send data to and receive data from a secure environment.

I'm currently verifying if both training and inference TF Ops are lowered to XLA ops.

### The Problem
I've made some [changes to the code](https://github.com/singam-sanjay/tensorflow/tree/trace_learning_XLA_ops) to highlight when some OpKernels and XlaOpKernels are being invoked / lowered. I've used the mnist_softmax_xla.py example to observe tensorflow's behaviour.

I found that ""ApplyGradientDescentGPU"" was being called 2000 times (2 layer network and 1000 iterations, clearly not JIT) instead of ResourceApplyGradientDescent, yet other Ops including MatMul and subtraction seem to have been optimized by XLA.

Why is gradient descent not optimized by XLA ?

### Source code / logs
The output text lost its structure when I pasted it. So, here are the screenshots.
![github 1](https://user-images.githubusercontent.com/6310523/28340192-80abeec2-6c2c-11e7-8fb3-d3ec6ae1676e.PNG)
![github 2](https://user-images.githubusercontent.com/6310523/28340198-8593c450-6c2c-11e7-9c62-dd06e93e2b5a.PNG)
![github 3](https://user-images.githubusercontent.com/6310523/28340204-8909b69e-6c2c-11e7-9e16-afe089ccde8e.PNG)
![github 4](https://user-images.githubusercontent.com/6310523/28340210-8c3c4a02-6c2c-11e7-93b1-a416035b3ed8.PNG) Note that the first 2 kernels have been called a 100 times each.

### Related
 Also, it would be helpful if someone could answer [Tensow - XLA | Passing tensors to external functions at runtime](https://stackoverflow.com/questions/45146444/tensorflow-xla-how-are-tf-ops-lowered-to-xla-for-training) , as it is **the** important assumption of my approach that Tensorflow can allow such XLA ops. I appreciate any information on its feasibility as I could start on a different approach as soon as possible.

Thanks,
Sanjay"
11588,GPU kernel for SVD,"### System information
-Linux Ubuntu 16.04
- TensorFlow installed from (source )
- python v3.5 
- **GPU 8gb**:

### Describe the problem
I tried to run code that used tf.svd on GPU but it gives me an error. Is it correct that tensorflow does not have GPU kernels for SVD?

### Source code / logs
[[Node: Svd = Svd[T=DT_FLOAT, compute_uv=false, full_matrices=false](x)]]
[[Node: map_1/while/nuclear_norm_09660f0e_1 = nuclear_norm_09660f0e[_device=""/job:localhost/replica:0/task:0/gpu:0""](map_1/while/Reshape_1, ^map_1/while/Identity)]]
[[Node: Adagrad_1/update/_64 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_690_Adagrad_1/update"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]"
11585,Keras API reproducibility,"If any new option was added to ensure a reproducible results by setting some seed parameter
?
In Keras the issue was discussed [here](https://github.com/fchollet/keras/issues/2743)
According the issue the instability in results is due to weights random initialization."
11583,"tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[49] = 60000 is not in [0, 60000)","I try to run the Sequence-to-Sequence Models with tensorflow, but when I run the training set, I have the problem like this

> Traceback (most recent call last):
>   File ""/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1022, in _do_call
>     return fn(*args)
>   File ""/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1004, in _run_fn
>     status, run_metadata)
>   File ""/Users/loohaze/anaconda/envs/python3/lib/python3.6/contextlib.py"", line 89, in __exit__
>     next(self.gen)
>   File ""/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 469, in raise_exception_on_not_ok_status
>     pywrap_tensorflow.TF_GetCode(status))
> tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[49] = 60000 is not in [0, 60000)
> 	 [[Node: model_with_buckets/embedding_attention_seq2seq_2/embedding_attention_decoder/embedding_lookup_15 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[""loc:@embedding_attention_seq2seq/embedding_attention_decoder/embedding""], validate_indices=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](embedding_attention_seq2seq/embedding_attention_decoder/embedding/read, _recv_decoder15_0)]]
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""translate.py"", line 322, in <module>
>     tf.app.run()
>   File ""/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 44, in run
>     _sys.exit(main(_sys.argv[:1] + flags_passthrough))
>   File ""translate.py"", line 319, in main
>     train()
>   File ""translate.py"", line 210, in train
>     target_weights, bucket_id, False)
>   File ""/Users/loohaze/Documents/models/tutorials/rnn/translate/seq2seq_model.py"", line 251, in step
>     outputs = session.run(output_feed, input_feed)
>   File ""/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 767, in run
>     run_metadata_ptr)
>   File ""/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 965, in _run
>     feed_dict_string, options, run_metadata)
>   File ""/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
>     target_list, options, run_metadata)
>   File ""/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
>     raise type(e)(node_def, op, message)
> tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[49] = 60000 is not in [0, 60000)
> 	 [[Node: model_with_buckets/embedding_attention_seq2seq_2/embedding_attention_decoder/embedding_lookup_15 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[""loc:@embedding_attention_seq2seq/embedding_attention_decoder/embedding""], validate_indices=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](embedding_attention_seq2seq/embedding_attention_decoder/embedding/read, _recv_decoder15_0)]]
> 
> Caused by op 'model_with_buckets/embedding_attention_seq2seq_2/embedding_attention_decoder/embedding_lookup_15', defined at:
>   File ""translate.py"", line 322, in <module>
>     tf.app.run()
>   File ""/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 44, in run
>     _sys.exit(main(_sys.argv[:1] + flags_passthrough))
>   File ""translate.py"", line 319, in main
>     train()
>   File ""translate.py"", line 178, in train
>     model = create_model(sess, False)
>   File ""translate.py"", line 136, in create_model
>     dtype=dtype)
>   File ""/Users/loohaze/Documents/models/tutorials/rnn/translate/seq2seq_model.py"", line 179, in __init__
>     softmax_loss_function=softmax_loss_function)
>   File ""/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py"", line 1180, in model_with_buckets
>     decoder_inputs[:bucket[1]])
>   File ""/Users/loohaze/Documents/models/tutorials/rnn/translate/seq2seq_model.py"", line 178, in <lambda>
>     lambda x, y: seq2seq_f(x, y, False),
>   File ""/Users/loohaze/Documents/models/tutorials/rnn/translate/seq2seq_model.py"", line 142, in seq2seq_f
>     dtype=dtype)
>   File ""/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py"", line 876, in embedding_attention_seq2seq
>     initial_state_attention=initial_state_attention)
>   File ""/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py"", line 772, in embedding_attention_decoder
>     embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs
>   File ""/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py"", line 772, in <listcomp>
>     embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs
>   File ""/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py"", line 111, in embedding_lookup
>     validate_indices=validate_indices)
>   File ""/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1359, in gather
>     validate_indices=validate_indices, name=name)
>   File ""/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
>     op_def=op_def)
>   File ""/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2395, in create_op
>     original_op=self._default_original_op, op_def=op_def)
>   File ""/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
>     self._traceback = _extract_stack()
> 
> InvalidArgumentError (see above for traceback): indices[49] = 60000 is not in [0, 60000)
> 	 [[Node: model_with_buckets/embedding_attention_seq2seq_2/embedding_attention_decoder/embedding_lookup_15 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[""loc:@embedding_attention_seq2seq/embedding_attention_decoder/embedding""], validate_indices=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](embedding_attention_seq2seq/embedding_attention_decoder/embedding/read, _recv_decoder15_0)]]
> 

It seems the problem happens randomly when running training set. It means when I have this problem,I can continue and this may not happen in next steps.
I checked this problem on previous issues, but I am sure that I have set the parameter of vocabulary size
Here are my input commands:

> python translate.py --data_dir /private/tmp/ch-en  --train_dir /private/tmp/ch-en/train_result --size=256 --num_layers=2 --steps_per_checkpoint=200 --from_vocab_size=60000 --to_vocab_size=60000

So how to solve this problem?
"
11582,Issue while using AttentionWrapper.,"I am getting issue related to miss match of state and output. But I am unable to figure the issue.
It would be really appreciated if someone can guide me. Thanks in advance.
I am using tensorfow-gpu==1.2.1, with 1080 Ti graphics.

Error is as below:
ValueError: Shapes (8, 522) and (8, 512) are incompatible

Error occurs in the file ""attention_wrapper.py"" in the method named ""call"" at line 708

cell_output, next_cell_state = self._cell(cell_inputs, cell_state)

I was able to figure out that it is adding the attention_size to the shape and so there is a mismatch.
But I have no idea how to fix it.
The code is as below, hyper-parameters are declared as below (test purpose).
`
batch_size= 8
number_of_units_per_layer= 512
number_of_layers = 3
attn_size= 10
def build_decoder_cell(enc_output, enc_state, source_sequence_length, attn_size, batch_size):

    encoder_outputs = enc_output
    encoder_last_state = enc_state
    encoder_inputs_length = source_sequence_length

    attention_mechanism = attention_wrapper.LuongAttention(
            num_units=attn_size, memory=encoder_outputs,
            memory_sequence_length=encoder_inputs_length,
            scale=True,
            name='LuongAttention' )

    # Building decoder_cell
    decoder_cell_list = [
        build_single_cell() for i in range(num_layers)]

    decoder_initial_state = encoder_last_state

    def attn_decoder_input_fn(inputs, attention):
        #if not self.attn_input_feeding:
        #    return inputs

        # Essential when use_residual=True
        _input_layer = Dense(size, dtype=tf.float32,
                            name='attn_input_feeding')
        return _input_layer(array_ops.concat([inputs, attention], -1))


    # AttentionWrapper wraps RNNCell with the attention_mechanism
    # Note: We implement Attention mechanism only on the top decoder layer
    decoder_cell_list[-1] = attention_wrapper.AttentionWrapper(
        cell=decoder_cell_list[-1],
        attention_mechanism=attention_mechanism,
        attention_layer_size=attn_size,
        #cell_input_fn=attn_decoder_input_fn,
        initial_cell_state=encoder_last_state[-1],
        alignment_history=False,
        name='Attention_Wrapper')

    # To be compatible with AttentionWrapper, the encoder last state
    # of the top layer should be converted into the AttentionWrapperState form
    # We can easily do this by calling AttentionWrapper.zero_state

    # Also if beamsearch decoding is used, the batch_size argument in .zero_state
    # should be ${decoder_beam_width} times to the origianl batch_size
    #batch_size = self.batch_size if not self.use_beamsearch_decode \
    #    else self.batch_size * self.beam_width
    initial_state = [state for state in encoder_last_state]

    initial_state[-1] = decoder_cell_list[-1].zero_state(
        batch_size=batch_size, dtype=tf.float32)
    decoder_initial_state = tuple(initial_state)

    return tf.contrib.rnn.MultiRNNCell(decoder_cell_list), decoder_initial_state`

Thank you once again."
11578,Cannot build TensorFLow with GPU support when using TensorFlow as an external dependency,"Currently my project uses TensorFlow as an external dependency (local repository). When I build my project with CPU only, the build is successful. However if I add --config=-cuda the build will succeed but without the GPU part (yes, I have run ./configure). The warning I get is: WARNING: Config values are not defined in any .rc file: cuda. This does not happen if I build my project inside TensorFlow's source tree.

"
11575,unique in special axis,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.2
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
cuDNN v5.1
- **GPU model and memory**:
pascal titan x
- **Exact command to reproduce**:
None

**Feature Request**
For getting unique row, we can use numpy's `unique` by specifying the axis. However, in tensorflow, `unique` only support 1D tensor, which is not convenient."
11574,The same code for tensorboard test generate .mm file which should be .user file,"I use my classmate's file for tensorboard test. But, this .py file generate .mm file not a .user file.

Codes are as follows:
import tensorflow as tf
import math
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)

def conv_layer(input,channels_in,channels_out,strides, name=""conv"",):
    with tf.name_scope(name):
        w = tf.Variable(tf.truncated_normal([5,5,channels_in,channels_out],stddev=0.1),name=""W"")
        b = tf.Variable(tf.ones([channels_out])/10,name=""B"")
        conv = tf.nn.conv2d(input=input,filter=w,strides=[1,strides,strides,1],padding=""SAME"")
        # print conv
        act = tf.nn.relu(conv + b)
        tf.summary.histogram(""weight"",w)
        tf.summary.histogram(""bias"",b)
        tf.summary.histogram(""activations"",act)
        # print act
        return act

def fc_layer(input,channels_in,channels_out, name=""fc""):
    with tf.name_scope(name):
        w = tf.Variable(tf.truncated_normal([channels_in,channels_out],stddev=0.1),name=""W"")
        b = tf.Variable(tf.ones([channels_out])/10,name=""B"")
        print input
        act = tf.nn.relu(tf.matmul(input, w) + b)
        # act = tf.matmul(input, w) + b
        return act

def compatible_convolutional_noise_shape(Y):
    noiseshape = tf.shape(Y)
    noiseshape = noiseshape * tf.constant([1,0,0,1]) + tf.constant([0,1,1,0])
    return noiseshape

x = tf.placeholder(dtype=tf.float32,shape=[None,784],name=""x"")
y = tf.placeholder(dtype=tf.float32,shape=[None,10],name=""lables"")

# learning rate
lr = tf.placeholder(tf.float32)
# dropout
pkeep = tf.placeholder(tf.float32)
pkeep_conv = tf.placeholder(tf.float32)
# batch_normal
tst = tf.placeholder(tf.bool)
iter = tf.placeholder(tf.float32)


x_image = tf.reshape(x,shape=[-1,28,28,1])
tf.summary.image(""input"",x_image,6)

# creat net
conv1 = conv_layer(x_image,1,4,1,""conv_1"")
# conv1_dr = tf.nn.dropout(conv1,pkeep_conv,compatible_convolutional_noise_shape(conv1))
# pool1 = tf.nn.max_pool(value=conv1,ksize=[1,2,2,1],strides=[1,2,2,1],padding=""SAME"")

conv2 = conv_layer(conv1,4,8,2,""conv_2"")
# conv1_dr = tf.nn.dropout(conv1,pkeep_conv,compatible_convolutional_noise_shape(conv1))
# pool2 = tf.nn.max_pool(value=conv2,ksize=[1,2,2,1],strides=[1,2,2,1],padding=""SAME"")
conv3 = conv_layer(conv2,8,16,2,""conv_3"")

flattened = tf.reshape(conv3,[-1, 7 * 7 * 16])
fc1 = fc_layer(flattened,7 * 7 * 16, 200,""fc1"")
# fc1_dr = tf.nn.dropout(fc1,pkeep)

w5 = tf.Variable(tf.truncated_normal([200,10],stddev=0.1))
b5 = tf.Variable(tf.ones([10])/10)
logits = tf.matmul(fc1,w5)+b5

with tf.name_scope(""loss""):
    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=logits))
    tf.summary.scalar(""loss"",cross_entropy)
with tf.name_scope(""BP""):
    train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)

with tf.name_scope(""accuracy""):
    correct_prediction = tf.equal(tf.argmax(logits,1),tf.argmax(y,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction,dtype=tf.float32))
    tf.summary.scalar(""accuracy"",accuracy)

# writer = tf.summary.FileWriter(""~/workspace/TensorFlow_Test/1"")

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    merged_summary = tf.summary.merge_all()
    writer = tf.summary.FileWriter(""/tmp/mnist_demo/"")
    writer.add_graph(sess.graph)

    # learning rate decay
    max_learning_rate = 0.02
    min_learning_rate = 0.001
    decay_speed = 2000
    for i in range(2001):
        batch = mnist.train.next_batch(100)

        learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-1/decay_speed)
        if i % 5 == 0:
            s = sess.run(merged_summary,feed_dict={x:batch[0],y:batch[1],lr:learning_rate,pkeep:0.9,pkeep_conv:0.9})
            writer.add_summary(s,i)

        if i  % 100 == 0:
            [train_accuracy] = sess.run([accuracy], feed_dict={x: mnist.test.images, y: mnist.test.labels,lr:learning_rate,pkeep:1.0,pkeep_conv:1.0})
            print(""step %d, test accuracy is %g"" %(i,train_accuracy))
        sess.run(train_step,feed_dict={x:batch[0],y:batch[1],lr:learning_rate,pkeep:0.9,pkeep_conv:0.9})

This file runs well on my classmate's machine........."
11572,How to install tensorflow c#,"Hi,
I am trying to build email spam filtering .I have already done when email has text only but the problem when deal with images



Any help how to install tensorflow and deal with images. 

Thanks"
11571,No Module Named '_pywrap_tensorflow_internal' (still without working solution),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
	Yes, it is one general import command (see below).
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
	Windows 10
- **TensorFlow installed from (source or binary)**:
	From source (nightly build  tensorflow_gpu-1.2.1-cp35-cp35m-win_amd64.whl from 2017/07/13).
- **TensorFlow version (use command below)**:
	1.2.1 (command does not work, it includes the failed command)
- **Python version**: 
	3.5.2
- **Bazel version (if compiling from source)**:
	Not compiled from source.
- **CUDA/cuDNN version**:
	CUDA 8.0, cudnn64_5.dll (Windows file description is: NVIDIA CUDA CUDNN Library. Version 8.0.54)
- **GPU model and memory**:
	 GPU NVidia Geforce 1050
- **Exact command to reproduce**:
	import tensorflow

### Describe the problem
Importing TensorFlow causes an error:
```
$ ipython
Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)]
Type 'copyright', 'credits' or 'license' for more information
IPython 6.1.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import tensorflow
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
c:\users\steph\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     17         try:
---> 18             return importlib.import_module(mname)
     19         except ImportError:

c:\users\steph\appdata\local\programs\python\python35\lib\importlib\__init__.py in import_module(name, package)
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127

c:\users\steph\appdata\local\programs\python\python35\lib\importlib\_bootstrap.py in _gcd_import(name, package, level)

c:\users\steph\appdata\local\programs\python\python35\lib\importlib\_bootstrap.py in _find_and_load(name, import_)

c:\users\steph\appdata\local\programs\python\python35\lib\importlib\_bootstrap.py in _find_and_load_unlocked(name, import_)

c:\users\steph\appdata\local\programs\python\python35\lib\importlib\_bootstrap.py in _load_unlocked(spec)

c:\users\steph\appdata\local\programs\python\python35\lib\importlib\_bootstrap.py in module_from_spec(spec)

c:\users\steph\appdata\local\programs\python\python35\lib\importlib\_bootstrap_external.py in create_module(self, spec)

c:\users\steph\appdata\local\programs\python\python35\lib\importlib\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)

ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
c:\users\steph\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     40     sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_GLOBAL)
---> 41   from tensorflow.python.pywrap_tensorflow_internal import *
     42   from tensorflow.python.pywrap_tensorflow_internal import __version__

c:\users\steph\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
     20             return importlib.import_module('_pywrap_tensorflow_internal')
---> 21     _pywrap_tensorflow_internal = swig_import_helper()
     22     del swig_import_helper

c:\users\steph\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     19         except ImportError:
---> 20             return importlib.import_module('_pywrap_tensorflow_internal')
     21     _pywrap_tensorflow_internal = swig_import_helper()

c:\users\steph\appdata\local\programs\python\python35\lib\importlib\__init__.py in import_module(name, package)
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127

ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-a649b509054f> in <module>()
----> 1 import tensorflow

c:\users\steph\appdata\local\programs\python\python35\lib\site-packages\tensorflow\__init__.py in <module>()
     22
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26

c:\users\steph\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\__init__.py in <module>()
     47 import numpy as np
     48
---> 49 from tensorflow.python import pywrap_tensorflow
     50
     51 # Protocol buffers

c:\users\steph\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     50 for some common reasons and solutions.  Include the entire stack trace
     51 above this error message when asking for help."""""" % traceback.format_exc()
---> 52   raise ImportError(msg)
     53
     54 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""c:\users\steph\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""c:\users\steph\appdata\local\programs\python\python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\steph\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\users\steph\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\users\steph\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""c:\users\steph\appdata\local\programs\python\python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```


### Source code / logs
check_tensorflow.py produces:

> $ python check_tensorflow.py
> ERROR: Failed to import the TensorFlow module.
> 
> - Python version is 3.5.
> 
> - TensorFlow is installed at: C:\Users\Steph\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow
> 
> - All required DLLs are present. Please open an issue on the
>   TensorFlow GitHub page: https://github.com/tensorflow/tensorflow/issues

The issue was already reported to Stackoverflow:
[Stackoverflow issue](https://stackoverflow.com/questions/45090440/no-module-named-pywrap-tensorflow-internal-still-without-working-solution/45091193#45091193)
"
11570,LSTM with Projection issue,"I wanna build a multi-layer LSTM CTC network with projection in LSTM.

        cell = tf.nn.rnn_cell.LSTMCell(FLAGS.n_hidden, 
                                       initializer=tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_AVG', uniform=True), 
                                       num_proj = FLAGS.n_hidden/2, 
                                       state_is_tuple=True)
        init_stat = cell.zero_state(FLAGS.batch_size, dtype=tf.float32)
        _inputs = output_conv
        for layer in range(FLAGS.recur_layer):
            with tf.name_scope('LSTM_{}'.format(layer+1)) as scope: 
                outputs, _ = tf.nn.dynamic_rnn(cell, _inputs, seq_len, initial_state=init_stat, dtype=tf.float32)
                _inputs = tf.nn.relu(outputs)
                tf.summary.histogram('LSTM_{}'.format(layer+1), _inputs)
                _inputs = tf.contrib.layers.batch_norm(_inputs, center=True, scale=True, is_training=is_training, updates_collections=None)
                tf.summary.histogram('LSTM_{}_bn'.format(layer+1), _inputs)
        outputs = _inputs

the _inputs size is 1024, FLAGS.n_hidden=1024,
but when I run it, there is an error:

 outputs, _ = tf.nn.dynamic_rnn(cell, _inputs, seq_len, initial_state=init_stat, dtype=tf.float32)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py"", line 574, in dynamic_rnn
    dtype=dtype)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py"", line 737, in _dynamic_rnn_loop
    swap_memory=swap_memory)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2770, in while_loop
    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2599, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2549, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py"", line 720, in _time_step
    skip_conditionals=True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py"", line 206, in _rnn_step
    new_output, new_state = call_cell()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py"", line 708, in <lambda>
    call_cell = lambda: cell(input_t, state)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 180, in __call__
    return super(RNNCell, self).__call__(inputs, state)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py"", line 441, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 542, in call
    lstm_matrix = _linear([inputs, m_prev], 4 * self._num_units, bias=True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 1017, in _linear
    initializer=kernel_initializer)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 1065, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 962, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 360, in get_variable
    validate_shape=validate_shape, use_resource=use_resource)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 183, in _rnn_get_variable
    variable = getter(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 352, in _true_getter
    use_resource=use_resource)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 669, in _get_single_variable
    found_var.get_shape()))
ValueError: Trying to share variable rnn/lstm_cell/kernel, but specified shape (1024, 4096) and found shape (1536, 4096).

How can I fix it?  Using the num_proj, is there any example of it?

Many thanks
Xin.q.
"
11569,Why use memory_layer in all cases?,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.2

In current AttentionWrapper, both [BahdanauAttention](https://github.com/tensorflow/tensorflow/blob/c996c7b381a8eb54f9c7d7b298b24b1715645b68/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L416-L417) and [LuongAttention](https://github.com/tensorflow/tensorflow/blob/c996c7b381a8eb54f9c7d7b298b24b1715645b68/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L295-L296) enforce to use a memory_layer. According to my understanding, it is needed only when the depth of memory is not matched with that of query_layer. Is it intended to be in this manner?

@ebrevdo , would you mind having a look at this?"
11564,XLA bugs on training accuracy,"-----------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.2.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0 / 6.0
- **GPU model and memory**: NVIDIA TITAN Xp 12GB
- **Exact command to reproduce**:

(At the tensorflow/model/inception directory)
bazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=32 --train_dir=/tmp/imagenet_train --data_dir=/tmp/imagenet_data
bazel-bin/inception/imagenet_eval --checkpoint_dir=/tmp/imagenet_train --eval_dir=/tmp/imagenet_eval

== cat /etc/issue ===============================================
Linux Ares 4.8.0-58-generic #63~16.04.1-Ubuntu SMP Mon Jun 26 18:08:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.2 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux Ares 4.8.0-58-generic #63~16.04.1-Ubuntu SMP Mon Jun 26 18:08:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.1)
protobuf (3.3.0)
tensorflow (1.2.1)
tensorflow-tensorboard (0.1.2)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.2.1
tf.GIT_VERSION = v1.2.1-2-gc996c7b
tf.COMPILER_VERSION = v1.2.1-2-gc996c7b
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64:/usr/local/cuda-8.0/lib64:/usr/local/cuda/extras/CUPTI/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Tue Jul 18 09:26:11 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 381.09                 Driver Version: 381.09                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN Xp            Off  | 0000:01:00.0     Off |                  N/A |
| 48%   75C    P2   287W / 250W |  11771MiB / 12189MiB |     54%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1005    G   /usr/lib/xorg/Xorg                              18MiB |
|    0     30938    C   /usr/bin/python                              11737MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7


### Describe the problem
I activated XLA and trained the Inception model.
When I verify the training results, the accuracy is always 0.001.
(It is considered that the training is not performed normally.)

When XLA is disabled, normal accuracy is achieved.

### Source code / logs
I added some codes of model/inception/inception/inception_train.py to enable XLA.
I attached the file.

[inception_train.zip](https://github.com/tensorflow/tensorflow/files/1154355/inception_train.zip)


"
11557,TF/Keras RNN initial_state with stateful = false,"I'm trying to put different initial hidden state into recurrent layers. I saw many issues about initializing states using keras and most of the solutions were using layer.reset_state(state). But in my application, I have to set stateful = False and I cannot use reset_state. As it is written in the document, I tried initial_state argument in calling layers. 

Using the code below, I compared the outputs and the resulting outputs were always the same. I tried different initializers and different initial states but the both outputs were the same.

**When the stateful is False, is there other possible way to set initial hidden state???**

for size in dimension[1:]:
    initial_ = tf.ones(shape=[batchnum, size])
    recurrent_layer = GRU(units=size, return_sequences=True, kernel_initializer=one,
                                 recurrent_initializer=one)
    recurrent_layertest = GRU(units=size, return_sequences=True, kernel_initializer=one,
                                 recurrent_initializer=one)

    x = recurrent_layer(inputs=x)
    xtest = recurrent_layertest(inputs=xtest, initial_state = initial_)

In my last issue https://github.com/tensorflow/tensorflow/issues/11553, there was a suggestion to put inputs as [state, initial state] and It didn't work saying GRU has no attribute 'states'. I think this happens because the stateful is False."
11554,Feature request: GPU support for tf.bincount,"Hi. In my opinion, `tf.bincount` is a very useful and simple function to build weighted histograms like in the example below, but recently i learned that unfortunately does not have GPU support. It could be very useful to run it on GPU, since i can't figure any simple way to do a weighted histogram efficiently. Thanks.

```
sess = tf.Session()

values = tf.random_uniform((1,50),10,20,dtype = tf.int32)
weights = tf.random_uniform((1,50),0,1,dtype = tf.float32)

counts = tf.bincount(values, weights = weights)

histogram = sess.run(counts)
print(histogram)
```
"
11553,initial_state in keras rnn," I'm trying to put different initial hidden state into recurrent layers. I saw many issues about initializing states using keras and most of the solutions were using **layer.reset_state(state)**. But in my application, I have to set stateful = False and I cannot use reset_state.
As it is written in the document, I tried **initial_state** argument in calling layers.

Using the code below, I compared the outputs and the resulting outputs were always the same.
I tried different initializers and different initial states but the both outputs were the same.

**When the stateful is False, is there other possible way to set initial hidden state???**

        for size in dimension[1:]:
            initial_ = tf.ones(shape=[batchnum, size])
            recurrent_layer = GRU(units=size, return_sequences=True, kernel_initializer=one,
                                         recurrent_initializer=one)
            recurrent_layertest = GRU(units=size, return_sequences=True, kernel_initializer=one,
                                         recurrent_initializer=one)

            x = recurrent_layer(inputs=x)
            xtest = recurrent_layertest(inputs=xtest, **initial_state = initial_**)"
11552,DecodeCSV outputs an unexpectedly result,"I recently feel confused while using `DecodeCSV` operator. The meaning of `record_defaults` is different from that described in the documentation.

Here is the description in documentation:
```
record_defaults: One tensor per column of the input record, with either a
  scalar default value for that column or empty if the column is required.
```

Problem 1 : The size of `record_defaults` limit the size of output
```python
>>> value=[""a,b,c""]
>>> parts_val = parsing_ops.decode_csv(value, field_delim="","", record_defaults = [[""""]])
>>> parts_val
[<tf.Tensor 'DecodeCSV_10:0' shape=(1,) dtype=string>]
```

Problem 2 : If `record_defaults` contains empty element, the type of output is always float32. In the same time, it failed to check the existence of value.
```python
>>> value=[""a,b,c""]
>>> parts_val = parsing_ops.decode_csv(value, field_delim="","", record_defaults = [[""""], [], []])
>>> parts_val
[<tf.Tensor 'DecodeCSV_13:0' shape=(1,) dtype=string>, <tf.Tensor 'DecodeCSV_13:1' shape=(1,) dtype=float32>, <tf.Tensor 'DecodeCSV_13:2' shape=(1,) dtype=float32>]
```

Maybe I understand it in a wrong way. Any comment is appreciated. Thanks."
11550,Segmentation fault occured when I install tensorflow r1.2  with bazel,"------------------------
Hi, when I builded tensorflow r1.2 I got following error:
`tensorflow-1.2--mkl/tensorflow/contrib/framework/BUILD:108:1: Executing genrule //tensorflow/contrib/framework:gen_variable_ops_pygenrule failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 11.
/bin/bash: line 1: 133735 Segmentation fault      (core dumped) bazel-out/host/bin/tensorflow/contrib/framework/gen_gen_variable_ops_py_wrappers_cc 0 > bazel-out/local-opt/genfiles/tensorflow/contrib/framework/python/ops/gen_variable_ops.py
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps`
### System information
- System: Red Hat 4.8.5
- Python: 2.7.5
- bazel: 0.5.2
- Tensorflow: r1.2

### Commands:
```
wget https://github.com/bazelbuild/bazel/releases/download/0.5.2/bazel-0.5.2-installer-linux-x86_64.sh
chmod +x bazel-0.5.2-installer-linux-x86_64.sh
sudo sh ./zel-0.5.2-installer-linux-x86_64.sh

git clone https://github.com/tensorflow/tensorflow/
cd tensorflow
git checkout r1.2
./configure
 
Please specify the location of python. [Default is /vir_tensorflow/bin/python]: 
Found possible Python library paths:
  /vir_tensorflow/lib/python2.7/site-packages
Please input the desired Python library path to use.  Default is [/vir_tensorflow/lib/python2.7/site-packages]

Using python library path: /vir_tensorflow/lib/python2.7/site-packages
Do you wish to build TensorFlow with MKL support? [y/N] y
MKL support will be enabled for TensorFlow
Do you wish to download MKL LIB from the web? [Y/n] y
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 
Do you wish to use jemalloc as the malloc implementation? [Y/n] 
jemalloc enabled
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] 
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] 
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] 
No XLA support will be enabled for TensorFlow
Do you wish to build TensorFlow with VERBS support? [y/N] 
No VERBS support will be enabled for TensorFlow
Do you wish to build TensorFlow with OpenCL support? [y/N] 
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] 
No CUDA support will be enabled for TensorFlow
INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.
Configuration finished

bazel build --config=opt --config=mkl --copt=""-DEIGEN_USE_VML"" //tensorflow/tools/pip_package:build_pip_package
```
Are there any mistakes ? "
11549,Running Model on tensorflow Distribution can't  save model for tensorflow serving,"### System information
- **OS Platform CentOS 7.1**:
- **TensorFlow installed from binary)**:
- **TensorFlow version 1.2.1**:
- **Python version 2.7**: 
- **Bazel version 0.4.5**:

### Describe the problem
Situation One: I add saving model for tensorflow serving based on mnist model distribution version.
when i run this model on the same machine and start one ps server and two workers. saving model can work well.
Situation Two: but if the model runs on three different machines(eg: A, B, C). I start ps server on A machine, and B, C machine runs worker, there is something wrong on saving model code.

Situation Three: then I try another situation, run ps server and one worker on A, another worker on B, the worker running on A machine saves model. it can work well again.

Situation Four: and run ps server on A, the other worker on B, it can also work.

I think it is an issue of tensorflow distribution on saving model using saved_model_builder.

### Source code / logs
my mnist distribution code as bellow:
```python
#!/usr/bin/env python2.7
""""""Train and export a simple Softmax Regression TensorFlow model.
The model is from the TensorFlow ""MNIST For ML Beginner"" tutorial. This program
simply follows all its training instructions, and uses TensorFlow SavedModel to
export the trained model with proper signatures that can be loaded by standard
tensorflow_model_server.
Usage: mnist_export.py [--training_iteration=x] [--model_version=y] export_dir
""""""

import os
import sys

# This is a placeholder for a Google-internal import.

import tensorflow as tf
from tensorflow.python.ops import variables
from tensorflow.core.protobuf import saver_pb2

from tensorflow.python.saved_model import builder as saved_model_builder
from tensorflow.python.saved_model import signature_constants
from tensorflow.python.saved_model import signature_def_utils
from tensorflow.python.saved_model import tag_constants
from tensorflow.python.saved_model import utils
from tensorflow.python.util import compat
from tensorflow.examples.tutorials.mnist import input_data

from six.moves import xrange

tf.app.flags.DEFINE_string(""ps_hosts"", """", ""Comma-separated list of hostname:port pairs"")

tf.app.flags.DEFINE_string(""worker_hosts"", """", ""Comma-separated list of hostname:port pairs"")

tf.app.flags.DEFINE_string(""job_name"", """", ""One of 'ps', 'worker'"")

tf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")
tf.app.flags.DEFINE_integer(""batch_size"", 100, ""Index of task within the job"")

tf.app.flags.DEFINE_integer('training_iteration', 2,
                            'number of training iterations.')
tf.app.flags.DEFINE_integer('model_version', 1, 'version number of the model.')
tf.app.flags.DEFINE_string('work_dir', 'model/', 'Working directory.')
tf.app.flags.DEFINE_string('train_dir', 'MNIST_data/', 'Working directory.')


FLAGS = tf.app.flags.FLAGS


def main(_):
    ps_hosts = FLAGS.ps_hosts.split("","")

    worker_hosts = FLAGS.worker_hosts.split("","")
    cluster = tf.train.ClusterSpec({""ps"": ps_hosts, ""worker"": worker_hosts})
    server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)

    if FLAGS.job_name == ""ps"":
        server.join()
    elif FLAGS.job_name == ""worker"":
        train(server, cluster)


def train(server, cluster):
    # Train model
    print('Training model...')
    with tf.device(
            tf.train.replica_device_setter(worker_device=""/job:worker/task:%d"" % FLAGS.task_index, cluster=cluster)):
        mnist = input_data.read_data_sets(FLAGS.train_dir, one_hot=True)
        serialized_tf_example = tf.placeholder(tf.string, name='tf_example')
        feature_configs = {'x': tf.FixedLenFeature(shape=[784], dtype=tf.float32), }
        tf_example = tf.parse_example(serialized_tf_example, feature_configs)

        x = tf.identity(tf_example['x'], name='x')  # use tf.identity() to assign name
        y_ = tf.placeholder('float', shape=[None, 10])

        w = tf.Variable(tf.zeros([784, 10]))
        b = tf.Variable(tf.zeros([10]))

        y = tf.nn.softmax(tf.matmul(x, w) + b, name='y')
        cross_entropy = -tf.reduce_sum(y_ * tf.log(y))

        global_step = tf.Variable(0)
        train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy, global_step=global_step)
        values, indices = tf.nn.top_k(y, 10)

        prediction_classes = tf.contrib.lookup.index_to_string(
            tf.to_int64(indices), mapping=tf.constant([str(i) for i in xrange(10)]))
        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))

        summary_op = tf.summary.merge_all()
        init_op = tf.global_variables_initializer()
        saver = tf.train.Saver()

        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0), logdir=""train_logs"", init_op=init_op,
                                 summary_op=summary_op, saver=saver, global_step=global_step, save_model_secs=600)

        with sv.managed_session(server.target) as sess:
            step = 0

            while not sv.should_stop() and step < 1000:
                batch_xs, batch_ys = mnist.train.next_batch(FLAGS.batch_size)
                train_feed = {x: batch_xs, y_: batch_ys}

                _, step = sess.run([train_step, global_step], feed_dict=train_feed)

                if step % 1000 == 0:
                    print(""global step: {} , accuracy:{}"".format(step, sess.run(accuracy,
                                                                                feed_dict=train_feed)))

            print('training accuracy %g' % sess.run(
                accuracy, feed_dict={x: mnist.test.images,
                                     y_: mnist.test.labels}))
            print('Done training!')
            if sv.is_chief:
                # Export model
                # WARNING(break-tutorial-inline-code): The following code snippet is
                # in-lined in tutorials, please update tutorial documents accordingly
                # whenever code changes.
                sess.graph._unsafe_unfinalize()
                export_path_base = FLAGS.work_dir
                export_path = os.path.join(
                    compat.as_bytes(export_path_base),
                    compat.as_bytes(str(FLAGS.model_version)))
                print('Exporting trained model to', export_path)
                builder = saved_model_builder.SavedModelBuilder(export_path)

                # Build the signature_def_map.
                classification_inputs = utils.build_tensor_info(serialized_tf_example)
                classification_outputs_classes = utils.build_tensor_info(prediction_classes)
                classification_outputs_scores = utils.build_tensor_info(values)

                classification_signature = signature_def_utils.build_signature_def(
                    inputs={signature_constants.CLASSIFY_INPUTS: classification_inputs},
                    outputs={
                        signature_constants.CLASSIFY_OUTPUT_CLASSES:
                            classification_outputs_classes,
                        signature_constants.CLASSIFY_OUTPUT_SCORES:
                            classification_outputs_scores
                    },
                    method_name=signature_constants.CLASSIFY_METHOD_NAME)

                tensor_info_x = utils.build_tensor_info(x)
                tensor_info_y = utils.build_tensor_info(y)

                prediction_signature = signature_def_utils.build_signature_def(
                    inputs={'images': tensor_info_x},
                    outputs={'scores': tensor_info_y},
                    method_name=signature_constants.PREDICT_METHOD_NAME)

                legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')

                builder.add_meta_graph_and_variables(
                    sess, [tag_constants.SERVING],
                    signature_def_map={
                        'predict_images':
                            prediction_signature,
                        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:
                            classification_signature,
                    },
                    legacy_init_op=legacy_init_op)

                builder.save()

                print('Done exporting!')

if __name__ == '__main__':
    tf.app.run()

```
### Error Log
```
Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
WARNING:tensorflow:From distribute_mnist_serving_model.py:85: index_to_string (from tensorflow.contrib.lookup.lookup_ops) is deprecated and will be removed after 2017-01-07.
Instructions for updating:
This op will be removed after the deprecation date. Please switch to index_to_string_table_from_tensor and call the lookup method of the returned table.
('Exporting trained model to', 'model/1')
2017-07-17 17:30:10.703382: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session e5aa2c66bff69f11 with config:
global step: 0 , accuracy:0.469999998808
training accuracy 0.5701
Done training!
Traceback (most recent call last):
  File ""distribute_mnist_serving_model.py"", line 176, in <module>
    tf.app.run()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""distribute_mnist_serving_model.py"", line 58, in main
    train(server, cluster)
  File ""distribute_mnist_serving_model.py"", line 159, in train
    legacy_init_op=legacy_init_op)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/saved_model/builder_impl.py"", line 362, in add_meta_graph_and_variables
    saver.save(sess, variables_path, write_meta_graph=False, write_state=False)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1488, in save
    raise exc
tensorflow.python.framework.errors_impl.NotFoundError: model/1/variables/variables_temp_962a99f708244380a378c7e2218c6865
         [[Node: save_1/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_INT32], _device=""/job:ps/replica:0/task:0/cpu:0""](save_1/ShardedFilename, save_1/SaveV2/tensor_names, save_1/SaveV2/shape_and_slices, Variable, Variable_1, Variable_2)]]
Caused by op u'save_1/SaveV2', defined at:
  File ""distribute_mnist_serving_model.py"", line 176, in <module>
    tf.app.run()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""distribute_mnist_serving_model.py"", line 58, in main
    train(server, cluster)
  File ""distribute_mnist_serving_model.py"", line 159, in train
    legacy_init_op=legacy_init_op)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/saved_model/builder_impl.py"", line 356, in add_meta_graph_and_variables
    allow_empty=True)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1139, in __init__
    self.build()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1170, in build
    restore_sequentially=self._restore_sequentially)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 685, in build
    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 361, in _AddShardedSaveOps
    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 335, in _AddShardedSaveOpsForV2
    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 276, in _AddSaveOps
    save = self.save_op(filename_tensor, saveables)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 219, in save_op
    tensors)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 745, in save_v2
    tensors=tensors, name=name)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()
NotFoundError (see above for traceback): model/1/variables/variables_temp_962a99f708244380a378c7e2218c6865
         [[Node: save_1/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_INT32], _device=""/job:ps/replica:0/task:0/cpu:0""](save_1/ShardedFilename, save_1/SaveV2/tensor_names, save_1/SaveV2/shape_and_slices, Variable, Variable_1, Variable_2)]]
```

### The Correct Log
```
Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
WARNING:tensorflow:From distribute_mnist_serving_model.py:83: index_to_string (from tensorflow.contrib.lookup.lookup_ops) is deprecated and will be removed after 2017-01-07.
Instructions for updating:
This op will be removed after the deprecation date. Please switch to index_to_string_table_from_tensor and call the lookup method of the returned table.
('Exporting trained model to', 'model/1')
2017-07-17 17:38:46.714706: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 2618b797d2ef99b4 with config:
global step: 0 , accuracy:0.479999989271
training accuracy 0.5513
Done training!
Done exporting!
```

### Exact command to reproduce

Situation One: work well
```
python distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=A:2223,A:2224 --job_name=ps --task_index=0

python distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=A:2223,A:2224 --job_name=worker --task_index=0

python distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=A:2223,A:2224 --job_name=worker --task_index=1
```

Situation Two: can't work
```
python distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=B:2222,C:2222 --job_name=ps --task_index=0

python distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=B:2222,C:2222 --job_name=worker --task_index=0

python distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=B:2222,C:2222 --job_name=worker --task_index=1
```

Situation Three: work well
```
python distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=A:2223,B:2222 --job_name=ps --task_index=0

python distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=A:2223,B:2222 --job_name=worker --task_index=0

python distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=A:2223,B:2222 --job_name=worker --task_index=1
```

Situation Four: work well
```
python distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=B:2223,B:2222 --job_name=ps --task_index=0

python distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=B:2223,B:2222 --job_name=worker --task_index=0

python distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=B:2223,B:2222 --job_name=worker --task_index=1
```

I Don't know what cause this problem. how can i fix this issue? thanks a lot."
11548,Different behavior of tf.extract_image_patches and tf.nn.conv2d for certain padding/stride/filter size combinations,"Hi!

I am trying to implement something using ``tf.extract_image_patches`` and ran into some troubles that made clear ``tf.extract_image_patches`` handles some combinations of padding, filter size and stride differently than ``tf.nn.conv2d``. Since ``tf.extract_image_patches`` is conceptually a ""part"" of a convolution operation, I think this might be unintended behavior. 

Specifically, I implemented a ""manual"" version of a convolution operation using ``tf.extract_image_patches``

```python
def manual_conv(input, filter, strides, padding):
  h_f, w_f, c_in, c_out = filter.get_shape().as_list()
  input_patches = tf.extract_image_patches(input, ksizes=[1, h_f, w_f, 1 ], strides=strides, rates=[1, 1, 1, 1], padding=padding)
  filters_flat = tf.reshape(filter, shape=[h_f*w_f*c_in, c_out])
  return tf.einsum(""ijkl,lm->ijkm"", input_patches, filters_flat)
```

and tested it like this

```python
import unittest
import tensorflow as tf

class TestManualConvToyData(unittest.TestCase):

  def runTest(self):
    m = 32
    c_in = 3
    c_out = 16

    image_sizes = [127, 64]
    filter_sizes = [1, 2, 3, 5, 11]
    strides = [1, 3, 4, 30]
    paddings = [""VALID"", ""SAME""]

    for fs in filter_sizes:
      for stri in strides:
        for imsize in image_sizes: 
          for pad in paddings:
            h = w = imsize
            h_f = w_f = fs
            print ""Testing for"", imsize, fs, stri, pad

            tf.reset_default_graph()
            X = tf.constant(1.0+np.random.rand(m, h, w, c_in), tf.float32)
            W = tf.truncated_normal([h_f, w_f, c_in, c_out])

            Z = tf.nn.conv2d(X, W, strides=[1, stri, stri, 1], padding=pad)
            Z_manual = manual_conv(X, W, strides=[1, stri, stri, 1], padding=pad)

            sess = tf.Session()
            sess.run(tf.global_variables_initializer())
            Z_, Z_manual_ = sess.run([Z, Z_manual])
            self.assertEqual(Z_.shape, Z_manual_.shape)
            self.assertTrue(np.allclose(Z_, Z_manual_, rtol=1e-05))
            sess.close()
 ```

This test fails for some combinations of padding, filter size and stride. I think it has to do with the fact that ``tf.extract_image_patches`` tries to center patches if possible, as discussed in [this][1] stackoverflow question.

[1]: https://stackoverflow.com/questions/40731433/understanding-tf-extract-image-patches-for-extracting-patches-from-an-image

------------------------

### System information
- Ubuntu 16.04.
- Python 2.7.12
- tensorflow version 1.2.1 installed via pip (CPU only)"
11547,Tensorflow installation issue,"Hi Team,

    I have anaconda with python 2.7 in my windows 7 machine, i tried to install tensorflow with the below command.
   c:\> conda create -n tensorflow python=2.7
   it has installed successfully.
activate tensorflow
  it changed the prompt to
(tensorflow) C:\>

  I have visual studio 2015, and selected the environment as python 2.7 and ran a sample program, it is giving the below error, please suggest.

    from keras.models import Sequential
  File ""D:\Anaconda2\Lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""D:\Anaconda2\Lib\site-packages\keras\utils\__init__.py"", line 6, in <mod
ule>
    from . import conv_utils
  File ""D:\Anaconda2\Lib\site-packages\keras\utils\conv_utils.py"", line 3, in <m
odule>
    from .. import backend as K
  File ""D:\Anaconda2\Lib\site-packages\keras\backend\__init__.py"", line 83, in <
module>
    from .tensorflow_backend import *
  File ""D:\Anaconda2\Lib\site-packages\keras\backend\tensorflow_backend.py"", lin
e 1, in <module>
    import tensorflow as tf
ImportError: No module named tensorflow
Press any key to continue . . .

"
11546,PyImport_Import crash,"I recently ran into a problem, while using bazel to build a project.  This project is compiled as a dynamic linking library, using PyImport_Import to import python module. when there is ""import tensorflow as tf"" in the python file , application who calls the dynamic linking library crashed everytime, but when it‘s not there ,everything works just fine. where is the problem?
my tensorflow version is 1.0.0,python 2.7.0,bazel 0.4.3


here is the console information when the application crashes:

> F tensorflow/core/framework/function.cc:1015] Check failed: GetOpGradFactory()->insert({op, func}).second Duplicated gradient for Softmax


here is the test python file looks like: 
`from __future__ import print_function
import tensorflow as tf
import os
import time
from itertools import izip
import numpy as np
import wrapt
import cv2
def get_int( ):
    a = 10
    b = 20
    return a + b

def get_str( s1, s2 ):
    #return s1 + s2  
    #return 'Hello , TY'  
    return ('Hello, World', 10, 20)`

here is the source code of .so file:
`   Py_Initialize();
    if ( !Py_IsInitialized() ) {
        return -1;
    }
    PyEval_InitThreads();
    PyThreadState *mainThreadState = NULL;
    //  save a pointer to the main PyThreadState object 
    mainThreadState = PyThreadState_Get();
    //  release the lock 
    PyEval_ReleaseLock();
    char* mockargv[1]={(char*)""""};
    PySys_SetArgv(1,mockargv);
    PyRun_SimpleString(""import sys"");
    PyRun_SimpleString(""sys.path.append('./')"");
    pName_ = PyString_FromString(""test_py"");
    displayPyObject(pName_);
    if(pName_ == NULL){
        return -1;
    }
    pModule_ = PyImport_Import(pName_);
    displayPyObject(pModule_);`

and here is the dynamic linking library part of my BUILD file
`cc_binary(
    name = ""test.so"",
    linkshared = 1,
    deps = [
        "":test_lib"",
    ],
)
cc_library(
    name = ""test_lib"",
    visibility = [""//visibility:__subpackages__""],
    srcs = glob([""test.cpp""
             ],
        ),
    includes=[""test.h""],
    linkopts = [
""-lm -lpthread -L/usr/lib/python2.7 -lpython2.7 -lopencv_core -lopencv_imgproc -lopencv_highgui -lopencv_ml -lfreeimage""
    ],
    deps = [
        ""//tensorflow/cc:cc_ops"",
        ""//tensorflow/core:framework"",
        ""//tensorflow/core:framework_internal"",
        ""//tensorflow/core:tensorflow"",
    ],
)
`


"
11544,PyImport_Import crash while using bazel to build the project,"I recently ran into a problem, while using bazel to build a project.  This project is compiled as a dynamic linking library, using PyImport_Import to import python module. when there is ""import tensorflow as tf"" in the python file , application who calls the dynamic linking library crashed everytime, but when it‘s not there ,everything works just fine. where is the problem?
my tensorflow version is 1.0.0,python 2.7.0,bazel 0.4.3


here is the console information when the application crashes:

> F tensorflow/core/framework/function.cc:1015] Check failed: GetOpGradFactory()->insert({op, func}).second Duplicated gradient for Softmax


here is the test python file looks like: 
`from __future__ import print_function
import tensorflow as tf
import os
import time
from itertools import izip
import numpy as np
import wrapt
import cv2
def get_int( ):
    a = 10
    b = 20
    return a + b

def get_str( s1, s2 ):
    #return s1 + s2  
    #return 'Hello , TY'  
    return ('Hello, World', 10, 20)`

here is the source code of .so file:
`   Py_Initialize();
    if ( !Py_IsInitialized() ) {
        return -1;
    }
    PyEval_InitThreads();
    PyThreadState *mainThreadState = NULL;
    //  save a pointer to the main PyThreadState object 
    mainThreadState = PyThreadState_Get();
    //  release the lock 
    PyEval_ReleaseLock();
    char* mockargv[1]={(char*)""""};
    PySys_SetArgv(1,mockargv);
    PyRun_SimpleString(""import sys"");
    PyRun_SimpleString(""sys.path.append('./')"");
    pName_ = PyString_FromString(""test_py"");
    displayPyObject(pName_);
    if(pName_ == NULL){
        return -1;
    }
    pModule_ = PyImport_Import(pName_);
    displayPyObject(pModule_);`

and here is the dynamic linking library part of my BUILD file
`cc_binary(
    name = ""test.so"",
    linkshared = 1,
    deps = [
        "":test_lib"",
    ],
)
cc_library(
    name = ""test_lib"",
    visibility = [""//visibility:__subpackages__""],
    srcs = glob([""test.cpp""
             ],
        ),
    includes=[""test.h""],
    linkopts = [
""-lm -lpthread -L/usr/lib/python2.7 -lpython2.7 -lopencv_core -lopencv_imgproc -lopencv_highgui -lopencv_ml -lfreeimage""
    ],
    deps = [
        ""//tensorflow/cc:cc_ops"",
        ""//tensorflow/core:framework"",
        ""//tensorflow/core:framework_internal"",
        ""//tensorflow/core:tensorflow"",
    ],
)
`


"
11542,Tensorflow feed_dict issue,"I am new to tensorflow. I understand that we need to create the tensorflow graph and then call sess.run() to get the values I want.

However, I am confused by how the feed_dict works. 
For example:
Will 1 be the same as 2?
```
1. accuracy, cost = sess.run([accuracy, cost], feed_dict = {X:X_batch, Y:Y_batch})

2. accuracy  = sess.run(accuracy, feed_dict = {X:X_batch, Y: Y_batch})
   cost = sess.run(cost, feed_dict = {X:X_batch, Y:Y_batch})
```
I don't know that if tensorflow receives the same feed_dict in cost and in tensorflow graph computing accuracy already computes cost, do it go over the neural net again to evaluate the value, or it will return the value computed without going through the net again?

Also, from [Hvass-Labs/TensorFlow-Tutorials/TensorFlow Tutorial #02 Convolutional Neural Network](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/02_Convolutional_Neural_Network.ipynb), in function plot_conv_weights(weights, input_channel=0)
```
weights = sess.run(conv_weigh)
```
Since training weights require we fill the placeholder X and Y with values, but here I saw no feed_dict.

So how exactly feed_dict works?"
11541,tfcompile won't work with graph that has no inputs,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.2.1 (git version v1.2.0-2140-g85d4102)
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 0.5.2
- **CUDA/cuDNN version**: 5.1.5
- **GPU model and memory**: GeForce GTX 970 4GB

### Describe the problem

I have been working on a char-rnn like network that is trained on a corpus of text, and then produces similar text as output. I'm trying to compile the graph, but tfcompile asserts that the list of feeds is non-empty. However, since for my network there are no inputs to specify as feeds, tfcompile fails.

### Source code / logs

The configuration I tried to use looks like this:

```
fetch {
  id { node_name: ""predictions"" }
}
```

The error message:
```
INVALID ARGUMENTS: feeds and fetches must be specified
```
which comes from https://github.com/tensorflow/tensorflow/blob/85d4102862c781af2346b4aa568054522e8946ea/tensorflow/compiler/aot/tfcompile_util.cc#L119

I don't really know anything about how the compiler works, but is this check in place because it was assumed that you'd always want to have an input, or because of some other limitation that would cause the compilation to fail with no feeds?"
11540,Unclear about how to make AttentionWrapper work,"Hello, I am using the TensorFlow 1.2.1 and I want to implement a Seq2Seq model with Attention through tf.contrib.seq2seq. I didn't find enough information of how to use ```tf.contrib.seq2seq.AttentionWrapper``` on documentation, however I successfully find how to use that in the [NMT tutorial](https://github.com/tensorflow/nmt#attention-wrapper-api).

Before adding Attention, I already make the Seq2Seq model work, however, it seems very difficult to insert the AttentionWrapper by simply wrapping the decoder cell. It is not fair to ask anyone to read my own code, which is quite long. So here I provide the minimal code to demonstrate the problem:

```python
import tensorflow as tf
from tensorflow.python.layers.core import Dense

# INPUT
X = tf.placeholder(tf.int32, [None, None])
Y = tf.placeholder(tf.int32, [None, None])
X_seq_len = tf.placeholder(tf.int32, [None])
Y_seq_len = tf.placeholder(tf.int32, [None])

# ENCODER         
encoder_out, encoder_state = tf.nn.dynamic_rnn(
    cell = tf.nn.rnn_cell.BasicLSTMCell(128), 
    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),
    sequence_length = X_seq_len,
    dtype = tf.float32)

# ATTENTION
attention_mechanism = tf.contrib.seq2seq.LuongAttention(
    num_units = 128, 
    memory = encoder_out,
    memory_sequence_length = X_seq_len)

decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
    cell = tf.nn.rnn_cell.BasicLSTMCell(128),
    attention_mechanism = attention_mechanism,
    attention_layer_size = 128)

# DECODER
Y_vocab_size = 10000
decoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))
projection_layer = Dense(Y_vocab_size)

training_helper = tf.contrib.seq2seq.TrainingHelper(
    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),
    sequence_length = Y_seq_len,
    time_major = False)
training_decoder = tf.contrib.seq2seq.BasicDecoder(
    cell = decoder_cell,
    helper = training_helper,
    initial_state = encoder_state,
    output_layer = projection_layer)
training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(
    decoder = training_decoder,
    impute_finished = True,
    maximum_iterations = tf.reduce_max(Y_seq_len))
training_logits = training_decoder_output.rnn_output

# LOSS
masks = tf.sequence_mask(Y_seq_len, tf.reduce_max(Y_seq_len), dtype=tf.float32)
loss = tf.contrib.seq2seq.sequence_loss(logits = training_logits, targets = Y, weights = masks)

# BACKWARD
params = tf.trainable_variables()
gradients = tf.gradients(loss, params)
clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)
train_op = tf.train.AdamOptimizer().apply_gradients(zip(clipped_gradients, params))
```
The graph cannot be built, due to error message
```
Traceback (most recent call last):
  File ""seq2seq_attn.py"", line 44, in <module>
    maximum_iterations = tf.reduce_max(Y_seq_len))
  File ""/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py"", line 286, in dynamic_decode
    swap_memory=swap_memory)
  File ""/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2766, in while_loop
    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2595, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2545, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py"", line 234, in body
    decoder_finished) = decoder.step(time, inputs, state)
  File ""/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py"", line 139, in step
    cell_outputs, cell_state = self._cell(inputs, state)
  File ""/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 180, in __call__
    return super(RNNCell, self).__call__(inputs, state)
  File ""/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 439, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py"", line 677, in call
    cell_inputs = self._cell_input_fn(inputs, state.attention)
AttributeError: 'LSTMStateTuple' object has no attribute 'attention'
```
I have also tried using GRU instead of LSTM, the error still exists"
11538,MultiRNNCell fails _like_rnncell check.,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Sierra 10.12.5
- **TensorFlow installed from (source or binary)**: binary, pip3 install
- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1
- **Python version**: 3.5.2
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A

### Describe the problem
I believe this is a bug. When initializing a MultiRNNCell like MultiRNNCell([lstm]*3) this subsequently may be passed into something like tf.nn.bidirectional_dynamic_rnn and it will pass the _like_rnncell check and everything proceeds normally. When initializing a MultiRNNCell like MultiRNNCell([lstm_factory() for _ in range num_layers]), such as in [this code](https://www.tensorflow.org/tutorials/recurrent#stacking_multiple_lstms) the _like_rnncell check on line 393 of [this code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py) will fail. If you trace it back to where that function is defined there are 4 qualifiers, the qualifiers that fail are hasattr(cell, 'output_size') and hasattr(cell, 'state_size'). The preferred way of initializing multi cell rnns that do not share input size which lead to later dimension mismatch seem to be this way. Initially I wrote my code with the former implementation but the former initialization leads to input dimension sharing and then mismatches later on which is another issue and why I can't use that one. (Also this probably fails anywhere a _like_rnncell check is used, I know for a fact my AttentionWrapper in my decoder fails for the same reason not just in the bidirectional_dynamic_rnn)


### Source code / logs
This is my encoder copied from my seq2seq model class. 
```
    LSTMCell = tf.nn.rnn_cell.LSTMCell
    MultiRNNCell = tf.nn.rnn_cell.MultiRNNCell
    DropoutWrapper = tf.nn.rnn_cell.DropoutWrapper

    def encode(
        self,
        num_units, peepholes, inputs,
        num_layers, seq_len, time_major,
        keep_prob=0.5
    ):
        multi_cell = MultiRNNCell(
            [
                (self._cell_factory(num_units, peepholes, keep_prob)
                    for x in range(num_layers))
            ]
        )
        enc_outputs, enc_state = tf.nn.bidirectional_dynamic_rnn(
            cell_fw=multi_cell,
            cell_bw=multi_cell,
            inputs=inputs,
            sequence_length=seq_len,
            dtype=tf.float32,
            time_major=time_major
        )
        return enc_outputs, enc_state

    def _cell_factory(self, num_units, peepholes, keep_prob):
        lstm = LSTMCell(num_units=num_units, use_peepholes=peepholes)
        dropout = DropoutWrapper(lstm, input_keep_prob=keep_prob)
        return dropout
```

Trace:
`  File ""execute.py"", line 168, in <module>
    main()
  File ""execute.py"", line 91, in main
    steps=10000
  File ""/Users/panda/Desktop/aura_ml/aura_model/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 241, in train
    loss = self._train_model(input_fn=input_fn, hooks=hooks)
  File ""/Users/panda/Desktop/aura_ml/aura_model/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 560, in _train_model
    model_fn_lib.ModeKeys.TRAIN)
  File ""/Users/panda/Desktop/aura_ml/aura_model/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 545, in _call_model_fn
    features=features, labels=labels, **kwargs)
  File ""execute.py"", line 134, in model_wrapper
    keep_prob=params['encode']['keep_probability']
  File ""/Users/panda/Desktop/aura_ml/model_1/model.py"", line 42, in encode
    time_major=time_major
  File ""/Users/panda/Desktop/aura_ml/aura_model/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 364, in bidirectional_dynamic_rnn
    raise TypeError(""cell_fw must be an instance of RNNCell"")
TypeError: cell_fw must be an instance of RNNCell`"
11536,[Feature Request] Feed tensors to feed_dict,"I think that a nice step towards ""dynamic computation graphs"" is the possibility to feed tensors to placeholders.

I am building an application which creates a network.
I create my training and testing batches with tf.train.batch. 
However, the training batches are unlimited (so no epoch limit to the tf.train.slice_input_producer) but I would like to also test during training so to only test one epoch on the testing data.
For both the training and testing images, I have a preprocessing pipeline which also uses a computation graph, but is different for testing and for training.

Now the problem is that I have to create the testing batch every time I want to test (because I want to test for 1 epoch, and I cannot otherwise limit the input producing...).
So it would definitely help, to be able to feed the training and testing batch respectively to the network input.
My current solution is to create the network every time I make the switch from training to testing with the new network inputs. But this takes quite an (unnecessary) long amount of time...

Is there perhaps another way to ""link"" two graphs in tensorflow?"
11534,[FeatureRequest] make categorical_x_entropy broadcastable,"For example:

```
>>> x = tf.placeholder('float32', [4,5,6])
>>> y = tf.placeholder('float32', [1,5,6])
>>> z = tf.nn.softmax_cross_entropy_with_logits(labels=x, logits=y)
Traceback (most recent call last):
  File ""/home/khaotik/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py"", line 671, in _call_cpp_shape_fn_impl
    input_tensors_as_shapes, status)
  File ""/home/khaotik/anaconda3/lib/python3.6/contextlib.py"", line 89, in __exit__
    next(self.gen)
  File ""/home/khaotik/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension 0 in both shapes must be equal, but are 5 and 20 for 'SoftmaxCrossEntropyWithLogits' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [5,6], [20,6].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/khaotik/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1594, in softmax_cross_entropy_with_logits
    precise_logits, labels, name=name)
  File ""/home/khaotik/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 2380, in _softmax_cross_entropy_with_logits
    features=features, labels=labels, name=name)
  File ""/home/khaotik/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/home/khaotik/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2508, in create_op
    set_shapes_for_outputs(ret)
  File ""/home/khaotik/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1873, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/home/khaotik/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1823, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""/home/khaotik/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py"", line 610, in call_cpp_shape_fn
    debug_python_shape_fn, require_shape_fn)
  File ""/home/khaotik/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py"", line 676, in _call_cpp_shape_fn_impl
    raise ValueError(err.message)
ValueError: Dimension 0 in both shapes must be equal, but are 5 and 20 for 'SoftmaxCrossEntropyWithLogits' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [5,6], [20,6].
```

But, intuitively, I'd like to broadcast `y` into shape `[4,5,6]` in the above code.

It's not too hard to get a workaround such as using `tf.tile`. However it would consume more precious GPU memory."
11529,Not able to import tensorflow python3.5.2 Tensorflow  installation from PIP3 on window machine ,"import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in impor
t_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in impor
t_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\__init__.py"", lin
e 24, in <module>
    from tensorflow.python import *
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\__init__.p
y"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in impor
t_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in impor
t_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_probl
ems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>
"
11528,Not able to import tensorflow..,"import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in impor
t_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in impor
t_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\__init__.py"", lin
e 24, in <module>
    from tensorflow.python import *
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\__init__.p
y"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in impor
t_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_ten
sorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in impor
t_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_probl
ems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>
"
11527,AttributeError: 'module' object has no attribute 'LinearClassifier',"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
11525,tf.where outputs the wrong tensor dtype,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution**: Ubuntu 14.04
- **TensorFlow installed from**: Source
- **TensorFlow version**: ('v1.2.0-5-g435cdfc', '1.2.1')
- **Python version**: 2.7.13
- **Bazel version**: N/A
- **CUDA/cuDNN version**: Cuda 8.0
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```
x = tf.Variable([2, 1], dtype=tf.float32)
y = tf.Variable(2, dtype=tf.float32)
z = tf.where(tf.equal(x, y))
print z

>> Tensor(""Where_1:0"", shape=(?, 1), dtype=int64)
```

### Describe the problem
In the documentation for [tf.where.py](https://www.tensorflow.org/api_docs/python/tf/where), it states that it returns `A Tensor with the same type and shape as x, y if they are non-None.` In the example above, we see that `x` and `y` are both dtype `tf.float32`. Yet, the output is `tf.int64`. Was this intended or a bug?

This bug is a bit problematic for me as I use `tf.where` in my loss function so I need to ensure that all my tensors are `tf.float32` since `tf.cast` is not a differentiable `ops`."
11524,Build fails on FreeBSD 11,"```
external/grpc/src/core/lib/iomgr/socket_utils_common_posix.c:101:39: error: use of undeclared identifier 'IP_PKTINFO'
  if (0 != setsockopt(fd, IPPROTO_IP, IP_PKTINFO, &get_local_ip,
                                      ^
```"
11521,[go bindings] Printing graph in a text format,"I was looking for a function  that allows me (like in python) to print the graph in a readable way.
 
If you open a GitHub issue, here is our policy:

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOs X
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.1.0
- **Python version**: 2.7.13
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**: 
- **Exact command to reproduce**:
In python i can do:

```python
import tensorflow as tf

sess = tf.InteractiveSession()
with tf.gfile.FastGFile(""graphname.pb"", 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
    _ = tf.import_graph_def(graph_def, name='')
    
# All operations
sess.graph.get_operations()
```

that prints the operations of my graph

In go i just have the option:

```
graph := tf.NewGraph()
if err := graph.Import(model, """"); err != nil {
		log.Fatal(err)
	}
f, err := os.Create(""logWritter.txt"")
	if err != nil {
		log.Fatal(err)
	}
	graph.WriteTo(f)
```
it prints the binary graph.

### Describe the problem
When using a model, previously trained by somebody else, it is very useful to know the nodes to reference them.  It should be nice to have the same option with go."
11519,import_pb_to_tensorboard.py fails with TypeError,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution**: Mac OSX Sierra 10.12.5
- **TensorFlow installed from**: Source.
- **TensorFlow version**: ('v0.10.0-1727-g484ca8a-dirty', '0.11.0rc0')
- **Python version**: 2.7.6
- **Bazel version**: 0.4.4-homebrew
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
```
python import_pb_to_tensorboard.py --model_dir ~/Code/AndroidTensorFlowMNISTExample/app/src/main/assets/mnist_model_graph.pb --log_dir /tmp/tensorboard/
```

### Describe the problem
I'm trying to use `import_pb_to_tensorboard.py` to import an existing TensorFlow model into TensorBoard.  It appears to always return a TypeError due to line 74:

```
python import_pb_to_tensorboard.py --model_dir ~/Code/AndroidTensorFlowMNISTExample/app/src/main/assets/mnist_model_graph.pb --log_dir /tmp/tensorboard/
Traceback (most recent call last):
  File ""import_pb_to_tensorboard.py"", line 74, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
TypeError: run() got an unexpected keyword argument 'argv'
```

I discovered `import_pb_to_tensorboard.py` in [this Stackoverflow answer](https://stackoverflow.com/a/44955005/112705)."
11517,ImportError: No module named pbr.version,Hi everybody. I've this problem also when i run openstack --version. Solutions?
11516,how to feed more than one input into my tensorflow model,i want to use python code to feed more than one input and then use concat method how can i do it ？by using more placeholder？
11514,tf.assign is much slower than tf.assign_add on CPU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v0.8.0rc0-16474-gac98d11', '1.2.1')
- **Python version**: Python 2.7.13 :: Anaconda custom (64-bit)
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**:cuda8.0/cudnn6.0
- **GPU model and memory**:old titan x / 12GB
- **Exact command to reproduce**: a python script

### Describe the problem
`tf.assign` is much slower than `tf.assign_add` on CPU when `intra_op_parallelism_threads` set to 10. Although tensorflow treats these two operations a little different(maybe because `tf.assign` allows uninitialized tensor, accepts more tensor types), they all use a same class in eigen. I find this problem when doing some test about #11411.

### Source code / logs
script is on [gist](https://gist.github.com/suiyuan2009/24315b35915bddbe2d53b764164bb8fb),
```
dtype is <dtype: 'float32'>
use tf.assign: 3480.53776469 MB/s
use tf.assign_add: 10737.1193186 MB/s
```
set `intra_op_parallelism_threads` to 1, 
```
dtype is <dtype: 'float32'>
use tf.assign: 3481.3296105 MB/s
use tf.assign_add: 4244.41816359 MB/s
```
on GPU,
```
dtype is <dtype: 'float32'>
use tf.assign: 120361.172131 MB/s
use tf.assign_add: 77835.7152633 MB/s
```"
11512,Error in table.lookup from tf.contrib.lookup when using dataset api,"

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: All platforms
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.2.1
- **Python version**: 3.6, 2.7

I try the following code:

```
vocab =tf.contrib.lookup.index_table_from_file('./vocab.txt', num_oov_buckets=1)
dataset = tf.contrib.data.Dataset.from_tensor_slices(tf.constant(['./test1.txt']))
dataset = dataset.flat_map(lambda filename: tf.contrib.data.TextLineDataset(filename))
dataset = dataset.map(lambda line: tf.string_split([line]).values)
dataset = dataset.map(lambda words: table.lookup(words))
```
gives me the following errors. This is different from the stack overflow issue:
https://stackoverflow.com/questions/44519045/new-dataset-map-transformation-and-lookup-table-incompatible-string-type and appears before referencing the iterator


```
----> 1 dataset.map(lambda x: table.lookup(x))

/Users/xxx/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/contrib/lookup/lookup_ops.pyc in lookup(self, keys, name)
    803             name=""hash_bucket"")
    804         if self._table:
--> 805           ids = self._table.lookup(values)
    806           buckets = math_ops.add(buckets, self._table.size())
    807           is_id_non_default = math_ops.not_equal(ids, self._table.default_value)

/Users/xxx/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/contrib/lookup/lookup_ops.pyc in lookup(self, keys, name)
    184       # pylint: disable=protected-access
    185       values = gen_lookup_ops._lookup_table_find(
--> 186           self._table_ref, key_tensor, self._default_value, name=scope)
    187       # pylint: enable=protected-access
    188 

/Users/xxx/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/gen_lookup_ops.pyc in _lookup_table_find(table_handle, keys, default_value, name)
    286   result = _op_def_lib.apply_op(""LookupTableFind"", table_handle=table_handle,
    287                                 keys=keys, default_value=default_value,
--> 288                                 name=name)
    289   return result
    290 

/Users/xxx/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)
    765         op = g.create_op(op_type_name, inputs, output_types, name=scope,
    766                          input_types=input_types, attrs=attr_protos,
--> 767                          op_def=op_def)
    768         if output_structure:
    769           outputs = op.outputs

/Users/xxx/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/contrib/data/python/framework/function.pyc in create_op(self, op_type, inputs, data_types, **kwargs)
     79           self.extra_args.append(ph)
     80     return super(_ExperimentalFuncGraph, self).create_op(op_type, inputs,
---> 81                                                          data_types, **kwargs)
     82 
     83   def _add_tensor_and_parents(self, tensor):

/Users/xxx/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/function.pyc in create_op(self, op_type, inputs, data_types, **kwargs)
    359           self.extra_args.append(ph)
    360     return super(_FuncGraph, self).create_op(op_type, inputs, data_types,
--> 361                                              **kwargs)
    362 
    363 

/Users/xxx/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)
   2504     ret = Operation(node_def, self, inputs=inputs, output_types=dtypes,
   2505                     control_inputs=control_inputs, input_types=input_types,
-> 2506                     original_op=self._default_original_op, op_def=op_def)
   2507     if compute_shapes:
   2508       set_shapes_for_outputs(ret)

/Users/xxx/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)
   1248                             self.node_def.name,
   1249                             [i.dtype for i in self._inputs],
-> 1250                             input_types))
   1251     self._input_types = input_types
   1252 

TypeError: In op 'string_to_index_Lookup/hash_table_Lookup', 
input types ([tf.string, tf.string, tf.int64]) are not compatible with expected types
 ([tf.string_ref, tf.string, tf.int64])
```
However if I use the following referencing python_lookup_ops as outline in NMT, there is no issue..
Why is this?

```
from tensorflow.python.ops import lookup_ops
table = lookup_ops.index_table_from_file('./vocab.txt', num_oov_buckets=1)
```"
11502,float16 depthwise conv support,"It seems that the current implementation of depthwise convolution doesn't support half-precision : 
`/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.pyc in depthwise_conv2d_native(input, filter, strides, padding, data_format, name)`
`TypeError: Value passed to parameter 'input' has DataType float16 not in list of allowed values: float32, float64`
Any chances of having float16 support for this important op anytime soon? So training on P100s would be faster.
"
11501,"tf.einsum does not have ""name"" variable",I noticed there is no ability to name a tensor generated from from tf.einsum(). Is this on purpose?
11500,allocate_output can result in OOM error,"TF-commit: 7f008453ca1c7b

### Describe the problem
When implementing custom operations, allocating output twice (as a bug) should give at least a warning. The following code

```c++
    Tensor* tensor = nullptr;
    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, output_shape, &tensor));
    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, output_shape, &tensor));
```

gives a nearly un-debuggable OOM error after many iterations during runtime. Is it possible to prevent  `allocate_output(i,...)` being called with the same `i` twice?"
11499,"Tensorflow runs matmul, bincount and other ""heavy"" funcions only on cpu","### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux virgo-wn100.roma1.infn.it 3.10.0-514.el7.x86_64 #1 SMP Thu Nov 3 15:10:49 CDT 2016 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""7.3 (Nitrogen)""
VERSION_ID=""7.3""
REDHAT_BUGZILLA_PRODUCT_VERSION=7.3
REDHAT_SUPPORT_PRODUCT_VERSION=""7.3""
- **TensorFlow installed from (source or binary)**: from pip3
- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1
- **Python version**: Python 3.5.2
- **CUDA/cuDNN version**: CUDA: 8.0, cuDNN : 5
- **GPU model and memory**: Tesla K20m, memory 4742MiB

### Describe the problem
Everytime i run a code with simple operations and heavy computation functions, the simple ones, like add or reshape, run on the gpu, while the heavy ones, like matmul, bincount, tensordot run always on cpu. This is very strange, because tensorflow sees the gpu but uses it only for simple functions, while i expect the inverse behavior.
I noticed it when i used Timeline to profile my codes.

### Source code / logs
A simple example of the code i use:
```
import numpy
import tensorflow as tf
from tensorflow.python.client import timeline

matrix1 = tf.zeros([5000,5000], dtype = tf.int32)
matrix2 = tf.ones([5000,5000], dtype = tf.int32)
matrix1 = tf.add(matrix1,2)
product = tf.matmul(matrix1,matrix2)
    
session = tf.Session(config=tf.ConfigProto(log_device_placement=True))

run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
run_metadata = tf.RunMetadata()

image = session.run(product, options=run_options, run_metadata=run_metadata)


    # Create the Timeline object, and write it to a json
tl = timeline.Timeline(run_metadata.step_stats)
ctf = tl.generate_chrome_trace_format()
with open('timelineDB.json', 'w') as f:
	f.write(ctf)
```

and here the logs:
``` 
2017-07-14 12:33:41.431036: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-14 12:33:41.431080: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-14 12:33:41.431089: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-07-14 12:33:41.431096: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-14 12:33:41.431104: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-07-14 12:33:42.860681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: Tesla K20m
major: 3 minor: 5 memoryClockRate (GHz) 0.7055
pciBusID 0000:02:00.0
Total memory: 4.63GiB
Free memory: 4.57GiB
2017-07-14 12:33:42.860741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2017-07-14 12:33:42.860778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2017-07-14 12:33:42.860791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K20m, pci bus id: 0000:02:00.0)
Device mapping:
/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: Tesla K20m, pci bus id: 0000:02:00.0
2017-07-14 12:33:42.901828: I tensorflow/core/common_runtime/direct_session.cc:265] Device mapping:
/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: Tesla K20m, pci bus id: 0000:02:00.0

Add: (Add): /job:localhost/replica:0/task:0/gpu:0
2017-07-14 12:33:42.902645: I tensorflow/core/common_runtime/simple_placer.cc:847] Add: (Add)/job:localhost/replica:0/task:0/gpu:0
MatMul: (MatMul): /job:localhost/replica:0/task:0/cpu:0
2017-07-14 12:33:42.902669: I tensorflow/core/common_runtime/simple_placer.cc:847] MatMul: (MatMul)/job:localhost/replica:0/task:0/cpu:0
Add/y: (Const): /job:localhost/replica:0/task:0/gpu:0
2017-07-14 12:33:42.902714: I tensorflow/core/common_runtime/simple_placer.cc:847] Add/y: (Const)/job:localhost/replica:0/task:0/gpu:0
ones: (Const): /job:localhost/replica:0/task:0/cpu:0
2017-07-14 12:33:42.902736: I tensorflow/core/common_runtime/simple_placer.cc:847] ones: (Const)/job:localhost/replica:0/task:0/cpu:0
zeros: (Const): /job:localhost/replica:0/task:0/gpu:0
2017-07-14 12:33:42.902756: I tensorflow/core/common_runtime/simple_placer.cc:847] zeros: (Const)/job:localhost/replica:0/task:0/gpu:0
2017-07-14 12:33:43.110880: I tensorflow/stream_executor/dso_loader.cc:139] successfully opened CUDA library libcupti.so.8.0 locally
```
I attach the .json generated (change .txt to .json and open it with chrome://tracing/)
[timeline.txt](https://github.com/tensorflow/tensorflow/files/1148128/timeline.txt)
"
11497,Tensorflow-serving(CPU) for xbox one/ps4/switch ?,"Hello,
First of all I would like to thank the developer of tensorflow for this great library.

Can tensorflow-serving(CPU) works on xbox1 or ps4? (i mean to compile it)

Thank you."
11496,Turning on XLA JIT Compilation during session crashes computer?,"## System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**: 0.5.2
- **CUDA/cuDNN version**: 8.0/5.1
- **GPU model and memory**: Titan X Pascal 12GB
- **Exact command to reproduce**: Running any code with the following configuration passed to a supervisor managed session:

```
# Config to turn on JIT compilation
config = tf.ConfigProto()
config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1

with sv.managed_session(config=config) as sess:
...
```

### Describe the problem
No idea why but when I tried to turn on the XLA JIT compilation according to this guide:
https://www.tensorflow.org/performance/xla/jit

My computer crashes immediately and I've no way of diagnosing the problem. Note that I installed TF from source and enabled the XLA JIT option. Does this option consume more power than normal? 

Previously, I encountered a similar problem when I ran the TF-slim VGG model with a pip-installed TensorFlow-gpu r1.2, and the computer crashes simply. I could not find any issue until I ran the exact same code in another computer (a laptop with weaker power supply) with source-compiled tensorflow, which works. I then replaced the pip-installed TensorFlow with a source-compiled one, and everything works fine --- this led me to think there might be some issue with either my installation or the source code (although I can't verify). 

Has anyone faced a similar problem? Also, is XLA activated by default if I configured it to be enabled during the source installation? i.e. it could be because I called the compilation twice when it was already activated by default, causing the system failure. Is there a way to verify whether XLA is activated - so far the only indication I see are the messages ""XLA service 0x62bb180 executing computations on platform Host"" and ""XLA service 0x62a43b0 executing computations on platform CUDA"". Is this sufficient, i.e. to say I do not have to manually activate XLA?
"
11495,MultiGPU multi-session ,"It would be nice if there is an official way to do limit devices initialised by tensorflow other than the os level environment variable settings specifically as in[ this](https://stackoverflow.com/a/34776814) answer posted by @mrry in stackoverflow. As of my understanding the setup mentioned in [another](https://stackoverflow.com/a/34200194) answer by @mrry will not prevent tensorflow from grabbing all the GPU resources available on a same workstation at initialization as it is in by `tf.train.Server()`.  It would be essentially helpful for those sharing a machine among multiple users, so that they could independently initiate tensorflow graphs in different GPU units. Although this could be achieved via `nvidia-docker `or other resource orchestration tools, a native API would be a great addition to tensorflow.  Please ignore this if there is already an official way or my understanding is wrong.  "
11494,Error reading filenames with special characters on Windows,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 3.6

### Describe the problem
My tensorflow script on Ubuntu 16.04 works with the following code.

```
file=tf.gfile.GFile('../../data/Gutenberg/txt/Anthony Trollope___La Vendée.txt')
text = file.read()
```

However on Windows 10
```
file=tf.gfile.GFile('../../data/Gutenberg/txt/Anthony Trollope___La Vendée.txt')
text = file.read()
```
gives the following error.  This happens when the filenames have special characters in them

```
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-22-669eb528a808> in <module>()
----> 1 text = file.read()

C:\Users\xxx\Anaconda3\lib\site-packages\tensorflow\python\lib\io\file_io.py in read(self, n)
    116       string if in string (regular) mode.
    117     """"""
--> 118     self._preread_check()
    119     with errors.raise_exception_on_not_ok_status() as status:
    120       if n == -1:

C:\Users\xxx\Anaconda3\lib\site-packages\tensorflow\python\lib\io\file_io.py in _preread_check(self)
     76       with errors.raise_exception_on_not_ok_status() as status:
     77         self._read_buf = pywrap_tensorflow.CreateBufferedInputStream(
---> 78             compat.as_bytes(self.__name), 1024 * 512, status)
     79 
     80   def _prewrite_check(self):

C:\Users\xxx\Anaconda3\lib\contextlib.py in __exit__(self, type, value, traceback)
     87         if type is None:
     88             try:
---> 89                 next(self.gen)
     90             except StopIteration:
     91                 return

C:\Users\xxx\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py in raise_exception_on_not_ok_status()
    464           None, None,
    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),
--> 466           pywrap_tensorflow.TF_GetCode(status))
    467   finally:
    468     pywrap_tensorflow.TF_DeleteStatus(status)

NotFoundError: NewRandomAccessFile failed to Create/Open: ../../data/Gutenberg/txt/Anthony Trollope___La Vendée.txt : The system cannot find the file specified.
```
Note: pythons open()/read() works well on both platforms.  This error occurs for other file names  with special characters on windows too. I moved my data readers from python to tf api and data readers, the pipeline works on ubuntu but breaks in windows.
"
11493,"Using TF-slim DatasetDataProvider generate batch data, but get an error","------------------------
### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:macOS
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.2.1
- **Python version**: 3.5.0

In the [StackOverflow](https://stackoverflow.com/search?q=slim+OutOfRangeError) I find similar problems but no one answered.
Please give me some advice. Thanks.
```
def read_and_decode():
    file_pattern = './voc*.tfrecord'
    keys_to_features = {
        'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),
        'image/format': tf.FixedLenFeature((), tf.string, default_value='jpeg'),
        'image/height': tf.FixedLenFeature([1], tf.int64),
        'image/width': tf.FixedLenFeature([1], tf.int64),
        'image/channels': tf.FixedLenFeature([1], tf.int64),
        'image/shape': tf.FixedLenFeature([3], tf.int64),
        'image/object/bbox/xmin': tf.VarLenFeature(dtype=tf.float32),
        'image/object/bbox/ymin': tf.VarLenFeature(dtype=tf.float32),
        'image/object/bbox/xmax': tf.VarLenFeature(dtype=tf.float32),
        'image/object/bbox/ymax': tf.VarLenFeature(dtype=tf.float32),
        'image/object/bbox/label': tf.VarLenFeature(dtype=tf.int64),
        'image/object/bbox/difficult': tf.VarLenFeature(dtype=tf.int64),
        'image/object/bbox/truncated': tf.VarLenFeature(dtype=tf.int64),
    }
    items_to_handlers = {
        'image': slim.tfexample_decoder.Image('image/encoded', 'image/format'),
        'shape': slim.tfexample_decoder.Tensor('image/shape'),
        'object/bbox': slim.tfexample_decoder.BoundingBox(
            ['ymin', 'xmin', 'ymax', 'xmax'], 'image/object/bbox/'),
        'object/label': slim.tfexample_decoder.Tensor('image/object/bbox/label'),
        'object/difficult': slim.tfexample_decoder.Tensor('image/object/bbox/difficult'),
        'object/truncated': slim.tfexample_decoder.Tensor('image/object/bbox/truncated'),
    }
    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)
    return slim.dataset.Dataset(
        data_sources=file_pattern,
        reader=tf.TFRecordReader,
        num_samples=1,
        decoder=decoder,
        items_to_descriptions={},
        num_classes=21)


slim = tf.contrib.slim
dataset = read_and_decode()
provider = slim.dataset_data_provider.DatasetDataProvider(dataset, num_readers=3, shuffle=False)

[image, shape, glabels, gbboxes] = provider.get(['image', 'shape', 'object/label', 'object/bbox'])
image = tf.expand_dims(image, 0)
image = tf.image.resize_images(image, [224,224],tf.image.ResizeMethod.BILINEAR)
glabels.set_shape([1])

img_batch, label_batch = tf.train.batch([image, glabels], 64, capacity=2000)
with tf.Session() as sess:
    ini_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
    sess.run(ini_op)
    coord = tf.train.Coordinator()
    thread = tf.train.start_queue_runners(sess=sess,coord=coord)
    for i in range(2):
        cur_example_batch,cur_label_batch = sess.run([img_batch,label_batch])
        print(cur_example_batch.shape)
    coord.request_stop()
    coord.join(thread)
```

The error infomation:
```
Traceback (most recent call last):
  File ""/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1139, in _do_call
    return fn(*args)
  File ""/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1121, in _run_fn
    status, run_metadata)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_3_batch/fifo_queue' is closed and has insufficient elements (requested 64, current size 0)
	 [[Node: batch = QueueDequeueManyV2[component_types=[DT_FLOAT, DT_INT64], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](batch/fifo_queue, batch/n)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/szp/Documents/github/RCNN/RCNN.py"", line 140, in <module>
    cur_example_batch,cur_label_batch = sess.run([img_batch,label_batch])
  File ""/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 789, in run
    run_metadata_ptr)
  File ""/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 997, in _run
    feed_dict_string, options, run_metadata)
  File ""/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1132, in _do_run
    target_list, options, run_metadata)
  File ""/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1152, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_3_batch/fifo_queue' is closed and has insufficient elements (requested 64, current size 0)
	 [[Node: batch = QueueDequeueManyV2[component_types=[DT_FLOAT, DT_INT64], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](batch/fifo_queue, batch/n)]]

Caused by op 'batch', defined at:
  File ""/Users/szp/Documents/github/RCNN/RCNN.py"", line 133, in <module>
    img_batch, label_batch = tf.train.batch([image, glabels], 64, capacity=2000)
  File ""/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/training/input.py"", line 919, in batch
    name=name)
  File ""/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/training/input.py"", line 716, in _batch
    dequeued = queue.dequeue_many(batch_size, name=name)
  File ""/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/ops/data_flow_ops.py"", line 457, in dequeue_many
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File ""/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 946, in _queue_dequeue_many_v2
    timeout_ms=timeout_ms, name=name)
  File ""/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

OutOfRangeError (see above for traceback): FIFOQueue '_3_batch/fifo_queue' is closed and has insufficient elements (requested 64, current size 0)
	 [[Node: batch = QueueDequeueManyV2[component_types=[DT_FLOAT, DT_INT64], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](batch/fifo_queue, batch/n)]]
```

"
11489,import_graph_def input_map not updating all attributes,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Reproducible script below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary/pip
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```py
import tensorflow as tf
from tensorflow.python.framework import graph_util
from tensorflow.core.framework import graph_pb2

orig_model = ""input_map_orig.pb""
mapped_model = ""input_map_modified.pb""

# Create a graph with uint8 placeholder that gets
# cast to float32 before going through image whitening
tf.reset_default_graph()
img = tf.placeholder(tf.uint8, shape=(1, None, None, 3), name=""input_tensor"")
float_img = tf.cast(img, tf.float32, ""ToFloat"")
whiten_fn = lambda img: tf.image.per_image_standardization(img)
whitened = tf.map_fn(whiten_fn, float_img, name='image_whitening')
identity = tf.identity(whitened, name=""id"")

# Write graph
output_names = [""id""]
with tf.Session() as sess:
  input_graph_def = sess.graph_def
  output_graph_def = graph_util.convert_variables_to_constants(sess,
                                                               input_graph_def,
                                                               output_names)
  tf.train.write_graph(output_graph_def, ""./"", orig_model, False)

# Load the graph again
tf.reset_default_graph()
graph_def = graph_pb2.GraphDef()
with open(orig_model, ""rb"") as f:
  graph_def.ParseFromString(f.read())

# Rewire the graph with a float32 placeholder directly
img = tf.placeholder(tf.float32, shape=(1, None, None, 3), name=""input"")
input_map = {
  ""ToFloat"": img,
  }
tf.import_graph_def(graph_def, input_map=input_map, name="""")

# Write the modified graph
graph = tf.get_default_graph()
output_names = [""id""]
with tf.Session(graph=graph) as sess:
  graph = tf.get_default_graph()
  input_graph_def = graph.as_graph_def()
  output_graph_def = graph_util.convert_variables_to_constants(sess,
                                                               input_graph_def,
                                                               output_names)
  tf.train.write_graph(output_graph_def, ""./"", mapped_model, False)

# Try to load the modified graph
tf.reset_default_graph()
graph_def = graph_pb2.GraphDef()
with open(mapped_model, ""rb"") as f:
  graph_def.ParseFromString(f.read())

g = tf.import_graph_def(graph_def)
```

### Describe the problem
I noticed this issue trying to modify (via input_map) an object detection model (from [tf/models/object_detection](https://github.com/tensorflow/models/tree/master/object_detection)) to directly accept float32 instead of uint8. I have managed to reproduce the using the above minimal example. It seems like one of the attributes of one of the operations doesn't get fully updated. Possibly related to  https://github.com/tensorflow/tensorflow/issues/9925 .

### Source code / logs
Traceback:
```
Traceback (most recent call last):
  File ""input_map_bug.py"", line 71, in <module>
    g = tf.import_graph_def(graph_def)
  File ""/home/s.antol/cv/py3virtualenvs/tf1.2/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 331, in import_graph_def
    op_to_bind_to, node.name))
ValueError: Specified colocation to an op that does not exist during import: ToFloat in image_whitening/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3
```

Looking through the protobuf text version, the node below has `s: ""loc:@ToFloat""` instead of (what I think should be) `s:""loc:@input""`:
```
node {
  name: ""image_whitening/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3""
  op: ""TensorArrayScatterV3""
  input: ""image_whitening/TensorArray""
  input: ""image_whitening/TensorArrayUnstack/range""
  input: ""input""
  input: ""image_whitening/TensorArray:1""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@ToFloat""
      }
    }
  }
}
```
"
11487,'return_state' in keras rnn layers,"Im using tensorflow r1.2.
 In the original Keras, There's an option to get the last hidden state of a rnn layer. (
https://github.com/fchollet/keras/blob/master/keras/layers/recurrent.py#L99 )
And it disappeared in tensorflow version of the keras.
(https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/keras/python/keras/layers/recurrent.py#L89)

Is there any reason for this?? I think the 'return_state' option is quite useful.
"
11486,Link for iOS doc doesn't work,https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples/
11484,tf.device with function leads to incompatible device names,"### System information
- **OS Platform and Distribution: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.2.0-rc2-21-g12f033d', '1.2.0')
- **Python version**:  2

```python
import tensorflow as tf
with tf.device('/gpu:0'):
    x = tf.constant([1, 2, 3])
    print(x.device)
with tf.device(lambda op: '/gpu:0'):
    y = tf.constant([1, 2, 3])
    print(y.device)
G = tf.get_default_graph()
with G.colocate_with(y):
    with tf.device('/gpu:0'):
        z = tf.constant([1,2,3])
        print(z.device)
```

Outputs:
```
/device:GPU:0
/gpu:0
WARNING:tensorflow:Tried to colocate Const_2 with an op Const_1 that had a different device: /device:GPU:0 vs /gpu:0. Ignoring colocation property.
/device:GPU:0
```

When using device function, '/gpu:0' doesn't get translated to '/device:GPU:0'. This can lead to very confusing warning messages when used together with colocate."
11479,Not able to install tensflow on Windows 7,"Hi,

I am not able to tensor flow on Windows7 machine. I searched it and found it seems to b a proxy server problem which is trying to connect to pypi.python.org. I configured it also & tried to install tensorflow, it's still not working. Can anyone tell me how can I install it without compromising the security?

Thanks, 
Yugank Narula
"
11478,Does inception model detect multiple object in one image?,I have use retrain.py to train tensorflow with my own dataset of traffic sign but it seems it doesn't capture multi-object in one image.I am using the label_image.py to detect the object in my image.I have an image of two road sign which exists in my dataset but i get only one sign with high accuracy.It doesn't detect other sign.
11476,Cannot restore faster_rcnn checkpoint,"I want to reload faster_rcnn_resnet101 model parameters and run as follows: 

python export_inference_graph.py --input_type image_tensor --pipeline_config_path samples/configs/faster_rcnn_resnet101_coco.config --checkpoint_path checkpoints/faster_rcnn_resnet101_coco_checkpoint/model.ckpt --inference_graph_path checkpoints/faster_rcnn_resnet101_coco_checkpoint/test.pb

But it poses OutOfRangeError.
OutOfRangeError (see above for traceback): Read less bytes than requested
	 [[Node: save/RestoreV2_408 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_408/tensor_names, save/RestoreV2_408/shape_and_slices)]]
	 [[Node: save/RestoreV2_528/_1 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_1164_save/RestoreV2_528"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

What can I do for this error? "
11474,fail to run android example,"### System information
bazel version:
Build label: 0.5.2-homebrew

java version: ""1.8.0_131""

### workspace config:
android_sdk_repository(
  name = ""androidsdk"",
  api_level = 23,
  build_tools_version = ""25.0.1"",
  path = ""/Users/scucheri/Library/Android/sdk"",
)

android_ndk_repository(
    name=""androidndk"",
    path=""/Users/scucheri/Library/Android/sdk/ndk-bundle"",
    api_level=14) 


### Describe the problem
error when I run 'bazel build //tensorflow/examples/android:tensorflow_demo' in terminal, could you tell me how to solve this?

### Source code / logs
bazel build //tensorflow/examples/android:tensorflow_demo
WARNING: The major revision of the Android NDK referenced by android_ndk_repository rule 'androidndk' is 15. The major revisions supported by Bazel are [10, 11, 12, 13, 14]. Defaulting to revision 14.
ERROR: /private/var/tmp/_bazel_scucheri/f55efc3e55d5b53a9cbb2375bb9bdf55/external/androidsdk/BUILD.bazel:64:1: Traceback (most recent call last):
        File ""/private/var/tmp/_bazel_scucheri/f55efc3e55d5b53a9cbb2375bb9bdf55/external/androidsdk/BUILD.bazel"", line 64
                create_system_images_filegroups(system_image_dirs = [""system-ima..."", <53 more arguments>])
        File ""/private/var/tmp/_bazel_scucheri/f55efc3e55d5b53a9cbb2375bb9bdf55/external/bazel_tools/tools/android/android_sdk_repository_template.bzl"", line 246, in create_system_images_filegroups
                int(apidir.split(""-"")[1])
invalid literal for int() with base 10: ""MNC"".
ERROR: /private/var/tmp/_bazel_scucheri/f55efc3e55d5b53a9cbb2375bb9bdf55/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:build-tools/25.0.1/lib/dx.jar' contains an error and its package is in error and referenced by '@androidsdk//:dx_jar'.
ERROR: /private/var/tmp/_bazel_scucheri/f55efc3e55d5b53a9cbb2375bb9bdf55/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:dx_jar' contains an error and its package is in error and referenced by '@androidsdk//:dx_jar_import'.
ERROR: /Users/scucheri/AllMyProjects/AndroidStudioProjects/TmallAndroidProjects/tensorflow/WORKSPACE:20:1: Target '@androidsdk//:dx_jar_import' contains an error and its package is in error and referenced by '//external:android/dx_jar_import'.
ERROR: Analysis of target '//tensorflow/examples/android:tensorflow_demo' failed; build aborted.
INFO: Elapsed time: 3.653s
"
11472,Sorry to Reply you.My problem is tensor-flow un-incompatible with windows IIS?,"Sorry, i just have time to reply you. Thanks for replying me.
I means I use Flask to call tensor-flow py. And mount Flask to windows IIS. Local run ok. But in IIS will show FastCgi error. Check local run is ok,no error. But when in IIS, when i import keras, it will show fastcgi error. but when don't import keras, it will ok. If tensor-flow isun-incompatible with IIS?
![image](https://user-images.githubusercontent.com/22673941/28158872-b733b454-67ed-11e7-914e-303340bdc081.png)
![image](https://user-images.githubusercontent.com/22673941/28158879-bc5bc0c0-67ed-11e7-90f2-e7f72fd69e02.png)
![image](https://user-images.githubusercontent.com/22673941/28158885-c147b828-67ed-11e7-96da-6bd9324bdba0.png)
![image](https://user-images.githubusercontent.com/22673941/28158983-0ff8cca0-67ee-11e7-8fc7-8a063950b19d.png)
"
11471,XLA crash on Wasserstein GAN,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: TensorFlow installed from source
- **TensorFlow version (use command below)**: v1.2.0-1382-g708cbaf 1.2.0
- **Python version**:  Python 3.5.2
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA 8 / cudnn 6
- **GPU model and memory**: 2x1080 Ti, 12 Gb
### Describe the problem
When I'm running this code: https://github.com/Randl/WassersteinGAN.tensorflow with XLA, I get the following error message:
```
LLVM ERROR: Cannot select: 0x7f4c300d4878: i16,ch = AtomicCmpSwap<Volatile LDST1[%_fusion.typed16(addrspace=1)]> 0x7f4c30093858, 0x7f4c300d5098, 0x7f4c300d5168, 0x7f4c300e6c20
  0x7f4c300d5098: i64,ch = CopyFromReg 0x7f4c30093858, Register:i64 %vreg0
    0x7f4c300d5100: i64 = Register %vreg0
  0x7f4c300d5168: i16,ch = CopyFromReg 0x7f4c30093858, Register:i16 %vreg3
    0x7f4c300d56b0: i16 = Register %vreg3
  0x7f4c300e6c20: i16 = and 0x7f4c300d5168, 0x7f4c300d5370
    0x7f4c300d5168: i16,ch = CopyFromReg 0x7f4c30093858, Register:i16 %vreg3
      0x7f4c300d56b0: i16 = Register %vreg3
    0x7f4c300d5370: i16 = AssertZext 0x7f4c300d4c20, ValueType:ch:i1
      0x7f4c300d4c20: i16,ch = CopyFromReg 0x7f4c30093858, Register:i16 %vreg1
        0x7f4c300d4948: i16 = Register %vreg1
In function: _fusion__1
*** Error in `python3': free(): invalid size: 0x00007f4ac8091fe0 ***
======= Backtrace: =========
/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f4e6c2507e5]
/lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7f4e6c25937a]
/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f4e6c25d53c]
/usr/lib/nvidia-375/libnvidia-ptxjitcompiler.so.375.66(+0x6b98aa)[0x7f4a9f3f48aa]
/usr/lib/nvidia-375/libnvidia-ptxjitcompiler.so.375.66(+0xdb80e)[0x7f4a9ee1680e]
/usr/lib/nvidia-375/libnvidia-ptxjitcompiler.so.375.66(+0xc034b)[0x7f4a9edfb34b]
/usr/lib/nvidia-375/libnvidia-ptxjitcompiler.so.375.66(__cuda_CallJitEntryPoint+0xdcc)[0x7f4a9edf1aec]
/usr/lib/nvidia-375/libnvidia-fatbinaryloader.so.375.66(fatBinaryCtl_Compile+0x302)[0x7f4df0b125c2]
/usr/lib/x86_64-linux-gnu/libcuda.so.1(+0x1a1ee2)[0x7f4e07c31ee2]
/usr/lib/x86_64-linux-gnu/libcuda.so.1(+0x1a2a63)[0x7f4e07c32a63]
/usr/lib/x86_64-linux-gnu/libcuda.so.1(+0x1a3133)[0x7f4e07c33133]
/usr/lib/x86_64-linux-gnu/libcuda.so.1(+0xcf7b3)[0x7f4e07b5f7b3]
/usr/lib/x86_64-linux-gnu/libcuda.so.1(cuModuleLoadDataEx+0x75)[0x7f4e07c77055]
/usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(+0x4d0b7ba)[0x7f4e1370d7ba]
/usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN5Eigen26NonBlockingThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi+0xff)[0x7f4e139e998f]
/usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZNSt17_Function_handlerIFvvEZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data+0x2d)[0x7f4e139e977d]
```

Without XLA it works OK.
"
11470,Variable values are not present in Java when saving a model in python and restoring in Java,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
Yes. Provided bellow
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS 10.12.5
- **TensorFlow installed from (source or binary)**: pip install 
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: No compiled
- **CUDA/cuDNN version**: No
- **GPU model and memory**: No
- **Exact command to reproduce**: 

### Describe the problem
I've a script where a graph and variables are being saved using add_meta_graph_and_variables and tried to load it in Java but the weights seems that not are present. 
I've created two mini-examples of what it seems to me a bug.

### Source code / logs
Here it's how I have saved the things:

```
with tf.Session(graph=tf.Graph()) as session:
    ##HERE IS THE CODE OF MY NETWORK (Very long)

    session.run(tf.global_variables_initializer())
    #Load
    saver = tf.train.Saver()
    saver.restore(session, ""newModel.chkpt"")

    features = loadFeatures([""cat2.jpg""])
    res = predictions.eval(
            feed_dict={
                x: features,
                keep_prob: 1.0, })
    print('Image {} has a prob {} '.format(image, res))

    b = saved_model_builder.SavedModelBuilder(pathToSaveModel)
    b.add_meta_graph_and_variables(session, [tf.saved_model.tag_constants.TRAINING])
    b.save()
```

And here is how I tried to load in the Java side:

```
public static void main(String[] args) throws Exception {

        final int IMG_SIZE = 128;
        final String value = ""Hello from "" + TensorFlow.version();

        byte[] imageBytes = readAllBytesOrExit(Paths.get(""./cat1.jpg""));
        Tensor image = constructAndExecuteGraphToNormalizeImage(IMG_SIZE, imageBytes);

        SavedModelBundle load = SavedModelBundle.load(""./tmpTestNew/model"", ""train"");

        long[] sitio2;
        try (Graph g = load.graph()) {
            try (Session s = load.session();
                 Tensor result = s.runner()
                         .feed(""keep_prob"", Tensor.create(1.0F))
                         .feed(""input_jm"", image)
                         .fetch(""predictions"").run().get(0))
            {
                sitio2 = result.copyTo(new long[1]);
                System.out.print(sitio2[0]+""\n"");
            }
        }
        load.close();
    }
```

I've checked the content of some Tensor variables and in the python side the values are correct but the same variable in the Java side have another values (I can paste the code if needed), as a result the predictions are always wrong. 

I can provide the model saved or any other thing that is needed.

Thanks. 
"
11469,No graph definition files were found. ,"hello I was learn to use the tensorflow, Now I can run softmax , nn , cnn and other model on it , this is a really good framework.
but now I suffered some problem.
I want to use the tensorboard, this is my code:
https://github.com/catpanda/tensorflow_demo/blob/master/minist/mnist_bp.py
you can see the result is very good. to celebrate that. I want to show them in graph.You see I use `writer = tf.summary.FileWriter('board/', graph=graph)` in line 56 and define the graph in line 53. then I run the tensorboard by `tensorboard --logdir='board/'`.

![qq 20170713125732](https://user-images.githubusercontent.com/6506928/28151076-040ad3f2-67cb-11e7-8f63-2cda654357ca.png)

but the page shows `No graph definition files were found. `。
![_2017-07-13t04-58-05 105z](https://user-images.githubusercontent.com/6506928/28151070-fd6f2746-67ca-11e7-8a45-1134e1dfecc6.png)

I use tensorflow on windows10 and by python3.5 and tensorflow1.2 I hope you can help me find out what is the problem.
"
11468,Fine tuning tutorial without Bazel,"I am using this tutorial provided by Google Research to fine tune the Inception model fro my own images.

https://github.com/tensorflow/models/blob/master/inception/README.md#how-to-fine-tune-a-pre-trained-model-on-a-new-task

I have access to a GPU provided by my institution. The problem is that the GPU doesn't have Bazel installed.

Is it possible to complete this tutorial without using Bazel? "
11466,Custom Beam Search Decoder with sampling,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.1.0
- **Python version**: 3.4.3
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 7.5
- **GPU model and memory**:
- **Exact command to reproduce**:



### Describe the problem
I want to incorporating sampling into the  beam search decoder for seq2seq model. Is there any example or document on how to implement it? 


"
11463,Copyright on first line of LICENSE (Apache 2) is incorrect,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
N/A
- **TensorFlow installed from (source or binary)**:
N/A
- **TensorFlow version (use command below)**:
N/A
- **Python version**: 
N/A
- **Bazel version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
N/A

### Describe the problem
The LICENSE file in the tensorflow github repository has an incorrect copyright assignment.
The first line of the file reads:
Copyright 2017 The TensorFlow Authors.  All rights reserved.

This is incorrect.  ""The TensorFlow Authors"" are not the copyright holders for the Apache 2.0 license.
This line should be removed.  It makes a legal claim which is expressly false.

A separate issue is whether the line in the LICENSE file which provides an example of how to attribute
the copyright for new code in the TensorFlow project (which also includes a line assigning copyright
to ""The TensorFlow Authors"") is incorrect as well.

In general I can't see how the copyright assignment works in the project.  Yuan Tang was recently
added to the AUTHORS file.  Does that person now have full copyright over the entire TensorFlow
work?  The Contributor agreement only grants a copyright *license* to Google and the downstream
recipients of the code.  It does not grant copyright to the contributed code.

### Source code / logs
N/A
"
11462,Retrieving LLVM IR from AOT tfcompile,"Is it possible to get LLVM intermediate representation (.ll) files from tfcompile instead of object code? If so, how can it be done? Alternatively, can one get source code instead of object code from tfcompile?"
11457,How to use pre-trained models for fine tuning on different dataset ?,"Hi,

I have downloaded a pre-trained model for inception v3 (inception_v3_2016_08_28.tar.gz file)from [https://github.com/tensorflow/models/tree/master/slim#Pretrained](url) which contains the file inception_v3.ckpt. Could anyone please help me how to use this file to fine-tune on a different dataset ?

Thanks in advance.
"
11456,GPU-Isolated docker containers fail in a distributed training with more than one worker.,"### System information
- **Have I written custom code**: Yes
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: No
- **TensorFlow version**: v1.2.0-rc2-21-g12f033d 1.2.0
- **Python version**: Python 3.5.2
- **Bazel version**: -
- **CUDA/cuDNN version**: 'CUDA_VERSION': '8.0.61' / 'CUDNN_VERSION': '5.1.10'
- **GPU model and memory**: 8x GCE K80 Tesla

### Context:
1) I have created a Cloud Compute Engine instance with 8 GPUs.
2) I have started a couple of GPU-isolated containers with intention to run each as a separate worker:
`sudo NV_GPU=0 nvidia-docker run -it --name tf_worker_0 -p 8000:8000 -v /$(pwd)/repo:/notebooks/repo gcr.io/tensorflow/tensorflow:latest-gpu-py3 /bin/bash` 

3) To create a loopback to the host I run following:
`echo $(netstat -nr | grep '^0\.0\.0\.0' | awk '{print $2}') dockerhost >> /etc/hosts`
Through `dockerhost` it properly detects and pings all the open ports on the host including processes running in other containers that are exposed with `-p hostport:containerport`.

### Problem:
*Case 1 - Normal*: When I run a distributed TensorFlow script using `dockerhost` with only one `worker` and any number of `ps` servers (e.g.  `--ps_hosts=dockerhost:7000,dockerhost:7001 --worker_hosts=dockerhost:8000`) everything works as intended - local grcp servers are launched, worker and ps communicate well. 

*Case 2 - Issue*: When I increase the number of worker jobs (e.g.  `--worker_hosts=dockerhost:8000,dockerhost:8001`) i get `Master init: Unavailable` and `Master init: Internal` Errors. 

*Case 3 - Normal*: BTW, everything works smoothly when I run all the workers inside a single container isolating GPUs with `CUDA_VISIBLE_DEVICES`.

*Case 4 - Issue*: Also, it is worth mentioning here, that using queues solution to shutdown `ps` proposed in https://github.com/tensorflow/tensorflow/issues/4713 fails in both Case 1 and Case 2. This results in immediate `tensorflow.python.framework.errors_impl.UnavailableError` when starting `ps`.

### Statement:
Is this an intended behaviour? Or does such setting is insufficient for a distributed training and a special `master` server or a cluster manager is needed? It just seems interesting that it would work for one worker but not for more.

**Reproducible Toy Example**:
[repr.txt](https://github.com/tensorflow/tensorflow/files/1142032/repr.txt)

**Log**:
[worker_log.txt](https://github.com/tensorflow/tensorflow/files/1142041/worker_log.txt)

"
11453,python create model and c++ uses model without bazel,I just have used python to create ckpt model. But i have to use c++ to test my picture without bazel. Some one can help me? I am so eager to solve this problem! Thanks!!
11452,20 minutes for compiling a single file(conv_grad_ops_3d.cc)? ,"-----------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
2533ada7dd45b84d60677b8735e013d21044651a

- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:


### Describe the problem

It took about 20 minutes to compile ""conv_grad_ops_3d.cc""
The whole build took:
INFO: Elapsed time: 2241.764s, Critical Path: 1509.12s

Therefore this file took about 50% compile time. Why? 

### Source code / logs
http://ci.tensorflow.org/job/tf-master-win-bzl/1239/consoleFull


"
11450,Link to Benchmarks not working,The link to [Benchmarks ](https://www.tensorflow.org/community/benchmarks)on the page [https://www.tensorflow.org/community/](https://www.tensorflow.org/community/) is not working. 
11447,Feature Request: sparse matrix triangular solver,"I have just written about this in stack overflow,
but I just realized it would be more relevant if be put here.

I think it would be very helpful if we could do solve linear equation Ax = b,
where A has a sparse matrix representation, for example,
containing lower triangular entries for a banded symmetric-matrix.

Because, AFAIK, in this case, we need to convert the sparse matrix A
with the `tf.sparse_to_dense()`, to run the `tf.matrix_triangular_solve()`.

But, if the dimension of A is very large, e.g., about 16000x16000,
and with very sparse entries, e.g., about 46000 non-zeros (0.018%),
it would take a huge amount of memory.

Or maybe there is another way to do it in Tensorflow?"
11446,Is tf.split() not create a new  object? Using tf.split() and tf.concat() will has some bugs......,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution , :
Linux Ubuntu 16.04
- **TensorFlow installed from *:
source
- **TensorFlow version (use command below)**:
1.2
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
8.0
- **GPU model and memory**:
- **Exact command to reproduce**:


### Describe the problem
tf.split() is not create a new  object. Using tf.split() and tf.concat() will has some bugs......
For example, I has a [64,20] tensor, and split to [64,15], and then concat it with a [64,1] tensor, but I obtain a [64,21] tensor instead [64,16]

### Source code / logs
```
`import tensorflow as tf
import  numpy
from tensorflow.python.ops import tensor_array_ops, control_flow_ops
gen_x = tensor_array_ops.TensorArray(dtype=tf.int32,size=1,dynamic_size=True, infer_shape=True)
tmp = tf.constant(-1,shape=[64,20])
t = tf.split(tmp,[15],1)
t = tf.squeeze(t)
gen_x = gen_x.write(0,tf.constant(1,shape=[64]))
gen_tmp = tf.transpose(gen_x.stack(), perm=[1, 0])
feature = tf.concat([t,gen_tmp],1)


config = tf.ConfigProto()
config.gpu_options.allow_growth = True
sess = tf.Session(config=config)
print sess.run(tf.shape(feature))`
```

Result:

$ python test.py 

[64 21]


"
11442,The window in tf.contrib.rnn.AttentionCellWrapper(),"It is very strange to have a fixed-sized window in the implementation of AttentionCellWrapper, because there is no description about it in the Bahdanau paper. Of course, the behave of the window is not clear, which makes me confused. Can anyone explain it?"
11441,wide_n_deep Tutorial processing large data,"I am a freshman of tensorflow, and want to use the wide & deep network.
## I see the code of wide_n_deep tutorial, find that in ""def train_and_eval()"" function reading the total data using pandas at once. 
## I confuse if the data is large maybe 5 GB or more, and I cannot read all the data in memory, and how can I processing it? 

Thank you~~~
"
11440,Linear Model Tutorial: How to extract prediction? ,"Similar to this issue: https://github.com/tensorflow/tensorflow/issues/97 

For the ""[TensorFlow Linear Model Tutorial](https://www.tensorflow.org/tutorials/wide)"", the project implies that it will end with a program that, based on input data, outputs a 0 or 1: 

> Given census data about a person such as age, gender, education and occupation (the features), we will try to predict whether or not the person earns more than 50,000 dollars a year (the target label). We will train a logistic regression model, **and given an individual's information our model will output a number between 0 and 1, which can be interpreted as the probability that the individual has an annual income of over 50,000 dollars.**

However, it seems that the tutorial is incomplete. The last steps have you calculate the accuracy of the trained model: 

> The first line of the output should be something like accuracy: 0.83557522, which means the accuracy is 83.6%. Feel free to try more features and transformations and see if you can do even better!

And then point you in the direction of the full example code: 

> If you'd like to see a working end-to-end example, you can download our [example code.](https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/learn/wide_n_deep_tutorial.py) and set the model_type flag to wide. 

When I run the final program, my output looks like this: 

> `accuracy: 0.989583`
> `accuracy/baseline_label_mean: 0.364583`
> `accuracy/threshold_0.500000_mean: 0.989583`
> `auc: 1.0`
> `auc_precision_recall: 1.0`
> `global_step: 3000`
> `labels/actual_label_mean: 0.364583`
> `labels/prediction_mean: 0.369466`
> `loss: 0.0242721`
> `precision/positive_threshold_0.500000_mean: 0.972222`
> `recall/positive_threshold_0.500000_mean: 1.0`

Only `accuracy` is explained in the instructions, and it doesn't seem that there are final steps to complete the tutorial: to take a set of given values, and predict `income_bracket.` Can someone provide a code example, or point to documentation on how to extract final predictions after training the model?

Thanks! "
11438,Switching Session to MonitoredTrainingSession Produces Negative Dimension Tensors from Queues,"I already [posted](https://stackoverflow.com/questions/44831580/receiving-negative-input-dimensions-with-tensorflow-monitoredtrainingsession) on StackOverflow, but received no response.

I've made some changes to my code since posting on StackOverflow, but the problem is still the same: the readers and queues I use without a problem with a vanilla `tf.Session()` work fine, but if I try to switch to a `tf.train.MonitoredTrainingSession`, I get the following error:

`InvalidArgumentError (see above for traceback): Shape [15,-1,4] has negative dimensions
	 [[Node: define_inputs/y = Placeholder[dtype=DT_FLOAT, shape=[15,?,4], _device=""/job:local/replica:0/task:0/cpu:0""]()]]`

I'm using TensorFlow v1.2.0-5-g435cdfc 1.2.1. Here's my code. This works:


    # fix random seed to permit comparison between training runs
    tf.set_random_seed(seed=0)

    # define graph
    model = import_model()

    with tf.Session() as monitored_sess:

        monitored_sess.run(tf.global_variables_initializer())

        # create coordinator to handle threading
        coord = tf.train.Coordinator()

        # start threads to enqueue input minibatches for training
        threads = tf.train.start_queue_runners(sess=monitored_sess, coord=coord)

        data = monitored_sess.run([training_data])
        x, y, x_lengths, y_lengths = data[0]

        # when done, ask the threads to stop
        coord.request_stop()

        # wait for threads to finish
        coord.join(threads)

But then, this code doesn't work:

    tf.set_random_seed(seed=0)

    # define graph
    model = import_model()

    # create a one process cluster with an in-process server
    server = tf.train.Server.create_local_server()

    # define hooks for writing summaries and model variables to disk
    hooks = construct_training_hooks(model.summary_op, model.loss, train_log_directory)

    # create monitored training session to write model variables and summaries to disk
    with tf.train.MonitoredTrainingSession(master=server.target,
                                           config=tf.ConfigProto(allow_soft_placement=True),
                                           is_chief=True,
                                           hooks=hooks) as monitored_sess:

        # create coordinator to handle threading
        coord = tf.train.Coordinator()

        # start threads to enqueue input minibatches for training
        threads = tf.train.start_queue_runners(sess=monitored_sess, coord=coord)

        # train
        data = monitored_sess.run([training_data])
        x, y, x_lengths, y_lengths = data[0]

        # when done, ask the threads to stop
        coord.request_stop()

        # wait for threads to finish
        coord.join(threads)

The function `construct_training_hooks` is pretty straightforward:


    def construct_training_hooks(summary_op, loss, train_log_directory):
        hooks = [tf.train.StopAtStepHook(last_step=tf.flags.FLAGS.max_steps),
                 tf.train.CheckpointSaverHook(checkpoint_dir=train_log_directory,
                                              saver=tf.train.Saver(),
                                              save_steps=5),
                 tf.train.SummarySaverHook(output_dir=train_log_directory,
                                           summary_op=summary_op,
                                           save_steps=1),
                 tf.train.NanTensorHook(loss_tensor=loss)]

        return hooks
"
11435,Feature request / possible bug: cannot take second derivative of a MaxPool,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX El Capitan
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.0.0-65-g4763edf-dirty 1.0.1, but I've tested it on later versions too, and looking at the source code in master it doesn't seem like it's available
- **Python version**: 3.5.1, but I've reproduced it on 2.7

### Describe the problem

If I understand correctly, MaxPool operations should be many-times differentiable, but in Tensorflow you can only differentiate them once, which makes it impossible to inspect second derivatives for CNNs or do [double backpropogation](http://yann.lecun.com/exdb/publis/pdf/drucker-lecun-91.pdf).

### Source code / logs

You can reproduce it very simply this way:
```python
import tensorflow as tf
X = tf.Variable(tf.random_normal([1,1,1,1]))
P = tf.nn.max_pool(X, [1,1,1,1], [1,1,1,1], 'SAME')
g1 = tf.gradients(P, X)[0]
g2 = tf.gradients(g1, X)[0] # this line fails
```
raises
```
LookupError: No gradient defined for operation 'gradients_17/MaxPool_10_grad/MaxPoolGrad' (op type: MaxPoolGrad)
```"
11434,"My CNN is not learning ,who can help me fix this problem? Thx !","[CNN.txt](https://github.com/tensorflow/tensorflow/files/1139040/CNN.txt)

That's the whole code of my CNN, the data for training, validation, test are all organized myself, which is not the problem.
While training this CNN, the result in Console showed that all weights and biases did not change. If I replace the RELU in six convolutional layers with Sigmoid, the CNN performed well.
Who can help me fix this problem? Thanks anyway ~"
11433,"My CNN is not learning , who can help me fix this porblem ?","import tensorflow as tf
import Transform_Data_New as td
import numpy as np

max_accuracy=0
timer=0
Epoch=300
learning_rate=1e-3

training_batch_size=100
validation_batch_size=40
test_batch_size=40

len_training_data, validation, test = td.Initialization(validation_batch_size,test_batch_size)
iteration_time=int(len_training_data/training_batch_size)+1

sess=tf.InteractiveSession()

def weight_variable(shape,Name):
    initial = tf.random_normal(shape,stddev=0.2,mean=0.5)
    return tf.Variable(initial,name=Name)

def bias_variable(shape,Name):
    initial=tf.constant(0.1, shape=shape,)
    return tf.Variable(initial,name=Name)

def conv2d(x,w):
    return tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME')

def max_pool_2x2(x):
    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='SAME')

with tf.name_scope('Input'):
    with tf.name_scope('Input_x'):
        x = tf.placeholder(tf.float32,shape=[None,1024])
    with tf.name_scope('Input_y'):
        y_ = tf.placeholder(tf.float32,shape=[None,7])

with tf.name_scope('Conv_1'):
    with tf.name_scope('W_conv1'):
        w_conv1=weight_variable([3,3,1,8],'w_conv1')
    with tf.name_scope('B_conv1'):
        b_conv1=bias_variable([8],'b_conv1')
    with tf.name_scope('x_image'):
        x_image=tf.reshape(x,[-1,32,32,1])
    with tf.name_scope('H_conv1'):
        h_conv1=tf.nn.relu(tf.nn.bias_add(conv2d(x_image,w_conv1),b_conv1))

with tf.name_scope('Conv_2'):
    with tf.name_scope('W_conv2'):
        w_conv2=weight_variable([3,3,8,16],'w_conv2')
    with tf.name_scope('B_conv2'):
        b_conv2=bias_variable([16],'b_conv2')
    with tf.name_scope('H_conv2'):
        h_conv2=tf.nn.relu(tf.nn.bias_add(conv2d(h_conv1,w_conv2),b_conv2))
    with tf.name_scope('H_pool2'):
        h_pool2=max_pool_2x2(h_conv2)

with tf.name_scope('Conv_3'):
    with tf.name_scope('W_conv3'):
        w_conv3=weight_variable([3,3,16,32],'w_conv3')
    with tf.name_scope('B_conv3'):
        b_conv3=bias_variable([32],'b_conv3')
    with tf.name_scope('H_conv3'):
        h_conv3=tf.nn.relu(tf.nn.bias_add(conv2d(h_pool2,w_conv3),b_conv3))

with tf.name_scope('Conv_4'):
    with tf.name_scope('W_conv4'):
        w_conv4=weight_variable([3,3,32,64],'w_conv4')
    with tf.name_scope('B_conv4'):
        b_conv4=bias_variable([64],'b_conv4')
    with tf.name_scope('H_conv4'):
        h_conv4=tf.nn.relu(tf.nn.bias_add(conv2d(h_conv3,w_conv4),b_conv4))
    with tf.name_scope('H_pool4'):
        h_pool4=max_pool_2x2(h_conv4)

with tf.name_scope('Conv_5'):
    with tf.name_scope('W_conv5'):
        w_conv5=weight_variable([3,3,64,128],'w_conv5')
    with tf.name_scope('B_conv5'):
        b_conv5=bias_variable([128],'b_conv5')
    with tf.name_scope('H_conv5'):
        h_conv5=tf.nn.relu(tf.nn.bias_add(conv2d(h_pool4,w_conv5),b_conv5))

with tf.name_scope('Conv_6'):
    with tf.name_scope('W_conv6'):
        w_conv6=weight_variable([3,3,128,256],'w_conv6')
    with tf.name_scope('B_conv6'):
        b_conv6=bias_variable([256],'b_conv6')
    with tf.name_scope('H_conv6'):
        h_conv6=tf.nn.relu(tf.nn.bias_add(conv2d(h_conv5,w_conv6),b_conv6))
    with tf.name_scope('H_pool6'):
        h_pool6=max_pool_2x2(h_conv6)

with tf.name_scope('Full_Connected_Layer_1'):
    with tf.name_scope('W_fc1'):
        w_fc1=weight_variable([4*4*256,1024],'w_fc1')
    with tf.name_scope('B_fc1'):
        b_fc1=bias_variable([1024],'b_fc1')
    with tf.name_scope('H_pool_flat'):
        h_pool_flat=tf.reshape(h_pool6,[-1,4*4*256])
    with tf.name_scope('H_fc1'):
        h_fc1=tf.nn.relu(tf.matmul(h_pool_flat,w_fc1)+b_fc1)

with tf.name_scope('Full_Connected_Layer_2'):
    with tf.name_scope('W_fc2'):
        w_fc2=weight_variable([1024,7],'w_fc2')
    with tf.name_scope('B_fc2'):
        b_fc2=bias_variable([7],'b_fc2')
    with tf.name_scope('Y_conv'):
        y_conv=tf.nn.softmax(tf.matmul(h_fc1,w_fc2)+b_fc2)

with tf.name_scope('Cross_Entropy'):
    cross_entropy=-tf.reduce_sum(y_*tf.log(tf.clip_by_value(y_conv,1e-10,1.0)))
    tf.summary.scalar('Cross_Entropy',cross_entropy)
with tf.name_scope('Train_Step'):
    train_step=tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)
with tf.name_scope('Correct_prediction'):
    distribution=[tf.arg_max(y_,1),tf.arg_max(y_conv,1)]
    correct_prediction=tf.equal(distribution[0],distribution[1])
with tf.name_scope('Accuracy'):
    accuracy=tf.reduce_mean(tf.cast(correct_prediction,""float""))
    tf.summary.scalar('Accuracy',accuracy)

merged=tf.summary.merge_all()
writer=tf.summary.FileWriter('D:/Log',sess.graph)
epoch=0

saver=tf.train.Saver()
First_training=True
checkpoint_dir='D:\\Checkpoint\\model.ckpt'

if First_training==False:
    ckpt=tf.train.get_checkpoint_state(checkpoint_dir)
    if ckpt and ckpt.model_checkpoint_path:
        saver.restore(sess,checkpoint_dir)
else:
    sess.run(tf.global_variables_initializer())

for i in range(Epoch*iteration_time+1):
    #Batch Size
    batch = td.next_batch(training_batch_size)
    train_step.run(feed_dict={x: batch[0], y_: batch[1]})
    if i % (iteration_time) == 0 and i != 0:
        epoch += 1
        train_accuracy=accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})
        validation_accuracy_resultSet = []
        for j in range(len(validation)):
            validation_accuracy = accuracy.eval(feed_dict={x: validation[j][0], y_: validation[j][1]})
            validation_accuracy_resultSet.append(validation_accuracy)
        validation_accuracy = int(np.sum(validation_accuracy_resultSet)) / len(validation_accuracy_resultSet)
        if validation_accuracy<=0.7:
            learning_rate=1e-3
        if validation_accuracy>0.7 and validation_accuracy<=0.8:
            learning_rate=5e-4
        if validation_accuracy>0.8 and validation_accuracy<=0.9:
            learning_rate=1e-4
        if validation_accuracy>0.9:
            learning_rate=5e-5
        print(""Epoch %d , training accuracy %g,Validation Accuracy: %g"" % (epoch, train_accuracy, validation_accuracy))

        result = sess.run(merged, feed_dict={x: batch[0], y_: batch[1]})
        writer.add_summary(result, epoch)
        saver.save(sess,checkpoint_dir,global_step=epoch)
        if validation_accuracy>max_accuracy:
            max_accuracy=validation_accuracy
            timer=0
        else:
            timer+=1
            if timer>10:   #原来是10
                break

confusion_matrics=np.zeros([7,7],dtype=""int"")
test_accuracy_resultSet=[]
for j in range(len(test)):
    matrix_row, matrix_col = sess.run(distribution, feed_dict={x: test[j][0], y_: test[j][1]})
    for m, n in zip(matrix_row, matrix_col):
        confusion_matrics[m][n] += 1
    test_accuracy = accuracy.eval(feed_dict={x: test[j][0], y_: test[j][1]})
    test_accuracy_resultSet.append(test_accuracy)
test_accuracy = np.sum(test_accuracy_resultSet) / len(test_accuracy_resultSet)

print(""Test Accuracy :"",test_accuracy)
print(np.array(confusion_matrics.tolist()))



That's the whole code of my CNN, the data for training, validation, test are all organized myself, which is not the problem.
While training this CNN, the result in Console showed that all weights and biases did not change. If I replace the RELU in six convolutional layers with Sigmoid, the CNN performed well.
Who can help me fix this problem? Thanks anyway ~
"
11431,Rank calculated incorrectly using 'gen_array_ops.rank()',"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 'v1.2.1-0-gb4957ff', '1.2.1'
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: No GPU
- **GPU model and memory**: No GPU
- **Exact command to reproduce**: //tensorflow/python/kernel_tests:transpose_op_test

### The problem
While running TensorFlow tests such as  `//tensorflow/python/kernel_tests:transpose_op_test
` or `//tensorflow/contrib/distributions:mixture_test`, 
on a big endian system they fail with error: `ValueError: Dimension must be 2 but is 0 for 'Mixture_1/sample/Categorical/sample/transpose' (op: 'Transpose') with input shapes: [1,4], [0].`  

After debugging a little, realized that, while calculating `rank` of a tensor in [transpose method](https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/python/ops/array_ops.py#L1276), the rank is calculated incorrectly via `gen_array_ops.rank(a)`.

I tried replacing `rank = gen_array_ops.rank(a)` to use the `array_ops.rank(a)` method instead and it works perfectly fine.

I tried the same change on an x86 machine and it doesn't break anything there.

I am unable to understand the difference between the two methods though. Also is there some major impact of above change which is not very obvious?"
11430,tf.matrix_inverse doesn't support complex tensor,"I'm trying to run the next code:

Code:

    x=tf.placeholder(tf.float64,[2,2])
    y=tf.matrix_inverse(x)

Result OK :

    <tf.Tensor 'MatrixInverse_2:0' shape=(2, 2) dtype=float64>

Code:

    x=tf.placeholder(tf.complex64,[2,2])
    y=tf.matrix_inverse(x)

Result (Error):

    Traceback (most recent call last):

    File ""<ipython-input-163-f259114be54a>"", line 2, in <module>
    y=tf.matrix_inverse(x)

    File ""c:\python\python35\lib\site-packages\tensorflow\python\ops\gen_linalg_ops.py"", line 330, in 
    matrix_inverse
    name=name)

    File ""c:\python\python35\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 585, 
    in apply_op
    param_name=input_name)

    File ""c:\python\python35\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 61, 
    in _SatisfiesTypeConstraint
    "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))

    TypeError: Value passed to parameter 'input' has DataType complex64 not in list of allowed values: 
    float64, float32



It seems that complex matrix inverse is not supported, Any idea for workaround ?"
11428,NameError: name 'eval_input_fn' is not defined,"OS : MacOS 10.12
TensorVersion : 1.2
PythonVersion : 2.7

Run the example of 'A custom model' in the document '[Getting Started With TensorFlow](https://www.tensorflow.org/get_started/get_started#a_custom_model)', occur the exception:
`Traceback (most recent call last):
  File ""linear_regression_tf_contrib_learn_custom_model.py"", line 35, in <module>
    eval_loss = estimator.evaluate(input_fn=eval_input_fn)
NameError: name 'eval_input_fn' is not defined`.

Fix : Need to add the code `eval_input_fn = tf.contrib.learn.io.numpy_input_fn({""x"": x_eval}, y_eval, 4, num_epochs=1000)` at line 30."
11427,"wrong with tf.norm, it will produce nan","I use version 1.0.0 in server, in my code, the result will be nan if i use :
```
result = tf.square(tf.norm(penalize))
``` 
but if i change it to be:
```
result = tf.reduce_sum(tf.square(penalize))
```
everything will be ok, and result is some float between 0 and 1. 

"
11426,Resource exhausted error for translate.py  Seq2seq model (CPU),"Windows 10
Tensorflow: 1.0.0
Python version : 3.5
 I get the following error when i try to run translate.py for Seq2seq model 

Resource exhausted: OOM when allocating tensor with shape[576,1024]
Is there any way i could reduce the size in translate.py so that the error does not occur?
Thanks
"
11422,No clue to inform what 'dimension' arg of argmin or argmax means in API docs,"In documentation of TF API 1.2, `tf.argmin` and `tf.argmax` have `dimension` argument.

* [tf.argmin](https://www.tensorflow.org/api_docs/python/tf/argmin)
* [tf.argmax](https://www.tensorflow.org/api_docs/python/tf/argmax)

However, there is no any explanation for what it means."
11419,KL divergence not in tf.contrib.distributions as of 1.2.0,"Apparently KL divergence is no longer in contrib.distributions.
This page is broken:
https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/kl

and this file is removed:
tensorflow/contrib/distributions/python/ops/kullback_leibler.py

but I couldn't find it being mentioned in the changelog. Did I miss it?"
11417,Apache License header does not include HTTPS link,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12.15
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.2
- **Python version**: 2.7.13

### Describe the problem
Various code files have `http://www.apache.org/licenses/LICENSE-2.0` instead of `https://www.apache.org/licenses/LICENSE-2.0`. Not sure if this is the result of bazel or simply that needs to be explicitly written.

### Source code / logs
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py
"
11416,Distributed training with synchronized SGD using 'grpc+verbs' sometimes hangs indefinitely,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2 LTS
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: v1.2.0-1755-gee4259a 1.2.1
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: 0.5.1
- **CUDA/cuDNN version**: 8.0/5.1
- **GPU model and memory**: GeForce GTX 1080 Ti
- **Exact command to reproduce**: Sorry, the command is not available since this is custom code
- **RDMA driver version**: libibverbs-dev (1.2.1mlnx1-OFED.4.0.1.5.3.40200)

### Describe the problem
I am training a seq2seq model with synchronized SGD (using `tf.train.SyncReplicasOptimizer` and `tf.train.Supervisor`), over RDMA (using `grpc+verbs` protocol), on 2 workers and 1 parameter server. Each worker has 8 GPUs, and the model on each GPU is the same.

I can train this model fine with the default `grpc` protocol using the same setting. When I switched to `grpc+verbs`, the behavior becomes unpredictable. Most of the times both workers hang (for at least 12 hours, so not because I didn't wait long enough). Sometimes the chief worker (worker 0) starts training, but worker 1 hangs. (This should not happen since I am programming with synchronized SGD. Worker 0 should be blocked if worker 1 is not ready.) In rare case it can go through and start training. I believe the RDMA driver is correctly installed.

When worker hangs, the CPU utilization stays low but not zero, and the GPU utilization is 0.

### Source code / logs
I attached below the log from worker 1 [worker1.txt](https://github.com/tensorflow/tensorflow/files/1136607/worker1.txt). In this case, worker 1 hangs, and worker 0 starts training. (There are some custom logs printed.) In the log you can find that worker 1 sent a `RDMA_MESSAGE_TENSOR_REQUEST` to the parameter server, but never got an ACK, and I have checked the log from the parameter server, which showed that it never received a `RDMA_MESSAGE_TENSOR_REQUEST` from worker 1.

I also attached the full trace [gdb_worker1.txt](https://github.com/tensorflow/tensorflow/files/1136672/gdb_worker1.txt) collected on worker 1 in gdb with `thread apply all bt`, during worker 1 hanging.

Thanks.



"
11414,TensorFlow needs a mascot,"Every open source project deserves a mascot. Here's Teensy the TensorFlow Pony, and he's ready to serve:

![img_20170710_073626](https://user-images.githubusercontent.com/161459/28030840-3a882240-655a-11e7-99ae-4469eec0a58a.jpg)
"
11413,Request genuine consecutive scheme batch generation for RNN Trainning,"The concept of “genuine consecutive scheme ” can be seen at [here(5.4. Batch)](http://www.sciencedirect.com/science/article/pii/S088523081400093X?via%3Dihub).

My scenario is as follows:

I have some files with different sequence lengths.

First, do buckecting to generate file-batches with parameter `batch_size`

Then, split each  file-batch with parameter `seq_len` to generate trainning sample-batches

Last, use each sample-batch for one step of trainning.

Following is my test code:

```
# -*- coding:utf8 -*-

import os
import time
import random
import tensorflow as tf
from tensorflow.contrib.training import bucket_by_sequence_length, batch_sequences_with_states


context_features = {
    ""length"": tf.FixedLenFeature([], dtype=tf.int64)
}

sequence_features = {
            ""inputs"": tf.FixedLenSequenceFeature([], dtype=tf.int64),
}

def GenerateFakeData():
    FILE_NUM = 100
    DATA_PATH = ""test_dataset""
    file_path_list, file_len_list = [], []
    for idx in range(FILE_NUM):
        filename = ""{fileno}-of-{idx}"".format(idx=idx+1, fileno=FILE_NUM)
        token_length = random.randint(50, 100)
        ex = tf.train.SequenceExample()
        ex.context.feature[""length""].int64_list.value.append(token_length)
        ###########################################
        ex_tokens = ex.feature_lists.feature_list[""inputs""]
        for tok in range(token_length):
            ex_tokens.feature.add().int64_list.value.append(tok)
        with tf.python_io.TFRecordWriter(os.path.join(DATA_PATH, filename) + "".tfrecord"") as filew:
            filew.write(ex.SerializeToString())
        file_len_list.append(token_length)
        file_path_list.append(os.path.join(DATA_PATH, filename) + "".tfrecord"")
    with open(""filelist.txt"", ""w"") as filew:
        for file_name, file_len in zip(file_path_list, file_len_list):
            filew.write(""{fn}\t{fl}\n"".format(fn=os.path.join(file_name), fl=file_len))

def LoadFileList(filepath):
    with open(filepath, ""r"") as filer:
        wfilelist, wfilelengthlist = tuple(zip(*[tuple(line.strip().split(""\t"")) for line in filer if line.strip() != """"]))
        return list(wfilelist), [int(item) for item in wfilelengthlist]

        
def InputProducer():
    batch_size = 2
    seq_len = 75
    state_size = 1024
    bucket_boundaries = [60, 70, 80, 90]
    #####################################
    filelist, filelengthlist = LoadFileList(""filelist.txt"")
    #####################################
    tf_file_queue = tf.train.string_input_producer(
            string_tensor = filelist, 
            num_epochs = 1, 
            shuffle = False, 
            seed = None, 
            capacity = 32, 
            shared_name = None,
            name = ""tf_file_queue"",
            cancel_op=None
    )
    ######################################
    tf_reader = tf.TFRecordReader()
    tf_key, tf_serialized = tf_reader.read(tf_file_queue)
    tf_context, tf_sequence = tf.parse_single_sequence_example(
            serialized = tf_serialized,
            context_features = context_features,
            sequence_features = sequence_features
    )
    ######################################
    tf_bucket_sequence_length, tf_bucket_outputs = bucket_by_sequence_length(
        input_length = tf.cast(tf_context[""length""], dtype=tf.int32), 
        tensors = tf_sequence, 
        batch_size = batch_size, 
        bucket_boundaries = bucket_boundaries, 
        num_threads=1, 
        capacity=32, 
        shapes=None, 
        dynamic_pad=True,
        allow_smaller_final_batch=False, 
        keep_input=True, 
        shared_name=None, 
        name=""bucket_files""
    )
    #######################################
    tf_bbucket_outputs = {}
    for fkey in tf_bucket_outputs:
        tf_bbucket_outputs[fkey]=tf_bucket_outputs[fkey][0]
    #######################################
    # Solution 1:
    tf_fb_key=time.strftime('%Y-%m-%d-%H-%M-%S',time.localtime(time.time())) + str(random.randint(1,100000000))
    initial_state_values = tf.zeros((state_size,), dtype=tf.float32)
    initial_states = {""lstm_state"": initial_state_values}
    tf_batch=batch_sequences_with_states(
        input_key = tf_fb_key, 
        input_sequences = tf_bbucket_outputs, 
        input_context = {}, 
        input_length = tf.reduce_max(tf_bucket_sequence_length), 
        initial_states=initial_states, 
        num_unroll=seq_len, 
        batch_size=batch_size, 
        num_threads=3, 
        capacity=1000, 
        allow_small_batch=False, 
        pad=True, 
        name=None)
    #######################################
    # Solution 2:
    '''
    tf_index_queue=tf.train.range_input_producer(
        limit=tf.reduce_max(tf_bucket_sequence_length),
        num_epochs=1, 
        shuffle=False, 
        seed=None, 
        capacity=32, 
        shared_name=None, 
        name=None
    )
    tf_index=tf_index_queue.dequeue()
    tf_batch=tf_bbucket_outputs[""inputs""][tf_index]
    '''
    #######################################
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess, coord)
        try:
            while True:
                #####################################
                # Test Bucketing
                #bucket_sequence_length, bucket_outputs = sess.run([tf_bucket_sequence_length, tf_bucket_outputs])
                #print(bucket_sequence_length)
                #print(bucket_outputs)
                #print(""#################"")
                #####################################
                # Test Solution 1:
                batch = sess.run(tf_batch)
                print(batch)
                #####################################
                # Test Solution 2:
                #bucket_sequence_length, bucket_outputs, index = sess.run([tf_bucket_sequence_length, tf_bucket_outputs, tf_index])
                #print(bucket_sequence_length)
                #print(bucket_outputs)
                #print(index)
                #print(""#################"")
        except tf.errors.OutOfRangeError:
            pass
        except tf.errors.InvalidArgumentError:
            pass
        finally:
            coord.request_stop()
        coord.join(threads)
    
if __name__ == ""__main__"":
    #GenerateFakeData()
    InputProducer()
    pass
```

With Solution 1, I raised the error as below:

```
Traceback (most recent call last):
  
    File ""/home/yangming/workspace/tfstudy-3.5.3-tf-1.1.0/BatchSchemas/make_test_dataset.py"", line 157, in <module>
    
InputProducer()
  
    File ""/home/yangming/workspace/tfstudy-3.5.3-tf-1.1.0/BatchSchemas/make_test_dataset.py"", line 107, in InputProducer
    
name=None)
  
    File ""/home/yangming/.pyenv/versions/tfstudy-3.5.3/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py"", line 1522, in batch_sequences_with_states
    
allow_small_batch=allow_small_batch)
  
    File ""/home/yangming/.pyenv/versions/tfstudy-3.5.3/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py"", line 849, in __init__
    
initial_states)
  
    File ""/home/yangming/.pyenv/versions/tfstudy-3.5.3/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py"", line 332, in _prepare_sequence_inputs
    
""sequence"", inputs.sequences, ignore_first_dimension=True)
  
    File ""/home/yangming/.pyenv/versions/tfstudy-3.5.3/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py"", line 326, in _assert_fully_defined
    
ignore_first_dimension else """", v.get_shape()))
ValueError: Shape for sequence inputs is not fully defined (ignoring first dimension): (?, ?)
```

See the document of `batch_sequences_with_states`, I found that 

1. it seems only support only one sequence and don't support multiple sequences .
2. it don't support the situation of `Shape for sequence inputs is not fully defined`, which means `bucket_by_sequence_length` can not be followed with `batch_sequences_with_states`.

What more, I have tried Solution 2, but I failed because of thread synchronization problem between `tf.train.string_input_producer` and `tf.train.range_input_producer`.

So, how to relize my request ?

Hope for your help."
11411,Fetching data in Distributed Tensorflow has too much latency ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

https://gist.github.com/ahaider3/ae4f6d2d790d963a93b346bb0138a41d

The above is a simple benchmark which tests the overhead of distributed TF. It fetches a configurable sized variable from the parameter server and does a matmul on the worker. It also does a matmul from a locally stored variable on the worker. The time difference between these two operations would be the overhead I am measuring. 

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Redhat  
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
tensorflow 1.2 
- **Python version**: 
python 2.7.7



- **Exact command to reproduce**:

python matmul_benchmark.py --num_features=256 --batch_size=128 --num_hidden=64 --job_name=ps &

python matmul_benchmark.py --num_features=256 --batch_size=128 --num_hidden=64 --job_name=worker

By increasing batch_size, the timing difference between local/remote computation eventually becomes negligible. 
However, for small batch sizes the overhead can become 2x/3x:

For example, here are two runs for different model/dataset sizes:

> 128 features, batch size of 32, hidden layer size of 256  returns:
Local GEMM Time:  0.0002624  Network Fetch GEMM Time: 0.0006798

> 256 features, batch size of 128, hidden layer size of 128  returns:
Local GEMM Time: 0.0002995 Network Fetch GEMM Time: 0.0006124

### Describe the problem
Distributed tensorflow introduces overhead due to its communication stack. By overhead I mean the additional time required for workers to receive data from parameter servers when compared to doing the same computation without fetching any remote data. 

This is a problem because due to this overhead I have to use 2/3 nodes to just provide performance on-par with non-distributed (single process) tensorflow. The number of nodes required to be on-par with single process TF increases further when I use gpus. 


Fetching small variables provides a constant overhead which limits scaling and efficiency . 
This overhead creates two issues in Distributed Tensorflow:
1) I have to add several workers just to equal the performance of a single process. 
2) The overhead of fetching model parameters doesn't scale but the amount of computation does 
decrease as I add more workers. Thus, once I get to a moderately small batch size for each worker I can't scale because the constant overhead of fetching remote model parameters. 

There have been several issues posted with distributed tensorflow. #6116 is an improvement to large tensor transfer while this problem exists for small tensors. #4498 might have been caused by CPU performance bottleneck and not network. However for my problem, network transfer is definitely the bottleneck. I have tried using RDMA and have seen minimal benefit. 

### Source code / logs
https://gist.github.com/ahaider3/ae4f6d2d790d963a93b346bb0138a41d



"
11407,how I upgrade my code(build on Tensorflow 1.0.1 python3) to compatible with TF1.2?,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/45007249/how-i-upgrade-my-codebuild-on-tensorflow-1-0-1-python3-to-compatible-with-tf1"
11405,Finding position of detected object,"Is it possible to find the position of detected object with accuracy in given image using tensor flow?﻿
I am using label_image.py to test my trained data how can I get the position of detected object as I am getting the accuracy of detected object?

Reference:
https://stackoverflow.com/questions/44942587/save-image-of-detected-object-from-image-using-tensor-flow?noredirect=1#comment76982154_44942587"
11404,Build TF 1.2 error: no such package '@nccl_archive//' OR undeclared inclusion(s) in rule '@nccl_archive//:nccl': this rule is missing dependency declarations for the following files included by 'external/nccl_archive/src/libwrap.cu.cc':,"Hi，I compiled tensorflow r1.0 successfully with Centos 7.0，cuda 8.0 and CUDNN 6.5，gcc 4.8.5.  Recently，We want follow the latest version of Tensorflow，error below are always occur:

`ERROR: /home/jiangbo/tensorflow/tensorflow/tools/pip_package/BUILD:76:1: no such package '@nccl_archive//': /home/jiangbo/.cache/bazel/_bazel_jiangbo/0c80707bb0528f07a36b6bc1e1bf9b14/external/nccl_archive/build (Operation not permitted) and referenced by '//tensorflow/tools/pip_package:licenses'.
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.
`
My bazel version have tried 0.4.5 and 0.5.2, but compile error.  Some method to solve this problem I have tried but no one helps , any body knows this problem, please tell me or let me know you thought.

After I compiled TF r1.2 error, I have checkout to r1.0. it is weird, the same error info again.
"
11403,Feature request: explicit state interfaces,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source 
- **TensorFlow version (use command below)**: 1.2.0
- **Python version**: 2.7.12
 
### Description:

The current API is immensely stateful. Each function and object creation affects and is affected by countless mutable global states. Running a declaration of `Variable` or summary mutates the graph and collections and may return different results based on the current `NameScope` or `VariableScope` or `control_dependencies`. A function running twice can easily return wildly different result, even though it *looks* pure (but it isn't because almost no tensorflow function is pure). Declaring something and then deleting it won't restore the global state, as it may or may not be added somewhere in the graph or the collections or some other hidden objects. Reasoning about what changes what is incredibly difficult.

What I would like to see is a API where all state mutations are explicit. When I declare a variable, it is just a variable, and not added to anything unless I call `add`. The current entangled interfaces can be built on top of that, preserving convenience functions such as `global_variable_initializers` for those who prefer."
