Issue Number,Issue Title,Issue Body
4931,Couldn't open CUDA library libcuda.so.1 in when building using nvidia docker cuda8 image,"We built tensorflow on nvidia-docker image nvidia/cuda:8.0-cudnn5-devel
""""""
However, when using the package we built, we run into this problem:
I tensorflow/stream_executor/dso_loader.cc:105] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:
""""""
In this special Dockerfile, we are using whatever LD_LIBRARY_PATH defined by nvidia docker.

It looks like we are looking specificly for the file libcuda.so.1
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/dso_loader.cc#L80

And it looks like we switched to doing this due to:
https://github.com/tensorflow/tensorflow/issues/2865

However, libcuda.so.1 is not linked/made available by nvidia-docker in the docker image.

Side note: it seems to be not available in our docker images based on 7.5, but that one somehow works.
No idea how.

@flx42 could you help us shed some light into this?
"
4930,Example MNIST_RNN not working,"Hello everyone,

I use the very last docker container, GPU Enabled (nvidia-docker). 
Everything works fine for what I could have tested so far.

Host : Ubuntu 14.04 Server with CUDA and NVIDIA drivers up to date

Except, this example is not working anymore :
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/mnist_rnn.py

**AttributeError: 'LSTMStateTuple' object has no attribute 'get_shape'**

![screen shot 2016-10-13 at 01 41 19](https://cloud.githubusercontent.com/assets/10923599/19331714/2fac0baa-90e6-11e6-8027-5f68dd10b160.png)
"
4929,Example Digits Not working,"Hello everyone,

I use the very last docker container, GPU Enabled (nvidia-docker). 
Everything works fine for what I could have tested so far.

Host : Ubuntu 14.04 Server with CUDA and NVIDIA drivers up to date

Except, this example is not working anymore : https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/digits.py

Can't manage to make it work and understand the error : InvalidArgumentError: No OpKernel was registered to support Op 'Conv2D' with these attrs.  Registered kernels:
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_HALF]

I have uploaded a screenshot of the error:

![screen shot 2016-10-13 at 01 36 23](https://cloud.githubusercontent.com/assets/10923599/19331543/83dea378-90e5-11e6-9a7b-27920d47f013.png)
"
4928,"When swig is not available, ./configure exits silently","### Environment info

Operating System:
Ubuntu 16.04 

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
cuda 8.0, cudnn does not matter, installation issue

If installed from binary pip package, provide:
Installing from sources
1. The commit hash (`git rev-parse HEAD`) 55bd9ccdb21a41b7c4046a1c1f2115066ed1f9bc
2. The output of `bazel version` 0.3.2
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

git clone github.com/tensorflow/tensorflow
./configure

exits before full config  is complete silently.

@caisq  @davidzchen any ideas?
"
4926,unable to build tensorflow for GPU,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
### Environment info

Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:
1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
### What other attempted solutions have you tried?
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).

bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
..
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
WARNING: /home/ammalik/TensorFlow/tensorflow/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future.
INFO: Found 1 target...
ERROR: /home/ammalik/.cache/bazel/_bazel_ammalik/27908a9a0ff0347c2aebe63a8fa99002/external/zlib_archive/BUILD:7:1: undeclared inclusion(s) in rule '@zlib_archive//:zlib':
this rule is missing dependency declarations for the following files included by 'external/zlib_archive/zlib-1.2.8/compress.c':
  '/opt/local/gcc/6.2.0/lib/gcc/x86_64-pc-linux-gnu/6.2.0/include-fixed/limits.h'
  '/opt/local/gcc/6.2.0/lib/gcc/x86_64-pc-linux-gnu/6.2.0/include-fixed/syslimits.h'
  '/opt/local/gcc/6.2.0/lib/gcc/x86_64-pc-linux-gnu/6.2.0/include/stddef.h'
  '/opt/local/gcc/6.2.0/lib/gcc/x86_64-pc-linux-gnu/6.2.0/include/stdarg.h'.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 962.954s, Critical Path: 0.69s
"
4924,Using xavier_initializer causes pyglet to fail to display,"Using tf.contrib.layers.xavier_initializer causes pyglet GLX initialization (and therefore gym) to fail. The stack trace is attached. It is not at all clear how calling xavier_initializer can cause GLX initialization to fail.

```
$ python test.py # See test.py below.
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
[2016-10-12 14:09:54,402] Making new env: Pong-v0
Traceback (most recent call last):
  File ""test.py"", line 11, in <module>
    env.render()
  File ""/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/gym/core.py"", line 192, in render
    return self._render(mode=mode, close=close)
  File ""/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/gym/envs/atari/atari_env.py"", line 119, in _render
    from gym.envs.classic_control import rendering
  File ""/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/gym/envs/classic_control/rendering.py"", line 23, in <module>
    from pyglet.gl import *
  File ""/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/gl/__init__.py"", line 236, in <module>
    import pyglet.window
  File ""/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/window/__init__.py"", line 1816, in <module>
    gl._create_shadow_window()
  File ""/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/gl/__init__.py"", line 205, in _create_shadow_window
    _shadow_window = Window(width=1, height=1, visible=False)
  File ""/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/window/xlib/__init__.py"", line 163, in __init__
    super(XlibWindow, self).__init__(*args, **kwargs)
  File ""/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/window/__init__.py"", line 504, in __init__
    config = screen.get_best_config(template_config)
  File ""/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/canvas/base.py"", line 161, in get_best_config
    configs = self.get_matching_configs(template)
  File ""/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/canvas/xlib.py"", line 179, in get_matching_configs
    configs = template.match(canvas)
  File ""/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/gl/xlib.py"", line 29, in match
    have_13 = info.have_version(1, 3)
  File ""/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/gl/glx_info.py"", line 86, in have_version
    client_version = self.get_client_version().split()[0]
  File ""/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/gl/glx_info.py"", line 118, in get_client_version
    return asstr(glXGetClientString(self.display, GLX_VERSION))
  File ""/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/compat.py"", line 88, in asstr
    return s.decode(""utf-8"")
AttributeError: 'NoneType' object has no attribute 'decode'
```

Here is my environment information.

Operating system: Ubuntu 14.04.5
CUDA 7.5 and CuDNN 5.1.3 See [cuda-versions.txt](https://github.com/tensorflow/tensorflow/files/525655/cuda-versions.txt) for details.
Python 3.4, Gym 0.4.2, Pyglet 1.2.4
TensorFlow 0.11.0rc0 installed from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp34-cp34m-linux_x86_64.whl.

Here is some minimal code that exhibits the problem (this is the test.py that causes the stack trace above.) Everything is fine until the random call tries to set up GLX.

```
import gym
import random
import tensorflow as tf

xavier_init = tf.contrib.layers.xavier_initializer()

env = gym.make('Pong-v0')
env.reset()
for _ in range(90):
    env.step(random.randint(0, env.action_space.n - 1))
    env.render()
```

Commenting the xavier_init line fixes the problem.
"
4923,"Tutorial on contrib (load_csv) no longer works, please suggest solution","I try to follow the turotial here:

https://www.tensorflow.org/versions/r0.11/tutorials/tflearn/index.html

But I get the error:

221:tensorflow_tutorial ME$ python contrib_learn_quick_origin.py 
Traceback (most recent call last):
  File ""contrib_learn_quick_origin.py"", line 13, in <module>
    training_set = tf.contrib.learn.datasets.base.load_csv(filename=IRIS_TRAINING,
AttributeError: 'module' object has no attribute 'load_csv'

Please suggests solutions and edit the online tutorial.
"
4922,Android: read model file from arbitrary directory rather than from assets folder,"Currently, TensorFlowInferenceInterface's method initializeTensorFlow() and its underlying C implementation (in tensorflow_inference_jni.cc and jni_utils.cc) require the usage of Android's AssetManager in order to read model files stored in the assets folder. 

How can we modify TF's code to provide an alternative way to run the classification using a model file that's not stored in the assets folder (i.e. a location described by a File object, or by a String path pointing to another location outside the assets folder).

Is there branch where someone might be working on this? I have poor C skills so as to make such change by myself, but maybe I can work on something with some guidance.

Thanks.

Bruno.
"
4921,FileSystem::GetMatchingPaths doesn't work for simple pattern,"With latest tensorflow.

``` bash
$ ls
test
```

``` cpp
std::vector<string> results;
auto ret = tensorflow::Env::Default()->GetMatchingPaths(""te*"", &results);
cout << results.size() << endl;  // 0
ret = tensorflow::Env::Default()->GetMatchingPaths(""./te*"", &results);
cout << results.size() << endl;  // 1
```

Reason:
In the implementation of `GetMatchingPaths` [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/file_system.cc#L94), when the pattern doesn't contain any directory, the prefix `.` is prepended to all elements in `all_files`, and then `MatchPath(""./test"", ""te*"")` evals to false.
"
4920,Better support for initializing variables that depend on each other,"Currently there's no easy way to properly initialize variables when some variables initial values depend on other variables, and initialization has to be split over several `.run` calls. This kind of initialization happens with data-dependent parameter init.

This could be solved if there were something like `var.initialized_value()`, but which only runs initializer if the variable hasn't been initialized already.

Example:

```
a = tf.get_variable(""a"", shape=())
b = tf.get_variable(""b"", initializer=a.initialized_value())
c = tf.placeholder(tf.float32, shape=())
d = tf.get_variable(""d"", initializer=a.initialized_value()+c)

sess.run([a.initializer, b.initializer])
sess.run([d.initializer], feed_dict={c: 0})
sess.run([a, b, d])

Out[]: [0.30858743, -1.2756943, 0.30858743]
```

Here, `a` and `b` end up with different values because initializer for `a` has been run twice, which is counter-intuitive, the user expects `a,b,d` to have same values
"
4917,QueueRunner deadlock when using all CPUs,"I'm building an input pipeline following the guidelines [here](https://www.tensorflow.org/versions/r0.11/how_tos/reading_data/index.html).  The pipeline needs to queue up a bunch of input examples, process each example using a python function (inserted into the tensorflow computation graph using `tf.py_func`), and return the processed results to an output queue.  I'd like to use `QueueRunner`'s ability to process multiple examples in parallel by launching one processing thread per CPU core.  Here's a simplified example of what I'm trying to do:

``` python
import numpy as np
import multiprocessing
import tensorflow as tf

n_cpus = multiprocessing.cpu_count()

sess = tf.Session()
a = tf.placeholder(tf.float32)
b = tf.placeholder(tf.float32)
mult = tf.mul(a, b)

def python_op(x):
    print ""python_op called with {}"".format(x)
    # In my real function, the np.cos and np.sin calls are replaced by
    # python calculations I can't do in tensorflow
    y = np.cos(x)
    z = sess.run(mult, feed_dict={a: y, b: x})
    print ""intermediate result is {}"".format(z)
    return np.sin(z)

n_inputs = n_cpus
input_queue = tf.FIFOQueue(10000, [tf.float32], shapes=[1])
load_input = input_queue.enqueue_many(np.random.random((n_inputs, 1)))

output_queue = tf.FIFOQueue(10000, [tf.float32], shapes=[1])
get_result = output_queue.dequeue_many(n_inputs)

def processing_pipeline():
    input_value = input_queue.dequeue()
    return output_queue.enqueue(tf.py_func(python_op, [input_value], [tf.float32], False))

# Here's the problem: If we use all CPUs here, the program will deadlock.
# If we change cpus to (cpus-1), it works as expected.
runner = tf.train.QueueRunner(output_queue, [processing_pipeline()] * (n_cpus))

coord = tf.train.Coordinator()
runner.create_threads(sess, coord=coord, start=True)

print ""Loading input""
sess.run(load_input)
sess.run(input_queue.close())

try:
    print ""waiting for result""
    result = sess.run(get_result)
    print ""RESULT: {}"".format(result)
except tf.errors.OutOfRangeError:
    print ""Input exhausted""

coord.request_stop()
coord.join()
print ""Done""
```

The program above deadlocks waiting for `sess.run` to complete in `python_op`:

```
$ python queuetest.py                                                                                                                                                                                                                                                  
Loading input
python_op called with [ 0.65624136]
 python_op called with [ 0.80651367]
python_op called with [ 0.31998941]
 python_op called with [ 0.726421]
 python_op called with [ 0.33133706]
python_op called with [ 0.4912357]
python_op called with [ 0.27365881]
python_op called with [ 0.32846987]
```

This is running on an 8-core machine; you can see that 8 `python_op`s are currently running but are failing to finish.  If, however, we don't use all CPUs (by changing `(n_cpus)` to `(n_cpus-1)` in the line that creates the `tf.train.QueueRunner`, then the program runs to completion:

```
$ python queuetest.py                                                                                                                                                                                                                                                        
Loading input
python_op called with [ 0.40804103]
python_op called with [ 0.0182138]
python_op called with [ 0.17579727]
 python_op called with [ 0.29143187]
python_op called with [ 0.11612369]
 intermediate result is [ 0.37454084]
python_op called with [ 0.679506]
python_op called with [ 0.50754625]
intermediate result is [ 0.01821078]
intermediate result is [ 0.52857631]
 intermediate result is [ 0.27914321]
waiting for result
python_op called with [ 0.68288684]
 intermediate result is [ 0.44356483]
intermediate result is [ 0.11534163]
 intermediate result is [ 0.17308778]
intermediate result is [ 0.52975237]
RESULT: [[ 0.36584523]
 [ 0.01820978]
 [ 0.50430447]
 [ 0.42916203]
 [ 0.11508605]
 [ 0.27553213]
 [ 0.1722248 ]
 [ 0.50531965]]
Done
```

The program also completes successfully if we pass in fewer examples than CPUs in the input queue.

I realize it's somewhat awkward for `python_op` to call back into the tensorflow session.  However, the [threading and queues](https://www.tensorflow.org/versions/r0.11/how_tos/threading_and_queues/index.html#threading-and-queues) section of the manual states:

""The TensorFlow Session object is multithreaded, so multiple threads can easily use the same session and run ops in parallel.""

So, I assumed that the use case I've outlined above should be supported, and it seems surprising that it works in some cases (fewer threads than CPUs) and doesn't in others (one thread per CPU).  Is this a bug, or is there some reason I shouldn't expect it to work?

As a side note, one option to work around my problems would be to break `python_op` into several smaller pieces and chain the intermediate results together in the tensorflow computation graph.  However, in my real pipeline, this isn't straightforward to do, since `python_op`'s real output depends on a number of intermediate, native python data structures that aren't easily converted into tensors that could be spliced into the tensorflow graph.

OS: Linux
Tensorflow versions: 0.10.0, 0.11.0-rc0 (CPU-only version)
"
4915,"Unable to build label_image example, master branch","I am on OSX 10.11.12 (El Capitan)
Installed from source, on master branch 
git rev-parse HEAD: a566a7701381a5cf7f70fce397759483764e482
bazel version: 0.3.2-homebrew

I am getting a series of errors when trying to build the label_image example from a freshly downloaded and configured (all default options, no CUDA, GPU, or Hadoop support) tensorflow source. 

bazel build tensorflow/examples/label_image ...

output:

ERROR: /Users/corey/Documents/school/hpc/tensorflow/tensorflow/core/BUILD:353:1: C++ compilation of rule '//tensorflow/core:string_ops_op_lib' failed: cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer '-std=c++0x' -MD -MF ... (remaining 92 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.

tensorflow/core/ops/string_ops.cc:186:11: error: return type 'const ::tensorflow::Status' must match previous return type 'tensorflow::Status' when lambda expression has unspecified explicit return type
          TF_RETURN_IF_ERROR(c->Merge(out, c->input(i), &out));
          ^
./tensorflow/core/lib/core/errors.h:43:42: note: expanded from macro 'TF_RETURN_IF_ERROR'
    if (TF_PREDICT_FALSE(!_status.ok())) return _status; \
                                         ^
tensorflow/core/ops/string_ops.cc:216:7: error: return type 'tensorflow::Status' must match previous return type 'const ::tensorflow::Status' when lambda expression has unspecified explicit return type
      return Status::OK();
      ^
tensorflow/core/ops/string_ops.cc:208:17: error: no viable conversion from 'tensorflow::(lambda at tensorflow/core/ops/string_ops.cc:208:17)' to 'tensorflow::Status (_)(shape_inference::InferenceContext *)'
    .SetShapeFn([](InferenceContext* c) {
                ^~~~~~~~~~~~~~~~~~~~~~~~~
tensorflow/core/ops/string_ops.cc:208:17: note: candidate function
./tensorflow/core/framework/op.h:247:16: note: passing argument to parameter 'fn' here
      Status (_fn)(shape_inference::InferenceContext*)) {
               ^
3 errors generated.

Thanks for any advice or help!
"
4914,Keeping gradient of sqrt(x) stable for x = 0,"I'm minimizing a function that contains a few `tf.sqrt(c * x)` terms. The `x` is a `tf.Variable` and `c` is a `tf.constant` that is sometimes zero. A `NaN` inevitably presents itself. In my case, the gradient is to `c`, which is `x * 0.5/sqrt(c * x)` and which equals `0 * inf = NaN` when `c` is `0`.

When such a `sqrt` is deeply buried in your function, it can be quite an effort to dig out where the `NaN` is coming from. I can understand and appreciate the fact that there is no check for zero in the `sqrt_grad` operator. However, I feel that debugging could be easier for ops that are known to be unstable in some numerical range.

Two possible fixes would be:
1. Add exceptions to the documentation of these ops. Right now this is not even indicated for `tf.div`, for instance. Since the use-cases of TensorFlow almost always mean that gradients will be involved, the allowed range should also be mentioned for the gradient, if different from that of the op itself.
2. Add debug-mode versions of the ops. These could include `NaN` and `inf` checks.

By the way, I was using the `tf.contrib.opt.ScipyOptimizerInterface` for the minimization, which does not support manually changing the gradients by using `compute_gradients` and `apply_gradients`. That's beside the point, though.

Below some example code for completeness' sake. The differences in outcome only add to the confusion.

``` python
from __future__ import absolute_import, division, print_function
import tensorflow as tf

c = tf.Variable(0.0)

sqrt_grad = tf.gradients(tf.sqrt(c), c)

# another possibility is when another factor in the argument is zero
x = tf.Variable(1.)
sqrt_x_grad = tf.gradients(tf.sqrt(x * c), x)

# try to use select to filter out the NaN
selsqrt_grad = tf.gradients(tf.select(c > 0, tf.sqrt(c), 0), c)

# try clipping of the sqrt
clipsqrt_grad = tf.gradients(tf.clip_by_value(tf.sqrt(c), 1e-10, 1), c)

# clip the argument of the sqrt --> only numerically stable option
clipargsqrt_grad = tf.gradients(tf.sqrt(tf.clip_by_value(c, 1e-10, 1)), c)

init_op = tf.initialize_all_variables()

with tf.Session() as sess:
    sess.run(init_op)

    print(sess.run([sqrt_grad, sqrt_x_grad, selsqrt_grad,
                    clipsqrt_grad, clipargsqrt_grad]))
    # [[inf], [nan], [nan], [nan], [0.0]]
```
"
4913,Saver returning paths that cause GetMatchingPaths to go digging around in parent dirs,"Note: This works without problems in 0.10.0rc0 (default pip install-ed according to website).
### Environment info

Operating System:
Ubuntu 15.10

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

```
-rw-r--r-- 1 root root 189170 Jan  1  2016 /usr/local/cuda/lib/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Jan  1  2016 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19 Jan  1  2016 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 311596 Jan  1  2016 /usr/local/cuda/lib/libcudart.so.7.5.18
-rw-r--r-- 1 root root 558020 Jan  1  2016 /usr/local/cuda/lib/libcudart_static.a
```

If installed from binary pip package, provide:
1. A link to the pip package you installed. Tried with two packages:
   -  `https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl
     `
   -  `https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl`
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

```
I tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcurand.so locally
0.11.0rc0
```
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
### What other attempted solutions have you tried?
### Logs or other output that would be helpful

Relevant part of error (the file `/tmp/6d7079c109e5_0-saver_853` is supposed to be loaded by `self.saver.restore` (tf.Saver instance), it exists).

```
Parameters according to early stopping: /tmp/6d7079c109e5_0-saver_853
Training finished after 854/854 iterations.
Traceback (most recent call last):
  File ""run.py"", line 297, in <module>
    run(config)
  File ""step.py"", line 109, in run
    step(config=config)
  File ""step.py"", line 237, in  step
    model.fit(dset, config)
  File ""model.py"", line 210, in fit
    self.saver.restore(self.sess, save_path=early_stop_name)
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1360, in restore
    if not file_io.get_matching_files(file_path):
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 254, in get_matching_files
    compat.as_bytes(filename), status)]
  File ""/anaconda/lib/python2.7/contextlib.py"", line 24, in __exit__
    self.gen.next()
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/errors.py"", line 463, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors.PermissionDeniedError: /tmp/systemd-private-a9da6f279d7b4869b7f687b66db5c7dd-rtkit-daemon.service-xozhXc
```

After restarting the computer and re-running the above, the `PermessionDeniedError` now points at `tensorflow.python.framework.errors.PermissionDeniedError: /tmp/systemd-private-bc171111db824721b4f4cec0e0a363ec-systemd-timesyncd.service-MARv8I`
"
4912,"""no module named tensorflow"" when I ""import tensorflow"" after I install tensorflow on windows successfully","I followed ..\tensorflow\contrib\cmake\README to install tensorflow on windows. Everything went well and I installed tensorflow successfully. But after I ""activate tensorflow"", and tried to ""import tensorflow"" using python, it went wrong, saying ""no module named tensorflow"". How can I fix this problem? Many thanks!
"
4911,Greenish image when using convertYUV420ToARGB8888,"I'm experimenting the [TensorFlow Android Camera Demo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android). but it seems that the [ImageUtils.convertYUV420ToARGB8888](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageListener.java#L164) used to convert images from the camera input to RGB  bytes array is not handling very well the [YUV_420_888 format](https://developer.android.com/reference/android/graphics/ImageFormat.html).

Here is the output of `ImageUtils.saveBitmap(rgbFrameBitmap);`

![](http://i.imgur.com/bsL1czy.png)
"
4910,"Ubuntu 16.04 + CUDA8.0, GPU build from source: C++ compilation fails (crosstool_wrapper_driver_is_not_gcc failed)","### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

[Issue 190](https://github.com/tensorflow/tensorflow/issues/190) is the same, but was closed & pointed to a [bazel issue that has been fixed](https://github.com/bazelbuild/bazel/issues/359).
### Environment info

Operating System: **Ubuntu 16.04**

Installed version of CUDA and cuDNN: **8.0.44 + 5.1.5**

(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

```
-rw-r--r-- 1 root root   558720 Okt 11 19:24 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Okt 11 19:24 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Okt 11 19:24 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rwxr-xr-x 1 root root   415432 Okt 11 19:24 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 Okt 11 19:24 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 Okt 11 19:47 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       17 Okt 11 19:47 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxr-xr-x 1 root root 79337624 Okt 11 19:47 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Okt 11 19:47 /usr/local/cuda/lib64/libcudnn_static.a
```

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`): **1975cd1e9d539e75a1b85b56f16448c91ef88d90**
2. The output of `bazel version`: **0.3.2**

```
Build label: 0.3.2
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Oct 7 17:25:10 2016 (1475861110)
Build timestamp: 1475861110
Build timestamp as int: 1475861110
```
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

It occurred when running both training example & pip build (same error message):

```
bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer
bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```
### What other attempted solutions have you tried?

I have followed both [this tutorial](http://www.computervisionbytecnalia.com/en/2016/06/deep-learning-development-setup-for-ubuntu-16-04-xenial/) and [this one](https://alliseesolutions.wordpress.com/2016/09/08/install-gpu-tensorflow-from-sources-w-ubuntu-16-04-and-cuda-8-0-rc/).
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).

Last message before the error was:

```
INFO: From Compiling tensorflow/core/kernels/string_split_op.cc:
```

The full error message, using `--verbose_failures`:

```
ERROR: /home/sebastien/tensorflow/tensorflow/core/kernels/BUILD:1199:1: C++ compilation of rule '//tensorflow/core/kernels:determinant_op' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/sebastien/.cache/bazel/_bazel_sebastien/f91199c4da2d428eb9d05b40a2d00b4e/execroot/tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=:/usr/local/cuda/lib64:/usr/progtools/caffe-nv/distribute/lib:/usr/local/cuda/lib64 \
    PATH=/usr/local/cuda/bin:/home/sebastien/bin:/home/sebastien/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -D_FORCE_INLINES -D_MWAITXINTRIN_H_INCLUDED '-std=c++11' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/determinant_op/tensorflow/core/kernels/determinant_op.d '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/determinant_op/tensorflow/core/kernels/determinant_op.o' -DEIGEN_MPL2_ONLY -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/protobuf -iquote bazel-out/local_linux-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/com_googlesource_code_re2 -iquote bazel-out/local_linux-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local_linux-opt/genfiles/external/farmhash_archive -iquote external/gif_archive -iquote bazel-out/local_linux-opt/genfiles/external/gif_archive -iquote external/highwayhash -iquote bazel-out/local_linux-opt/genfiles/external/highwayhash -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local_linux-opt/genfiles/external/zlib_archive -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -iquote external/local_config_cuda -iquote bazel-out/local_linux-opt/genfiles/external/local_config_cuda -isystem external/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/farmhash_archive -isystem bazel-out/local_linux-opt/genfiles/external/farmhash_archive -isystem external/gif_archive -isystem bazel-out/local_linux-opt/genfiles/external/gif_archive -isystem external/highwayhash -isystem bazel-out/local_linux-opt/genfiles/external/highwayhash -isystem external/jpeg_archive -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive -isystem external/png_archive -isystem bazel-out/local_linux-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local_linux-opt/genfiles/external/zlib_archive -isystem external/eigen_archive -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem external/local_config_cuda/cuda/include -isystem bazel-out/local_linux-opt/genfiles/external/local_config_cuda/cuda/include -isystem external/local_config_cuda/cuda -isystem bazel-out/local_linux-opt/genfiles/external/local_config_cuda/cuda -fno-exceptions -DEIGEN_AVOID_STL_ARRAY '-DGOOGLE_CUDA=1' -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c tensorflow/core/kernels/determinant_op.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/determinant_op/tensorflow/core/kernels/determinant_op.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
virtual memory exhausted: Cannot allocate memory
Target //tensorflow/cc:tutorials_example_trainer failed to build
```

[build_error_msg.txt](https://github.com/tensorflow/tensorflow/files/524061/build_error_msg.txt)
"
4909,Empty array as loss function causes unhandled exception,"If the loss fed to an optimizer is an empty array and attempt is made to train on a GPU, an unhandled exception occurs that kills the python kernel. The logs reveal the following error message

```
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:03:00.0
Total memory: 12.00GiB
Free memory: 4.89GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)
F tensorflow/stream_executor/cuda/cuda_dnn.cc:423] could not set cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM
```

Code to reproduce the problem:

``` python
import tensorflow as tf
import numpy as np

# Define a useless network
with tf.Graph().as_default() as graph:
    placeholder = tf.placeholder(tf.float32)
    filter = tf.Variable(np.random.gamma(1, 1, (10, 10, 1, 1)).astype(np.float32))
    loss = tf.nn.conv2d(placeholder, filter, [1, 1, 1, 1], 'VALID')
    optimizer = tf.train.AdamOptimizer()
    train_op = optimizer.minimize(loss)
    init_op = tf.initialize_all_variables()

gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)
session = tf.Session(graph=graph, config=tf.ConfigProto(gpu_options=gpu_options))
session.run(init_op)


def run(x):
    print(""loss"", session.run(loss, {placeholder: x}))
    session.run(train_op, {placeholder: x})
    print(""executed one training step"")


# This succeeds
run(np.ones((1, 10, 10, 1)))

# This kills the kernel because the 'VALID' padding in the convolutional layer
# leads to an empty array which the optimizer cannot handle
run(np.ones((1, 10, 9, 1)))
```
"
4907,DeepDream tutorial and TFSlim,"What do you think to switch the DeepDream tutorial  notebook to MetaGraph? I see that  fine tuning and new inception models are more oriented on tfslim and slim models use the new meta+check point.
"
4906,"The ""cos"" merge mode when using tf as backend cann't get the expect result","I have two batch of sentences as inputs, like:

```
a = np.array([[0, 1, 2, 3, 4], [0,1,0,0,0]], dtype='int32')
b = np.array([[0, 1, 2, 3, 4], [0,0,2,0,0]], dtype='int32')
vec = model.predict([a, b])
```

In my model, I trying to get the cos score between two vector, so I use 'cos' model in merge
`cos_score = merge([vec_a, vec_b], mode='cos', dot_axes=1)`
When I use theano as backend, it works fine, but when I use tf as backend, the output_shape=[batch_size, batch_size]

The result by 'tf' backend:

```
[[ 0.99999994  0.97052568]
 [ 1.0303694   0.80238068]]
```

The result by 'theano' backend:

```
[[ 1.        ]
 [ 0.80238068]]
```

This is caused by [tf.batch_matmul](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard5/tf.batch_matmul.md), 

> The input tensors x and y are 3-D or higher

But I just input 2-D vectors. I think it have to be expanded when the input shape is 2-D

BTW, My solution is:

```
if (keras.backend.backend() == ""theano""):
    cos_score = merge([vec_a, vec_b], mode='cos', dot_axes=1)
else:
    vec_a = RepeatVector(1)(vec_a)
    vec_b = RepeatVector(1)(vec_b)
    cos_score = merge([vec_a, vec_b], mode='cos', dot_axes=2)
    cos_score = Flatten()(cos_score)
```

Sorry about my English ðŸ˜ƒ 
"
4904,is the documentation about sampled_softmax_loss correct?,"Hi, 
""tf.nn.sampled_softmax_loss(weights, biases, inputs, labels, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=True, partition_strategy='mod', name='sampled_softmax_loss')""
the document says:
""At inference time, you can compute full softmax probabilities with the expression tf.nn.softmax(tf.matmul(inputs, tf.transpose(weights)) + biases).""

I am not sure if this describes the behavior correctly. First, the labels are not used in the softmax computation; Second, the weights matrix is merely a lookup table that maps each word to a vector (word2vec), the above matmul operation is not meaningful.

Here is my understanding of the code:
Given any word w, we have a vector encoding V1 from inputs, and then with label, we look up the weight matrix to find a vector encoding V2. Then we compute the softmax loss (cross-entropy loss) between V1 and V2. Finally, we sum up the loss for the sampled words.
"
4903,tensorboard command broken,"Running tensorboard results in this:

```
    logdir = os.path.expanduser(FLAGS.logdir)
AttributeError: 'NoneType' object has no attribute 'logdir'
```

Best practice in this case is arg(opt)parse.
"
4902,Consistently unable to run camera example iOS app,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Closest match: https://github.com/tensorflow/tensorflow/issues/4640
### Environment info

Operating System: iOS 10.0.1
Xcode 8 + macOS Sierra
1. The commit hash (`git rev-parse HEAD`) - 781603968c60bb14a40cf00653f9b31be7826f20
2. The output of `bazel version` - bazel command not found
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

tensorflow/contrib/ios_examples/camera in Xcode
### Logs or other output that would be helpful

Exception Type:  EXC_CRASH (SIGABRT)
Exception Codes: 0x0000000000000000, 0x0000000000000000
Exception Note:  EXC_CORPSE_NOTIFY
Triggered by Thread:  0

Application Specific Information:
abort() called

Filtered syslog:
None found

Thread 0 name:  Dispatch queue: com.apple.main-thread
Thread 0 Crashed:
0   libsystem_kernel.dylib          0x000000018127e014 **pthread_kill + 8
1   libsystem_pthread.dylib         0x0000000181345460 pthread_kill + 112
2   libsystem_c.dylib               0x00000001811f23f4 abort + 140
3   CameraExample                   0x00000001005d8a64 tensorflow::internal::LogMessageFatal::~LogMessageFatal() + 0
4   CameraExample                   0x00000001005d8a8c tensorflow::internal::LogMessageFatal::~LogMessageFatal() + 40
5   CameraExample                   0x000000010129fecc -[CameraExampleViewController viewDidLoad](CameraExampleViewController.mm:392)
6   UIKit                           0x00000001880f85c8 -[UIViewController loadViewIfRequired] + 1056
7   UIKit                           0x00000001880f8190 -[UIViewController view] + 28
8   UIKit                           0x00000001880fe93c -[UIWindow addRootViewControllerViewIfPossible] + 76
9   UIKit                           0x00000001880fbddc -[UIWindow _setHidden:forced:] + 272
10  UIKit                           0x000000018816e604 -[UIWindow makeKeyAndVisible] + 48
11  CameraExample                   0x00000001012a38ac -[CameraExampleAppDelegate application:didFinishLaunchingWithOptions:](CameraExampleAppDelegate.m:23)
12  UIKit                           0x000000018816a61c -[UIApplication _handleDelegateCallbacksWithOptions:isSuspended:restoreState:] + 400
13  UIKit                           0x000000018837ad60 -[UIApplication _callInitializationDelegatesForMainScene:transitionContext:] + 3524
14  UIKit                           0x0000000188380ad0 -[UIApplication _runWithMainScene:transitionContext:completion:] + 1656
15  UIKit                           0x0000000188395270 __84-[UIApplication _handleApplicationActivationWithScene:transitionContext:completion:]_block_invoke.3134 + 48
16  UIKit                           0x000000018837dab4 -[UIApplication workspaceDidEndTransaction:] + 168
17  FrontBoardServices              0x0000000183e51904 __FBSSERIALQUEUE_IS_CALLING_OUT_TO_A_BLOCK** + 36
18  FrontBoardServices              0x0000000183e51770 -[FBSSerialQueue _performNext] + 176
19  FrontBoardServices              0x0000000183e51b18 -[FBSSerialQueue _performNextFromRunLoopSource] + 56
20  CoreFoundation                  0x000000018225e278 __CFRUNLOOP_IS_CALLING_OUT_TO_A_SOURCE0_PERFORM_FUNCTION__ + 24
21  CoreFoundation                  0x000000018225dbc0 __CFRunLoopDoSources0 + 524
22  CoreFoundation                  0x000000018225b7c0 __CFRunLoopRun + 804
23  CoreFoundation                  0x000000018218a048 CFRunLoopRunSpecific + 444
24  UIKit                           0x00000001881637cc -[UIApplication _run] + 608
25  UIKit                           0x000000018815e550 UIApplicationMain + 208
26  CameraExample                   0x000000010129bce0 main (main.mm:23)
27  libdyld.dylib                   0x000000018116c5b8 start + 4
"
4898,Dequeue op loses TensorShape,"I am not able to find related issues online.

Operating System:
Ubuntu, Mac OSX

Installed version of CUDA and cuDNN: 
CUDA 7.5, cuDNN v5.1

If installed from binary pip package, provide:
1. A link to the pip package you installed:
`TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl`
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
`I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally`
`I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally`
`I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally`
`I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally`
`I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally`
`0.11.0rc0`

To reproduce the problem:

`a = tf.placeholder(dtype=tf.int32, shape=[None, 10])`
`print a.get_shape()`
`queue = tf.FIFOQueue(20, dtypes=[tf.int32])`
`queue.enqueue([a])`
`b = queue.dequeue()`
`print b.get_shape()`

The first print output <?, 10>, which is OK, and there is a TensorShape for a.
But the second print output is <unknown>, and there is no TensorShape for b.

TensorShape information is lost after dequeue operation.

And this TensorShape is required in LSTM cell class.

Can we add this TensorShape information back to the dequeued tensors?

Thanks.
"
4897,Provide unaggregated gradients tensors,"As described here, TF is inflexible when it comes to access to the gradients:
http://stackoverflow.com/questions/35731506/unaggregated-gradients-gradients-per-example-in-tensorflow?rq=1

Please provide a method where the user can retrieve the raw gradients, not the averaged gradients. Requiring the user to compute their own gradients is impractical -- the framework should work for the user, not the other way around.

Use case: this is needed for reinforcement learning, where the gradients of one net needs to be backpropagated through another net (in separate steps).
"
4895,Tensorflow fails to find GPU device (CUDA 8.0),"- Operating System: Ubuntu 16.04
- Installed version of CUDA and cuDNN: CUDA 8.0.27, cuDNN 5.1.5

```
$ ls ~/bin/cuda-8.0/lib64/libcud*
/home/maxim/bin/cuda-8.0/lib64/libcudadevrt.a    /home/maxim/bin/cuda-8.0/lib64/libcudart.so.8.0.27  /home/maxim/bin/cuda-8.0/lib64/libcudnn.so.5
/home/maxim/bin/cuda-8.0/lib64/libcudart.so      /home/maxim/bin/cuda-8.0/lib64/libcudart_static.a   /home/maxim/bin/cuda-8.0/lib64/libcudnn.so.5.1.5
/home/maxim/bin/cuda-8.0/lib64/libcudart.so.8.0  /home/maxim/bin/cuda-8.0/lib64/libcudnn.so          /home/maxim/bin/cuda-8.0/lib64/libcudnn_static.a
```

`nvidia-smi` shows the GPU.
- Environment variables:

```
declare -x CUDA_HOME=""/home/maxim/bin/cuda-8.0""
declare -x PATH=""/home/maxim/bin/anaconda2/bin:/home/maxim/bin/cuda-8.0/bin:/home/maxim/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin""
declare -x LD_LIBRARY_PATH=""/home/maxim/bin/cuda-8.0/lib64:""
```
- Tensorflow version: 0.10.0rc0 (installed by Anaconda)
- Note that on the same machine Theano _works perfectly_:

```
$ python theano_check1.py 
Using gpu device 0: GeForce GTX 960M (CNMeM is disabled, cuDNN 5105)
```

But Tensorflow does not:

```
Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0
```
"
4894,virtualenv pip3 install fails,"I am trying to install TensorFlow in virtualenv using Python 3.4.

I did:

```
$ pyvenv-3.4 --system-site-packages tensorflow
$ source tensorflow/bin/activate
$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp34-cp34m-linux_x86_64.whl
$ pip3 install --upgrade $TF_BINARY_URL
```

The pip3 install ends in this error:

```
Installing collected packages: tensorflow, protobuf, wheel, numpy, six, setuptools                                                                                                                                                                                        â”‚
Cleaning up...                                                                                                                                                                                                                                                            â”‚
  Removing temporary dir /tmp/pip_build_ashwin...                                                                                                                                                                                                                         â”‚
Exception:                                                                                                                                                                                                                                                                â”‚
Traceback (most recent call last):                                                                                                                                                                                                                                        â”‚
  File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 122, in main                                                                                                                                                                                             â”‚
    status = self.run(options, args)                                                                                                                                                                                                                                      â”‚
  File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 283, in run                                                                                                                                                                                         â”‚
    requirement_set.install(install_options, global_options, root=options.root_path)                                                                                                                                                                                      â”‚
  File ""/usr/lib/python3/dist-packages/pip/req.py"", line 1436, in install                                                                                                                                                                                                 â”‚
    requirement.install(install_options, global_options, *args, **kwargs)                                                                                                                                                                                                 â”‚
  File ""/usr/lib/python3/dist-packages/pip/req.py"", line 672, in install                                                                                                                                                                                                  â”‚
    self.move_wheel_files(self.source_dir, root=root)                                                                                                                                                                                                                     â”‚
  File ""/usr/lib/python3/dist-packages/pip/req.py"", line 902, in move_wheel_files                                                                                                                                                                                         â”‚
    pycompile=self.pycompile,                                                                                                                                                                                                                                             â”‚
  File ""/usr/lib/python3/dist-packages/pip/wheel.py"", line 206, in move_wheel_files                                                                                                                                                                                       â”‚
    clobber(source, lib_dir, True)                                                                                                                                                                                                                                        â”‚
  File ""/usr/lib/python3/dist-packages/pip/wheel.py"", line 175, in clobber                                                                                                                                                                                                â”‚
    os.makedirs(dest)                                                                                                                                                                                                                                                     â”‚
  File ""/usr/lib/python3.4/os.py"", line 237, in makedirs                                                                                                                                                                                                                  â”‚
    mkdir(name, mode)                                                                                                                                                                                                                                                     â”‚
PermissionError: [Errno 13] Permission denied: '/usr/lib/python3.4/site-packages' 
```

Why is pip3 install inside a virtual environment trying to write in `/usr/lib/python3.4`? How to make this install work?

Operating System: Ubuntu 14.04
Installed version of CUDA and cuDNN: CUDA 7.5 and cuDNN 5

```
lrwxrwxrwx 1 root root   16 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcublas.so -> libcublas.so.7.5                                                                                                                                                                       â”‚
lrwxrwxrwx 1 root root   19 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcublas.so.7.5 -> libcublas.so.7.5.18                                                                                                                                                                â”‚
-rwxr-xr-x 1 root root  23M Aug 15  2015 /usr/local/cuda-7.5/lib64/libcublas.so.7.5.18                                                                                                                                                                                    â”‚
-rw-r--r-- 1 root root  28M Aug 16  2015 /usr/local/cuda-7.5/lib64/libcublas_device.a                                                                                                                                                                                     â”‚
-rw-r--r-- 1 root root  27M Aug 16  2015 /usr/local/cuda-7.5/lib64/libcublas_static.a                                                                                                                                                                                     â”‚
-rw-r--r-- 1 root root 316K Aug 16  2015 /usr/local/cuda-7.5/lib64/libcudadevrt.a                                                                                                                                                                                         â”‚
lrwxrwxrwx 1 root root   16 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so -> libcudart.so.7.5                                                                                                                                                                       â”‚
lrwxrwxrwx 1 root root   19 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18                                                                                                                                                                â”‚
-rwxr-xr-x 1 root root 375K Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18                                                                                                                                                                                    â”‚
-rw-r--r-- 1 root root 704K Aug 16  2015 /usr/local/cuda-7.5/lib64/libcudart_static.a                                                                                                                                                                                     â”‚
lrwxrwxrwx 1 root root   15 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcufft.so -> libcufft.so.7.5                                                                                                                                                                         â”‚
lrwxrwxrwx 1 root root   16 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcufftw.so -> libcufftw.so.7.5                                                                                                                                                                       â”‚
lrwxrwxrwx 1 root root   19 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcufftw.so.7.5 -> libcufftw.so.7.5.18                                                                                                                                                                â”‚
-rwxr-xr-x 1 root root 438K Aug 15  2015 /usr/local/cuda-7.5/lib64/libcufftw.so.7.5.18                                                                                                                                                                                    â”‚
-rw-r--r-- 1 root root  42K Aug 16  2015 /usr/local/cuda-7.5/lib64/libcufftw_static.a                                                                                                                                                                                     â”‚
lrwxrwxrwx 1 root root   18 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcufft.so.7.5 -> libcufft.so.7.5.18                                                                                                                                                                  â”‚
-rwxr-xr-x 1 root root 107M Aug 15  2015 /usr/local/cuda-7.5/lib64/libcufft.so.7.5.18                                                                                                                                                                                     â”‚
-rw-r--r-- 1 root root 110M Aug 16  2015 /usr/local/cuda-7.5/lib64/libcufft_static.a                                                                                                                                                                                      â”‚
lrwxrwxrwx 1 root root   17 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcuinj64.so -> libcuinj64.so.7.5                                                                                                                                                                     â”‚
lrwxrwxrwx 1 root root   20 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcuinj64.so.7.5 -> libcuinj64.so.7.5.18                                                                                                                                                              â”‚
-rwxr-xr-x 1 root root 5.5M Aug 15  2015 /usr/local/cuda-7.5/lib64/libcuinj64.so.7.5.18                                                                                                                                                                                   â”‚
-rw-r--r-- 1 root root 1.6M Aug 16  2015 /usr/local/cuda-7.5/lib64/libculibos.a                                                                                                                                                                                           â”‚
lrwxrwxrwx 1 root root   16 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcurand.so -> libcurand.so.7.5                                                                                                                                                                       â”‚
lrwxrwxrwx 1 root root   19 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcurand.so.7.5 -> libcurand.so.7.5.18                                                                                                                                                                â”‚
-rwxr-xr-x 1 root root  50M Aug 15  2015 /usr/local/cuda-7.5/lib64/libcurand.so.7.5.18                                                                                                                                                                                    â”‚
-rw-r--r-- 1 root root  50M Aug 16  2015 /usr/local/cuda-7.5/lib64/libcurand_static.a                                                                                                                                                                                     â”‚
lrwxrwxrwx 1 root root   18 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcusolver.so -> libcusolver.so.7.5                                                                                                                                                                   â”‚
lrwxrwxrwx 1 root root   21 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcusolver.so.7.5 -> libcusolver.so.7.5.18                                                                                                                                                            â”‚
-rwxr-xr-x 1 root root  36M Aug 15  2015 /usr/local/cuda-7.5/lib64/libcusolver.so.7.5.18                                                                                                                                                                                  â”‚
-rw-r--r-- 1 root root  16M Aug 16  2015 /usr/local/cuda-7.5/lib64/libcusolver_static.a                                                                                                                                                                                   â”‚
lrwxrwxrwx 1 root root   18 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcusparse.so -> libcusparse.so.7.5                                                                                                                                                                   â”‚
lrwxrwxrwx 1 root root   21 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcusparse.so.7.5 -> libcusparse.so.7.5.18                                                                                                                                                            â”‚
-rwxr-xr-x 1 root root  36M Aug 15  2015 /usr/local/cuda-7.5/lib64/libcusparse.so.7.5.18                                                                                                                                                                                  â”‚
-rw-r--r-- 1 root root  43M Aug 16  2015 /usr/local/cuda-7.5/lib64/libcusparse_static.a
```
"
4893,How to create `input_fn` using `read_batch_examples` with `num_epochs` set?,"TF Version: 0.10.0rc
## Update

Solved the issue, see [StackOverflow post here for solution](http://stackoverflow.com/q/39877710/6557588).
## Original non-Issue...

I have a basic `input_fn` that can be used with Tensorflow Estimators below. It works flawlessly without setting the `num_epochs` parameter; the obtained tensor has a discrete shape. Pass in `num_epochs` as anything other than `None` results in an unknown shape. My issue lies with constructing sparse tensors whilst using `num_epochs`; I cannot figure out how to generically create said tensors without knowing the shape of the input tensor.

Can anyone think of a solution to this problem? I'd like to be able to pass `num_epochs=1` to be able to evaluate only 1 time over the data set, as well as to pass to `predict` to yield a set of predictions the size of the data set, no more no less.

``` python
    def input_fn(batch_size):
        examples_op = tf.contrib.learn.read_batch_examples(
            FILE_NAMES,
            batch_size=batch_size,
            reader=tf.TextLineReader,
            num_epochs=1,
            parse_fn=lambda x: tf.decode_csv(x, [tf.constant([''], dtype=tf.string)] * len(HEADERS)))

        examples_dict = {}
        for i, header in enumerate(HEADERS):
            examples_dict[header] = examples_op[:, i]

        continuous_cols = {k: tf.string_to_number(examples_dict[k], out_type=tf.float32)
                           for k in CONTINUOUS_FEATURES}

        # Problems lay here while creating sparse categorical tensors
        categorical_cols = {
            k: tf.SparseTensor(
                indices=[[i, 0] for i in range(examples_dict[k].get_shape()[0])],
                values=examples_dict[k],
                shape=[int(examples_dict[k].get_shape()[0]), 1])
            for k in CATEGORICAL_FEATURES}

        feature_cols = dict(continuous_cols)
        feature_cols.update(categorical_cols)
        label = tf.string_to_number(examples_dict[LABEL], out_type=tf.int32)

        return feature_cols, label
```
"
4890, TypeError: __init__() got an unexpected keyword argument 'state_is_tuple',"Please help me resolved the error:

```
I tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)
..
======================================================================
ERROR: testAttentionDecoderStateIsTuple (__main__.Seq2SeqTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py"", line 308, in testAttentionDecoderStateIsTuple
    cell = tf.nn.rnn_cell.BasicLSTMCell(2, state_is_tuple=True)
TypeError: __init__() got an unexpected keyword argument 'state_is_tuple'

======================================================================
ERROR: testDynamicAttentionDecoder1 (__main__.Seq2SeqTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py"", line 271, in testDynamicAttentionDecoder1
    enc_outputs, enc_state = tf.nn.dynamic_rnn(cell, inp, dtype=tf.float32)
TypeError: dynamic_rnn() takes at least 3 arguments (3 given)

======================================================================
ERROR: testDynamicAttentionDecoder2 (__main__.Seq2SeqTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py"", line 290, in testDynamicAttentionDecoder2
    enc_outputs, enc_state = tf.nn.dynamic_rnn(cell, inp, dtype=tf.float32)
TypeError: dynamic_rnn() takes at least 3 arguments (3 given)

======================================================================
ERROR: testEmbeddingAttentionDecoder (__main__.Seq2SeqTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py"", line 368, in testEmbeddingAttentionDecoder
    embedding_size=2, output_size=3)
TypeError: embedding_attention_decoder() got an unexpected keyword argument 'embedding_size'

======================================================================
ERROR: testEmbeddingAttentionSeq2Seq (__main__.Seq2SeqTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py"", line 382, in testEmbeddingAttentionSeq2Seq
    cell = tf.nn.rnn_cell.BasicLSTMCell(2, state_is_tuple=True)
TypeError: __init__() got an unexpected keyword argument 'state_is_tuple'

======================================================================
ERROR: testEmbeddingRNNDecoder (__main__.Seq2SeqTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py"", line 85, in testEmbeddingRNNDecoder
    cell = tf.nn.rnn_cell.BasicLSTMCell(2, state_is_tuple=True)
TypeError: __init__() got an unexpected keyword argument 'state_is_tuple'

======================================================================
ERROR: testEmbeddingRNNSeq2Seq (__main__.Seq2SeqTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py"", line 105, in testEmbeddingRNNSeq2Seq
    cell = tf.nn.rnn_cell.BasicLSTMCell(2, state_is_tuple=True)
TypeError: __init__() got an unexpected keyword argument 'state_is_tuple'

======================================================================
ERROR: testEmbeddingTiedRNNSeq2Seq (__main__.Seq2SeqTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py"", line 170, in testEmbeddingTiedRNNSeq2Seq
    cell = tf.nn.rnn_cell.BasicLSTMCell(2, state_is_tuple=True)
TypeError: __init__() got an unexpected keyword argument 'state_is_tuple'

======================================================================
ERROR: testModelWithBooleanFeedPrevious (__main__.Seq2SeqTest)
Test the model behavior when feed_previous is True.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py"", line 764, in testModelWithBooleanFeedPrevious
    TestModel(model)
  File ""tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py"", line 695, in TestModel
    enc_inp, dec_inp_fp_true, feed_previous=True)
  File ""tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py"", line 685, in ForwardBackward
    dec_op, _ = seq2seq(enc_inp, dec_inp, feed_previous=feed_previous)
  File ""tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py"", line 726, in EmbeddingRNNSeq2SeqF
    cell = tf.nn.rnn_cell.BasicLSTMCell(2, state_is_tuple=True)
TypeError: __init__() got an unexpected keyword argument 'state_is_tuple'

======================================================================
ERROR: testModelWithBuckets (__main__.Seq2SeqTest)
Larger tests that does full sequence-to-sequence model training.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py"", line 617, in testModelWithBuckets
    _, losses = SampleGRUSeq2Seq(inp, out, weights)
  File ""tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py"", line 609, in SampleGRUSeq2Seq
    softmax_loss_function=SampledLoss)
  File ""/usr/local/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/seq2seq.py"", line 926, in model_with_buckets
    decoder_inputs[:bucket[1]])
  File ""tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py"", line 598, in GRUSeq2Seq
    state_is_tuple=True)
TypeError: __init__() got an unexpected keyword argument 'state_is_tuple'

======================================================================
ERROR: testModelWithBucketsScopeAndLoss (__main__.Seq2SeqTest)
Test that variable scope reuse is not reset after model_with_buckets.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py"", line 568, in testModelWithBucketsScopeAndLoss
    _, losses1 = SampleGRUSeq2Seq(inp, out, weights, per_example_loss=False)
  File ""tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py"", line 561, in SampleGRUSeq2Seq
    per_example_loss=per_example_loss)
  File ""/usr/local/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/seq2seq.py"", line 926, in model_with_buckets
    decoder_inputs[:bucket[1]])
  File ""tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py"", line 554, in GRUSeq2Seq
    state_is_tuple=True)
TypeError: __init__() got an unexpected keyword argument 'state_is_tuple'

======================================================================
ERROR: testOne2ManyRNNSeq2Seq (__main__.Seq2SeqTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py"", line 452, in testOne2ManyRNNSeq2Seq
    cell = tf.nn.rnn_cell.BasicLSTMCell(2, state_is_tuple=True)
TypeError: __init__() got an unexpected keyword argument 'state_is_tuple'

----------------------------------------------------------------------
Ran 20 tests in 1.935s
```
"
4888,"""src: warning: directory does not exist.â€œ when I build syntaxnet","when I build syntaxnet, a problem occurs that.
ERROR: /home/zwg/.cache/bazel/_bazel_zwg/532f17a1037f0f671972f45f175b09ce/external/protobuf/BUILD:560:1: null failed: protoc failed: error executing command bazel-out/host/bin/external/protobuf/protoc '--python_out=bazel-out/local-opt/genfiles/src' -Isrc external/protobuf/src/google/protobuf/any.proto external/protobuf/src/google/protobuf/api.proto ... (remaining 10 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
src: warning: directory does not exist.
external/protobuf/src/google/protobuf/any.proto: File does not reside within any path specified using --proto_path (or -I).  You must specify a --proto_path which encompasses this file.  Note that the proto_path must be an exact prefix of the .proto file names -- protoc is too dumb to figure out when two paths (e.g. absolute and relative) are equivalent (it's harder than you think).

 Build Syntaxnet on my machine with the following configuration:
OS:
[zwg@localhost example]$ cat /proc/version
Linux version 2.6.32-358.el6.x86_64 (mockbuild@c6b8.bsys.dev.centos.org) (gcc version 4.4.7 20120313 (Red Hat 4.4.7-3) (GCC) ) #1 SMP Fri Feb 22 00:31:26 UTC 2013

Python:
[zwg@localhost example]$ python
Python 2.7.10 (default, Oct 10 2016, 02:48:27) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-3)] on linux2

GCC:
[zwg@localhost example]$ gcc -v
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/opt/rh/devtoolset-2/root/usr/libexec/gcc/x86_64-redhat-linux/4.8.2/lto-wrapper
Target: x86_64-redhat-linux
Configured with: ../configure --prefix=/opt/rh/devtoolset-2/root/usr --mandir=/opt/rh/devtoolset-2/root/usr/share/man --infodir=/opt/rh/devtoolset-2/root/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --enable-languages=c,c++,fortran,lto --enable-plugin --with-linker-hash-style=gnu --enable-initfini-array --disable-libgcj --with-isl=/dev/shm/home/centos/rpm/BUILD/gcc-4.8.2-20140120/obj-x86_64-redhat-linux/isl-install --with-cloog=/dev/shm/home/centos/rpm/BUILD/gcc-4.8.2-20140120/obj-x86_64-redhat-linux/cloog-install --with-mpc=/dev/shm/home/centos/rpm/BUILD/gcc-4.8.2-20140120/obj-x86_64-redhat-linux/mpc-install --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux
Thread model: posix
gcc version 4.8.2 20140120 (Red Hat 4.8.2-15) (GCC)

JAVA:
[zwg@localhost example]$ java -version
java version ""1.8.0_101""
Java(TM) SE Runtime Environment (build 1.8.0_101-b13)
Java HotSpot(TM) 64-Bit Server VM (build 25.101-b13, mixed mode)

BAZEL:
[zwg@localhost example]$ bazel version
Build label: 0.2.2b- (@non-git)
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Mon Oct 10 13:48:34 2016 (1476107314)
Build timestamp: 1476107314
Build timestamp as int: 1476107314
"
4887,resnet model in tf slim does not take is_training param,"Compare the [resnet_v2 model](https://github.com/tensorflow/tensorflow/blob/8e48ec6ea0492e2cb9fd19c0a2ccf41afc7b4dc6/tensorflow/contrib/slim/python/slim/nets/resnet_v2.py) to the [vgg model](https://github.com/tensorflow/tensorflow/blob/8e48ec6ea0492e2cb9fd19c0a2ccf41afc7b4dc6/tensorflow/contrib/slim/python/slim/nets/vgg.py). The vgg model takes `is_training` whereas the resnet model does not. This param should be taken and passed to the `batch_norm` layers.

The docs for the v1 model do reference `is_training`, but I don't see it used.
"
4886,"Gradient of tf.reduce_max and tf.nn.max_pool don't agree with each other, and theano","``` python
a = tf.placeholder(dtype=tf.float32, name='a', shape=[4])
b = tf.reduce_max(a)
b2 = tf.nn.max_pool(tf.reshape(a, [1, 2, 2, 1]),
        [1,2,2,1], [1,1,1,1], 'VALID')[0,0,0,0]
c = tf.gradients([b], [a])[0]
c2 = tf.gradients([b2], [a])[0]

with tf.Session() as sess:
    v = np.asarray([1, 2, 2, 2], dtype='float32')
    print sess.run(c, feed_dict={a:v})  # 0, 0.3, 0.3, 0.3
    print sess.run(c2, feed_dict={a:v})  # 0, 1, 0, 0

import theano.tensor as T
import theano.tensor.signal.downsample as D
a = T.fvector('a')
b = T.max(a)
b2 = T.reshape(a, [1,2,2])
b2 = D.max_pool_2d(b2, [2,2])[0,0,0]
c = T.grad(b, a)
c2 = T.grad(b2, a)
print c.eval({a: v})    # 0, 1, 1, 1
print c2.eval({a: v})    # 0, 1, 1, 1
```

Bug or feature? ðŸ˜¦ Although as subgradient they all seem to make sense, but is there a reason to justify the difference of reduce_max and max_pool?

I saw the [comment](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L71) for reduce_max and it seems like the first one is feature. 
"
4885,"After some iterations my accuracy has decrease to 0, but the loss and cross entropy are not nan.","Hello
I can't understand this question:
I can trained my net in a correct result in beginning but after about 4K iterations ,the accuracy suddenly decrease to zero, and the loss and cross entropy suddenly increased to a high value, it is like this curves:
train accuracy:
![image](https://cloud.githubusercontent.com/assets/5306116/19257677/2f14d1ec-8fa4-11e6-9f31-50255f2cef0e.png)
val accuracy:
![image](https://cloud.githubusercontent.com/assets/5306116/19257685/3c3b26f0-8fa4-11e6-854f-3d5b459158f6.png)
cross entropy:
![image](https://cloud.githubusercontent.com/assets/5306116/19257709/54f385de-8fa4-11e6-88c7-d5e90af88f94.png)
regular loss:
![image](https://cloud.githubusercontent.com/assets/5306116/19257716/676f5f1c-8fa4-11e6-8b87-79d92f35202d.png)

I think you can reproduce it in this link(3D Convolution, UCF101):
https://github.com/hx173149/C3D-tensorflow

thanks~
"
4883,Ability to directly set the gradient of a node for use in backpropagation.,"Imagine a tiny network defined as follows, where linear is a typical helper function defining TensorFlow variables for a weight matrix and activation function:

`final_layer = linear(linear(_input,10,tf.nn.tanh),20)`

Normally this would be optimized via gradient descent on a loss:

`loss = tf.reduce_sum(tf.square(final_layer - _target))
train_step = tf.train.AdamOptimizer().minimmize(loss)`

But assume I'm getting the derivatives of the loss w.r.t. final_layer from an external  source (e.g. a tf.placeholder named _deriv). I would like to be able to use this gradient information with one of the builtin optimizers to backpropagate and update the network parameters, but this appears to be currently impossible.

The workaround I'm currently using is to construct an artificial loss consisting of the inner product between _deriv and final_layer (since the derivatives of this loss w.r.t. final_layer will be equal to _deriv). 

`loss = tf.reduce_sum(final_layer*_deriv)
train_step = tf.train.AdamOptimizer().minimmize(loss)`

This is very wasteful though, as it needs to do this unnecessary inner product and calculate its derivative for every training step even though I already know this information.

For those thinking this an odd thing to need to do, it is necessary for implementing [synthetic gradients](https://arxiv.org/abs/1608.05343).
"
4882,S_IREAD and S_IWRITE errors from gif_archive in CentOS 6.8 Build,"Hi there,

I've been attempting off and on for the last 1-2 weeks to get Tensorflow to build on CentOS 6.8. I've spent a lot of time reading through old issues and piecing/hacking together suggestions from developers and other users on how to configure the build. I'm using a hand-built version of GCC (v. 4.9.3) in a non-default location with the system version of binutils (more info below). 

I've managed to get the tutorial_trainers_example to build and run successfully, but I'm now running into trouble when attempting to build the pip wheel (see below). It looks like there may be some missing header files.
### Relevant Threads

The most helpful threads I've found during this process have been the following:

https://github.com/tensorflow/tensorflow/issues/110 (especially comments from @rdipietro)
https://github.com/bazelbuild/bazel/issues/760 (thanks @damienmg)

The only mention I can find of the specific error (granted, for a completely different platform) is in this recent Stackoverflow thread:

http://stackoverflow.com/questions/39855672/tensorflow-how-to-compile-libtensorflow-cc-so-for-android

The fact that this is a recent thread makes me think that perhaps there was some sort of error that was introduced into the build config in the last few weeks/months.
### Environment info

Operating System:

```
$ lsb_release -a
LSB Version:    :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch
Distributor ID: CentOS
Description:    CentOS release 6.8 (Final)
Release:    6.8
Codename:   Final
$ uname -a
Linux vmp1250 2.6.32-642.1.1.el6.x86_64 #1 SMP Tue May 31 21:57:07 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
```

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

```
$ ls -lh /gpfs22/local/centos6/cuda-7.5/lib/libcud*
-rw-r--r--. 1 root root 185K Sep 17  2015 /gpfs22/local/centos6/cuda-7.5/lib/libcudadevrt.a
lrwxrwxrwx. 1 root root   16 Sep 17  2015 /gpfs22/local/centos6/cuda-7.5/lib/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx. 1 root root   19 Sep 17  2015 /gpfs22/local/centos6/cuda-7.5/lib/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x. 1 root root 305K Sep 17  2015 /gpfs22/local/centos6/cuda-7.5/lib/libcudart.so.7.5.18
-rw-r--r--. 1 root root 545K Sep 17  2015 /gpfs22/local/centos6/cuda-7.5/lib/libcudart_static.a
```

```
$ ls -lh /gpfs22/local/centos6/cudnn-7.5-v5/lib64/
total 227M
lrwxrwxrwx. 1 1000 1000  13 May 25 10:38 libcudnn.so -> libcudnn.so.5
lrwxrwxrwx. 1 1000 1000  17 May 25 10:38 libcudnn.so.5 -> libcudnn.so.5.0.5
-rwxrwxr-x. 1 1000 1000 58M Apr 22 19:15 libcudnn.so.5.0.5
-rw-rw-r--. 1 1000 1000 57M Apr 22 19:15 libcudnn_static.a
```

If installed from source, provide 

The commit hash (`git rev-parse HEAD`):
- Most recent (df871edcff2faf643975b9863100ed41b6da9c3f)

The output of `bazel version`:

```
$ bazel version
Build label: 0.3.2- (@non-git)
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Oct 7 20:15:17 2016 (1475871317)
Build timestamp: 1475871317
Build timestamp as int: 1475871317
```
### Custom Build Config for Bazel & Tensorflow

For the bazel build:
- Modified `tools/cpp/CROSSTOOL` to point to C and C++ compilers in non-default locations 
- Added an extra `linker_flag` line in `tools/cpp/CROSSTOOL`
- Added a bunch of `cxx_builtin_include_directory` lines to point to all the appropriate header files within GCC 4.9.3 in `tools/cpp/CROSSTOOL` 

For the Tensorflow build:
- Modified `third_party/gpus/crosstool/CROSSTOOL.tpl` to point to C++ compiler in custom location 
- Added an extra `linker_flag` line to `third_party/gpus/crosstool/CROSSTOOL.tpl` 
- Updated `third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl` to point to the versions of NVCC and GCC that are in non-default locations 
- Commented out the line `cmd = 'PATH=' + PREFIX_DIR + ' ' + cmd` in `third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl` (otherwise `as` cannot be found). 

I can provide the exact lines I used if you'd like. Just let me know. 
### Description of Problem

Environment setup (I realize some of this may be overkill):

```
$ export PATH=/gpfs22/local/centos6/swig/3.0.5/x86_64/gcc49/nonet/bin:/gpfs22/local/centos6/cuda-7.5/bin:/gpfs22/local/centos6/python2/anaconda2/bin:/gpfs21/scratch/frenchwr/tensorflow/bazel-0.3.2/output:/gpfs22/local/centos6/gcc/4.9.3/x86_64/bin:/gpfs22/local/centos6/java/1.8.0/bin:/usr/local/git/latest/x86_64/gcc46/nonet/bin:/usr/local/git/latest/x86_64/gcc46/nonet/libexec/git-core:/usr/scheduler/slurm/sbin:/usr/scheduler/slurm/bin:/usr/lpp/mmfs/bin:/usr/local/bin:/usr/local/common/bin:/usr/bin:/bin:/usr/scheduler/slurm/sbin:/usr/scheduler/slurm/bin:/usr/lpp/mmfs/bin:/usr/local/bin:/usr/local/common/bin:/usr/bin:/bin:/usr/local/git/latest/x86_64/gcc46/nonet/bin:/usr/local/git/latest/x86_64/gcc46/nonet/libexec/git-core:/usr/scheduler/slurm/sbin:/usr/scheduler/slurm/bin:/usr/lpp/mmfs/bin:/usr/local/bin:/usr/local/common/bin:/usr/bin:/bin:/usr/scheduler/slurm/sbin:/usr/scheduler/slurm/bin:/usr/lpp/mmfs/bin:/usr/local/bin:/usr/local/common/bin:/usr/bin:/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/var/cfengine/bin:/var/cfengine/bin
$ export LD_LIBRARY_PATH=/gpfs22/local/centos6/cudnn-7.5-v5/lib64:/gpfs22/local/centos6/swig/3.0.5/x86_64/gcc49/nonet/share/swig/3.0.5:/gpfs22/local/centos6/cuda-7.5/lib:/gpfs22/local/centos6/cuda-7.5/lib64:/gpfs22/local/centos6/java/1.8.0/lib:/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib:/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib64:/gpfs22/local/centos6/java/1.8.0/lib/:/gpfs22/local/centos6/python2/anaconda2/lib
$ export JAVA_HOME=/gpfs22/local/centos6/java/1.8.0
$ export JAVA_ROOT=/gpfs22/local/centos6/java/1.8.0
$ export CXX=/gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/g++
$ export CC=/gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/gcc
$ export GCC_ROOT=/gpfs22/local/centos6/gcc/4.9.3/x86_64
$ export LDFLAGS=""-L/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib -L/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib64 -L/gpfs22/local/centos6/cuda-7.5/lib64 -L/gpfs22/local/centos6/cuda-7.5/lib -L/gpfs22/local/centos6/cudnn-7.5-v5/lib64 -L/gpfs22/local/centos6/python2/anaconda2/lib""
$ export CXXFLAGS=""-L/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib -L/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib64 -L/gpfs22/local/centos6/cuda-7.5/lib64 -L/gpfs22/local/centos6/cuda-7.5/lib -L/gpfs22/local/centos6/cudnn-7.5-v5/lib64 -L/gpfs22/local/centos6/python2/anaconda2/lib""
$ export CPLUS_INCLUDE_PATH=""/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib/gcc/:/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib/gcc/x86_64-unknown-linux-gnu/4.9.3/:/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib/gcc/x86_64-unknown-linux-gnu/4.9.3/include/:/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib/gcc/x86_64-unknown-linux-gnu/4.9.3/include-fixed/:/gpfs22/local/centos6/gcc/4.9.3/x86_64/include/c++/4.9.3/:/gpfs22/local/centos6/gcc/4.9.3/x86_64/include/c++/4.9.3/x86_64-unknown-linux-gnu/:/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib64/:/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib/:/gpfs22/local/centos6/cuda-7.5/include/:/gpfs22/local/centos6/cudnn-7.5-v5/include/""
$ export PYTHONPATH=/gpfs22/local/centos6/python2/anaconda2/lib/python2.7:$PYTHONPATH
$ export PYTHON_ROOT=/gpfs22/local/centos6/python2/anaconda2
$ export PYTHON27_INC=$PYTHON_ROOT/include/python2.7
$ export CUDA_HOME=/gpfs22/local/centos6/cuda-7.5
$ export SWIG_ROOT=/gpfs22/local/centos6/swig/3.0.5/x86_64/gcc49/nonet
$ export NVVM_ROOT=/gpfs22/local/centos6/cuda-7.5/nvvm
$ export LIBRARY_PATH=/gpfs22/local/centos6/cudnn-7.5-v5/lib64:$LIBRARY_PATH
$ export CPATH=/gpfs22/local/centos6/cudnn-7.5-v5/include:$CPATH
$ export CUDNN_ROOT=/gpfs22/local/centos6/cudnn-7.5-v5
$ export SWIG_PATH=$(which swig)
```

Tensorflow Configuration:

```
$ $ ./configure
/scratch/frenchwr/tensorflow/tensorflow-head /scratch/frenchwr/tensorflow/tensorflow-head
Please specify the location of python. [Default is /gpfs22/local/centos6/python2/anaconda2/bin/python]: 
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] N
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] N
No Hadoop File System support will be enabled for TensorFlow
Found possible Python library paths:
  /gpfs22/local/centos6/python2/anaconda2/lib/python2.7/site-packages
  /gpfs22/local/centos6/python2/anaconda2/lib/python2.7
Please input the desired Python library path to use.  Default is [/gpfs22/local/centos6/python2/anaconda2/lib/python2.7/site-packages]

/gpfs22/local/centos6/python2/anaconda2/lib/python2.7/site-packages
Do you wish to build TensorFlow with GPU support? [y/N] y
GPU support will be enabled for TensorFlow
Please specify which gcc should be used by nvcc as the host compiler. [Default is /gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/gcc]: 
Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5
Please specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /gpfs22/local/centos6/cuda-7.5
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 5.0.5
Please specify the location where cuDNN 5.0.5 library is installed. Refer to README.md for more details. [Default is /gpfs22/local/centos6/cuda-7.5]: /gpfs22/local/centos6/cudnn-7.5-v5
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 5.2
INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
.................
INFO: All external dependencies fetched successfully.
Configuration finished
```

Tutorials Example Trainer Build:

```
bazel build -c opt --config=cuda --verbose_failures --subcommands //tensorflow/cc:tutorials_example_trainer
```

Tutorials Example Trainer Run:

```
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5.0.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:02:00.0
Total memory: 12.00GiB
Free memory: 11.87GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)
000000/000001 lambda = 1.841570 x = [0.669396 0.742906] y = [3.493999 -0.669396]
000000/000009 lambda = 1.841570 x = [0.669396 0.742906] y = [3.493999 -0.669396]
...
...
```

Pip Wheel Build (extra command line options suggested by @rdipietro in https://github.com/tensorflow/tensorflow/issues/110):

```
$ bazel build -c opt --config=cuda --linkopt '-lrt' --copt=""-DGPR_BACKWARDS_COMPATIBILITY_MODE"" --conlyopt=""-std=c99"" //tensorflow/tools/pip_package:build_pip_package
...
...
INFO: From Compiling external/protobuf/src/google/protobuf/compiler/cpp/cpp_message.cc:
external/protobuf/src/google/protobuf/compiler/cpp/cpp_message.cc:376:8: warning: 'std::string google::protobuf::compiler::cpp::{anonymous}::MessageTypeProtoName(const google::protobuf::FieldDescriptor*)' defined but not used [-Wunused-function]
 string MessageTypeProtoName(const FieldDescriptor* field) {
        ^
ERROR: /gpfs22/home/frenchwr/.cache/bazel/_bazel_frenchwr/4ceedd0aac0f37a9bc9063198121367a/external/gif_archive/BUILD:6:1: C++ compilation of rule '@gif_archive//:gif' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 37 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
external/gif_archive/egif_lib.c: In function 'EGifOpenFileName':
external/gif_archive/egif_lib.c:62:6: error: 'S_IREAD' undeclared (first use in this function)
      S_IREAD | S_IWRITE);
      ^
external/gif_archive/egif_lib.c:62:6: note: each undeclared identifier is reported only once for each function it appears in
external/gif_archive/egif_lib.c:62:16: error: 'S_IWRITE' undeclared (first use in this function)
      S_IREAD | S_IWRITE);
                ^
external/gif_archive/egif_lib.c: In function 'EGifOpenFileHandle':
external/gif_archive/egif_lib.c:119:5: warning: implicit declaration of function 'fdopen' [-Wimplicit-function-declaration]
     f = fdopen(FileHandle, ""wb"");    /* Make it into a stream: */
     ^
external/gif_archive/egif_lib.c:119:7: warning: assignment makes pointer from integer without a cast
     f = fdopen(FileHandle, ""wb"");    /* Make it into a stream: */
       ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 403.336s, Critical Path: 394.04s
```
"
4881,"Multi gpu cifa10 example, putting ops outside of towerloss to cpu actually hurt performance","From cifa 10 example tutorial  in train.py  ""with tf.Graph().as_default(), tf.device('/cpu:0'):"" putting all the ops but in tower loss to cpu. 
as in the tutorial [cifa 10 tutorial](https://www.tensorflow.org/versions/r0.11/tutorials/deep_cnn/index.html#cifar-10-model)
""This setup requires that all GPUs share the model parameters. A well-known fact is that transferring data to and from GPUs is quite slow. For this reason, we decide to store and update all model parameters on the CPU (see green box). A fresh set of model parameters is transferred to the GPU when a new batch of data is processed by all GPUs.""

However I find in my rnn application(tower loss 4GPU), without setting  tf.device('/cpu:0'): actually is faster.
Setting   tf.device('/cpu:0'):
about 44s per 100 step, take 9min43s to finish the first 1000 steps
2016-10-10 17:37:53 epoch:0.1664 train_step:100 duration:0.474 elapsed:56.103 train_avg_metrics:['loss:0.496']  ['loss:0.486']
2016-10-10 17:38:38 epoch:0.3328 train_step:200 duration:0.520 elapsed:45.064 train_avg_metrics:['loss:0.464']  ['loss:0.448']
2016-10-10 17:39:25 epoch:0.4992 train_step:300 duration:0.408 elapsed:46.612 train_avg_metrics:['loss:0.383']  ['loss:0.238']
2016-10-10 17:40:10 epoch:0.6656 train_step:400 duration:0.471 elapsed:44.819 train_avg_metrics:['loss:0.308']  ['loss:0.317']
2016-10-10 17:40:54 epoch:0.8319 train_step:500 duration:0.431 elapsed:44.716 train_avg_metrics:['loss:0.269']  ['loss:0.245']
2016-10-10 17:41:39 epoch:0.9983 train_step:600 duration:0.418 elapsed:44.256 train_avg_metrics:['loss:0.242']  ['loss:0.234']
2016-10-10 17:42:35 epoch:1.1647 train_step:700 duration:0.424 elapsed:56.733 train_avg_metrics:['loss:0.214']  ['loss:0.168']
2016-10-10 17:43:19 epoch:1.3311 train_step:800 duration:0.402 elapsed:43.769 train_avg_metrics:['loss:0.209']  ['loss:0.213']
2016-10-10 17:44:01 epoch:1.4975 train_step:900 duration:0.475 elapsed:41.747 train_avg_metrics:['loss:0.201']  ['loss:0.193']
2016-10-10 17:44:43 epoch:1.6639 train_step:1000 duration:0.423 elapsed:42.351 train_avg_metrics:['loss:0.190']  ['loss:0.238']
2016-10-10 17:44:43 0:08:14 epoch:1.6639 eval_step: 1000 train_avg_loss:['loss:0.298'] 
Not setting tf.device('/cpu:0'):
about 35s per 100 step, take 6min48s to finsish the first 1000 steps
2016-10-11 04:24:43 epoch:0.1664 train_step:100 duration:0.371 elapsed:47.976 train_avg_metrics:['loss:0.496']  ['loss:0.485']
2016-10-11 04:25:19 epoch:0.3328 train_step:200 duration:0.385 elapsed:36.630 train_avg_metrics:['loss:0.470']  ['loss:0.408']
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 1680058 get requests, put_count=1680141 evicted_count=3000 eviction_rate=0.00178556 and unsatisfied allocation rate=0.00195172
2016-10-11 04:25:56 epoch:0.4992 train_step:300 duration:0.319 elapsed:37.185 train_avg_metrics:['loss:0.386']  ['loss:0.336']
2016-10-11 04:26:32 epoch:0.6656 train_step:400 duration:0.411 elapsed:35.670 train_avg_metrics:['loss:0.320']  ['loss:0.303']
2016-10-11 04:27:07 epoch:0.8319 train_step:500 duration:0.360 elapsed:35.204 train_avg_metrics:['loss:0.268']  ['loss:0.272']
2016-10-11 04:27:43 epoch:0.9983 train_step:600 duration:0.333 elapsed:35.526 train_avg_metrics:['loss:0.248']  ['loss:0.167']
2016-10-11 04:28:31 epoch:1.1647 train_step:700 duration:0.378 elapsed:48.393 train_avg_metrics:['loss:0.218']  ['loss:0.170']
2016-10-11 04:29:06 epoch:1.3311 train_step:800 duration:0.334 elapsed:34.472 train_avg_metrics:['loss:0.208']  ['loss:0.264']
2016-10-11 04:29:38 epoch:1.4975 train_step:900 duration:0.322 elapsed:32.613 train_avg_metrics:['loss:0.199']  ['loss:0.198']
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 7824854 get requests, put_count=7824942 evicted_count=13000 eviction_rate=0.00166135 and unsatisfied allocation rate=0.00169639
2016-10-11 04:30:11 epoch:1.6639 train_step:1000 duration:0.320 elapsed:32.889 train_avg_metrics:['loss:0.199']  ['loss:0.219']
2016-10-11 04:30:11 0:06:48 epoch:1.6639 eval_step: 1000 train_avg_loss:['loss:0.301'] 
"
4880,Needed to install `pbr` and `funcsigs`,"I was following the [Pip Installation](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#pip-installation) instructions.  When I ran the [Test the TensorFlow installation](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#test-the-tensorflow-installation) steps, I hit two errors, listed below.
### The Fix!

To fix, I ran `pip install pbr funcsigs`, then the test Python program worked.
### Environment info

Operating System: Mac OSX 10.10.5

If installed from binary pip package, provide:
1. A link to the pip package you installed:
   I installed this one:

```
# Mac OS X, CPU only, Python 2.7:
$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0rc0-py2-none-any.whl
```
1. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

```
0.11.0rc0
```
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

I was following the existing instructions, I don't _think_ I missed any steps...
### Logs or other output that would be helpful

Here are the two errors that I saw:

1) 

```
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/__init__.py"", line 23, in <module>
    from tensorflow.python import *
  File ""/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 98, in <module>
    from tensorflow.python.platform import test
  File ""/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/platform/test.py"", line 77, in <module>
    import mock                # pylint: disable=g-import-not-at-top,unused-import
  File ""/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/mock/__init__.py"", line 2, in <module>
    import mock.mock as _mock
  File ""/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/mock/mock.py"", line 69, in <module>
    from pbr.version import VersionInfo
ImportError: No module named pbr.version
```

2)

```
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/__init__.py"", line 23, in <module>
    from tensorflow.python import *
  File ""/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 98, in <module>
    from tensorflow.python.platform import test
  File ""/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/platform/test.py"", line 77, in <module>
    import mock                # pylint: disable=g-import-not-at-top,unused-import
  File ""/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/mock/__init__.py"", line 2, in <module>
    import mock.mock as _mock
  File ""/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/mock/mock.py"", line 80, in <module>
    import funcsigs
ImportError: No module named funcsigs
```
"
4877,fully_connected() doesn't reshape input,"[According to the docs](https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.layers.html#fully_connected),  if `inputs`' rank is >2 fully_connected() flattens it. However, running with `inputs.shape = (None, m, n)` I get:

```
ValueError: logits and targets must have the same shape ((?, m, 1) vs (?, 1))
```

And when I run with `inputs.shape = (None, m*n)` everything is fine.

On `0.11rc0`.
"
4876,"Eigen::TensorEvaluator `m_impl` is private, compiling cxx11_tensor_cuda.cu","i know this is a bug for Eigen repo, but the owner of the file seems to be Benoit Steiner, so tentatively thinking here might be a good place to file it?

If I build cxx11_tensor_cuda.cu using standard eigen build process, it does in fact compile.

However, if I build using clang, I get the following error:

```
In file included from test/eigen/cxx11_tensor_cuda.cu:19:
In file included from /usr/local/eigen/unsupported/Eigen/CXX11/Tensor:95:
/usr/local/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReductionCuda.h:687:50: error: 
      'm_impl' is a private member of 'Eigen::TensorEvaluator<const
      Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>, const
      Eigen::array<long, 2>, const Eigen::TensorMap<Eigen::Tensor<float, 4, 0,
      long>, 0> >, Eigen::GpuDevice>'
      typename Self::CoeffReturnType val = input.m_impl.coeff(j * num_pr...
...
test/eigen/cxx11_tensor_cuda.cu:234:30: note: in instantiation of function
      template specialization
      'Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<float, 2, 0, long>,
      0>,
      Eigen::GpuDevice>::operator=<Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>,
      const Eigen::array<long, 2>, const Eigen::TensorMap<Eigen::Tensor<float,
      4, 0, long>, 0> > >' requested here
  gpu_out.device(gpu_device) = gpu_in1.maximum(reduction_axis);
                             ^
/usr/local/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:732:36: note: 
      declared private here
  TensorEvaluator<ArgType, Device> m_impl;
```

I'm building as follows:
- (optional) comment out lines 15-17, to disable fp16
- run, from cloned eigen repo:

```
 clang++-3.8 -std=c++11 -I. -Itest -Ibuild/test -I/usr/local/cuda-7.5/include unsupported/test/cxx11_tensor_cuda.cu --cuda-host-only -emit-llvm  -O3 -S -o cxx11_tensor_cuda-hostraw.ll
```

Thoughts?  @benoitsteiner 

(edited to have a correct commandline, tested from root of cloned eigen repo)
"
4875,Error for Mac El-capitan while installing tensorflow GPU version on CUDA 8.0,"Below is the error I am getting when I import tensorflow:

import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/d/anaconda/lib/python2.7/site-packages/tensorflow/**init**.py"", line 23, in <module>
    from tensorflow.python import *
  File ""/Users/d/anaconda/lib/python2.7/site-packages/tensorflow/python/**init**.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Users/d/anaconda/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/Users/d/anaconda/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
ImportError: dlopen(/Users/d/anaconda/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.7.5.dylib
  Referenced from: /Users/d/anaconda/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
  Reason: image not found

I don't understand why it is asking for libcudart.7.5.dylib when I have installed CUDA 8.0. Theano is working perfectly fine for me on GPU but tensorflow is not. Any help would be appreciated. I have exactly followed the steps mentioned on tensorflow website. I also tried disabling SIP protection in El-capitan as mentioned in other similar github issues but no luck.
"
4871,"Can I back-propagate the already calculated gradients w.r.t the outputs to all the parameters, and then apply the gradients to the whole network? ","I can get the gradients w.r.t the outputs of the network(calculated by python or other ways), Can I just back-propagate the gradients w.r.t the outputs to all the parameters, and then apply them to the whole network? 
"
4867,[0.11]Can't bind GPU with tf.device(),"I installed the latest TF v0.11  and found that it can't bind specific GPU with `tf.device(""gpu:1"")`.  It always applies all GPU resource when I run `tf.Session()`, which is unreasonable.
For example:

``` python
import tensorflow as tf
with tf.device(""gpu:1""):
  sess = tf.Session()
```

GPU resource:

```
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      7895    C   python                                       10963MiB |
|    1      7895    C   python                                       10922MiB |
|    2      7895    C   python                                       10923MiB |
|    3      7895    C   python                                       10922MiB |
+-----------------------------------------------------------------------------+
```

But when I switch to TF v0.9, everything is fine.
GPU resource:

```
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      5161    C   python                                          65MiB |
|    1      5161    C   python                                          65MiB |
|    2      5161    C   python                                          65MiB |
|    3      5161    C   python                                          65MiB |
+-----------------------------------------------------------------------------+
```

I think it's a bug and caused by the update of `tf.Session()`.
I hope I can fix it ASAP
"
4865,GraphDef duplication in event log,"This is a little difficult to explain, and I'm guessing someone has to have run into it before, but it would be nice to at least get an explanation for what's going on here.

The issue I'm finding is that when you specify a `model_dir` argument for any tflearn model, the first time you fit that model, the tensorboard warning like the following will always appear:

`WARNING:tensorflow:Found more than one graph event per run. Overwriting the graph with the newest event.`

If you clear the contents of that directory and rerun the same model, the warning no longer appears.

For example, here is a piece of code to reproduce:

```
import tensorflow as tf
import random

from tensorflow.contrib.learn.python import learn
from tensorflow.contrib.learn.python.learn import datasets
from tensorflow.contrib.learn.python.learn.estimators._sklearn import accuracy_score
from tensorflow.contrib.learn.python.learn.estimators._sklearn import train_test_split
random.seed(42)

iris = datasets.load_iris()
x_train, x_test, y_train, y_test = train_test_split(iris.data,
                                                    iris.target,
                                                    test_size=0.2,
                                                    random_state=42)

def custom_optimizer(learning_rate):
    return tf.train.MomentumOptimizer(learning_rate, 0.9)

classifier = learn.TensorFlowDNNClassifier(
    hidden_units=[10, 20, 10],
    n_classes=3,
    steps=400,
    learning_rate=0.01,
    optimizer=custom_optimizer,
    model_dir='/tmp/tf/bug_test_1')
classifier.fit(x_train, y_train)
```

If you run this in a jupyter notebook, wait until it is done, cd into `/tmp/tf/bug_test_1`, and run `tensorboard --logdir=.`, the warning will appear.  If you clear the contents and run again, it will no longer appear.  If you, in the same notebook within the same kernel, change the directory to something else (e.g. `/tmp/tf/bug_test_2`) and run again, the warning will again appear.  And the warning will again go away if you clear the contents and run again with the same directory.

I also tried pre-creating the directory but that appears to make no difference.

I wouldn't imagine this indicates a major problem but I try to take those warnings seriously since they have indicated more problematic issues in the past with running multiple models in the same kernel, so it would be great to know what this means.
### Environment info

Operating System: OS X Yosemite (10.10.5)
Tensorflow Version: 0.10.0
"
4863,iOS error: No OpKernel was registered to support Op 'Mul' with these attrs [T=DT_INT32],"I have some issues performing a multiplication of int32 data on iOS. Session::Run fails with the error shown below, indicating that this particular multiplication op is not supported. I've already checked tf_op_files.txt, and 'tensorflow/core/kernels/cwise_op_mul.cc' is there, obviously, and it looks to like the int32 version should also get registered there.

Do I need to take any extra steps to enable int32 multiplication on iOS?

This is the exact error message I'm getting:

```
No OpKernel was registered to support Op 'Mul' with these attrs
     [[Node: mul = Mul[T=DT_INT32](Cast, Cast)]]
```

I'm using TensorFlow 0.10. Here is how I create the graph def file in Python:

```
import tensorflow as tf
from tensorflow.python.framework import graph_util

input = tf.placeholder(tf.float32, shape=(1,4), name='input')

v = tf.cast(input, tf.int32)
v = v * v
output = tf.cast(v, tf.float32, name='output')

with tf.Session() as sess:
    output_graph_def = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['output'])

with tf.gfile.GFile('/tmp/test_graph.pb', 'wb') as f:
    f.write(output_graph_def.SerializeToString())
```

Thanks a lot,
Peter
"
4861,Example mnist_rnn Not Working with Docker Image,"# Issue: Example mnist_rnn does run on docker image.

```
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-6-3bb5b939d552> in <module>()
      3 from __future__ import print_function
      4 
----> 5 from sklearn import metrics, preprocessing
      6 
      7 import tensorflow as tf

ImportError: No module named sklearn

```
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

An example in the code base does not work with the docker image. It is the opinion of the filer that all examples should run without any need for configuration on the docker image because the project has control over what is installed on the docker image.

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/mnist_rnn.py
### Environment info

Operating System:

docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow

> Installed version of CUDA and cuDNN: 

**NONE, CPU based container**

If installed from binary pip package, provide:

> The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

```
# python -c ""import tensorflow; print(tensorflow.__version__)""
0.11.0rc0
```

> If installed from source, provide 

Not installed from source

> If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Example given at beginning of ticket.

> What other attempted solutions have you tried?

Removed all references to sklearn. Application works.

```
# It's useful to scale to ensure Stochastic Gradient Descent will do the right thing
#scaler = preprocessing.StandardScaler()
#X_train = scaler.fit_transform(X_train)
#X_test = scaler.fit_transform(X_test)
```

> Logs or other output that would be helpful

No logs produced.
"
4859,incorrect gradient for complex matmul,"The `MatMul` kernel is registered for `complex64` and `complex128` on CPU and GPU. However the gradient is incorrect. This issue has not been caught because there is no test for a complex-valued `MatMul` gradient in `tensorflow\python\kernel_tests\matmul_op_test.py`.

When computing the complex gradient we need to apply the conjugate of the input. Several cwise gradient ops in TF were ignoring this step until it was addressed by @girving and @tensorflower-gardener in https://github.com/tensorflow/tensorflow/commit/821063df9f0e6a0eec8cb78cb0ddc5c5b2b91b9f. However it seems that `MatMul` snuck through the cracks.

**To Reproduce**

``` python
ops.reset_default_graph()
sess = tf.Session()
x = (np.linspace(-3, 3, 6) + 1j*np.linspace(3, -3, 6)).reshape(3, 2)
y = (np.linspace(-3, 3, 8) + 1j*np.linspace(3, -3, 8)).reshape(2, 4)
with sess.as_default():
    with tf.device('/cpu'):
        x_tf = tf.constant(x, dtype=tf.complex64, name=""x"")
        y_tf = tf.constant(y, dtype=tf.complex64, name=""y"")
        m = tf.matmul(x_tf, y_tf, name=""matmul"")
        err = tf.test.compute_gradient_error(x_tf, [3, 2], m, [3, 4])
        print(err)

with sess.as_default():
    with tf.device('/gpu'):
        x_tf = tf.constant(x, dtype=tf.complex64, name=""x"")
        y_tf = tf.constant(y, dtype=tf.complex64, name=""y"")
        m = tf.matmul(x_tf, y_tf, name=""matmul"")
        err = tf.test.compute_gradient_error(x_tf, [3, 2], m, [3, 4])
        print(err)
```

``` python
>>> 6.00002098083
>>> 6.00002098083
```

**Proposed Fix**
Replace [these lines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L697-L714) with:

``` python
@ops.RegisterGradient(""MatMul"")
def _MatMulGrad(op, grad):
  with ops.control_dependencies([grad.op]):
      inp0 = math_ops.conj(op.inputs[0])
      inp1 = math_ops.conj(op.inputs[1])
      t_a = op.get_attr(""transpose_a"")
      t_b = op.get_attr(""transpose_b"")
      if not t_a and not t_b:
        return (math_ops.matmul(grad, inp1, transpose_b=True),
                math_ops.matmul(inp0, grad, transpose_a=True))
      elif not t_a and t_b:
        return (math_ops.matmul(grad, inp1),
                math_ops.matmul(grad, inp0, transpose_a=True))
      elif t_a and not t_b:
        return (math_ops.matmul(op.inp1, grad, transpose_b=True),
                math_ops.matmul(op.inp0, grad))
      elif t_a and t_b:
        return (math_ops.matmul(op.inp1, grad, transpose_a=True,
                                transpose_b=True),
                math_ops.matmul(grad, op.inp0, transpose_a=True,
                                transpose_b=True))
```

and add a complex gradient test to `tensorflow\python\kernel_tests\matmul_op_test.py`.
"
4858,"macbook pro GPU version, successfully opened CUDA library, but not found the GPU","Hi ALL:
I am a chinese student, so may be English is bas. Sorry.
now I use the tensorflow GPU version. I user the pip to down the python3 GPU version, and down the all about the GPU file.
But i can successfully opened CUDA library, but not found the GPU, like this:
`In [1]: import tensorflow as tf
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.dylib locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.dylib locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.dylib locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.1.dylib locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.dylib locally
`

`
In [2]: tf.Session()
E tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:153] retrieving CUDA diagnostic information for host: wangxiaoweideWindows.local
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: wangxiaoweideWindows.local
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: 346.3.6
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: Invalid argument: expected %d.%d or %d.%d.%d form for driver version; got """"
I tensorflow/core/common_runtime/gpu/gpu_init.cc:81] No GPU devices available on machine.
`

and I find the I can't deviceQuery the GPU:

`âžœ  ~ nvcc -V
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2015 NVIDIA Corporation
Built on Mon_Apr_11_13:23:40_CDT_2016
Cuda compilation tools, release 7.5, V7.5.26
`

`
âžœ  ~ ~/cuda-samples/bin/x86_64/darwin/release/deviceQuery
/Users/codeMan/cuda-samples/bin/x86_64/darwin/release/deviceQuery Starting...
 CUDA Device Query (Runtime API) version (CUDART static linking)
cudaGetDeviceCount returned 38
-> no CUDA-capable device is detected
Result = FAIL`

now I use the mac os, and Xcode8, how to solve it ?
"
4857,tf.extract_image_patches Trying stride only on row,"Hi, Tried to use tf.extract_image_patche() of a tensor [N, sequence_length, embeding_size, 1] to do n-gram with patch size [1,sequence_length-#gram+1, embedding_size, 1]. Thus I set the stride to be [1,1,0,1]. However this leds to the error: `ZeroDivisionError: integer division or modulo by zero`

Is there a way to only stride on one dim and avoid this error?

below is the skeleton of my code and error message:

``` (python)
self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)
self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)
slic = tf.(self.embedded_chars_expanded, [1,i,embedding_size,1], [1,1,0,1], [1,1,1,1], 'VALID')
```

```
---------------------------------------------------------------------------
ZeroDivisionError                         Traceback (most recent call last)
/home/zijia/nlp/A2/train.py in <module>()
     85             embedding_size=FLAGS.embedding_dim,
     86             n_gram=FLAGS.n_gram,
---> 87             l2_reg_lambda=FLAGS.l2_reg_lambda)
     88
     89         # Define Training procedure

/home/zijia/nlp/A2/text_cnn.py in __init__(self, sequence_length, num_classes, vocab_size, embedding_size, n_gram, l2_reg_lambda)
     32                 grams = [ self.embedded_chars_expanded ]
     33                 for i in range(2,n_gram+1):
---> 34                     slic = tf.extract_image_patches(self.embedded_chars_expanded, [1,i,embedding_size,1], [1,1,0,1], [1,1,1,1], 'VALID')
     35                     print slic.get_shape()
     36                     slic = tf.reduce_sum( slic, 3 )

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.pyc in extract_image_patches(images, ksizes, strides, rates, padding, name)
    918   result = _op_def_lib.apply_op(""ExtractImagePatches"", images=images,
    919                                 ksizes=ksizes, strides=strides, rates=rates,
--> 920                                 padding=padding, name=name)
    921   return result
    922

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)
    701           op = g.create_op(op_type_name, inputs, output_types, name=scope,
    702                            input_types=input_types, attrs=attr_protos,
--> 703                            op_def=op_def)
    704           outputs = op.outputs
    705           return _Restructure(ops.convert_n_to_tensor(outputs),

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)
   2317                     original_op=self._default_original_op, op_def=op_def)
   2318     if compute_shapes:
-> 2319       set_shapes_for_outputs(ret)
   2320     self._add_op(ret)
   2321     self._record_op_seen_by_control_dependencies(ret)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)
   1709       raise RuntimeError(""No shape function registered for standard op: %s""
   1710                          % op.type)
-> 1711   shapes = shape_func(op)
   1712   if shapes is None:
   1713     raise RuntimeError(

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.pyc in _ExtractImagePatchesShape(op)
   2290                                                             ksize_c_eff,
   2291                                                             stride_r, stride_c,
-> 2292                                                             padding)
   2293
   2294   out_depth = None if in_depth is None else ksize_r * ksize_c * int(in_depth)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in get2d_conv_output_size(input_height, input_width, filter_height, filter_width, row_stride, col_stride, padding_type)
    182   return get_conv_output_size((input_height, input_width),
    183                               (filter_height, filter_width),
--> 184                               (row_stride, col_stride), padding_type)
    185
    186

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in get_conv_output_size(input_size, filter_size, strides, padding_type)
    159     output_size = [
    160         _valid(in_dim, k_dim, s_dim)
--> 161         for in_dim, k_dim, s_dim in zip(input_size, filter_size, strides)
    162     ]
    163   elif padding_type == b""SAME"":

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in _valid(in_dim, k_dim, s_dim)
    153     def _valid(in_dim, k_dim, s_dim):
    154       if in_dim is not None and k_dim is not None:
--> 155         return (in_dim - k_dim + s_dim) // s_dim
    156       else:
    157         return None

ZeroDivisionError: integer division or modulo by zero
```
"
4856,TensorBoard charts blank on Firefox,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

I have not found any related issues
### Environment info

Operating System:

Ubuntu 16.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

```

rik@rik-MS-7971:~$ ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root   560184 Sep 23 09:09 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Sep 23 09:09 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Sep 23 09:09 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 root root   394472 Sep 23 09:09 /usr/local/cuda/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 root root   737516 Sep 23 09:09 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 rik  rik        13 Jul 27 01:55 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 rik  rik        17 Jul 27 01:55 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxrwxr-x 1 rik  rik  79337624 Sep 26 21:55 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-rw-r-- 1 rik  rik  69756172 Sep 26 21:55 /usr/local/cuda/lib64/libcudnn_static.a
```

If installed from binary pip package, provide:
1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

```
rik@rik-MS-7971:~$ python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
0.11.0rc0
```

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
### What other attempted solutions have you tried?

Tried erasing files from logdir and re-running. Tried running mnist with summaries tutorial, this is also blank charts
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).

---

All, when running [MNIST with Summaries example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py) charts exist on events and distribution tabs, they are blank. Histograms and appear correct. Also, code is running slow compated to non-tensorboard version. Please help, Thanks. Below is output.

```
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.
Extracting /tmp/data/train-images-idx3-ubyte.gz
Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.
Extracting /tmp/data/train-labels-idx1-ubyte.gz
Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.
Extracting /tmp/data/t10k-images-idx3-ubyte.gz
Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.
Extracting /tmp/data/t10k-labels-idx1-ubyte.gz
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.797
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 7.36GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
Accuracy at step 0: 0.0802
Accuracy at step 10: 0.6788
Accuracy at step 20: 0.8217
Accuracy at step 30: 0.8495
Accuracy at step 40: 0.8734
Accuracy at step 50: 0.8794
Accuracy at step 60: 0.8847
Accuracy at step 70: 0.8863
Accuracy at step 80: 0.8898
Accuracy at step 90: 0.8939
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcupti.so locally
Adding run metadata for 99
Accuracy at step 100: 0.9015
Accuracy at step 110: 0.9132
Accuracy at step 120: 0.9163
Accuracy at step 130: 0.9197
Accuracy at step 140: 0.9252
Accuracy at step 150: 0.9182
Accuracy at step 160: 0.9283
Accuracy at step 170: 0.9243
Accuracy at step 180: 0.9231
Accuracy at step 190: 0.9266
Adding run metadata for 199
Accuracy at step 200: 0.9327
Accuracy at step 210: 0.9336
Accuracy at step 220: 0.9329
Accuracy at step 230: 0.9311
Accuracy at step 240: 0.9308
Accuracy at step 250: 0.9214
Accuracy at step 260: 0.9324
Accuracy at step 270: 0.9393
Accuracy at step 280: 0.9327
Accuracy at step 290: 0.9401
Adding run metadata for 299
Accuracy at step 300: 0.9442
Accuracy at step 310: 0.9405
Accuracy at step 320: 0.9424
Accuracy at step 330: 0.9425
Accuracy at step 340: 0.9468
Accuracy at step 350: 0.9461
Accuracy at step 360: 0.9427
Accuracy at step 370: 0.9434
Accuracy at step 380: 0.9474
Accuracy at step 390: 0.9449
Adding run metadata for 399
Accuracy at step 400: 0.9443
Accuracy at step 410: 0.9449
Accuracy at step 420: 0.9474
Accuracy at step 430: 0.9455
Accuracy at step 440: 0.952
Accuracy at step 450: 0.9505
Accuracy at step 460: 0.9525
Accuracy at step 470: 0.9493
Accuracy at step 480: 0.9489
Accuracy at step 490: 0.9515
Adding run metadata for 499
Accuracy at step 500: 0.9517
Accuracy at step 510: 0.9515
Accuracy at step 520: 0.9499
Accuracy at step 530: 0.9539
Accuracy at step 540: 0.9548
Accuracy at step 550: 0.9573
Accuracy at step 560: 0.9548
Accuracy at step 570: 0.9569
Accuracy at step 580: 0.9552
Accuracy at step 590: 0.9548
Adding run metadata for 599
Accuracy at step 600: 0.9551
Accuracy at step 610: 0.9535
Accuracy at step 620: 0.96
Accuracy at step 630: 0.9599
Accuracy at step 640: 0.9603
Accuracy at step 650: 0.9616
Accuracy at step 660: 0.9617
Accuracy at step 670: 0.9623
Accuracy at step 680: 0.9613
Accuracy at step 690: 0.9614
Adding run metadata for 699
Accuracy at step 700: 0.9592
Accuracy at step 710: 0.9614
Accuracy at step 720: 0.9595
Accuracy at step 730: 0.9646
Accuracy at step 740: 0.9636
Accuracy at step 750: 0.964
Accuracy at step 760: 0.9656
Accuracy at step 770: 0.9616
Accuracy at step 780: 0.9631
Accuracy at step 790: 0.964
Adding run metadata for 799
Accuracy at step 800: 0.9637
Accuracy at step 810: 0.9637
Accuracy at step 820: 0.9653
Accuracy at step 830: 0.9636
Accuracy at step 840: 0.9615
Accuracy at step 850: 0.9654
Accuracy at step 860: 0.9668
Accuracy at step 870: 0.9653
Accuracy at step 880: 0.965
Accuracy at step 890: 0.9645
Adding run metadata for 899
Accuracy at step 900: 0.9664
Accuracy at step 910: 0.967
Accuracy at step 920: 0.9658
Accuracy at step 930: 0.9664
Accuracy at step 940: 0.9671
Accuracy at step 950: 0.967
Accuracy at step 960: 0.9663
Accuracy at step 970: 0.968
Accuracy at step 980: 0.9679
Accuracy at step 990: 0.9666
Adding run metadata for 999
```

---
"
4855,DeepDream tutorial Unsupervised,"Is it possibile to have a unsupervised version of the DeepDream tutorial?
And what kind of solution could be adopted? VAE? DCGAN? 
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/deepdream
"
4854,Can we ignore 'tensorflow.python.framework.errors.DataLossError: corrupted record at 0',"I will face this problem when reading tf records on hdfs, may be running one hour or two hours, then my program will crash down due to this assertion fail from tensorflow c++ code.
Can we just ignore this, ie. ignore the batch with DataLossErro, so can continue training next batch without having to stop the program.
"
4853,Windows version of Tensorflow won't compile,"@mrry 
Thank you for your great work for the windows version of tensorflow. I've tried to build tensorflow on windows based on your tensorflow CMake build guide.  The only difference is that I didn't use anaconda. 

I've tried to build the tf_tutorials_example_trainer target. However it seems to depend on gif and png target, which are not able to get built. 

For the gif target, it seems to be invoking a configure shell script which is not able to be executed in windows cmd. The error reads like:

```
Performing download step (download, verify and extract) for 'gif'
  -- verifying file...
         file='C:/Users/v-liaha/Documents/GitHub/tensorflow/tensorflow/contrib/cmake/build/downloads/giflib-5.1.4.tar.gz'
  -- File already exists and hash match (skip download):
    file='C:/Users/v-liaha/Documents/GitHub/tensorflow/tensorflow/contrib/cmake/build/downloads/giflib-5.1.4.tar.gz'
    SHA256='34a7377ba834397db019e8eb122e551a49c98f49df75ec3fcc92b9a794a4f6d1'
  -- extracting...
       src='C:/Users/v-liaha/Documents/GitHub/tensorflow/tensorflow/contrib/cmake/build/downloads/giflib-5.1.4.tar.gz'
       dst='C:/Users/v-liaha/Documents/GitHub/tensorflow/tensorflow/contrib/cmake/build/gif/src/gif'
  -- extracting... [tar xfz]
  -- extracting... [analysis]
  -- extracting... [rename]
  -- extracting... [clean up]
  -- extracting... done
  No update step for 'gif'
  No patch step for 'gif'
  Performing configure step for 'gif'
  'C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\gif\src\gif\configure' is not recognized as an internal or external command,
  operable program or batch file.
C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(171,5): error MSB6006: ""cmd.exe"" exited with code 9009. [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\gif.vcxproj]
```

For the png target, the error reads like:

```
C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png.vcxproj]


  ""C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png\src\png-build\ALL_BUILD.vcxproj"" (default target) (1) ->
  ""C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png\src\png-build\png12.vcxproj"" (default target) (3) ->
  (Link target) -> 
png.obj : error LNK2019: unresolved external symbol __imp_inflateReset referenced in function png_reset_zstream [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png\src\png-build\png12.vcxproj] [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png.vcxproj]
pngrutil.obj : error LNK2001: unresolved external symbol __imp_inflateReset [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png\src\png-build\png12.vcxproj] [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png.vcxproj]
png.obj : error LNK2019: unresolved external symbol __imp_crc32 referenced in function png_calculate_crc [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png\src\png-build\png12.vcxproj] [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png.vcxproj]
pngpread.obj : error LNK2019: unresolved external symbol __imp_inflate referenced in function png_process_IDAT_data [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png\src\png-build\png12.vcxproj] [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png.vcxproj]
pngread.obj : error LNK2001: unresolved external symbol __imp_inflate [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png\src\png-build\png12.vcxproj] [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png.vcxproj]
pngrutil.obj : error LNK2001: unresolved external symbol __imp_inflate [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png\src\png-build\png12.vcxproj] [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png.vcxproj]
pngread.obj : error LNK2019: unresolved external symbol __imp_inflateEnd referenced in function png_read_destroy [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png\src\png-build\png12.vcxproj] [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png.vcxproj]
pngread.obj : error LNK2019: unresolved external symbol __imp_inflateInit_ referenced in function png_create_read_struct_2 [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png\src\png-build\png12.vcxproj] [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png.vcxproj]
pngwrite.obj : error LNK2019: unresolved external symbol __imp_deflate referenced in function png_write_flush [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png\src\png-build\png12.vcxproj] [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png.vcxproj]
pngwutil.obj : error LNK2001: unresolved external symbol __imp_deflate [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png\src\png-build\png12.vcxproj] [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png.vcxproj]
pngwrite.obj : error LNK2019: unresolved external symbol __imp_deflateEnd referenced in function png_write_destroy [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png\src\png-build\png12.vcxproj] [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png.vcxproj]
pngwutil.obj : error LNK2019: unresolved external symbol __imp_deflateReset referenced in function png_write_compressed_data_out [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png\src\png-build\png12.vcxproj] [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png.vcxproj]
pngwutil.obj : error LNK2019: unresolved external symbol __imp_deflateInit2_ referenced in function png_write_IHDR [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png\src\png-build\png12.vcxproj] [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png.vcxproj]
C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png\src\png-build\Release\libpng12.dll : fatal error LNK1120: 9 unresolved externals [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png\src\png-build\png12.vcxproj] [C:\Users\v-liaha\Documents\GitHub\tensorflow\tensorflow\contrib\cmake\build\png.vcxproj]

      51 Warning(s)
      14 Error(s)

  Time Elapsed 00:00:04.48
  The command exited with code 1.
```

And I checked issue [#4798](https://github.com/tensorflow/tensorflow/issues/4798), @laudney and @elcombato seems not having such a problem. 
"
4852,tensorflow.python.framework.errors.PermissionDeniedError: File isn't open for reading,"`
File ""./encoder.py"", line 103, in <module>
 tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""./encoder.py"", line 99, in main
    np.savez_compressed(code_file, shape=int_codes.shape, codes=export)
  File ""/usr/local/lib/python2.7/dist-packages/numpy/lib/npyio.py"", line 600, in savez_compressed
    _savez(file, args, kwds, True)
  File ""/usr/local/lib/python2.7/dist-packages/numpy/lib/npyio.py"", line 642, in _savez
    zipf.write(tmpfile, arcname=fname)
  File ""/usr/lib/python2.7/zipfile.py"", line 1146, in write
    zinfo.header_offset = self.fp.tell()    # Start of header bytes
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py"", line 139, in tell
    ""File isn't open for reading"")
tensorflow.python.framework.errors.PermissionDeniedError: File isn't open for reading
Exception tensorflow.python.framework.errors.PermissionDeniedError: PermissionDeniedError() in <bound method ZipFile.__del__ of <zipfile.ZipFile object at 0x7fc6f2011450>> ignored
`
"
4851,missing dependency declarations for the following files included by 'external/protobuf/python/google/protobuf/internal/api_implementation.cc',"when I install the syntaxnetã€‚I encounter with the error""missing dependency declarations"". what below is the details.
ERROR: /vol6/home/para11/.cache/bazel/_bazel_para11/05959d3334ad62caf308f1ce677a2caf/external/protobuf/BUILD:516:1: undeclared inclusion(s) in rule '@protobuf//:internal/_api_implementation.so':
this rule is missing dependency declarations for the following files included by 'external/protobuf/python/google/protobuf/internal/api_implementation.cc':
  '/vol6/python2.7/include/python2.7/Python.h'
  '/vol6/python2.7/include/python2.7/patchlevel.h'
  '/vol6/python2.7/include/python2.7/pyconfig.h'
  '/vol6/python2.7/include/python2.7/pymacconfig.h'
  '/vol6/python2.7/include/python2.7/pyport.h'
  '/vol6/python2.7/include/python2.7/pymath.h'
  '/vol6/python2.7/include/python2.7/pymem.h'
  '/vol6/python2.7/include/python2.7/object.h'
  '/vol6/python2.7/include/python2.7/objimpl.h'
  '/vol6/python2.7/include/python2.7/pydebug.h'
  '/vol6/python2.7/include/python2.7/unicodeobject.h'
  '/vol6/python2.7/include/python2.7/intobject.h'
  '/vol6/python2.7/include/python2.7/boolobject.h'
  '/vol6/python2.7/include/python2.7/longobject.h'
  '/vol6/python2.7/include/python2.7/floatobject.h'
  '/vol6/python2.7/include/python2.7/complexobject.h'
  '/vol6/python2.7/include/python2.7/rangeobject.h'
  '/vol6/python2.7/include/python2.7/stringobject.h'
  '/vol6/python2.7/include/python2.7/memoryobject.h'
  '/vol6/python2.7/include/python2.7/bufferobject.h'
  '/vol6/python2.7/include/python2.7/bytesobject.h'
  '/vol6/python2.7/include/python2.7/bytearrayobject.h'
  '/vol6/python2.7/include/python2.7/tupleobject.h'
  '/vol6/python2.7/include/python2.7/listobject.h'
  '/vol6/python2.7/include/python2.7/dictobject.h'
  '/vol6/python2.7/include/python2.7/enumobject.h'
  '/vol6/python2.7/include/python2.7/setobject.h'
  '/vol6/python2.7/include/python2.7/methodobject.h'
  '/vol6/python2.7/include/python2.7/moduleobject.h'
  '/vol6/python2.7/include/python2.7/funcobject.h'
  '/vol6/python2.7/include/python2.7/classobject.h'
  '/vol6/python2.7/include/python2.7/fileobject.h'
  '/vol6/python2.7/include/python2.7/cobject.h'
  '/vol6/python2.7/include/python2.7/pycapsule.h'
  '/vol6/python2.7/include/python2.7/traceback.h'
  '/vol6/python2.7/include/python2.7/sliceobject.h'
  '/vol6/python2.7/include/python2.7/cellobject.h'
  '/vol6/python2.7/include/python2.7/iterobject.h'
  '/vol6/python2.7/include/python2.7/genobject.h'
  '/vol6/python2.7/include/python2.7/descrobject.h'
  '/vol6/python2.7/include/python2.7/warnings.h'
  '/vol6/python2.7/include/python2.7/weakrefobject.h'
  '/vol6/python2.7/include/python2.7/codecs.h'
  '/vol6/python2.7/include/python2.7/pyerrors.h'
  '/vol6/python2.7/include/python2.7/pystate.h'
  '/vol6/python2.7/include/python2.7/pyarena.h'
  '/vol6/python2.7/include/python2.7/modsupport.h'
  '/vol6/python2.7/include/python2.7/pythonrun.h'
  '/vol6/python2.7/include/python2.7/ceval.h'
  '/vol6/python2.7/include/python2.7/sysmodule.h'
  '/vol6/python2.7/include/python2.7/intrcheck.h'
  '/vol6/python2.7/include/python2.7/import.h'
  '/vol6/python2.7/include/python2.7/abstract.h'
  '/vol6/python2.7/include/python2.7/compile.h'
  '/vol6/python2.7/include/python2.7/code.h'
  '/vol6/python2.7/include/python2.7/eval.h'
  '/vol6/python2.7/include/python2.7/pyctype.h'
  '/vol6/python2.7/include/python2.7/pystrtod.h'
  '/vol6/python2.7/include/python2.7/pystrcmp.h'
  '/vol6/python2.7/include/python2.7/dtoa.h'
  '/vol6/python2.7/include/python2.7/pyfpe.h'.
"
4849,"Method docs contain unclosed verbatim, which causes exceeding verbatim block in HTML docs","The HTML docs for `tf.constant_initializer` at  https://www.tensorflow.org/versions/master/api_docs/python/state_ops.html#constant_initializer contain unclosed verbatim block, which continues past the method and contains several following methods -- see for example the `tf.random_normal_initializer`.

I am not much familiar with the process of documentation generation, but my guess is that the verbatim ending block at https://github.com/tensorflow/tensorflow/blob/2c4af2e65a2018540d949cfdba1fcb15d0121f80/tensorflow/python/ops/init_ops.py#L142 has wrong indentation (it is indented by two more spaces than the start of the block).
"
4848,"""bazel clean"" will undo ""./configure"" when source configured to build for GPU.","### Environment info

Operating System:
ubuntu 14.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
cuda 8.0.44, cudnn 5.5

If installed from binary pip package, provide:
No

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
   c8d4896e3231c3ac32f174fd0af051867645fbb5
2. The output of `bazel version`
   Build label: 0.3.1
   Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
   Build time: Fri Jul 29 09:09:52 2016 (1469783392)
   Build timestamp: 1469783392
   Build timestamp as int: 1469783392
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

./configure   # configure with GPU support)
bazel test -c opt --config=cuda tensorflow/core:common_runtime_direct_session_test # this one should pass
bazel clean
bazel test -c opt --config=cuda tensorflow/core:common_runtime_direct_session_test  # this one fails, complaining no GPU support configured.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
java.lang.RuntimeException: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@ae
3629bd' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@1374ec1e', 'CONFIGURATION_FRAGMENT:com
.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@3686b55', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue
$ConfigurationFragmentKey@a93d9174')
        at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1070)
        at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:474)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

David, Damien, looks like bazel clean undid some output of ""./configure"" at head.
Any idea where things went wrong?
"
4847,input tensor lost in --mode eightbit quantization,"Quantizing with --mode eightbit somehow loses the input tensor, whereas other methods like weights_rounded don't. If I do this:

`python quantize_graph.py --input ~/proc/frozen_inference_optimized.pb --output ~/proc/frozen_inference_optimized_quantized.pb --output_node_names on_logits --mode eightbit`

the following error pops up in the iOS tensorflow runtime:

`Running model failed:Not found: FeedInputs: unable to find feed output preprocess/centered_bgr`

Here `preprocess/centered_bgr` is the node I want to use as the input. If instead I use something like `--mode weights_rounded` no error is raised and the network functions as expected.
### Environment info

Operating System: ubuntu / iOS

Installed version of CUDA and cuDNN: 

```
root@0f1fd91e29a1:~/w/tensorflow/tensorflow/contrib/quantization/tools# ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root 322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18
-rw-r--r-- 1 root root 720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a
```

Commit I'm using: 8915f0f8072c406ae3fe0dff888f51b4cad02d7d. bazel version 0.3.0.

cc @petewarden 
"
4846,Integrating Metal API on iOS?,"Any plans to use the Metal API on iOS? E.g. https://developer.apple.com/reference/metalperformanceshaders/mpscnnconvolution.
"
4844,Tensorboard - Alphabetic ordering of Runs in Graph tab,"In all the tabs in tensorboard, the different runs are neatly ordered. However, this is not the case for the Graph tab. See screenshot.
I'm going crosseyed looking for the correct model each time.
![image](https://cloud.githubusercontent.com/assets/7721540/19213241/c1de13a4-8d6e-11e6-9fcb-68c98f743a9f.png)
"
4842,TypeError: The value of a feed cannot be a tf.Tensor object,"Hi, thanks before you see it.
I got an error when i run tensorflow model of cnn.

TypeError: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, or numpy ndarrays.

 This error tell me that I cant feed tensor,but i dont know how to modify my code .Any one can tell me how can i modify my code ? 

Here is my code.
https://github.com/xuhuapeng/cifar/blob/master/cnn_1.py

  https://github.com/xuhuapeng/cifar

Thanks very much.
"
4841,Problem on building target with GPU support,"Result of `bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer`:

```
ERROR: /home/darth/.cache/bazel/_bazel_darth/08554d152596e5a7df399506682a63f3/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):
    File ""/home/darth/.cache/bazel/_bazel_darth/08554d152596e5a7df399506682a63f3/external/local_config_cuda/crosstool/BUILD"", line 4
        error_gpu_disabled()
    File ""/home/darth/.cache/bazel/_bazel_darth/08554d152596e5a7df399506682a63f3/external/local_config_cuda/crosstool/error_gpu_disabled.bzl"", line 3, in error_gpu_disabled
        fail(""ERROR: Building with --config=c..."")
ERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.
ERROR: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/darth/.cache/bazel/_bazel_darth/08554d152596e5a7df399506682a63f3/external/local_config_cuda/crosstool/BUILD.
INFO: Elapsed time: 0.097s
```

And yes, I have GPU enabled when I did `sudo ./configure`
"
4840,retrain.py input PNG picture,"Hi all, 

I try to change the retrain.py from jpeg to png so that I can input the png image. ( replace ""jpeg"" in the code to ""png"")

but I got the error **'Requested return_element %r not found in graph_def.' % name)
ValueError: Requested return_element 'DecodePng/contents:0' not found in graph_def.**

How to fix it ? 
I try **input_map={'DecodePng:0': decoded_png}** in **tf.import_graph_def** but **decoded_png** locate under the **create_inception_graph()** so I don't know where is the **decoded_png** to add with **""DecodePng:0""** 

thank you and regards,
Khoa
"
4838,AttributeError during pip install when building from source,"I was trying to build TensorFlow from source, and all went well until I was in the last step 
`sudo pip install /tmp/tensorflow-pkg/tensorflow-0.11.0rc0-py2-none-any.whl
`
it crashes with this error message: 

> Exception:
> Traceback (most recent call last):
>   File ""/usr/lib/python2.7/dist-packages/pip/basecommand.py"", line 209, in main
>     status = self.run(options, args)
>   File ""/usr/lib/python2.7/dist-packages/pip/commands/install.py"", line 317, in run
>     requirement_set.prepare_files(finder)
>   File ""/usr/lib/python2.7/dist-packages/pip/req/req_set.py"", line 360, in prepare_files
>     ignore_dependencies=self.ignore_dependencies))
>   File ""/usr/lib/python2.7/dist-packages/pip/req/req_set.py"", line 448, in _prepare_file
>     req_to_install, finder)
>   File ""/usr/lib/python2.7/dist-packages/pip/req/req_set.py"", line 387, in _check_skip_installed
>     req_to_install.check_if_exists()
>   File ""/usr/lib/python2.7/dist-packages/pip/req/req_install.py"", line 1011, in check_if_exists
>     self.req.project_name
> AttributeError: 'Requirement' object has no attribute 'project_name'

I did some search online and found that this may be due to an upstream bug and it's been happening in other projects too. Is there anyway to work around this issue? I currently can't use a pre-compiled whl because I'm using gtx1080 and CUDA8, i think the precompiled whl only supports 7.5

I'm using Ubuntu 16.04, python 2.7, pip 8.1.1
"
4836,ZeroMQ Operator,"Currently (it seems like) the encouraged way to read data is either through the provided reader/producer ops which are not quite flexible, or through python feed_dict which is slower.

Is there a plan to support something like a ZeroMQReaderOp, which keeps reading and deserializing tensors from a zmq socket and produce them? This way we can use any comfortable way to process data perhaps in an independent process, and send them through zmq for training.

// UPDATE: wrote my own at https://github.com/tensorpack/zmq_ops
  "
4834,"Error: no instance of constructor ""Eigen::array<T, n>::array [with T=int, n=2UL]"" matches the argument list             argument types are: ({...})","Hi, when I try to build tensowflow from source on a aarch64 16.04 ubuntu, I meet this error. I have been struggling with this for several days already, but still no luck.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

[#2143](https://github.com/tensorflow/tensorflow/issues/2143)
[#1066](https://github.com/tensorflow/tensorflow/issues/1066#issuecomment-200580370)
[#3786](https://github.com/tensorflow/tensorflow/issues/3786)
[#4430](https://github.com/tensorflow/tensorflow/issues/4430)
[#3985](https://github.com/tensorflow/tensorflow/issues/3985)
[Errors when building tensorflow on linux](http://stackoverflow.com/questions/38585357/errors-when-building-tensorflow-on-linux)
[](https://github.com/xman/tensorflow/commit/0cdb90d6024ead10e9dc87baf4730dc0962fde3c)
### Environment info

Operating System:
ubuntu 16.04
Linux tegra-ubuntu 3.10.96-tegra #1 SMP PREEMPT Wed Sep 28 17:51:08 PDT 2016 aarch64 aarch64 aarch64 GNU/Linux

Graphic: NVIDIA Tegra X1 (nvgpu)/integrated

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

CUDA version: 8.0
cuDNN version; 5.1.5
gcc version: 5.3 (also have tried 4.8, but no luck)

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)

v0.11.0.rc0
1. The output of `bazel version`

```
ubuntu@tegra-ubuntu:~/ws/tensorflow/tensorflow$ bazel version
Build label: 0.3.2-2016-10-04 (@5604163)
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Oct 4 23:34:52 2016 (1475624092)
Build timestamp: 1475624092
Build timestamp as int: 1475624092
```
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```
bazel clean --expunge

./configure (enable GPU support)

 bazel build -c opt --local_resources 1024,4.0,1.0 --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package
```
### What other attempted solutions have you tried?

I have tried gcc5.3 and gcc4.8, but not work.  I also tried to add these to third_party/gpus/crosstool/CROSSTOOL:

```
cxx_flag: ""-D_MWAITXINTRIN_H_INCLUDED""
cxx_flag: ""-D_FORCE_INLINES""
cxx_flag: ""-D__STRICT_ANSI__""
```

And I insert this to CROSSTOOL too:

```
cxx_builtin_include_directory: ""/usr/local/cuda-8.0/include""
```

but the errors are still there.
### Logs or other output that would be helpful

```
tensorflow/core/kernels/cwise_op_gpu_select.cu.cc(46): error: no instance of constructor ""Eigen::array<T, n>::array [with T=int, n=2UL]"" matches the argument list
            argument types are: ({...})
          detected during instantiation of ""void tensorflow::functor::BatchSelectFunctor<tensorflow::functor::GPUDevice, T>::operator()(const tensorflow::functor::GPUDevice &, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix, tensorflow::TTypes<tensorflow::functor::base<__nv_bool, Eigen::internal::scalar_boolean_not_op<__nv_bool>, __nv_bool>::out_type, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix) [with T=Eigen::half]"" 
(74): here

tensorflow/core/kernels/cwise_op_gpu_select.cu.cc(46): error: no instance of constructor ""Eigen::array<T, n>::array [with T=int, n=2UL]"" matches the argument list
            argument types are: ({...})
          detected during instantiation of ""void tensorflow::functor::BatchSelectFunctor<tensorflow::functor::GPUDevice, T>::operator()(const tensorflow::functor::GPUDevice &, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix, tensorflow::TTypes<tensorflow::functor::base<__nv_bool, Eigen::internal::scalar_boolean_not_op<__nv_bool>, __nv_bool>::out_type, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix) [with T=float]"" 
(75): here

tensorflow/core/kernels/cwise_op_gpu_select.cu.cc(46): error: no instance of constructor ""Eigen::array<T, n>::array [with T=int, n=2UL]"" matches the argument list
            argument types are: ({...})
          detected during instantiation of ""void tensorflow::functor::BatchSelectFunctor<tensorflow::functor::GPUDevice, T>::operator()(const tensorflow::functor::GPUDevice &, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix, tensorflow::TTypes<tensorflow::functor::base<__nv_bool, Eigen::internal::scalar_boolean_not_op<__nv_bool>, __nv_bool>::out_type, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix) [with T=double]"" 
(76): here

tensorflow/core/kernels/cwise_op_gpu_select.cu.cc(46): error: no instance of constructor ""Eigen::array<T, n>::array [with T=int, n=2UL]"" matches the argument list
            argument types are: ({...})
          detected during instantiation of ""void tensorflow::functor::BatchSelectFunctor<tensorflow::functor::GPUDevice, T>::operator()(const tensorflow::functor::GPUDevice &, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix, tensorflow::TTypes<tensorflow::functor::base<__nv_bool, Eigen::internal::scalar_boolean_not_op<__nv_bool>, __nv_bool>::out_type, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix) [with T=tensorflow::int32]"" 
(77): here

tensorflow/core/kernels/cwise_op_gpu_select.cu.cc(46): error: no instance of constructor ""Eigen::array<T, n>::array [with T=int, n=2UL]"" matches the argument list
            argument types are: ({...})
          detected during instantiation of ""void tensorflow::functor::BatchSelectFunctor<tensorflow::functor::GPUDevice, T>::operator()(const tensorflow::functor::GPUDevice &, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix, tensorflow::TTypes<tensorflow::functor::base<__nv_bool, Eigen::internal::scalar_boolean_not_op<__nv_bool>, __nv_bool>::out_type, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix) [with T=tensorflow::int64]"" 
(78): here

tensorflow/core/kernels/cwise_op_gpu_select.cu.cc(46): error: no instance of constructor ""Eigen::array<T, n>::array [with T=int, n=2UL]"" matches the argument list
            argument types are: ({...})
          detected during instantiation of ""void tensorflow::functor::BatchSelectFunctor<tensorflow::functor::GPUDevice, T>::operator()(const tensorflow::functor::GPUDevice &, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix, tensorflow::TTypes<tensorflow::functor::base<__nv_bool, Eigen::internal::scalar_boolean_not_op<__nv_bool>, __nv_bool>::out_type, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix) [with T=tensorflow::functor::complex64]"" 
(79): here

tensorflow/core/kernels/cwise_op_gpu_select.cu.cc(46): error: no instance of constructor ""Eigen::array<T, n>::array [with T=int, n=2UL]"" matches the argument list
            argument types are: ({...})
          detected during instantiation of ""void tensorflow::functor::BatchSelectFunctor<tensorflow::functor::GPUDevice, T>::operator()(const tensorflow::functor::GPUDevice &, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix, tensorflow::TTypes<tensorflow::functor::base<__nv_bool, Eigen::internal::scalar_boolean_not_op<__nv_bool>, __nv_bool>::out_type, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix) [with T=tensorflow::functor::complex128]"" 
(80): here

7 errors detected in the compilation of ""/tmp/tmpxft_00002cb1_00000000-7_cwise_op_gpu_select.cu.cpp1.ii"".
ERROR: /home/ubuntu/ws/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:1170:1: output 'tensorflow/core/kernels/_objs/cwise_op_gpu/tensorflow/core/kernels/cwise_op_gpu_select.cu.pic.o' was not created.
ERROR: /home/ubuntu/ws/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:1170:1: not all outputs were created.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /home/ubuntu/ws/tensorflow/tensorflow/tensorflow/tools/pip_package/BUILD:23:1 not all outputs were created.

```
"
4833,Slow IDCT used in JPEG decode,"This issue is related to #4807, yet a bit distinct. Regardless of whether libjpeg or libjpeg-turbo is used, I was wondering if there was a reason to choose the slow but more accurate variant of IDCT in libjpeg ([code link](https://github.com/tensorflow/tensorflow/blob/6b1d4fd8090d44d20fdadabf06f1a9b178c3d80c/tensorflow/core/lib/jpeg/jpeg_mem.cc#L143)). For human image viewing pleasure, accuracy may matter, but I was wondering if for model training, the decision was made to use the slower IDCT because that improved model convergence? In case the training is immune to using the faster but less accurate IDCT, it may be worth changing the default. Has anyone done any studies as to the trained model impact of the faster IDCT? From a performance point of view, I'm getting a significant performance gain (2x GTX 1080, 6-core high-clock Core i7, 500 MB/s SSD), around 20% for AlexNet against precomputed ImageNet AlexNet may be more CPU bound for decode since the network passes themselves are relatively cheap, so maybe the gains for Inception v3 or ResNet would be less, but they would still likely be significant.
"
4832,Android Demo build error,"The error I saw is as follows, don't quit know how this permission error occurs

```
ERROR: /home/toxido/.cache/bazel/_bazel_root/e7444205eb7a78f2fb0f6da55e24d78b/external/androidsdk/BUILD:5:1: Executing genrule @androidsdk//:aapt_runner failed: linux-sandbox failed: error executing command /home/toxido/.cache/bazel/_bazel_root/e7444205eb7a78f2fb0f6da55e24d78b/execroot/tensorflow/_bin/linux-sandbox ... (remaining 5 argument(s) skipped).
src/main/tools/linux-sandbox-pid1.cc:233: ""mount(/, /home/toxido/.cache/bazel/_bazel_root/e7444205eb7a78f2fb0f6da55e24d78b/bazel-sandbox/28d014be-3643-4295-b588-2ed7789511a4-1/tmp, NULL, MS_BIND | MS_REC, NULL)"": Permission denied
Target //tensorflow/examples/android:tensorflow_demo failed to build
Use --verbose_failures to see the command lines of failed build steps.
```
"
4830,tensorboard produces a lot of IOError,"Hi, I'm using the tensorboard comes with 0.11rc.0 and it outputs a lot of warning on IOError for not being able to find file or directory, e.g. /path/to/tensorflow/tensorboard/paper-item/all-imports.html. I checked the directory and there are indeed no such files. The board still runs but wonder if we can get rid of the warnings. My python env is anaconda 2.7.
"
4829,TensorBoard logdir assignment issue,"Dear all,
As instructed I type
`$tensorboard --logdir=/tmp/tensorflowlogs --debug`
but the debug message showing that the tensorboard logdir is still in my working dir.
`INFO:tensorflow:TensorBoard path_to_run is: {'/home/USER_NAME/Documents/workspace/python_dir/tensor_test/=': None}
`
However after I deleted the ""="" sign and replace it with a space
`$tensorboard --logdir /tmp/tensorflowlogs --debug`

the debug message show the right direcory
`INFO:tensorflow:TensorBoard path_to_run is: {'/tmp/tensorflowlogs': None}
`
I don't know its a typo or something in the instruction.

So just FYI
"
4828,"PIP install OS X, requirement to install","Today I tried to install TensorFlow on my MacBook pro following the guidelines on [tensorflow.org](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#installing-from-sources).

Operating System: OS X 10.11.6
Pip: 8.1.2 (python 3.5)
My MBP doesn't have an actual GPU so I tried using the CPU only version: [link](https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0rc0-py3-none-any.whl)

When trying to run `sudo -H pip3.5 install --upgrade $TG_BINARY_URL`
Pip logged `You must give at least one requirement to install (see ""pip help install"")`

I'm pretty new to pip, is there something obvious that I'm missing ?
"
4827,Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: :/usr/local/cuda-7.5/lib64,"I have a system where I installed CUDA 7.5 for TensorFlow 0.10.0. The specs of my system is written in #4825 . However, I have a small problem when importing `tensorflow` in my Python 3.5.2

Screenshot of the problem.

![screenshot from 2016-10-08 01-11-11](https://cloud.githubusercontent.com/assets/11130276/19199007/8a0a861c-8cf4-11e6-8155-6a71ff932947.png)

Any help would be great!
"
4825,Failure to install from source,"So, I have installed TensorFlow 0.10.0 using `pip3`. I have Python 3.5.2 GCC 5.4.0. And am running it on Ubuntu 16.04 LTS (x64) with the following specifications:

<ul>
<li>CPU: Intel Core i5-6300HQ 2.3GHz-3.2GHz</li>
<li>RAM: 8GB DDR3 1600MHz</li>
<li>GPU: NVidia GeForce GTX 960M DDR5 4GB</li>
<li>1 TB Hybrid HD + 8GB Cache</li>
</ul>


When I'm trying to install `tensorflow` from source, i.e. `git clone https://github.com/tensorflow/tensorflow`, `cd tensorflow`, and `bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`. It gives me the following error:

![screenshot from 2016-10-07 23-56-18](https://cloud.githubusercontent.com/assets/11130276/19196833/af667cdc-8ce9-11e6-82a2-eda8c620651e.png)

Even if I use `sudo`, the same error arises. I want to use `tensorflow` with GPU enabled in my system. Any help would be much appreciated. Thanks!
"
4823,Tensorflow not using GPU (according to TensorBoard),"GTX 1070, ubuntu 16.04.

I am retraining [inception](https://github.com/tensorflow/models/tree/master/inception) model on my own data. Everything is fine until the final command :

```
bazel-bin/inception/flowers_train \
  --config=cuda \
  --train_dir=""${TRAIN_DIR}"" \
  --data_dir=""${OUTPUT_DIRECTORY}"" \
  --pretrained_model_checkpoint_path=""${MODEL_PATH}"" \
  --fine_tune=True \
  --initial_learning_rate=0.001 \
  --input_queue_memory_factor=1
```

According to the logs, **Tensorflow seems to be using the GPU** :

```
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties:
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.7715
pciBusID 0000:03:00.0
Total memory: 7.92GiB
Free memory: 7.77GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:03:00.0)
```

But when I am checking the learning in TensorBoard, **the net is using mainly the CPU** (blue /device:CPU:0, green /device:GPU:0):

TensorBoard graph:

![graph](https://cloud.githubusercontent.com/assets/4989990/19196523/33a2e6c2-8cb6-11e6-85b1-1f35950657e3.png)

I have tried this two TensorFlow setups :
1. Install from the source (3b75eb34ea2c4982fb80843be089f02d430faade) with nvidia-367 drivers, CUDA8 8.0, cuDNN
   v5,    source from the master (16/10/06 - r11?). compiled for GPU
   use:
   
   ```
   bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer
   bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu    
   bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
   ```
2. docker GPU image of Tensorflow on a PC with a GTX
   1070 8Go
   
   ```
   nvidia-docker run -it -p 8888:8888 -p 6006:6006 gcr.io/tensorflow/tensorflow:latest-gpu /bin/bash
   ```

According to the doc Cuda >=7.0 can be used when installed from sources. 

> The GPU version works best with Cuda Toolkit 7.5 and cuDNN v5. Other versions are supported (Cuda toolkit >= 7.0 and cuDNN >= v3) only when installing from sources

And the dockerfile begins with `FROM nvidia/cuda:7.5-cudnn5-devel`. So it is not using cuda and cudnn of the host system
"
4821,Request for documentation: quantize_training in python,"Poking around, I find https://github.com/tensorflow/tensorflow/search?utf8=%E2%9C%93&q=quantize_training

However there is no documents that I can find describe how this can be used. Can we add some example and also documentation around it so that people can start to play with it?
"
4820,Thread created by SummaryWriter not killed,"Hi,

I noticed that the _EventLoggerThread created by summary writer does not get killed by the close method, which will make the number of threads keep increasing until it exceeds the system capacity. Is there any particular reason to not kill these threads?

Best 
"
4818,jkjk,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
### Environment info

Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:
1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
### What other attempted solutions have you tried?
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
"
4816,tensroflow 0.11rc conflict with boost.python generated so,"My application will use both tensorflow and some so generated by boost.python(1.53) running on centos 6.3 with cuda 7.5 and cudnn 4 .
The problem is everything is fine with tensorflow 0.10, but changing to tensorflow 0.11rc, then 
I will always face segmentation fault (double free) before the program stop running, especially if using numpy also, and if changing import library sequence  might decrease the chance of facing double free, but still not fully solve the problem.
"
4815,Neural Network Exchange Format (NNEF),"Do you want to support and contribute to the standardizzation of provisional [NNEF](https://www.khronos.org/nnef) effort by Kronos Group? Google is still in the [promoter member list](https://www.khronos.org/members/member_list)
"
4814,[Feature Request] Make streaming metrics resettable,"Hi! 
Currently, the streaming metrics are (as far as I know) not resettable. I'd like to be able to e.g. reset the counter after each epoch. This way, having e.g. a very bad accuracy in the beginning of training will not still influence the accuracy value ten epochs later. It makes it easier to compare my results to runs obtained outside tensorflow.

The only workaround I found is to do `sess.run(tf.initialize_local_variables())` after each epoch, but of course this can have bad side effects if I have other local variables that I don't want to reset.

Or is there a way to achieve what I want that I didn't think of?
"
4813,OpenCL support,
4812,How do you check the code runtime-result without build it ?,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
### Environment info

Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:
1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
### What other attempted solutions have you tried?
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
"
4811,Wrong Answer,
4810,#bug# How do you check the code runtime-result without build it ? ,"According to this [link](http://stackoverflow.com/questions/39890170/tensorflow-build-fail-at-october-6-2016),I think some developer can't build it .
I'm not that familiar with the source code now and can't coding without debug.
"
4809,Getting Send/Recv timings for distributed TF,"I'm trying to troubleshoot some slowness in our distributed models , and it would be useful to have access to timing of Send/Recv ops across graph partition.

cc @suharshs because maybe StatsPublisherInterface is relevant?

For instance, a toy benchmark [here](https://gist.github.com/yaroslavvb/1124bb02a9fd4abce3d86caf2f950cb2) adds 100MB vectors of 1's in one process to variable in another process on local machine. If I look at timeline/stepstats, I see 120ms of emptiness, followed by 20ms in `AddOp` followed by another 700ms of missing time.

Because it's a toy benchmark, I can figure out that 120ms is spent in transferring 100MB from one TF runtime to another, and 700ms is spent making the result available to Python client. But it's harder to do this on a large model

<img width=""1298"" alt=""screen shot 2016-10-06 at 5 27 51 pm"" src=""https://cloud.githubusercontent.com/assets/23068/19175296/47ddfe64-8bea-11e6-9882-a4a59a9823c8.png"">
"
4808,.ckpt file cann't be freezed into .pb after run eval_image_classifier.py,"This is a very strange issue i meet. i do not know what changes are made by eval_image_classifier.py
### Environment info

Ubuntu 16.04
### Steps:
1. Train inception v1 (slim) model and get .ckpt files.
2. Run freeze_graph to freeze .ckpt file into .pb file, like cmd below:

**bazel-bin/tensorflow/python/tools/freeze_graph \
 --input_graph=/home/tensorflow/v1.3sgd_copy/graph.pbtxt \
 --input_checkpoint=/home/tensorflow/v1.3sgd_copy/model.ckpt-73074 \
 --output_graph=/home/tensorflow/v1.3sgd_copy/freeze.pb \
 --output_node_names=InceptionV1/Logits/Predictions/Softmax**

you will get:

**Converted 173 variables to const ops.
1766 ops in the final graph.**

 3, Run model evaluation by cmd below

**python eval_image_classifier.py \
  --checkpoint_path=/home/tensorflow/v1.3sgd_copy/model.ckpt-73074 \
  --eval_dir=/home/tensorflow/v1.3sgd_copy/ \
  --dataset_name=animals \
  --dataset_split_name=validation \
  --dataset_dir=/home/data/datadic_train/ \
  --model_name=inception_v1 \
  --batch_size=256 \
  --labels_offset=0**

then you will get the result like: 
**I tensorflow/core/kernels/logging_ops.cc:79] eval/Accuracy[0.7301592]**

 4, Now run the cmd in step 2 again to freeze the same .ckpt file

CMD:

**bazel-bin/tensorflow/python/tools/freeze_graph \
 --input_graph=/home/tensorflow/v1.3sgd_copy/graph.pbtxt \
 --input_checkpoint=/home/tensorflow/v1.3sgd_copy/model.ckpt-73074 \
 --output_graph=/home/tensorflow/v1.3sgd_copy/freeze.pb \
 --output_node_names=InceptionV1/Logits/Predictions/Softmax**

Then you will get errors: **Attempting to use uninitialized value accuracy/total**
### Log

W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
Traceback (most recent call last):
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 134, in <module>
    tf.app.run()
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 131, in main
    FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 120, in freeze_graph
    sess, input_graph_def, output_node_names.split("",""))
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/graph_util.py"", line 226, in convert_variables_to_constants
    returned_variables = sess.run(variable_names)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 717, in run
    run_metadata_ptr)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 915, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 965, in _do_run
    target_list, options, run_metadata)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 985, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.FailedPreconditionError: Attempting to use uninitialized value accuracy/total
     [[Node: _send_accuracy/total_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=381203147379632588, tensor_name=""accuracy/total:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](accuracy/total)]]
"
4807,Upgrade to libjpeg-turbo,"TensorFlow seems to be using the classic libjpeg v 0.9, but libjpeg-turbo is heavily optimized to leverage SIMD on various platforms (x86, ARM, POWER, etc.), and is both API and ABI compatible with libjpeg, at least up to libjpeg 0.8. Given TensorFlow's goal of portability across a wide varierty of platforms, I'm wondering if libjpeg was decided upon instead of libjpeg-turbo for portability, or was this just an omission? 

libjpeg-turbo speeds up JPEG decoding by about 2x, and since that reduces decoding latency and increases throughput, it not only improves performance of TensorFlow on the CPU, but especially on the GPU, which tends to wait for the CPU to provide decoded images. Moreover, that could result in a major performance improvement on more CPU-underpowered platforms such as ARM (e.g. via NEON SIMD on the NVIDIA TX1 platform which uses the NEON-capable Cortex A57, or on ARM Cortex A8 on a Samsung Galaxy S).
"
4799,Error building PIP package on Windows,"Hi,

I followed the new instructions for the [CMake build on Windows](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md).
The C++ example program `tf_tutorials_example_trainer.exe` build and run successfully.
But the build of the PIP package exited with the following error:

```
ld_pip_package.vcxproj"" (Standardziel) (1) ->
""C:\Users\jonas\projects\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensor flow.vcxproj"" (Standardziel) (3) ->
(ClCompile Ziel) ->
C:\Users\jonas\projects\tensorflow\tensorflow\python\lib\core\py_func.cc(165): 
error C2466: cannot allocate an array of constant size 0 [C:\Users\jonas\projec ts\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow.vcxproj]

    2398 Warnung(en)
    1 Fehler
```
"
4798,Error building on Windows,"(Copied from @laudney's comment on #17)

> I've tried to follow your instructions. All seem to work flawlessly (for about 45min) until it failed with:
> 
> ```
> The target ""BeforeGenerateProjectPriFile"" listed in a BeforeTargets attribute at ""C:\Program Files (x86)\MSBuild\Microsoft\NuGet\Microsoft.NuGet.targets (186,61)"" does not exist in the project, and will be ignored.
> Done Building Project ""H:\PycharmProjects\tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj"" (default targets) -- FAILED.
> 
> ""H:\PycharmProjects\tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj"" (default target) (1) ->
> ""H:\PycharmProjects\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow.vcxproj"" (default target) (3) ->
> ""H:\PycharmProjects\tensorflow\tensorflow\contrib\cmake\build\tf_core_cpu.vcxproj"" (default target) (4) ->
> ""H:\PycharmProjects\tensorflow\tensorflow\contrib\cmake\build\tf_core_framework.vcxproj"" (default target) (5) ->
> (CustomBuild target) ->
>   C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(171,5): error MSB6006: ""cmd.exe"" exited with code 1. [H:\PycharmProjects\t
> ensorflow\tensorflow\contrib\cmake\build\tf_core_framework.vcxproj]
> 
>     30 Warning(s)
>     1 Error(s)
> ```
"
4797,Error following the wide and deep tutorial ,"I've been trying to follow through the wide and deep tutorial on a different data set and I am getting the below error.

I am using this version of tensorflow: tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl
on python: Python 2.7.5

All my fields are strings (features and labels)

I am adding the relevant code snippets

```
f1Col = tf.contrib.layers.sparse_column_with_keys(""f1"", keys = [""l1"", ""l2"", ""l3""])
..
..
I've defined all columns similar to this one (no hash bucket ones)
..
..

```

```
wide_columns = [
  tf.contrib.layers.crossed_column(columns = [f1Col, f2Col, f3Col] , hash_bucket_size = 1e7),
  ...
  defined all crossed columns like the above one with smaller hash buckets for ones with less features
  ...
]
```

```
deep_columns = [
    tf.contrib.layers.embedding_column(f1Col, dimension=8),
    ...
   defined all my features here as embedding columns exactly as the one above
   ....
]
```

tried this with both settings for enable_centered_bias the error changes a bit but is still the same

```
import tempfile
model_dir = tempfile.mkdtemp()
m = tf.contrib.learn.DNNLinearCombinedClassifier(
    model_dir=model_dir,
    linear_feature_columns=wide_columns,
    dnn_feature_columns=deep_columns,
    dnn_hidden_units=[1000, 500, 250], enable_centered_bias = False)
```

```
def input_fn(df):
  categorical_cols = {k: tf.SparseTensor(
      indices=[[i, 0] for i in range(df[k].size)],
      values=df[k].values,
      shape=[df[k].size, 1])
                      for k in my_features_col_list}
  label = tf.constant(df['my_label_col'].values)
  return categorical_cols, label

def train_input_fn():
  return input_fn(features)

def eval_input_fn():
  return input_fn(features)
```

then when I try to fit like this:
`m.fit(input_fn=train_input_fn, steps=200)`

I get the following error. I am not sure if I hit a bug or did something wrong. I am getting some warnings in the earlier code about combiner function defaults changing

Then again when executing the fit I get the 2 warnings and then the error below them:

```
WARNING:tensorflow:Given features: {'f1': <tensorflow.python.framework.ops.SparseTensor object at 0xc5bf690>, ... required signatures: {'f1': TensorSignature(dtype=tf.string, shape=None, is_sparse=True), ...

WARNING:tensorflow:Given targets: Tensor(""Const:0"", shape=(2019,), dtype=string), required signatures: TensorSignature(dtype=tf.string, shape=TensorShape([Dimension(2019)]), is_sparse=False).

```

```
---------------------------------------------------------------------------
UnimplementedError                        Traceback (most recent call last)
<ipython-input-29-8f5351c1fdf8> in <module>()
----> 1 m.fit(input_fn=train_input_fn, steps=200)

/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)
    331                              steps=steps,
    332                              monitors=monitors,
--> 333                              max_steps=max_steps)
    334     logging.info('Loss for final step: %s.', loss)
    335     return self

/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _train_model(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss, max_steps)
    706           fail_on_nan_loss=fail_on_nan_loss,
    707           hooks=hooks,
--> 708           max_steps=max_steps)
    709 
    710   def _extract_metric_update_ops(self, eval_dict):

/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.pyc in _monitored_train(graph, output_dir, train_op, loss_op, global_step_tensor, init_op, init_feed_dict, init_fn, log_every_steps, supervisor_is_chief, supervisor_master, supervisor_save_model_secs, keep_checkpoint_max, supervisor_save_summaries_steps, feed_fn, steps, fail_on_nan_loss, hooks, max_steps)
    283       while not super_sess.should_stop():
    284         _, loss = super_sess.run([train_op, loss_op], feed_fn() if feed_fn else
--> 285                                  None)
    286       return loss
    287 

/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/monitored_session.pyc in run(self, fetches, feed_dict, options, run_metadata)
    366                           feed_dict=feed_dict,
    367                           options=options,
--> 368                           run_metadata=run_metadata)
    369 
    370   def should_stop(self):

/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/monitored_session.pyc in run(self, fetches, feed_dict, options, run_metadata)
    519                               feed_dict=feed_dict,
    520                               options=options,
--> 521                               run_metadata=run_metadata)
    522       except errors.AbortedError:
    523         self.close()

/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/monitored_session.pyc in run(self, *args, **kwargs)
    486 
    487   def run(self, *args, **kwargs):
--> 488     return self._sess.run(*args, **kwargs)
    489 
    490 

/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/monitored_session.pyc in run(self, fetches, feed_dict, options, run_metadata)
    617                                   feed_dict=feed_dict,
    618                                   options=options,
--> 619                                   run_metadata=run_metadata)
    620 
    621     for hook in self._hooks:

/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/monitored_session.pyc in run(self, *args, **kwargs)
    486 
    487   def run(self, *args, **kwargs):
--> 488     return self._sess.run(*args, **kwargs)
    489 
    490 

/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)
    715     try:
    716       result = self._run(None, fetches, feed_dict, options_ptr,
--> 717                          run_metadata_ptr)
    718       if run_metadata:
    719         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)
    913     if final_fetches or final_targets:
    914       results = self._do_run(handle, final_targets, final_fetches,
--> 915                              feed_dict_string, options, run_metadata)
    916     else:
    917       results = []

/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
    963     if handle is None:
    964       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
--> 965                            target_list, options, run_metadata)
    966     else:
    967       return self._do_call(_prun_fn, self._session, handle, feed_dict,

/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)
    983         except KeyError:
    984           pass
--> 985       raise type(e)(node_def, op, message)
    986 
    987   def _extend_graph(self):

UnimplementedError: Cast string to float is not supported
     [[Node: ToFloat = Cast[DstT=DT_FLOAT, SrcT=DT_STRING, _device=""/job:localhost/replica:0/task:0/cpu:0""](ExpandDims)]]

Caused by op u'ToFloat', defined at:
  File ""/usr/lib64/python2.7/runpy.py"", line 162, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib64/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/usr/lib/python2.7/site-packages/ipykernel/__main__.py"", line 3, in <module>
    app.launch_new_instance()
  File ""/usr/lib/python2.7/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/usr/lib/python2.7/site-packages/ipykernel/kernelapp.py"", line 474, in start
    ioloop.IOLoop.instance().start()
  File ""/usr/lib/python2.7/site-packages/zmq/eventloop/ioloop.py"", line 162, in start
    super(ZMQIOLoop, self).start()
  File ""/usr/lib/python2.7/site-packages/tornado/ioloop.py"", line 887, in start
    handler_func(fd_obj, events)
  File ""/usr/lib/python2.7/site-packages/tornado/stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/usr/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/usr/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/usr/lib/python2.7/site-packages/tornado/stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 276, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/usr/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 228, in dispatch_shell
    handler(stream, idents, msg)
  File ""/usr/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 390, in execute_request
    user_expressions, allow_stdin)
  File ""/usr/lib/python2.7/site-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/usr/lib/python2.7/site-packages/ipykernel/zmqshell.py"", line 501, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2717, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2827, in run_ast_nodes
    if self.run_code(code, result):
  File ""/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-29-8f5351c1fdf8>"", line 1, in <module>
    m.fit(input_fn=train_input_fn, steps=200)
  File ""/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 333, in fit
    max_steps=max_steps)
  File ""/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 662, in _train_model
    train_op, loss_op = self._get_train_ops(features, targets)
  File ""/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py"", line 195, in _get_train_ops
    features)
  File ""/usr/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/target_column.py"", line 206, in training_loss
    loss_unweighted = self._loss_fn(logits, target)
  File ""/usr/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/target_column.py"", line 389, in _log_loss_with_two_classes
    math_ops.to_float(target))
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 661, in to_float
    return cast(x, dtypes.float32, name=name)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 616, in cast
    return gen_math_ops.cast(x, base_type, name=name)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 419, in cast
    result = _op_def_lib.apply_op(""Cast"", x=x, DstT=DstT, name=name)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 749, in apply_op
    op_def=op_def)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2380, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1298, in __init__
    self._traceback = _extract_stack()

UnimplementedError (see above for traceback): Cast string to float is not supported
     [[Node: ToFloat = Cast[DstT=DT_FLOAT, SrcT=DT_STRING, _device=""/job:localhost/replica:0/task:0/cpu:0""](ExpandDims)]]
```
"
4795,Make //tensorflow/cc:cc_ops compile for Android,"It seems that the bazel target //tensorflow/cc:cc_ops is currently not compilable for Android. From what I saw, this is at least because of the missing platform specific headers (e.g. in tensorflow/core/platform/gif.h).

Making the cc_ops compilable for Android would allow to fully use tensorflow on Android. In particular, I'm trying to build tensorflow for Android with the operations to be able to provide [javacpp-presets](https://github.com/bytedeco/javacpp-presets) which allow an automated generation of complete Java bindings. Therefore they can be used to completely omit the jni/C++ code in Android applications that want to use tensorflow.
"
4794,Configure failed on NFS: Device or resource busy,"On Rocks OS (CentOS 6.5) cluster, filesystem is on NFS.

When run ""./configure"", it failed at ""ERROR: /home/shiyemin/.cache/bazel/_bazel_shiyemin/df39eb3667102dfbd2ad9a81b93b57e9/server/.nfs0000000000cdec5400000995 (Device or resource busy)."". Then the configuration stoped.

I've seen this error, but never like this. I think maybe ""configure"" did not kill all sub-processes in time.

And i found it failed at ""bazel clean --expunge"".
"
4791,libtensorflow.so target not public,"Is there a particular reason that the libtensorflow.so target isn't public? I working on a tensorflow application in C++ and it would be nice to not have to link against tensorflow statically since that's taking ~6min for me.
"
4790,iOS cannot run the new trained Inception v1 model <no registered kernels>,"### Environment info

iOS
### steps:

1, i train inception v1 (slim) on a subset of ImageNet dataset (269 of 1000)
2, convert .ckpt into .pb by freeze_graph
3, convert .pb file into 8bit precision
4, load into iOS and run
### Logs

simple/RunModelViewController.mm:222] Running model failed: Invalid argument: **No OpKernel was registered to support Op 'RandomUniform' with these attrs.  Registered kernels:**

 <**no registered kernels**>

[[Node: InceptionV1/Logits/Dropout_0b/dropout/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, _output_shapes=[[128,1,1,1024]], dtype=DT_FLOAT, seed=0, seed2=0](InceptionV1/Logits/Dropout_0b/dropout/Shape)]]

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""ArgMin"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT32 } } } host_memory_arg: ""dimension""')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""ArgMin"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: ""dimension""')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""ArgMax"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT32 } } } host_memory_arg: ""dimension""')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""ArgMax"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: ""dimension""')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""AvgPoolGrad"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: ""orig_input_shape""')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""BatchNormWithGlobalNormalizationGrad"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_FLOAT } } }')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""BroadcastGradientArgs"" device_type: ""GPU"" host_memory_arg: ""s0"" host_memory_arg: ""s1"" host_memory_arg: ""r0"" host_memory_arg: ""r1""')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""BroadcastGradientArgs"" device_type: ""CPU"" host_memory_arg: ""s0"" host_memory_arg: ""s1"" host_memory_arg: ""r0"" host_memory_arg: ""r1""')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""BiasAddGrad"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT32 } } }')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""BiasAddGrad"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_FLOAT } } }')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""BiasAddV1"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT32 } } }')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""BiasAddV1"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_FLOAT } } }')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""CheckNumerics"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_FLOAT } } }')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""ConcatOffset"" device_type: ""GPU"" host_memory_arg: ""concat_dim"" host_memory_arg: ""shape"" host_memory_arg: ""offset""')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""ConcatOffset"" device_type: ""CPU""')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""Concat"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT32 } } } host_memory_arg: ""concat_dim""')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""Concat"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: ""concat_dim""')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""Concat"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_QUINT8 } } } host_memory_arg: ""concat_dim""')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""Concat"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_QINT8 } } } host_memory_arg: ""concat_dim""')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""Concat"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_QUINT16 } } } host_memory_arg: ""concat_dim""')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""Concat"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_QINT16 } } } host_memory_arg: ""concat_dim""')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""Concat"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_QINT32 } } } host_memory_arg: ""concat_dim""')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""Concat"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_BFLOAT16 } } } host_memory_arg: ""concat_dim""')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""Placeholder"" device_type: ""GPU""')

I tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: ""Placeholder"" device_type: ""CPU""')
"
4785,test,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
### Environment info

Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:
1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
### What other attempted solutions have you tried?
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
"
4784,"Cannot compile with ""-mavx""","I tried to compile tensorflow 0.10 with avx support. The command I used is 
`bazel build -c opt --copt=-mavx //tensorflow/tools/pip_package:build_pip_package`. 
However bazel output errors

```
...
ERROR: /home/oci/yfu/tensorflow_xeonphi/tensorflow/core/kernels/BUILD:1210:1: C++ compilation of rule '//tensorflow/core/kernels:batch_matmul_op' failed: gcc failed: error executing command /opt/rh/devtoolset-4/
root/usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/opt/rh/devtoolset-4/root/usr/bin -B/usr/bin -Wunused-but-set-parameter ... (remaining 105 argument(s) skip
ped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
...
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:222:1: note:   template argument deduction/substitution failed:
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:442:0,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/kernels/batch_matmul_op_impl.h:21,
                 from tensorflow/core/kernels/batch_matmul_op_real.cc:16:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h:648:24: note:   cannot convert '0' (type 'int') to type 'const type& {aka const Eigen::half&}'
     res = pset1<Packet>(0);
                        ^
```

The latest Eigen library seems to have explicit type conversion [here](https://github.com/RLovelett/eigen/blob/master/Eigen/src/Core/ProductEvaluators.h#L660)
I compiled tensorflow with gcc 5.2.1 on CentOS 7.
There is a related Stackoverflow [question](http://stackoverflow.com/questions/38493835/tensorflow-avx-support) but no response on why the compilation is failed. 
"
4782,tf.decode_raw should support tf.float16,"Hi!

It should be trivial to add, I guess.
"
4781,`tf.scan` has Unexpected Behavior on Python CLI/Shell,"### Version Info

'0.10.0rc0'
OS: Ubuntu
### Issue

While this doesn't affect runtime behavior, using the high level function `tf.scan` in the python shell results in unexpected behavior. Running the op whilst functional yields expected results. Running again with bad parameters (wrong dtype) raises exception, which is also expected. Running the original op again raises the same exception, which is not expected.
### To Reproduce

Via python CLI, run a working `tf.scan` function, run one that doesn't work, then run the original.

```
>>> import tensorflow as tf

>>> tf.Session().run(tf.scan(lambda a, x: [a[0] + 1, tf.to_int64(0)], tf.constant([1,2,3,4,5],dtype=tf.int64), initializer=[tf.to_int64(-1), tf.to_int64(0)]))

[array([0, 1, 2, 3, 4]), array([0, 0, 0, 0, 0])]

>>> tf.Session().run(tf.scan(lambda a, x: [1, tf.to_int64(0)], tf.constant([1,2,3,4,5],dtype=tf.int64), initializer=[tf.to_int64(-1), tf.to_int64(0)]))
E tensorflow/core/client/tensor_c_api.cc:485] Input 1 of node scan_1/while/Merge_1 was passed int32 from scan_1/while/NextIteration_1:0 incompatible with expected int64.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 382, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 655, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 723, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 743, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.InvalidArgumentError: Input 1 of node scan_1/while/Merge_1 was passed int32 from scan_1/while/NextIteration_1:0 incompatible with expected int64.

>>> tf.Session().run(tf.scan(lambda a, x: [a[0] + 1, tf.to_int64(0)], tf.constant([1,2,3,4,5],dtype=tf.int64), initializer=[tf.to_int64(-1), tf.to_int64(0)]))
E tensorflow/core/client/tensor_c_api.cc:485] Input 1 of node scan_1/while/Merge_1 was passed int32 from scan_1/while/NextIteration_1:0 incompatible with expected int64.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 382, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 655, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 723, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 743, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.InvalidArgumentError: Input 1 of node scan_1/while/Merge_1 was passed int32 from scan_1/while/NextIteration_1:0 incompatible with expected int64.
```
"
4779,Accessing data for post-processing on the GPU without transfer to host,"In TensorFlow's C++ API, after the session Run has been completed, is it possible to access the data while it's still on the GPU?
I would like to do post-processing on the GPU with the output of TensorFlow. 

Currently the output is sent back to the host and I will have to move it back to the GPU for post-processing. This data transfer is something that I am looking to avoid. 
"
4775,"Compile tensorflow with compile option ""-xMIC-AVX512"" does not work","Hi, All

I want to use AVX512 with tensorflow because vectorization can increase performance.
In addition, building any program with icpc(Intel compiler) and compile option ""-xMIC-AVX512"" can vectorize automatically in state-of-art Intel machine.
So, I use the command as below:

`CC=icpc bazel build -c opt --copt=-xMIC-AVX512 //tensorflow/tools/pip_package:build_pip_package`

But when I type the command, build does not work.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
- Nothing exist related my issue in GitHub or StackOverflow
### Environment info
- Operating System: Centos 7.2.1511 
- Tensorflow version: r0.10.0 source code version
- Only use CPU
### What other attempted solutions have you tried?

As far as I know, linear algebra library for Tensorflow is ""Eigen library"" and SIMD vectorizations(like SSE, AVX, etc.) are applied to the Eigen library.
So, I changed Eigen source code to apply the AVX512 and gcc compile option ""-mavx512f"" referring two links([eigen bug report](http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1306), [Benoit Steiner's Bitbucket](https://bitbucket.org/benoitsteiner/eigen-avx512)), but I didn't apply AVX512 because there was little information about that.

If someone know the method about applying AVX512 or the date to release Eigen version with AVX512,  please tell me about that.

Thank you very much.
"
4774,weights_initializer in tf.contrib.layers.fully_connected,"Hi, 

At present contrib.layers.fully_connected is unable to accept a Tensor as an initializer for the weights.

This because when calling variable_scope.get_variable (which accepts a Tensor as an initializer) it specifies both the initializer and the shape. Is this expected behavior or a bug? 

I have put together a small [gist ](https://gist.github.com/marcoadurno/60faf136193d8eccc83d14932d188d25) that illustrates the issue.

At present what's the recommended way to pass a Tensor as an initializer? Would a [dummy init_fn](https://gist.github.com/marcoadurno/60faf136193d8eccc83d14932d188d25#file-layers_weights_test-py-L34) be acceptable?

I'm currently running rev: 9c11fe2f1db1ccf5bcdb0724c18cc462ff5fbbd7.

Thanks,

Marco
"
4773,"ValueError: Shape (?, 4096) must have rank 1","Operating System: _CentOS 6 - box for vagrant_

Installed version of CUDA and cuDNN: 
ls -l /path/to/cuda/lib/libcud*: ""No such file or directory""
_No CUDA installed._

Installation:
1.  Installed with Anaconda 4.0.9;
2. `python -c ""import tensorflow; print(tensorflow.__version__)""`: _0.10.0rc0_;
3. Python 3.5, numpy 1.11.2, scipy 0.18.1;
### Reproduce

It is standard VGG model with 16 layers. I tried extract vgg.fc2 layer:
http://pastebin.com/A1qd7g2e

I also get this error after `session.run` was called.
### What other attempted solutions have you tried?

I'm a new user of tenserflow and don't know any attempted solutions. I found this issue - https://github.com/tensorflow/tensorflow/issues/3815, but it is closed.
### Logs or other output that would be helpful

Stacktrace - http://pastebin.com/eaQi1T0R
"
4772,tensorflow/models/compression does not work,"To run the compression model:

`python encoder.py --input_image=/your/image/here.png \ --output_codes=output_codes.pkl --iteration=15 --model=residual_gru.pb`

However, this error shows up:

> [libprotobuf ERROR google/protobuf/src/google/protobuf/io/coded_stream.cc:207] A protocol message was rejected because it was too big (more than 67108864 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.

I see from v0.8.0 RC0 that it has been fixed:

> Added instructions and binaries for ProtoBuf library with fast serialization and without 64MB limit

Could this error have been reintroduced? I'm using 0.10.0.
"
4770,bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu Failed With Error:could not insert 'nvidia_340_uvm': Invalid argument,"I am installing tensorflow from source code since my GTX1080 need CUDA8.0. My installing steps are as following:
1. pre-installing: Ubuntu14.04 os, Nvidia driver of version numbered 367.48, CUDA8.0 by cuda_8.0.44_linux.run, cuDNN5.1, bazel and jdk8.
2. download tensorflow source from github, type yes for GPU support during running ./configure and these operations are successful .
3. But errors stop me to continue tensorflow installing: 

```
zyl@zyl-PC$ bazel build -c opt --config=cuda  //tensorflow/cc:tutorials_example_trainer

- [ ] INFO: From Compiling tensorflow/core/kernels/tile_ops_gpu.cu.cc:
nvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.
nvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.
Target //tensorflow/cc:tutorials_example_trainer up-to-date:bazel-bin/tensorflow/cc/tutorials_example_trainer

- [ ] INFO: Elapsed time: 1233.946s, Critical Path: 1168.09s
zyl@zyl-PC$ bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally

- [ ] modprobe: ERROR: could not insert 'nvidia_340_uvm': Invalid argument
E tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_UNKNOWN
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:153] retrieving CUDA diagnostic information for host: zyl-PC
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: zyl-PC
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: 367.48.0
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:356] driver version 
file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.48  Sat Sep  3 18:21:08 PDT 2016 
GCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04.3)""""""    
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 367.48.0
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:293] kernel version seems to match DSO: 367.48.0

- [ ] F tensorflow/cc/tutorials/example_trainer.cc:129] Check failed:
::tensorflow::Status::OK() == (session->Run({{""x"", x}}, 
{""y:0"", ""y_normalized:0""}, {}, &outputs)) 
(OK vs. Invalid argument: Cannot assign a device to node 'Cast': Could not satisfy explicit device specification '/gpu:0' 
because no devices matching that specification are registered in this process; 
available devices: /job:localhost/replica:0/task:0/cpu:0 

- [ ] [[Node:Cast = Cast[DstT=DT_FLOAT, SrcT=DT_INT32, _device=""/gpu:0""](Const)]])
Aborted (core dumped)""
```

---

the tensorflow is failed to install from source code at the step of ""$ bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu"" as discribed above. Obviously, both gpu and tensorflow would not be available.
Anyone who sucessed in installing tensorflow and cuda8.0 can help me to fix it?
"
4769,Tensorflow running problem with gpu_device.cc:170,"I am having  a problem when running a tutorial example of tensorflow. It is an example for deep neural network with 2 hidden layers. Following is the output of my error file. 

I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 

name: Tesla K40t
major: 3 minor: 5 
memoryClockRate (GHz) 0.8755
pciBusID 0000:82:00.0
Total memory: 11.25GiB
Free memory: 11.09GiB

I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40t, pci bus id: 0000:82:00.0)
F tensorflow/core/common_runtime/gpu/gpu_device.cc:170] Check failed: err == cudaSuccess (71 vs. 0)
/var/spool/PBS/mom_priv/jobs/2004520.wlm01.SC: line 10: 18679 Aborted

I am using CentOS 6.6, python 2.7.11

I can run the example smoothly when using the cpu version of tensorflow. Does anyone have idea and solution of it? Thanks.               
"
4768,kernel version 352.63.0 does not match DSO version 361.93.2,"Hi, 

I've installed TensorFlow with CUDA and CuDNN. 

Why I run a session I've the following error:

```
>>> sess = tf.Session()
E tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_INVALID_VALUE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:153] retrieving CUDA diagnostic information for host: next-gpu1
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: next-gpu1
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: 361.93.2
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:347] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.63  Sat Nov  7 21:25:42 PST 2015
GCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04.1)
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 352.63.0
E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:296] kernel version 352.63.0 does not match DSO version 361.93.2 -- cannot find working devices in this configuration
I tensorflow/core/common_runtime/gpu/gpu_init.cc:81] No GPU devices available on machine.
```
"
4767,Problem initializing DT_DOUBLE variables in distributed TF,"If you save code below as `init_bug.py` and run as `python init_bug.py`, it crashes with error below when running `init_op`. It works fine using local session or when changing `dtype` to `np.float32`. Also fails for `np.int32` type. Tried on 0.11rc0 on MacOS

`tensorflow.python.framework.errors.InternalError: Output 0 of type float_ref does not match declared output type double_ref for node Variable = Variable[container="""", dtype=DT_DOUBLE, shape=[], shared_name="""", _device=""/job:worker/replica:0/task:0/cpu:0""]()
`

```
import subprocess, sys
import tensorflow as tf
import numpy as np

worker_ip=""127.0.0.1:12222""
cluster = {""worker"": [worker_ip]}
clusterspec = tf.train.ClusterSpec(cluster).as_cluster_def()

def launch_worker():
  def runcmd(cmd): subprocess.Popen(cmd, shell=True, stderr=subprocess.STDOUT)
  runcmd(""python init_bug.py worker"")

if __name__=='__main__':
  if len(sys.argv)<2:
    dtype=np.float64
    global_param_var = tf.Variable(np.array(1).astype(dtype), dtype=dtype)
    init_op = tf.initialize_all_variables()
    launch_worker()
    sess = tf.Session(""grpc://""+worker_ip)
    sess.run(init_op)

  else:
    print(""Launching worker"")
    server = tf.train.Server(clusterspec, job_name=""worker"")
    server.join()
```
"
4766,freeze graph for inception v1 fine tuned .ckpt model file fail. Attempting to use uninitialized value,"I try to fine tune inception v1 model on my own dataset, and i choose optimizer: rmsprop

the cmd is as below:

---

python train_image_classifier.py \
  --train_dir=/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/v1_finetune/ \
  --dataset_name=animals \
  --dataset_split_name=train \
  --dataset_dir=/home/scopeserver/RaidDisk/DeepLearning/mwang/data/datadic_train/ \
  --model_name=inception_v1 \
  **--checkpoint_path=/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/inceptionv1/inception_v1.ckpt \**
  --checkpoint_exclude_scopes=InceptionV1/Logits \
  --trainable_scopes=InceptionV1/Logits \
  --max_number_of_steps=60435 \
  --batch_size=128 \
  --learning_rate=0.001 \
  --save_interval_secs=3600 \
  --save_summaries_secs=3600 \
  --log_every_n_steps=200 \
  **--optimizer=rmsprop \**
  --weight_decay=0.00004

---

once i get .ckpt file, i try to freeze the graph into a .pb file. 

bazel-bin/tensorflow/python/tools/freeze_graph \
 --input_graph=/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/v1_finetune/graph.pbtxt \
 **--input_checkpoint=/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/v1_findtune/model.ckpt-23293 \**
 --output_graph=/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/v1_findtune/freeze.pb 
 --output_node_names=InceptionV1/Logits/Predictions/Softmax

---

however, i get many issues, they are all: 

**Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1**

---

W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: **Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1**
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: **Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1**
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
W tensorflow/core/framework/op_kernel.cc:968] Failed precondition: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
Traceback (most recent call last):
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 134, in <module>
    tf.app.run()
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 131, in main
    FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 120, in freeze_graph
    sess, input_graph_def, output_node_names.split("",""))
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/graph_util.py"", line 226, in convert_variables_to_constants
    returned_variables = sess.run(variable_names)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 717, in run
    run_metadata_ptr)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 915, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 965, in _do_run
    target_list, options, run_metadata)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 985, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.FailedPreconditionError: Attempting to use uninitialized value InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1
     [[Node: _send_InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=2995273176636161830, tensor_name=""InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV1/Logits/Conv2d_0c_1x1/biases/RMSProp_1)]]
"
4763,Ability to Save Checkpoints Every N Steps using Estimator,"TensorFlow Version: r0.10.0rc0

For TensorFlow/SKFlow/tf,learn Estimators, there is no option for specifying when an Estimator saves checkpoints other than `save_checkpoints_secs` in a `RunConfig`, which isn't usable for a monitor such as `ValidationMonitor`. For example, if I want to do early stopping and check every 10 steps, it is definitely not guaranteed that new checkpoints were saved for the model, even if I specify `save_checkpoints_secs` to be 1 second.
### Tried:

It is currently not possible to use a `CheckpointMonitor` with an `Estimator` (e.g. `LinearClassifier`) as far as I can tell. There is no saver nor Scaffold I can specify that captures the same variables it does normally.

When can we expect this capability, and are there workarounds currently?
"
4762,Request for documentation: Loop implementation,"I am trying to understand the implementation of `tf.while_loop` and everything that is built on top of it, because I am implementing a custom `tf.Graph` subclass, and finding that the way `tf.while_loop` is handled during gradient computation is important for what I am doing. 

However, I cannot find any documentation on the ops that comprise `tf.while_loop` â€“ is there any internal documentation on this implementation?

I am finding myself confused about some of the following concepts:
- `WhileContext`, and, in general, the stack of contexts
- Flows vs tensors
- Frames

So far I've gotten quite a bit by just reading the source code, but it's pretty hard to build yourself a good mental model by going completely bottom up without having any high level picture of how the entire thing is organized.

If there is no intention to document these things (which would be completely understandable) please feel free to close this issue (although I would definitely appreciate some help or pointers as to where I can figure out the high-level overview of looping implementation).
"
4760,Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms),"I've the following error. I'm using a conda installation of tensorflow. I'm struggling to try to use it with my GPU.

```
Loaded runtime CuDNN library: 5005 (compatibility version 5000) but source was compiled with 5103 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.
F tensorflow/core/kernels/conv_ops.cc:526] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)
Aborted (core dumped)
```

which nvcc returns
`/usr/local/cuda-7.5/bin/nvcc`

nvcc version returns
`Cuda compilation tools, release 7.5, V7.5.17`

I tried downloading CuDNN v5.1 and did the following but it didn't work either

```
sudo cp lib* /usr/local/cuda-7.5/lib64/
sudo cp include/cudnn.h /usr/local/cuda-7.5/include/
sudo ldconfig

```

I tried on the other folder too

```
sudo cp lib* /usr/local/cuda/lib64/
sudo cp include/cudnn.h /usr/local/cuda/include/
sudo ldconfig
```
"
4759,AttributeError: 'module' object has no attribute 'NodeDef',"I tried to run the distributed Tensorflow for inception-v3 example, but I have the following errors in worker nodes:

```
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job ps -> {0 -> 10.149.0.1:2222}
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222, 1 -> 10.149.0.3:2222}
I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:206] Started server with target: grpc://localhost:2222
Traceback (most recent call last):
  File ""/cm/shared/scratch/rengan/DL/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/imagenet_distributed_train.py"", line 65, in <module>
    tf.app.run()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""/cm/shared/scratch/rengan/DL/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/imagenet_distributed_train.py"", line 61, in main
    inception_distributed_train.train(server.target, dataset, cluster_spec)
  File ""/cm/shared/scratch/rengan/DL/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/inception_distributed_train.py"", line 120, in train
    global_step = slim.variables.global_step()
  File ""/cm/shared/scratch/rengan/DL/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/slim/scopes.py"", line 155, in func_with_args
    return func(*args, **current_args)
  File ""/cm/shared/scratch/rengan/DL/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/slim/variables.py"", line 242, in global_step
    with tf.device(variable_device(device, 'global_step')):
  File ""/cm/shared/scratch/rengan/DL/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/slim/variables.py"", line 214, in variable_device
    var_def = graph_pb2.NodeDef(name=var_name, op='Variable')
AttributeError: 'module' object has no attribute 'NodeDef'
```

Based on the information, the line 214 is 
var_def = graph_pb2.NodeDef(name=var_name, op='Variable')

And at the beginning of variables.py, it has the line
from tensorflow.core.framework import graph_pb2

But I could not find any graph_pb2 from tensorflow/tensorflow/core/framework. Any idea why I got this error? 
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

No other related posts were found.

Environment info

Operating System: Redhat 7.2

Installed version of CUDA and cuDNN: 
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
-rw-r--r-- 1 root root 560184 May 8 02:00 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root 16 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root 19 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 root root 394472 May 8 02:00 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 root root 737516 May 8 02:00 /usr/local/cuda-8.0/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so
-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.3
-rw-r--r-- 1 root root 59715990 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn_static.a

If installed from source, provide 
1. The commit hash (git rev-parse HEAD): a63b0cbcabc79531e155a0663a08656debf2fe07
2. The output of bazel version:
.
Build label: 0.3.1- (@non-git)
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Sep 29 22:19:27 2016 (1475187567)
Build timestamp: 1475187567
Build timestamp as int: 1475187567
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
"
4758,AttributeError: type object 'NewBase' has no attribute 'is_abstract',"I uninstalled anaconda and reinstalled tensorflow using the pip installation. 

Originally it was having trouble updating numpy, but after I fixed that now I'm getting this error:

```
import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Python/2.7/site-packages/tensorflow/__init__.py"", line 23, in <module>
    from tensorflow.python import *
  File ""/Library/Python/2.7/site-packages/tensorflow/python/__init__.py"", line 98, in <module>
    from tensorflow.python.platform import test
  File ""/Library/Python/2.7/site-packages/tensorflow/python/platform/test.py"", line 63, in <module>
    from tensorflow.python.framework import test_util
  File ""/Library/Python/2.7/site-packages/tensorflow/python/framework/test_util.py"", line 43, in <module>
    from tensorflow.python.platform import googletest
  File ""/Library/Python/2.7/site-packages/tensorflow/python/platform/googletest.py"", line 32, in <module>
    from tensorflow.python.platform import benchmark  # pylint: disable=unused-import
  File ""/Library/Python/2.7/site-packages/tensorflow/python/platform/benchmark.py"", line 122, in <module>
    class Benchmark(six.with_metaclass(_BenchmarkRegistrar, object)):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six.py"", line 566, in with_metaclass
    return meta(""NewBase"", bases, {})
  File ""/Library/Python/2.7/site-packages/tensorflow/python/platform/benchmark.py"", line 117, in __new__
    if not newclass.is_abstract():
AttributeError: type object 'NewBase' has no attribute 'is_abstract'
```

Could this be because uninstalling anaconda left some residual effects. I noticed my $PATH still has anaconda prepended to it
`$PATH -bash: /Users/usr/anaconda/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/usr/local/git/bin:/usr/texbin: No such file or directory`
"
4757,import meta graph bug reading model with scope name changed,"Same issue for tf 0.10 and 0.11.rc0.
The problem is if you read one model, change the top scope name, write a new model, then you read the new model, but still you got the old top scope name.
How to reproduce 
- create model
  python create-model.py
  
  ```
  import tensorflow as tf  
  
  sess = tf.InteractiveSession()  
  
  with tf.variable_scope('old'):  
  
      w = tf.get_variable('w', shape=[1], initializer=tf.constant_initializer(1.0))  
  
  sess.run(tf.initialize_all_variables())  
  
  tf.train.Saver().save(sess, '/tmp/old.model')  
  ```
- read this model (using import meta graph), then rename top scope and save to one new model
  python rename-scope.py
  old_vars: [u'old/w:0']
  new_vars: [u'new/w']  
  
  ```
  import tensorflow as tf  
  
  sess = tf.InteractiveSession()  
  
  saver = tf.train.import_meta_graph('/tmp/old.model.meta')  
  
  saver.restore(sess, '/tmp/old.model')  
  
  src_vars = [v for v in tf.all_variables() if v.name.startswith('old')]  
  
  print('old_vars:', [v.name for v in src_vars])
  
  out_vars = {v.name[:v.name.rfind(':')].replace('old', 'new', 1): v for v in src_vars}  
  
  print('new_vars:', [key for key in out_vars])  
  
  tf.train.Saver(var_list=out_vars).save(sess, '/tmp/new.model') 
  ```
- read new model(It is ok if you buid graph from scratch with new scope, but not ok if using import meta graph again)
  # -------------------this is ok
  
  python read-renamed-buildgraph.py  
  tf.all_variables: [u'new/w:0']
  w val: [ 1.]
  
  ```
  import tensorflow as tf  
  
  sess = tf.InteractiveSession()
  
  with tf.variable_scope('new'):
      w = tf.get_variable('w', shape=[1], initializer=tf.constant_initializer(2.0))  
  
  tf.train.Saver().restore(sess, '/tmp/new.model')  
  
  print('tf.all_variables:', [v.name for v in tf.all_variables()])  
  
  print('w val:',  w.eval())
  ```
  # ---------------this is wrong
  
  python read-renamed-metagraph.py
  tf.all_variables: [u'old/w:0']
  
  ```
   import tensorflow as tf  
  
   sess = tf.InteractiveSession()  
  
   saver = tf.train.import_meta_graph('/tmp/new.model.meta')
  
   saver.restore(sess, '/tmp/new.model')
  
   print('tf.all_variables:', [v.name for v in tf.all_variables()])
  ```
"
4754,Nan values after applying gradients,"Not sure it is a tf bug, I have also posted to stackoverflow so if consider not proper to be here close this. I find one similar question on stackoverlfow but no reply yet.
[nan-in-summary-histogram](http://stackoverflow.com/questions/39854390/nan-in-summary-histogram)
[why-am-i-getting-invalid-argument-nan-in-summary-histogram-for-histogramsummar](http://stackoverflow.com/questions/39828550/why-am-i-getting-invalid-argument-nan-in-summary-histogram-for-histogramsummar)
My program will face this some times(not every run will face this..), then if face this I can always reproduce this error loading from the last model I have saved before program crash due to nan. When rerun from this model, first train process seems fine using the model to generate loss(I have printed loss and shows no problem), but after applying gradients, the values of embedding variables will turn to Nan. So Nan in embedding will casue histogram collecting info crash.  If not use histogram, program will not crash(assertion fail) but since Nan exists the model is corrupted.

So what is the root cause of the nan problem? Confused as not know how to debug further and this program with same data and params will mostly run ok and only face this problem during some run..

Loading existing model from: /home/gezi/temp/image-caption//model.flickr.rnn2.nan/model.ckpt-18000 Train from restored model: /home/gezi/temp/image-caption//model.flickr.rnn2.nan/model.ckpt-18000 I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 5235 get requests, put_count=4729 evicted_count=1000 eviction_rate=0.211461 and unsatisfied allocation rate=0.306781 I tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 100 to 110 2016-10-04 21:45:39 epoch:1.87 train_step:18001 duration:0.947 elapsed:0.947 train_avg_metrics:['loss:0.527'] ['loss:0.527'] 2016-10-04 21:45:39 epoch:1.87 eval_step: 18001 duration:0.001 elapsed:0.948 ratio:0.001 W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: rnn/HistogramSummary_1 [[Node: rnn/HistogramSummary_1 = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](rnn/HistogramSummary_1/tag, rnn/image_text_sim/image_mlp/w_h/read/_309)]] W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: rnn/HistogramSummary_1 [[Node: rnn/HistogramSummary_1 = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](rnn/HistogramSummary_1/tag, rnn/image_text_sim/image_mlp/w_h/read/_309)]] W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: rnn/HistogramSummary_1 [[Node: rnn/HistogramSummary_1 = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](rnn/HistogramSummary_1/tag, rnn/image_text_sim/image_mlp/w_h/read/_309)]] W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: rnn/HistogramSummary_1 [[Node: rnn/HistogramSummary_1 = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](rnn/HistogramSummary_1/tag, rnn/image_text_sim/image_mlp/w_h/read/_309)]] W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: rnn/HistogramSummary_1 [[Node: rnn/HistogramSummary_1 = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](rnn/HistogramSummary_1/tag, rnn/image_text_sim/image_mlp/w_h/read/_309)]] Traceback (most recent call last): File ""./train.py"", line 308, in tf.app.run()
"
4753,tf.gfile.Glob blocks forever,"### minimal reproducible example

``` python
import tensorflow as tf
tf.gfile.Glob('/path/to/imagenet/validation-*')
```
### environment info

Operating System: Ubuntu 14.04 
TensorFlow version (installed using the official binary): 0.11.0rc0
Python version: 3.4.3
"
4750,"Error loading from meta graph, KeyError: u'RsqrtGrad'","On centos 6.3, I have tested both on tf 0.10 and 0.11rc0, all facing this problem.
Loading meta graph from saved model meta graph file.

 File ""/home/gezi/mine/tensorflow-exp/util/melt/inference/predictor.py"", line 61, in restore
    saver = tf.train.import_meta_graph(meta_graph)
  File ""/home/img/common/tensorflow.gpu.bigdata/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1458, in import_meta_graph
    return _import_meta_graph_def(read_meta_graph_file(meta_graph_or_file))
  File ""/home/img/common/tensorflow.gpu.bigdata/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1348, in _import_meta_graph_def
    producer_op_list=producer_op_list)
  File ""/home/img/common/tensorflow.gpu.bigdata/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 252, in import_graph_def
    op_def = op_dict[node.op]
KeyError: u'RsqrtGrad'
"
4749,Minor instruction issue in functional_ops.py/scan,"I had been using functional_ops.py/scan for implementing RNNs.

I've just checked the instruction, and think this is little bit wrong. 

In master branch, from line 419 in functional_ops.py, it says

```
  Args:
    fn: The callable to be performed.  It accepts two arguments.  The first
      will have the same (possibly nested) structure as `elems`.  The second
      will have the same structure as `initializer` if one is provided,
      otherwise it will have the same structure as `elems`.  Its output
      must have the same structure as `initializer` if one is provided,
      otherwise it must have the same structure as `elems`.
```

.

But actually the first argument of _fn_ should have the same structure as _initializer_, 

and the second should have the same structure as _elems_.

As we can see this in few examples just below, from line 448,

``````
  Examples:
    ```python
    elems = np.array([1, 2, 3, 4, 5, 6])
    sum = scan(lambda a, x: a + x, elems)
    # sum == [1, 3, 6, 10, 15, 21]
    ```
    ```python
    elems = np.array([1, 2, 3, 4, 5, 6])
    initializer = np.array(0)
    sum_one = scan(
        lambda a, x: x[0] - x[1] + a, (elems + 1, elems), initializer)
    # sum_one == [1, 2, 3, 4, 5, 6]
    ```
    ```python
    elems = np.array([1, 0, 0, 0, 0, 0])
    initializer = (np.array(0), np.array(1))
    fibonaccis = scan(lambda a, _: (a[1], a[0] + a[1]), elems, initializer)
    # fibonaccis == ([1, 1, 2, 3, 5, 8], [1, 2, 3, 5, 8, 13])
    ```
``````

, the instruction is slightly wrong.
"
4748,Configure fail when install from source code,"I Installed with following settings: centOS 7, CUDA 7.5, cudnn v4, python 2.7

After I filling in the configurations using

`./configure`
and I got this error

```
ERROR: /home/[...]/external/bazel_tools/src/tools/android/java/com/google/devtools/build/android/dexer/BUILD:3:1: no such target '//external:android/dx_jar_import': target 'android/dx_jar_import' not declared in package 'external' defined by /home/[...]/tensorflow/WORKSPACE and referenced by '@bazel_tools//src/tools/android/java/com/google/devtools/build/android/dexer:dexer'.
ERROR: Evaluation of query ""deps((//... union @bazel_tools//tools/jdk:toolchain))"" failed: errors were encountered while computing transitive closure.
Configuration finished
```

When I tried with v0.9 there is no such problem. And I don't need those android features.

Anybody encountered the same problem? How can I fix this issue?
"
4746,ksize in max_pool,"I am trying to build a convolution (followed by max_pool) for variable length input size. Since the length of the input in a batch should be the same, I set the batch size to 1. However, for some reason ksize in tf.nn.max_pool is a list attribute required at run-time. This would prevent any neural network that uses variable input size to apply max pool. Is there a workaround for this? Should ksize be an input tensor otherwise?
"
4744,Poor performance with multi-GPU,"I tried to train the example inception-v3 network on multiple GPUs. The following commands are used:
/\* use 4 GPUs */
bazel-bin/inception/imagenet_train --num_gpus=4 --batch_size=64 --max_steps=2000 --train_dir=... --data_dir=...

/*use 2 GPUs */
export CUDA_VISIBLE_DEVICES=0,1
bazel-bin/inception/imagenet_train --num_gpus=2 --batch_size=32 --max_steps=2000 --train_dir=... --data_dir=...

/*use 1 GPU */
export CUDA_VISIBLE_DEVICES=0
bazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=16 --max_steps=2000 --train_dir=... --data_dir=...

The performance results are as follows:
GPUs   Training time(s)       samples/sec
1           657                          52.7
2           844                          97.3
4           1104                        150

The training time was got from the log file by subtracting the first time from the last time and therefore excluded the start-up time. For example
2016-10-03 15:28:23.239148: step 0, loss = 13.08 (4.6 examples/sec; 13.916 sec/batch)
....
2016-10-03 15:46:47.830959: step 1990, loss = 12.21 (174.6 examples/sec; 0.367 sec/batch)

Now the part that I do not understand is that the samples/sec scales normally with increasing number of GPUs, but the training time does not reduce. Instead the training also increases with more GPUs. 

Has anyone observed the similar case? 
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

I tried to set --input_queue_memory_factor=0 as in the post https://github.com/tensorflow/models/issues/47, but it does not help.

Also the post https://github.com/tensorflow/tensorflow/issues/4272 has similar performance issue, but it was not solved.
### Environment info

Operating System: Redhat 7.2

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
-rw-r--r-- 1 root root   560184 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 root root   394472 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 root root   737516 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so
-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.3
-rw-r--r-- 1 root root 59715990 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn_static.a

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`): a63b0cbcabc79531e155a0663a08656debf2fe07
2. The output of `bazel version`: 

.
Build label: 0.3.1- (@non-git)
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Sep 29 22:19:27 2016 (1475187567)
Build timestamp: 1475187567
Build timestamp as int: 1475187567
### Logs or other output that would be helpful

By the way, before the actual computation, the overhead of the initialization took >7minutes, is that normal? And I also got many warnings:
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)
WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.

Will all these warnings impact the performance?
"
4742,Atrous convolution does not preserve tensor shape,"For an input with an undefined batch size, `atrous_conv2d` emits tensors where all except the final dimension are undefined:

``` python
input = tf.placeholder(tf.float32, (None, 256, 256, 3))

conv = tf.nn.conv2d(input, tf.zeros((3, 3, 3, 16)), strides=[1, 1, 1, 1], padding='SAME')
print(conv.get_shape()) # Correctly displays (?, 256, 256, 16)

dilated = tf.nn.atrous_conv2d(input, tf.zeros((3, 3, 3, 16)), rate=2, padding='SAME')
print(dilated.get_shape()) # Displays (?, ?, ?, 16)
```

(For concrete batch sizes, everything works as expected.)

Tested on `0.10.0rc0`
"
4740,fully_connected_preloaded_var.py unexpectedly slow,"From http://stackoverflow.com/questions/39840323/benchmark-of-howto-reading-data -- `examples/tutorials/mnist/fully_connected_feed.py` runs roughly 10x faster per step (`feed_dict`) than `examples/tutorials/mnist/fully_connected_preloaded_var.py` (TF variable with `slice_input_producer`). Looking at Timeline (attached), it seems the training is bottlenecked on QueueDequeueMany which takes 13ms
which is about 100x slower than corresponding segment on `feed_dict` example [timeline](%28https://github.com/tensorflow/tensorflow/files/507483/timeline.feed.json.zip%29). Also, the timing taken by `QueueDequeueMany` scales linearly with batch-size. IE, increasing batch-size to 200 makes it take 22ms.

[timeline.var.json.zip](https://github.com/tensorflow/tensorflow/files/507476/timeline.var.json.zip)
"
4739,Feature request: Implement QR decomposition,"Feature request:  A tensorflow op that performs numerically stable (works on non full rank matrices) QR decomposition along with gradients so that it can be used in layers and cost functions.
"
4738,Feature Request: Include function argument defaults in the documentation,"I have recently started using functions from tf.contrib. They speed things up a lot. Thank you. 

I did notice that some of the prototypes in the documents have their arguments replaced with `(*args, **kwargs)`. See tf.contrib.layers: https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.layers.html#layers-contrib. Perhaps this is automatically done if the number of arguments exceeds some number. Unfortunately the prototype is the only place where the argument defaults are shown. I have been looking at the source file, layers.py, to see the full prototype for the defaults which is fine, but probably not idea. 

My request is to include the argument defaults somewhere in the documentation. I wouldn't mind long prototypes, i.e. getting rid of the `(*args, **kwargs)`. Or if people want the shorter prototypes, maybe the argument defaults could be automatically included somewhere in the description.
"
4737,No gradient defined for operation EluGrad,"tf.nn.elu does not support second derivatives currently. 

This would be awesome, as in variational autoencoder models these activation functions improve performance by a few nats in the objective compared to tanh, sigmoid (which do support second derivatives).

In the meantime I'm using `elu = lambda x: tf.select(x < 0., tf.exp(x) - 1., x)` which seems to work:

```
In [29]: elu = lambda x: tf.select(x < 0., tf.exp(x) - 1., x)

In [30]: sess.run(tf.gradients(elu(z), z), {z: -1.})
Out[30]: [0.36787945]

In [31]: sess.run(tf.gradients(elu(z), z), {z: 1.})
Out[31]: [1.0]
```
"
4736,"Build Failing Compiling with CUDA 8.0, CUDNN 5.1.5, Bazel 0.3.1","Building Tensorflow with Bazel 0.3.1 + CUDA 8.0 + CUDNN 5.1.5 and latest GIT is failing

`# bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures`

Here is the end of the error.

```
external/local_config_cuda/cuda/include/math_functions.h(8897): error: cannot overload functions distinguished by return type alone

external/local_config_cuda/cuda/include/math_functions.h(8901): error: cannot overload functions distinguished by return type alone

./tensorflow/core/framework/allocator.h(154): warning: missing return statement at end of non-void function ""tensorflow::Allocator::RequestedS
ize""

2 errors detected in the compilation of ""/tmp/tmpxft_000027b0_00000000-7_spacetobatch_functor_gpu.cu.cpp1.ii"".
ERROR: /root/src/tensorflow/tensorflow/core/kernels/BUILD:1673:1: output 'tensorflow/core/kernels/_objs/batch_space_ops_gpu/tensorflow/core/ke
rnels/spacetobatch_functor_gpu.cu.pic.o' was not created.
ERROR: /root/src/tensorflow/tensorflow/core/kernels/BUILD:1673:1: not all outputs were created.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
### Environment info

**Graphics card capability 3.0**

`# lspci -v | grep -i nvidia`

```
01:00.0 VGA compatible controller: NVIDIA Corporation GK104 [GeForce GTX 760] (rev a1) (prog-if 00 [VGA controller])
        Kernel driver in use: nvidia
        Kernel modules: nouveau, nvidia_drm, nvidia
01:00.1 Audio device: NVIDIA Corporation GK104 HDMI Audio Controller (rev a1)
```

`# git rev-parse HEAD`
`3d35376a66cde4f3e614c746d3c8708d15caa1b5`

`# bazel version`

```
Build label: 0.3.1- (@non-git)
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Sun Oct 2 01:11:55 2016 (1475370715)
Build timestamp: 1475370715
Build timestamp as int: 1475370715
```

**CUDNN 5.1.5**

```
ls -l /opt/cuda/lib64 | grep cudnn
lrwxrwxrwx 1 root root        13 Oct  1 20:55 libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root        17 Oct  1 20:55 libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxr-xr-x 1 root root  79337624 Oct  1 20:55 libcudnn.so.5.1.5
-rw-r--r-- 1 root root  69756172 Oct  1 20:55 libcudnn_static.a`
```

**CUDA 8.0.44**

```
# ls -l /opt/cuda/lib64 | grep libcuda
-rw-r--r-- 1 root root    558720 Sep 28 17:41 libcudadevrt.a
lrwxrwxrwx 1 root root        16 Sep 28 17:41 libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root        19 Sep 28 17:41 libcudart.so.8.0 -> libcudart.so.8.0.44
-rwxr-xr-x 1 root root    415432 Sep 28 17:41 libcudart.so.8.0.44
-rw-r--r-- 1 root root    775162 Sep 28 17:41 libcudart_static.a
```

**GCC 6.2.1**

```
# gcc --version
gcc (GCC) 6.2.1 20160830
Copyright (C) 2016 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```
### Other Issues Experienced

First compile error was described here, i used @junyer's proposed solution
https://github.com/google/re2/issues/102
"
4735,Resource exhausted error in the middle of training,"I train the inception v1 (slim) model on my own data set. 269 classes total.
The max training step is 60435 and batch size is 256 as below
**--max_number_of_steps=60435
--batch_size=256**
The model runs under GPU mode and I have 4 Tian X GPUs, with each has 12G GPU memory.
The Resource exhausted error happen after at least 46831 trainning steps, since i can see the last check point file is model.ckpt-46831.

I do not know why the issue happen in the middle, but not at very beginning of the training process.

The error log report by Tensor Fow is as below:

//other lines above.
I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 11.16GiB
I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats: 
**Limit:                 12049707828
InUse:                 11984328960
MaxInUse:              12038083584
NumAllocs:               248036306
MaxAllocSize:           2831155200**

W tensorflow/core/common_runtime/bfc_allocator.cc:274] *************************************************************************************************_xx
W tensorflow/core/common_runtime/bfc_allocator.cc:275] *_Ran out of memory trying to allocate 39.81MiB.**  See logs for memory state.
W tensorflow/core/framework/op_kernel.cc:968] Resource exhausted: OOM when allocating tensor with shape[256,832,7,7]
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors.ResourceExhaustedError'>, OOM when allocating tensor with shape[256,112,112,64]
     [[Node: InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/moments/sufficient_statistics/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](InceptionV1/InceptionV1/Conv2d_1a_7x7/Conv2D, InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/Add)]]
     [[Node: InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool/_1231 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3156_InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op u'InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/moments/sufficient_statistics/SquaredDifference', defined at:
  File ""train_image_classifier.py"", line 585, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""train_image_classifier.py"", line 482, in main
    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/deployment/model_deploy.py"", line 195, in create_clones
    outputs = model_fn(_args, *_kwargs)
  File ""train_image_classifier.py"", line 466, in clone_fn
    logits, end_points = network_fn(images)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/nets/nets_factory.py"", line 103, in network_fn
    return func(images, num_classes, is_training=is_training)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/nets/inception_v1.py"", line 288, in inception_v1
    net, end_points = inception_v1_base(inputs, scope=scope)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/nets/inception_v1.py"", line 61, in inception_v1_base
    net = slim.conv2d(inputs, 64, [7, 7], stride=2, scope=end_point)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 177, in func_with_args
    return func(_args, *_current_args)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 445, in convolution2d
    outputs = normalizer_fn(outputs, *_normalizer_params)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 177, in func_with_args
    return func(_args, **current_args)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 250, in batch_norm
    mean, variance = nn.moments(inputs, axis, shift=shift)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn.py"", line 835, in moments
    y, axes, shift=shift, keep_dims=keep_dims, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn.py"", line 762, in sufficient_statistics
    v_ss = math_ops.squared_difference(x, shift)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 2347, in squared_difference
    result = _op_def_lib.apply_op(""SquaredDifference"", x=x, y=y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 749, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2386, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1298, in __init__
    self._traceback = _extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[256,112,112,64]
     [[Node: InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/moments/sufficient_statistics/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](InceptionV1/InceptionV1/Conv2d_1a_7x7/Conv2D, InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/Add)]]
     [[Node: InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool/_1231 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3156_InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Traceback (most recent call last):
  File ""train_image_classifier.py"", line 585, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""train_image_classifier.py"", line 581, in main
    sync_optimizer=optimizer if FLAGS.sync_replicas else None)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 781, in train
    raise
  File ""/usr/lib/python2.7/contextlib.py"", line 35, in **exit**
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 969, in managed_session
    self.stop(close_summary_writer=close_summary_writer)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 797, in stop
    stop_grace_period_secs=self._stop_grace_secs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 386, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 296, in stop_on_exception
    yield
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 481, in run
    self.run_loop()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 999, in run_loop
    self._sv.global_step])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 717, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 915, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 965, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 985, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shape[256,112,112,64]
     [[Node: InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/moments/sufficient_statistics/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](InceptionV1/InceptionV1/Conv2d_1a_7x7/Conv2D, InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/Add)]]
     [[Node: InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool/_1231 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3156_InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op u'InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/moments/sufficient_statistics/SquaredDifference', defined at:
  File ""train_image_classifier.py"", line 585, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""train_image_classifier.py"", line 482, in main
    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/deployment/model_deploy.py"", line 195, in create_clones
    outputs = model_fn(_args, *_kwargs)
  File ""train_image_classifier.py"", line 466, in clone_fn
    logits, end_points = network_fn(images)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/nets/nets_factory.py"", line 103, in network_fn
    return func(images, num_classes, is_training=is_training)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/nets/inception_v1.py"", line 288, in inception_v1
    net, end_points = inception_v1_base(inputs, scope=scope)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/nets/inception_v1.py"", line 61, in inception_v1_base
    net = slim.conv2d(inputs, 64, [7, 7], stride=2, scope=end_point)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 177, in func_with_args
    return func(_args, *_current_args)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 445, in convolution2d
    outputs = normalizer_fn(outputs, *_normalizer_params)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 177, in func_with_args
    return func(_args, **current_args)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 250, in batch_norm
    mean, variance = nn.moments(inputs, axis, shift=shift)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn.py"", line 835, in moments
    y, axes, shift=shift, keep_dims=keep_dims, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn.py"", line 762, in sufficient_statistics
    v_ss = math_ops.squared_difference(x, shift)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 2347, in squared_difference
    result = _op_def_lib.apply_op(""SquaredDifference"", x=x, y=y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 749, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2386, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1298, in __init__
    self._traceback = _extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[256,112,112,64]
     [[Node: InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/moments/sufficient_statistics/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](InceptionV1/InceptionV1/Conv2d_1a_7x7/Conv2D, InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/Add)]]
     [[Node: InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool/_1231 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3156_InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
"
4734,Unable to restore session with batch_norm,"I am saving my model during training with saver = tf.train.Saver() and then conditionally saver.save(sess,filename). It works for my model and allows me to restore the session later if I do not use batch normalization. With batch_norm layers in the model (either the one in contrib.layers or contrib.slim) I get the following error when I try to restore the session:

`FailedPreconditionError: Attempting to use uninitialized value Variable
     [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[""loc:@Variable""], _device=""/job:localhost/replica:0/task:0/gpu:0""](Variable)]]
     [[Node: conv0/moments/sufficient_statistics/Shape/_32 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_483_conv0/moments/sufficient_statistics/Shape"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
Caused by op u'Variable/read', defined at:
  File ""/usr/lib/python2.7/dist-packages/spyderlib/widgets/externalshell/start_ipython_kernel.py"", line 197, in <module>
    __ipythonkernel__.start()
  File ""/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/kernelapp.py"", line 458, in start
    ioloop.IOLoop.instance().start()
  File ""/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py"", line 160, in start
    super(ZMQIOLoop, self).start()
  File ""/usr/lib/python2.7/dist-packages/zmq/eventloop/minitornado/ioloop.py"", line 678, in start
    self._handlers[fd](fd, events)
  File ""/usr/lib/python2.7/dist-packages/zmq/eventloop/minitornado/stack_context.py"", line 302, in wrapped
    ret = fn(*args, **kwargs)
  File ""/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py"", line 433, in _handle_events
    self._handle_recv()
  File ""/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py"", line 465, in _handle_recv
    self._run_callback(callback, msg)
  File ""/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py"", line 407, in _run_callback
    callback(*args, **kwargs)
  File ""/usr/lib/python2.7/dist-packages/zmq/eventloop/minitornado/stack_context.py"", line 302, in wrapped
    ret = fn(*args, **kwargs)
  File ""/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py"", line 279, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py"", line 247, in dispatch_shell
    handler(stream, idents, msg)
  File ""/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py"", line 396, in execute_request
    shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py"", line 2660, in run_cell
    interactivity=interactivity, compiler=compiler)
  File ""/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py"", line 2770, in run_ast_nodes
    if self.run_code(code):
  File ""/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py"", line 2820, in run_code
    exec code_obj in self.user_global_ns, self.user_ns
  File ""<ipython-input-1-3936af1513f1>"", line 1, in <module>
    runfile('/home/jason/code/test_prototypes.py', wdir='/home/jason/code')
  File ""/usr/lib/python2.7/dist-packages/spyderlib/widgets/externalshell/sitecustomize.py"", line 540, in runfile
    execfile(filename, namespace)
  File ""/home/jason/code/test_prototypes.py"", line 53, in <module>
    l2_reg=l2_reg)
  File ""model.py"", line 147, in __init__
    W = _uniform_weight(filter_shape)
  File ""model.py"", line 27, in _uniform_weight
    return tf.Variable(initial)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py"", line 215, in __init__
    dtype=dtype)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py"", line 327, in _init_from_args
    self._snapshot = array_ops.identity(self._variable, name=""read"")
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 1106, in identity
    result = _op_def_lib.apply_op(""Identity"", input=input, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 703, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2333, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1252, in __init__
    self._traceback = _extract_stack()`
"
4733,Bidirectional RNN: forward and backward passing to the middle of the sentence,"Hi all, I have a task that requires me to split a sentence into two and model the two parts respectively using bidirectional rnn, e.g.:

**""I think of A, but I am going to B tomorrow""** split into **""I think of A""** and **""but I am going to B tomorrow""**; forward passing goes from ""I"" to ""A"" while backward passing goes from ""tomorrow"" to ""but"". This way two outputs are trained using directional rnn and then concatenated before feeding to the softmax layer.

My question is, is there an easier to achieve this using bidirectional_rnn in Tensorflow that I am not aware of? Or I have to split the sentence first, reverse the order of the 2nd part, and then model the two parts using two rnn networks respectively (which is what I am doing now). 

Thanks!! 
"
4732,Input ops fed networks operate considerably slower than direct feed ones,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

http://stackoverflow.com/questions/39794149/when-introducing-the-optimizer-variables-under-variable-scope-get-recreated-twic
### Environment info

Tried both OS X and Linux (Ubuntu 16)
On both utilizing CPU only.
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Any simple operation (i.e. calculating logits with a 3 layer deep network with simple regression in each layer) in which the data was fed with either a `parse_example`, `parse_single_example`, building a `CustomRunner` that feeds a `RandomShuffleQueue` or utilizing `QueueRunner`.  In all of those the operation and time to complete 1 epoch took considerably more than if I were to save the data in a bumpy array and feed it during a call to `sess.run()` with the `feed_dict`.
### What other attempted solutions have you tried?

Tried to score the web for solutions and ran into a couple blog posts describing the problem but can't seem to find solutions.
### Logs or other output that would be helpful

Tried this on both TF 0.10 and the latest RC for 0.11.
"
4729,"IOS #include ""unsupported/Eigen/CXX11/Tensor""","### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
### Environment info

Operating System: Mac OS X EI captain 10.11.6

Installed version of CUDA and cuDNN: 
No

If installed from binary pip package, provide:
1. A link to the pip package you installed:
   I install tensorflow from source following this site: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
   But It seems i cant get the tensorflow version command, there is no module named tensorflow; I also installed tensorflow with docker on my Mac.

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`) 
   6218ac2be3cc530da866ec32da4cb86c6ac5bb85
2. The output of `bazel version`
   Build label: 0.3.1-homebrew
   Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
   Build time: Thu Aug 4 09:58:27 2016 (1470304707)
   Build timestamp: 1470304707
   Build timestamp as int: 1470304707
### What other attempted solutions have you tried?

Search on stackoverflow

I followed the ios_example from here, https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples
I successfully installed the static library containing the core code of tensorflow;
I downloaded the incetption v1, and I have xcode 7.3.1 installed on my Mac;
When I launch xcode from tf_ios_makefile_example.xcodeproj in the subfolder tensorflow/tensorflow/contrib/ios_examples/simple, I got un error:
# include ""unsupported/Eigen/CXX11/Tensor""

What did I miss? the version of eigen is not right? I do install eigen.

Any suggestion will be appreciated
Thanks
"
4728,Eigen errors.,"/apps/.GCC/6.2.0/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=0' -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -g0 -DNDEBUG -pipe -DMKL_LP64 -I/usr/include/ncurses -I/apps/INTEL/2017.0-035/mkl/include -I/apps/PYTHON/3.5.2_ML/include/python3.5m -m64 '-march=native' '-mtune=native' -Ofast -s -fmath-errno -fno-unsafe-math-optimizations -fno-finite-math-only -fno-cx-limited-range -freciprocal-math -ftree-vectorize -fomit-frame-pointer -fno-stack-protector -ffunction-sections -fdata-sections -flto -fuse-linker-plugin -Wl,--as-needed -L/apps/.GLIBC/2.24/lib -L/apps/INTEL/2017.0-035/mkl/lib/intel64 -lmkl_rt -lpthread -lm -ldl -Wl,--gc-sections -Wl,-rpath,/apps/.GLIBC/2.24/lib -Wl,-rpath,/apps/.GCC/6.2.0/lib64 -Wl,-rpath,/apps/.GCC/6.2.0/lib -Wl,-rpath,/apps/INTEL/2017.0-035/mkl/lib/intel64 -Wl,--dynamic-linker,/apps/.GLIBC/2.24/lib/ld-linux-x86-64.so.2 '-std=c++11' -MD -MF bazel-out/local_mn-py3-opt/bin/tensorflow/core/kernels/_objs/batch_matmul_op/tensorflow/core/kernels/batch_matmul_op_real.pic.d '-frandom-seed=bazel-out/local_mn-py3-opt/bin/tensorflow/core/kernels/_objs/batch_matmul_op/tensorflow/core/kernels/batch_matmul_op_real.pic.o' -fPIC -DEIGEN_MPL2_ONLY -iquote . -iquote bazel-out/local_mn-py3-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_mn-py3-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/local_mn-py3-opt/genfiles/external/eigen_archive -iquote external/protobuf -iquote bazel-out/local_mn-py3-opt/genfiles/external/protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/local_mn-py3-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local_mn-py3-opt/genfiles/external/farmhash_archive -iquote external/gif_archive -iquote bazel-out/local_mn-py3-opt/genfiles/external/gif_archive -iquote external/highwayhash -iquote bazel-out/local_mn-py3-opt/genfiles/external/highwayhash -iquote external/jpeg_archive -iquote bazel-out/local_mn-py3-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_mn-py3-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local_mn-py3-opt/genfiles/external/zlib_archive -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/local_mn-py3-opt/genfiles/external/eigen_archive -isystem external/protobuf/src -isystem bazel-out/local_mn-py3-opt/genfiles/external/protobuf/src -isystem external/farmhash_archive -isystem bazel-out/local_mn-py3-opt/genfiles/external/farmhash_archive -isystem external/gif_archive -isystem bazel-out/local_mn-py3-opt/genfiles/external/gif_archive -isystem external/highwayhash -isystem bazel-out/local_mn-py3-opt/genfiles/external/highwayhash -isystem external/jpeg_archive -isystem bazel-out/local_mn-py3-opt/genfiles/external/jpeg_archive -isystem external/png_archive -isystem bazel-out/local_mn-py3-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local_mn-py3-opt/genfiles/external/zlib_archive -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -pthread '--sysroot=/apps/.GLIBC/2.24' -c tensorflow/core/kernels/batch_matmul_op_real.cc -o bazel-out/local_mn-py3-opt/bin/tensorflow/core/kernels/_objs/batch_matmul_op/tensorflow/core/kernels/batch_matmul_op_real.pic.o
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:331:0,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/kernels/batch_matmul_op_impl.h:21,
                 from tensorflow/core/kernels/batch_matmul_op_real.cc:16:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h: In instantiation of â€˜typename Eigen::internal::unpacket_traits<T>::type Eigen::internal::predux(const Packet&) [with Packet = Eigen::internal::Packet8h; typename Eigen::internal::unpacket_traits<T>::type = Eigen::half]â€™:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/products/GeneralMatrixVector.h:548:23:   required from â€˜static void Eigen::internal::general_matrix_vector_product<Index, LhsScalar, LhsMapper, 1, ConjugateLhs, RhsScalar, RhsMapper, ConjugateRhs, Version>::run(Index, Index, const LhsMapper&, const RhsMapper&, Eigen::internal::general_matrix_vector_product<Index, LhsScalar, LhsMapper, 1, ConjugateLhs, RhsScalar, RhsMapper, ConjugateRhs, Version>::ResScalar_, Index, Eigen::internal::general_matrix_vector_product<Index, LhsScalar, LhsMapper, 1, ConjugateLhs, RhsScalar, RhsMapper, ConjugateRhs, Version>::ResScalar) [with Index = long int; LhsScalar = Eigen::half; LhsMapper = Eigen::internal::const_blas_data_mapper<Eigen::half, long int, 1>; bool ConjugateLhs = false; RhsScalar = Eigen::half; RhsMapper = Eigen::internal::const_blas_data_mapper<Eigen::half, long int, 0>; bool ConjugateRhs = false; int Version = 0; Eigen::internal::general_matrix_vector_product<Index, LhsScalar, LhsMapper, 1, ConjugateLhs, RhsScalar, RhsMapper, ConjugateRhs, Version>::ResScalar = Eigen::half]â€™
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GeneralProduct.h:318:132:   required from â€˜static void Eigen::internal::gemv_dense_selector<2, 1, true>::run(const Lhs&, const Rhs&, Dest&, const typename Dest::Scalar&) [with Lhs = Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Rhs = Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>; Dest = Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; typename Dest::Scalar = Eigen::half]â€™
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h:367:34:   required from â€˜static void Eigen::internal::generic_product_impl<Lhs, Rhs, Eigen::DenseShape, Eigen::DenseShape, 7>::scaleAndAddTo(Dest&, const Lhs&, const Rhs&, const Scalar&) [with Dest = Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Lhs = Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Rhs = Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>; Eigen::internal::generic_product_impl<Lhs, Rhs, Eigen::DenseShape, Eigen::DenseShape, 7>::Scalar = Eigen::half]â€™
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h:349:27:   required from â€˜static void Eigen::internal::generic_product_impl_base<Lhs, Rhs, Derived>::scaleAndAddTo(Dst&, const Lhs&, const Rhs&, const Scalar&) [with Dst = Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Lhs = Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Rhs = Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>; Derived = Eigen::internal::generic_product_impl<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>, Eigen::DenseShape, Eigen::DenseShape, 7>; Eigen::internal::generic_product_impl_base<Lhs, Rhs, Derived>::Scalar = Eigen::half]â€™
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h:337:33:   required from â€˜static void Eigen::internal::generic_product_impl_base<Lhs, Rhs, Derived>::evalTo(Dst&, const Lhs&, const Rhs&) [with Dst = Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Lhs = Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Rhs = Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>; Derived = Eigen::internal::generic_product_impl<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>, Eigen::DenseShape, Eigen::DenseShape, 7>]â€™
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h:144:43:   [ skipping 4 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]
./tensorflow/core/kernels/batch_matmul_op_impl.h:181:17:   required from â€˜static void tensorflow::{anonymous}::SequentialMatMulKernel<Scalar>::Run(const tensorflow::Tensor&, const tensorflow::Tensor&, bool, bool, tensorflow::Tensor_, int, int) [with Scalar = Eigen::half]â€™
./tensorflow/core/kernels/batch_matmul_op_impl.h:238:50:   required from â€˜tensorflow::LaunchBatchMatMul<Eigen::ThreadPoolDevice, Scalar>::Launch(tensorflow::OpKernelContext_, const tensorflow::Tensor&, const tensorflow::Tensor&, bool, bool, tensorflow::Tensor_)::<lambda(int, int)> [with Scalar = Eigen::half]â€™
./tensorflow/core/kernels/batch_matmul_op_impl.h:237:42:   required from â€˜struct tensorflow::LaunchBatchMatMul<Eigen::ThreadPoolDevice, Scalar>::Launch(tensorflow::OpKernelContext_, const tensorflow::Tensor&, const tensorflow::Tensor&, bool, bool, tensorflow::Tensor_) [with Scalar = Eigen::half]::<lambda(int, int)>â€™
./tensorflow/core/kernels/batch_matmul_op_impl.h:235:12:   required from â€˜static void tensorflow::LaunchBatchMatMul<Eigen::ThreadPoolDevice, Scalar>::Launch(tensorflow::OpKernelContext_, const tensorflow::Tensor&, const tensorflow::Tensor&, bool, bool, tensorflow::Tensor_) [with Scalar = Eigen::half]â€™
./tensorflow/core/kernels/batch_matmul_op_impl.h:427:46:   required from â€˜void tensorflow::BatchMatMul<Device, Scalar>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; Scalar = Eigen::half]â€™
tensorflow/core/kernels/batch_matmul_op_real.cc:33:1:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:324:10: error: could not convert â€˜aâ€™ from â€˜const Eigen::internal::Packet8hâ€™ to â€˜Eigen::internal::unpacket_traitsEigen::internal::Packet8h::type {aka Eigen::half}â€™
 { return a; }
          ^
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:442:0,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/kernels/batch_matmul_op_impl.h:21,
                 from tensorflow/core/kernels/batch_matmul_op_real.cc:16:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h: In instantiation of â€˜static void Eigen::internal::etor_product_packet_impl<1, -1, Lhs, Rhs, Packet, LoadMode>::run(Eigen::Index, Eigen::Index, const Lhs&, const Rhs&, Eigen::Index, Packet&) [with Lhs = Eigen::internal::evaluator<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; Rhs = Eigen::internal::evaluator<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; Packet = Eigen::internal::Packet8h; int LoadMode = 0; Eigen::Index = long int]â€™:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h:545:20:   required from â€˜const PacketType Eigen::internal::product_evaluator<Eigen::Product<Lhs, Rhs, 1>, ProductTag, Eigen::DenseShape, Eigen::DenseShape>::packet(Eigen::Index, Eigen::Index) const [with int LoadMode = 0; PacketType = Eigen::internal::Packet8h; Lhs = Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Rhs = Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; int ProductTag = 8; typename Eigen::internal::traits<typename Eigen::Product<Lhs, Rhs, 1>::Rhs>::Scalar = Eigen::half; typename Eigen::internal::traits<typename Eigen::Product<Lhs, Rhs, 1>::Lhs>::Scalar = Eigen::half; Eigen::Index = long int]â€™
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/AssignEvaluator.h:652:5:   required from â€˜void Eigen::internal::generic_dense_assignment_kernel<DstEvaluatorTypeT, SrcEvaluatorTypeT, Functor, Version>::assignPacket(Eigen::Index, Eigen::Index) [with int StoreMode = 16; int LoadMode = 0; PacketType = Eigen::internal::Packet8h; DstEvaluatorTypeT = Eigen::internal::evaluator<Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; SrcEvaluatorTypeT = Eigen::internal::evaluator<Eigen::Product<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, 1> >; Functor = Eigen::internal::assign_op<Eigen::half, Eigen::half>; int Version = 0; Eigen::Index = long int]â€™
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/AssignEvaluator.h:666:48:   required from â€˜void Eigen::internal::generic_dense_assignment_kernel<DstEvaluatorTypeT, SrcEvaluatorTypeT, Functor, Version>::assignPacketByOuterInner(Eigen::Index, Eigen::Index) [with int StoreMode = 16; int LoadMode = 0; PacketType = Eigen::internal::Packet8h; DstEvaluatorTypeT = Eigen::internal::evaluator<Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; SrcEvaluatorTypeT = Eigen::internal::evaluator<Eigen::Product<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, 1> >; Functor = Eigen::internal::assign_op<Eigen::half, Eigen::half>; int Version = 0; Eigen::Index = long int]â€™
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/AssignEvaluator.h:551:9:   required from â€˜static void Eigen::internal::dense_assignment_loop<Kernel, 4, 0>::run(Kernel&) [with Kernel = Eigen::internal::generic_dense_assignment_kernel<Eigen::internal::evaluator<Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >, Eigen::internal::evaluator<Eigen::Product<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, 1> >, Eigen::internal::assign_op<Eigen::half, Eigen::half>, 0>]â€™
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/AssignEvaluator.h:713:37:   required from â€˜void Eigen::internal::call_dense_assignment_loop(const DstXprType&, const SrcXprType&, const Functor&) [with DstXprType = Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; SrcXprType = Eigen::Product<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, 1>; Functor = Eigen::internal::assign_op<Eigen::half, Eigen::half>]â€™
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/AssignEvaluator.h:862:31:   [ skipping 8 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]
./tensorflow/core/kernels/batch_matmul_op_impl.h:185:17:   required from â€˜static void tensorflow::{anonymous}::SequentialMatMulKernel<Scalar>::Run(const tensorflow::Tensor&, const tensorflow::Tensor&, bool, bool, tensorflow::Tensor_, int, int) [with Scalar = Eigen::half]â€™
./tensorflow/core/kernels/batch_matmul_op_impl.h:238:50:   required from â€˜tensorflow::LaunchBatchMatMul<Eigen::ThreadPoolDevice, Scalar>::Launch(tensorflow::OpKernelContext_, const tensorflow::Tensor&, const tensorflow::Tensor&, bool, bool, tensorflow::Tensor_)::<lambda(int, int)> [with Scalar = Eigen::half]â€™
./tensorflow/core/kernels/batch_matmul_op_impl.h:237:42:   required from â€˜struct tensorflow::LaunchBatchMatMul<Eigen::ThreadPoolDevice, Scalar>::Launch(tensorflow::OpKernelContext_, const tensorflow::Tensor&, const tensorflow::Tensor&, bool, bool, tensorflow::Tensor_) [with Scalar = Eigen::half]::<lambda(int, int)>â€™
./tensorflow/core/kernels/batch_matmul_op_impl.h:235:12:   required from â€˜static void tensorflow::LaunchBatchMatMul<Eigen::ThreadPoolDevice, Scalar>::Launch(tensorflow::OpKernelContext_, const tensorflow::Tensor&, const tensorflow::Tensor&, bool, bool, tensorflow::Tensor_) [with Scalar = Eigen::half]â€™
./tensorflow/core/kernels/batch_matmul_op_impl.h:427:46:   required from â€˜void tensorflow::BatchMatMul<Device, Scalar>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; Scalar = Eigen::half]â€™
tensorflow/core/kernels/batch_matmul_op_real.cc:33:1:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h:648:24: error: no matching function for call to â€˜pset1(int)â€™
     res = pset1<Packet>(0);
           ~~~~~~~~~~~~~^~~
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:331:0,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/kernels/batch_matmul_op_impl.h:21,
                 from tensorflow/core/kernels/batch_matmul_op_real.cc:16:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:222:1: note: candidate: template<class Packet> Packet Eigen::internal::pset1(const typename Eigen::internal::unpacket_traits<Packet>::type&)
 pset1(const typename unpacket_traits<Packet>::type& a) { return a; }
 ^~~~~
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:222:1: note:   template argument deduction/substitution failed:
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:442:0,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/kernels/batch_matmul_op_impl.h:21,
                 from tensorflow/core/kernels/batch_matmul_op_real.cc:16:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h:648:24: note:   cannot convert â€˜0â€™ (type â€˜intâ€™) to type â€˜const type& {aka const Eigen::half&}â€™
     res = pset1<Packet>(0);
"
4727,Minor issues with C++ custom ops documentation,"I was following the instructions below to define a custom op called `zero_out`.

https://github.com/tensorflow/tensorflow/blob/aaeb50c55427350841f7a0c226af9db174397d55/tensorflow/g3doc/how_tos/adding_an_op/index.md

I found a few issues:
1. You cannot load a `so` file without giving an absolute path (or prefixing with `./`). 
2. The zero_out_module has no zero_out component. 

---

```
In [1]: import tensorflow as tf

In [2]: zero_out_module = tf.load_op_library('zero_out.so')
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-2-e4878fcfc44f> in <module>()
----> 1 zero_out_module = tf.load_op_library('zero_out.so')

/home/hholst/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/load_library.py in load_op_library(library_filename)
     73           return _OP_LIBRARY_MAP[library_filename]
     74       # pylint: disable=protected-access
---> 75       raise errors._make_specific_exception(None, None, error_msg, error_code)
     76       # pylint: enable=protected-access
     77   finally:

NotFoundError: zero_out.so: cannot open shared object file: No such file or directory

In [3]: zero_out_module = tf.load_op_library('./zero_out.so')

In [4]: 

```

![image](https://cloud.githubusercontent.com/assets/6200749/19033213/9620c9ec-895d-11e6-8f4e-db477b4b5397.png)
"
4726,Inception v3 failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED,"I was following the instructions in https://github.com/tensorflow/models/tree/master/inception to train inception v3. I created the TFRecord data successfully. But when I tried to train the network using the following command:
bazel-bin/inception/imagenet_train --num_gpus=4 --batch_size=128 --train_dir=... --data_dir=...
It has the following warning and error:

WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.
F tensorflow/stream_executor/cuda/cuda_dnn.cc:1979] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED
### Environment info

Operating System: Redhat 7.2

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
-rw-r--r-- 1 root root   560184 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 root root   394472 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 root root   737516 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so
-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.3
-rw-r--r-- 1 root root 59715990 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn_static.a

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`):
   f98c5ded31d7da0c2d127c28b2c16f0307a368f0
2. The output of `bazel version`
   .
   Build label: 0.3.1- (@non-git)
   Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
   Build time: Thu Sep 29 22:19:27 2016 (1475187567)
   Build timestamp: 1475187567
   Build timestamp as int: 1475187567
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
Except the above warning and error, there are the following warnings:
WARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
"
4725,tutorials_example_trainer failed to build,"when I build the tensorflow with the command:
bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer
error happens :
./tensorflow/core/framework/allocator.h(154): warning: missing return statement at end of non-void function ""tensorflow::Allocator::RequestedSize""

./tensorflow/core/framework/allocator.h(154): warning: missing return statement at end of non-void function ""tensorflow::Allocator::RequestedSize""

gcc: error trying to exec 'as': execvp: No such file or directory
ERROR: /home/lonny/tensorflow/tensorflow/core/kernels/BUILD:1673:1: output 'tensorflow/core/kernels/_objs/batch_space_ops_gpu/tensorflow/core/kernels/spacetobatch_functor_gpu.cu.o' was not created.
ERROR: /home/lonny/tensorflow/tensorflow/core/kernels/BUILD:1673:1: not all outputs were created.
Target //tensorflow/cc:tutorials_example_trainer failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1092.740s, Critical Path: 902.88s

Operating System:Ubuntu 16.04

Installed version of CUDA and cuDNN: 
CUDA toolkit 8.0rc

how can I solve the error?
"
4723,How to convert a csv file to TFrecord tensorFlow format?,"Hello everybody i need to convert a csv file to TFrecord for TensorFlow. I really appreciate your help.
An example of csv file that  i need to convert is:
Col1 Col2 Col3 Col4 Target
2.56 0.98 0.45  7.8    0.189
3.10 5.78  4.78 9.0    0.78
....
Thank you very much!!!
"
4722,einsum not fully implemented,"I am glad to see the newly added einsum function. The documentation claims that its usage is the same as numpy. However, it can do almost nothing as compared to numpy. For example, it only supports subscripts in the form of '_->_'. Unfortunately, even matrix transpose does not work, i.e., 'ij->ji'. 
Numpy works:

```
>>> A
array([[ 0.3828997 , -0.39114848, -0.09727838, -0.20430113],
       [ 0.48020577, -0.47122706,  0.42830791,  0.25665744],
       [-0.30885863,  0.21669025,  0.31648793,  0.22417514],
       [ 0.32505724,  0.30478035,  0.48655034,  0.20040547]])
>>> einsum('ij->ji',A)
array([[ 0.3828997 ,  0.48020577, -0.30885863,  0.32505724],
       [-0.39114848, -0.47122706,  0.21669025,  0.30478035],
       [-0.09727838,  0.42830791,  0.31648793,  0.48655034],
       [-0.20430113,  0.25665744,  0.22417514,  0.20040547]])
```

Tensorflow does not work:

```
pseudo-code:
M_ = tf.Variable(tf.random_normal([4,4]))
N_ = tf.einsum('ij->ji',M_)              
print [M_, N_]

output:
[array([[ 0.80474716, -1.38590837, -0.3379252 , -1.24965811],
       [ 2.57852983,  0.05492432,  0.23039417, -0.74263287],
       [-2.42627382,  1.70774114,  1.19503212,  0.43006262],
       [-1.04652011, -0.32753903, -1.26430523,  0.8810069 ]], dtype=float32), 
array([[ 0.80474716, -1.38590837, -0.3379252 , -1.24965811],
       [ 2.57852983,  0.05492432,  0.23039417, -0.74263287],
       [-2.42627382,  1.70774114,  1.19503212,  0.43006262],
       [-1.04652011, -0.32753903, -1.26430523,  0.8810069 ]], dtype=float32)]
```

I want to multiply a matrix with every frame vector in every batch. Or similar operations which can be done by a simple tensor product. It seems that I still have to duplicate the matrix so many times and perform a batch_matmul, which is very inconvenient and slow and memory consuming.

I suggest tensorflow to implement either the tensordot or einsum function which can perform tensor product.

It is quite a shame that tensorflow cannot even perform basic tensor product so far :(
"
4721,Make Image Summary in tensorboard more dynamic,"The image summary in Tensorboard is very nice, it's the most static tab on Tensorboard.
For example, when I am doing a visual test of my auto-encoder, for example MNIST, I see this:
![image](https://cloud.githubusercontent.com/assets/7721540/19021497/5d761600-88c3-11e6-90c8-85f8049f0a14.png)

Then I wait a few minutes, and I refresh and it shows:
![image](https://cloud.githubusercontent.com/assets/7721540/19021501/7187f2d0-88c3-11e6-88f1-8374dbd48356.png)

So it works. But I would like to be able to somehow review the progress, other than my current F5-stategy.
I recon all image summaries are saved inside, but I cannot obtain them through Tensorboard.
"
4718,i can't import tf.python.ops,"Python 3.5.2 (default, Oct  2 2016, 13:49:15)
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
/>>> import tensorflow as tf
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
/>>> tf.**version**
'0.11.0rc0'
/>>> tf.python.ops
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'tensorflow.python' has no attribute 'ops'
/>>> tf.python.platform
<module 'tensorflow.python.platform' from '/home/zhusf/lib/python3.5/site-packages/tensorflow/python/platform/__init__.py'>
/>>> tf.python.framework
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'tensorflow.python' has no attribute 'framework'
/>>> tf.python.summary
<module 'tensorflow.python.summary.summary' from '/home/zhusf/lib/python3.5/site-packages/tensorflow/python/summary/summary.py'>
/>>> tf.python.training
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'tensorflow.python' has no attribute 'training'
"
4715,shapes incompatible after upgrade,"After upgrade from 0.10.0 to 0.11.0rc0 my code no longer runs:

```
(802, 277)
(802, 1)
WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.
WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.
Traceback (most recent call last):
  File ""brain.py"", line 47, in <module>
    classifier.fit(x=trainX, y=trainY, steps=2000)
  File ""/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py"", line 435, in fit
    max_steps=max_steps)
  File ""/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 333, in fit
    max_steps=max_steps)
  File ""/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 662, in _train_model
    train_op, loss_op = self._get_train_ops(features, targets)
  File ""/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 963, in _get_train_ops
    _, loss, train_op = self._call_model_fn(features, targets, ModeKeys.TRAIN)
  File ""/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 944, in _call_model_fn
    return self._model_fn(features, targets, mode=mode, params=self.params)
  File ""/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py"", line 258, in _dnn_classifier_model_fn
    weight=_get_weight_tensor(features, weight_column_name))
  File ""/usr/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py"", line 329, in sigmoid_cross_entropy
    logits.get_shape().assert_is_compatible_with(multi_class_labels.get_shape())
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py"", line 750, in assert_is_compatible_with
    raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))
ValueError: Shapes (?, 1) and (?,) are incompatible
```

My code:

```
...
print shape(trainX) # (802, 277)
print shape(trainY) # (802, 1)
classifier.fit(x=trainX, y=trainY, steps=2000)
...
```

This worked fine on 0.10.0. Notice the shapes in the beginning of output (looks like a valid combination to me).
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

none
### Environment info

Operating System: Fedora 23, TF 0.11.0rc0
"
4714,TensorBoard Feature Request: [DASHBOARD] Static Variable Display Tab,"I do a lot of different runs which contain descriptions as well as different meta parameters.  It would be nice to have a tab on TensorBoard so I can tell which experiment I am viewing.

My thought would be to simply add static variables from the Graph which might include Strings, Ints, Floats, (maybe matrix in table format if possible, like PCA parameters, maybe confusion matrix, etc).  These may include a text description, current learning rate, network meta parameters, etc.  It would be good to simple present the ""current"" value of these variables so if there is a search on Discount Rates for RL or variation such as learning rate, those can be viewed.  This would be different from simply graphing the Learning rate as that shows the history rather than ""current"".  I think of this as a TensorFlow ""Dashboard"" that I can setup.

Something like:
tf.dashboard_summary(tags, tensor/value, collections=None, name=None)

eg:
tf.dashboard_summary(""Description:"",description_string_tensor_or_python_string)
tf.dashboard_summary(""Learning Rate:"",lr_tensor)
tf.dashboard_summary(""DiscountRate:"",discountRate_python_variable)

In this case, a ""DASHBOARD"" tab on tensorflow would contain the 3 labels above and the tensor/value.  If the value is a python variable, then it should be considered constant and will not change over the graph lifecycle.  If it is a tensor/variable, then it should be pulled from the graph at each iteration.

I can help but not sure where to jump in to get this started.
"
4713,How to close parameter server with python?,"In the TensorFlow documentation, the tf.train.Server.join() will block forever, but I would like to close it after training. Are there any possible solutions for it?

Thanks.
"
4712,Add 3.7 to list of default compute capabilities,"The Tesla K80 has a compute capabilities of 3.7. These are the cards in [AWS' new `p2` GPU offering](https://aws.amazon.com/blogs/aws/new-p2-instance-type-for-amazon-ec2-up-to-16-gpus/) and in [Azure's GPU offering](https://azure.microsoft.com/en-us/blog/azure-n-series-preview-availability/) which means that usage of these cards will be quite widespread.

Given that there are some difference between CC 3.5 and 3.7, it would be ideal to have builds that are able to use the newer features / settings in 3.7.
"
4710,download_dependencies.sh throws error on osx 10.10 - syntax issue,"I receive this error on osx 10.10

```
$ sh tensorflow/contrib/makefile/download_dependencies.sh
tensorflow/contrib/makefile/download_dependencies.sh: line 51: syntax error near unexpected token `<'
```

It was fixed by changing line 51 to: 

```
curl -Ls ""${url}"" | tar -C ""${dir}"" --strip-components=1 -xz
```
"
4709,Is tensorflow consuming much more memory than torch?,"I am trying to replicate the stacked hourglass architecture which is implemented in torch, https://github.com/anewell/pose-hg-train/. However, I am unable to train it using settings in the opts.lua file which set the train batch size as 6, number of train iteration as 8000, test batch size as 1 and number of test iteration as 1000. I understand that I can reduce the batch size or iteration number to fit the network. However, I just want to know if tensorflow is consuming more memory than torch, if the same architecture and settings can be implemented in tensorflow.
"
4707,Create API for creating slot variables,"To create or modify Adadelta, or Adam requires editing the Eigen tensor implementations of each.  However if it were possible to create a slot variable from tensorflow then one could perform tests without needing to recompile the whole tensorflow library.
"
4706,"0.11.rc0 version, using all gpu's memory while running on only one","With 0.11.rc0 tf, cudnn 7.5
+------------------------------------------------------+  
| NVIDIA-SMI 352.39     Driver Version: 352.39         |  
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K40m          On   | 0000:03:00.0     Off |                    0 |
| N/A   47C    P0   127W / 235W |  10995MiB / 11519MiB |     87%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K40m          On   | 0000:04:00.0     Off |                    0 |
| N/A   39C    P0    63W / 235W |  10953MiB / 11519MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla K40m          On   | 0000:83:00.0     Off |                    0 |
| N/A   40C    P0    62W / 235W |  10954MiB / 11519MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla K40m          On   | 0000:84:00.0     Off |                    0 |
| N/A   39C    P0    62W / 235W |  10953MiB / 11519MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      8828    C   ...mpiler/gcc-4.8.2/lib/ld-linux-x86-64.so.2 10964MiB |
|    1      8828    C   ...mpiler/gcc-4.8.2/lib/ld-linux-x86-64.so.2 10923MiB |
|    2      8828    C   ...mpiler/gcc-4.8.2/lib/ld-linux-x86-64.so.2 10923MiB |
|    3      8828    C   ...mpiler/gcc-4.8.2/lib/ld-linux-x86-64.so.2 10923MiB

With 0.10.0 tf, same code running with gpu status below
| NVIDIA-SMI 352.39     Driver Version: 352.39         |  
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K40m          On   | 0000:03:00.0     Off |                    0 |
| N/A   46C    P0   129W / 235W |  10993MiB / 11519MiB |     89%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K40m          On   | 0000:04:00.0     Off |                    0 |
| N/A   38C    P0    63W / 235W |    108MiB / 11519MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla K40m          On   | 0000:83:00.0     Off |                    0 |
| N/A   39C    P0    62W / 235W |    107MiB / 11519MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla K40m          On   | 0000:84:00.0     Off |                    0 |
| N/A   38C    P0    62W / 235W |    107MiB / 11519MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0     20051    C   ...mpiler/gcc-4.8.2/lib/ld-linux-x86-64.so.2 10959MiB |
|    1     20051    C   ...mpiler/gcc-4.8.2/lib/ld-linux-x86-64.so.2    74MiB |
|    2     20051    C   ...mpiler/gcc-4.8.2/lib/ld-linux-x86-64.so.2    74MiB |
|    3     20051    C   ...mpiler/gcc-4.8.2/lib/ld-linux-x86-64.so.2    74MiB 
"
4705,Error building with cuda after bazel server dies,"This is effectively reopening #4105 as this is another way to reproduce the issue that I am seeing repeatedly. The fix for #4105 was to make sure after running configure clean+fetch is run.

I see this issue almost every day so I'm not sure if bazel flushes some caches or the daemon is dying, but the issue reliably reproduces by killing the bazel daemon. Having to re-run configure and restart the build from scratch is unreasonable, since there is a workaround that avoids a rebuild so this looks very much like a bug.

```
$ bazel version 
.
Build label: 0.3.1
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jul 29 09:09:52 2016 (1469783392)
Build timestamp: 1469783392
Build timestamp as int: 1469783392
```

Steps to reproduce and ""fix"":

_NB_: $bazel_build_dir is something like `~/.cache/bazel/_bazel_foo/6e3dd6b174494dc75d93de11da03e7e7`

``````
# Run ./configure, enabling GPU support.

# The crosstool BUILD file looks good now and building with --config=cuda works
tensorflow$ cat $bazel_build_dir/external/local_config_cuda/crosstool/BUILD
licenses([""restricted""])```

package(default_visibility = [""//visibility:public""])

cc_toolchain_suite(
    name = ""toolchain"",
    toolchains = {
        ""local|compiler"": "":cc-compiler-local"",
        ""darwin|compiler"": "":cc-compiler-darwin"",
    },
)

cc_toolchain(
    name = ""cc-compiler-local"",
    all_files = "":empty"",
    compiler_files = "":empty"",
    cpu = ""local"",
    dwp_files = "":empty"",
    dynamic_runtime_libs = ["":empty""],
    linker_files = "":empty"",
    objcopy_files = "":empty"",
    static_runtime_libs = ["":empty""],
    strip_files = "":empty"",
    supports_param_files = 0,
)

cc_toolchain(
    name = ""cc-compiler-darwin"",
    all_files = "":empty"",
    compiler_files = "":empty"",
    cpu = ""darwin"",
    dwp_files = "":empty"",
    dynamic_runtime_libs = ["":empty""],
    linker_files = "":empty"",
    objcopy_files = "":empty"",
    static_runtime_libs = ["":empty""],
    strip_files = "":empty"",
    supports_param_files = 0,
)

filegroup(
    name = ""empty"",
    srcs = [],
)

# Backup the crosstool directory so it can be restored later
mkdir -p /tmp/crosstool
cp -aR $bazel_build_dir/external/local_config_cuda/crosstool/* /tmp/crosstool/

# Kill bazel server.
kill $(ps aux | awk '/baze[l]/ {print $2; exit}')
tensorflow$ bazel build -c opt --config=cuda //tensorflow/tools/...
.
ERROR: $bazel_build_dir/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):
    File ""$bazel_build_dir/external/local_config_cuda/crosstool/BUILD"", line 4
        error_gpu_disabled()
    File ""$bazel_build_dir/external/local_config_cuda/crosstool/error_gpu_disabled.bzl"", line 3, in error_gpu_disabled
        fail(""ERROR: Building with --config=c..."")
ERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.
Unhandled exception thrown during build; message: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@808aaea9' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@9a9c82e5', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@66ba3b8e', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@97095481')
INFO: Elapsed time: 0.543s
java.lang.RuntimeException: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@808aaea9' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@9a9c82e5', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@66ba3b8e', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@97095481')
    at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1070)
    at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:474)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: com.google.devtools.build.lib.packages.NoSuchTargetException: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by $bazel_build_dir/external/local_config_cuda/crosstool/BUILD
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.getCrosstoolProtofromBuildFile(CrosstoolConfigurationLoader.java:179)
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.findCrosstoolConfiguration(CrosstoolConfigurationLoader.java:239)
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.readCrosstool(CrosstoolConfigurationLoader.java:281)
    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.createParameters(CppConfigurationLoader.java:128)
    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.create(CppConfigurationLoader.java:73)
    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.create(CppConfigurationLoader.java:48)
    at com.google.devtools.build.lib.skyframe.ConfigurationFragmentFunction.compute(ConfigurationFragmentFunction.java:78)
    at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1016)
    ... 4 more
Caused by: com.google.devtools.build.lib.packages.NoSuchTargetException: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by $bazel_build_dir/external/local_config_cuda/crosstool/BUILD
    at com.google.devtools.build.lib.packages.Package.makeNoSuchTargetException(Package.java:559)
    at com.google.devtools.build.lib.packages.Package.getTarget(Package.java:543)
    at com.google.devtools.build.lib.skyframe.SkyframePackageLoaderWithValueEnvironment.getTarget(SkyframePackageLoaderWithValueEnvironment.java:71)
    at com.google.devtools.build.lib.skyframe.ConfigurationFragmentFunction$ConfigurationBuilderEnvironment.getTarget(ConfigurationFragmentFunction.java:193)
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.getCrosstoolProtofromBuildFile(CrosstoolConfigurationLoader.java:177)
    ... 11 more
java.lang.RuntimeException: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@808aaea9' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@9a9c82e5', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@66ba3b8e', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@97095481')
    at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1070)
    at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:474)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: com.google.devtools.build.lib.packages.NoSuchTargetException: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by $bazel_build_dir/external/local_config_cuda/crosstool/BUILD
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.getCrosstoolProtofromBuildFile(CrosstoolConfigurationLoader.java:179)
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.findCrosstoolConfiguration(CrosstoolConfigurationLoader.java:239)
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.readCrosstool(CrosstoolConfigurationLoader.java:281)
    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.createParameters(CppConfigurationLoader.java:128)
    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.create(CppConfigurationLoader.java:73)
    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.create(CppConfigurationLoader.java:48)
    at com.google.devtools.build.lib.skyframe.ConfigurationFragmentFunction.compute(ConfigurationFragmentFunction.java:78)
    at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1016)
    ... 4 more
Caused by: com.google.devtools.build.lib.packages.NoSuchTargetException: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by $bazel_build_dir/external/local_config_cuda/crosstool/BUILD
    at com.google.devtools.build.lib.packages.Package.makeNoSuchTargetException(Package.java:559)
    at com.google.devtools.build.lib.packages.Package.getTarget(Package.java:543)
    at com.google.devtools.build.lib.skyframe.SkyframePackageLoaderWithValueEnvironment.getTarget(SkyframePackageLoaderWithValueEnvironment.java:71)
    at com.google.devtools.build.lib.skyframe.ConfigurationFragmentFunction$ConfigurationBuilderEnvironment.getTarget(ConfigurationFragmentFunction.java:193)
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.getCrosstoolProtofromBuildFile(CrosstoolConfigurationLoader.java:177)
    ... 11 more

# So now the crosstool dir has been trashed
tensorflow$ cat $bazel_build_dir/external/local_config_cuda/crosstool/BUILD

load(""//crosstool:error_gpu_disabled.bzl"", ""error_gpu_disabled"")

error_gpu_disabled()

# Eh? I didn't disable it! Bazel just died...
# Let's restore from backup
cp -aR /tmp/crosstool/* $bazel_build_dir/external/local_config_cuda/crosstool/
# And restart the bazel server... Almost... Just doing ""bazel version"" isn't enough; the server restarts but at the next build, the same issue occurs and we need to restore the crosstool directory again.
# Running this seems to do the trick:
tensorflow$ bazel fetch //tensorflow/...
# But the crosstool directory has been trashed again
tensorflow$ cat $bazel_build_dir/external/local_config_cuda/crosstool/BUILD

load(""//crosstool:error_gpu_disabled.bzl"", ""error_gpu_disabled"")

error_gpu_disabled()

# Restore once more again
cp -aR /tmp/crosstool/* $bazel_build_dir/external/local_config_cuda/crosstool/
# Now we can build fine!
``````

In short, the workaround whenever the bazel server dies is, assuming you've backed up your crosstool directory in /tmp/crosstool/:

```
bazel fetch //tensorflow/...
cp -aR /tmp/crosstool/* $bazel_build_dir/external/local_config_cuda/crosstool/
```

This looks like a bug to me and the resolution of #4105 just doesn't seem related to this, it just coincidentally fixes it.
"
4703,Conditional print doesn't work appropriately as the control flow describe,"I stucked with this problem. I want to check if gradients or variables are NaN before applying them. Print conditionally the name of the variable and a value for both gradient and variable. 
### Environment info

Operating System: Linux
Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
-rw-r--r-- 1 root root 189170 Jul 11 23:19 /opt/cuda/lib/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Jul 11 23:19 /opt/cuda/lib/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19 Jul 11 23:19 /opt/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 311596 Jul 11 23:19 /opt/cuda/lib/libcudart.so.7.5.18
-rw-r--r-- 1 root root 558020 Jul 11 23:19 /opt/cuda/lib/libcudart_static.a
### Tensorflow version:

I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
Backend Qt5Agg is interactive backend. Turning interactive mode on.
0.9.0
### Bazel

If installed from source, provide 

Extracting Bazel installation...
Build label: 0.3.1- (@non-git)
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Aug 31 19:34:37 2016 (1472672077)
Build timestamp: 1472672077
Build timestamp as int: 1472672077
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

I reproduced with MNIST. I tried many different way in my code. Checked documentation, and try to explicitly restrict control flow, and still no success. The tf.print runs anyway, I didn't find anything that this is on purpose, or how to manage it. 

```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse

# Import data
from tensorflow.examples.tutorials.mnist import input_data

import tensorflow as tf

FLAGS = None


def main(_):
    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)

    # Create the model
    x = tf.placeholder(tf.float32, [None, 784])
    W = tf.Variable(tf.zeros([784, 10]))
    b = tf.Variable(tf.zeros([10]))
    y = tf.matmul(x, W) + b

    # Define loss and optimizer
    y_ = tf.placeholder(tf.float32, [None, 10])

    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))
    train_step = tf.train.GradientDescentOptimizer(0.5)
    gvs = train_step.compute_gradients(cross_entropy)

    gvs = [(tf.select(tf.is_nan(grad), grad, tf.Print(grad, [grad, var, tf.is_nan(grad), tf.is_nan(var)])), var) for grad, var in gvs]

    minimizer = train_step.apply_gradients(gvs)

    sess = tf.InteractiveSession()
    # Train
    tf.initialize_all_variables().run()
    for _ in range(1000):
        batch_xs, batch_ys = mnist.train.next_batch(100)
        sess.run(minimizer, feed_dict={x: batch_xs, y_: batch_ys})

    # Test trained model
    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    print(sess.run(accuracy, feed_dict={x: mnist.test.images,
                                        y_: mnist.test.labels}))


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_dir', type=str, default='/tmp/data',
                        help='Directory for storing data')
    FLAGS = parser.parse_args()
    tf.app.run()
```
### What other attempted solutions have you tried?
## 1 with control flow restriction + replacing the condition with tf.greater or tf.is_finite, etc.

```
    new_gvs = list()
    for grad, var in gvs :
        # c = tf.greater(grad, 1)
        c = tf.is_finite(var)
        with tf.control_dependencies([c]):
            newGrad = tf.select(c, tf.Print(grad, [grad, var, tf.is_nan(grad), c]), grad)
            with tf.control_dependencies([newGrad]):
                new_gvs.append((newGrad, var))
    gvs = new_gvs
```
## 2 with is_nan 'var'

```
gvs = [(tf.select(tf.is_nan(var), tf.Print(grad, [grad, var, tf.is_nan(grad), tf.is_nan(var)]), grad), var) for grad, var in gvs]
```
### Logs or other output that would be helpful

It just logs everything, as if there were no condition.
"
4701,"Unable to run ""bazel build tensorflow/examples/image_retraining:retrain""","Hi! I was going through the re-trainer tutorial but i keep encountering the following error.

<code>Executing genrule //tensorflow/core:version_info_gen failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
error: Could not expand include path '~/.gitcinclude'
fatal: bad config file line 49 in /usr/local/git/etc/gitconfig
Traceback (most recent call last):
  File ""tensorflow/tools/git/gen_git_source.py"", line 254, in <module>
    generate(args.generate)
  File ""tensorflow/tools/git/gen_git_source.py"", line 207, in generate
    write_version_info(dest_file, git_version)
  File ""tensorflow/tools/git/gen_git_source.py"", line 160, in write_version_info
    if b""\"""" in git_version or b""\"" in git_version:
TypeError: 'in <string>' requires string as left operand, not bytes
Target //tensorflow/examples/image_retraining:retrain failed to build
Use --verbose_failures to see the command lines of failed build steps. </code>

Am i doing something incorrectly?
"
4698,freeze graph from .ckpt to .pd file fail,"Now i am using tensor flow to train inception v1 (in slim model folder) on my data set. i am using the latest nightly build 0.10.
 i want to convert the big .ckpt to .pd file. i try ""freeze graph"" in the tool box but fail.

the command is: (the graph format is default pbtxt)
## bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/home/DeepLearning/tensorflow/v1/graph.pbtxt --input_checkpoint=/home/DeepLearning/tensorflow/v1/model.ckpt-1315 --output_graph=/home/DeepLearning/tensorflow/v1/test.pb --output_node_names=softmax

the error message is:

Traceback (most recent call last):
  File ""/home/DeepLearning/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 134, in <module>
    tf.app.run()
  File ""/home/DeepLearning/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""/home/DeepLearning/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 131, in main
    FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes)
  File ""/home/DeepLearning/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 103, in freeze_graph
    _ = tf.import_graph_def(input_graph_def, name="""")
  File ""/home/DeepLearning/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/importer.py"", line 252, in import_graph_def
    op_def = op_dict[node.op]
**KeyError: u'RestoreV2'**

---

I am not clear about **KeyError: u'RestoreV2**. is there anyone meet the same issue?

thanks!
"
4697,Can't import reader.py correctly for rnn/ptb/,"I cloned the code and downloaded the dataset. However when I run 
`python ptb_word_lm.py --data_path=simple-examples/data/ --model small`

it gives the following error message:

Traceback (most recent call last):
  File ""ptb_word_lm.py"", line 369, in <module>
    tf.app.run()
  File ""/Users/fanyang1/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""ptb_word_lm.py"", line 329, in main
    train_input = PTBInput(config=config, data=train_data, name=""TrainInput"")
  File ""ptb_word_lm.py"", line 93, in **init**
    self.input_data, self.targets = reader.ptb_producer(
**AttributeError: 'module' object has no attribute 'ptb_producer'**

It seems that for some unknown reason, the reader.py file is not imported correctly. 

Is this a python problem or more TensorFlow related problem? 

Thanks!
"
4693,http://commondatastorage.googleapis.com/books1000/ no longer available,"This is not tensorflow issue but an issue with the first tutorial in the udacity program https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb

The url is not found and is not accessible via a browser, tried Chrome 53 on Mac OS X 10.11
"
4688,Unable to evaluate dense tensor obtained from sparse complex tensor,"I am unable to evaluate/print/run  a dense tensor which is obtained from a complex sparse tensor.

a = tf.SparseTensor(indices=[[0, 0, 0], [1, 2, 1]], values=[1.0+2j, 2.0], shape=[3, 4, 2])

b = tf.sparse_tensor_to_dense(a, default_value=0.0)

sess  = tf.Session()

sess.run(b)

returns the following error:

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'SparseToDense' with these attrs.  Registered kernels:
  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_STRING]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_STRING]; Tindices in [DT_INT64]

```
 [[Node: SparseToDense_3 = SparseToDense[T=DT_COMPLEX128, Tindices=DT_INT64, validate_indices=true](SparseTensor_2/indices, SparseTensor_2/shape, SparseTensor_2/values, SparseToDense_3/default_value)]]
```

I also tried 

br = tf.real(b)

sess.run(br)

However, this gives the same error as above.

Note, however, that following works fine:

x = tf.ones((3,3), dtype=tf.complex128)

sess.run(x)

I am currently using tensorflow v 0.10.0 on macosx (cpu only).

I am not sure why I am unable to evaluate complex tensor obtained from sparse_tensor_to_dense operator operation. 
"
4685,ResourceExhaustedError when formating ImageNet data to TFRecord,"Hi All Tensorflow users,

I followed the steps in https://github.com/tensorflow/models/tree/master/inception to build the imagenet to TFRecord format. But when I execute the command 
""bazel-bin/inception/download_and_preprocess_imagenet ""${DATA_DIR}"""", 
I have the following errors:

Exception in thread Thread-6:
Traceback (most recent call last):
  File ""/usr/lib64/python2.7/threading.py"", line 811, in **bootstrap_inner
  File ""/usr/lib64/python2.7/threading.py"", line 764, in run
  File ""/home/rengan/.cache/bazel/_bazel_rengan/2c9ddf610cf302ceda3f2b852669382c/execroot/inception/bazel-out/local-fastbuild/bin/inception/build_imagenet_data.runfiles/inception/inception/data/build_imagenet_data.py"", line 388, in _process_image_files_batch
    image_buffer, height, width = _process_image(filename, coder)
  File ""/home/rengan/.cache/bazel/_bazel_rengan/2c9ddf610cf302ceda3f2b852669382c/execroot/inception/bazel-out/local-fastbuild/bin/inception/build_imagenet_data.runfiles/inception/inception/data/build_imagenet_data.py"", line 316, in _process_image
    image_data = tf.gfile.FastGFile(filename, 'r').read()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 102, in read
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 72, in _preread_check
  File ""/usr/lib64/python2.7/contextlib.py"", line 24, in __exit**
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/errors.py"", line 463, in raise_exception_on_not_ok_status
ResourceExhaustedError: /cm/shared/scratch/rengan/Data/imagenet-data/raw-data/train//n03124170/n03124170_675.JPEG

There are similar errors in other threads. It successfully generated 128 validation data but less than 100 training data (should be 1024).
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?: no related issues were found.
### Environment info

Operating System: Redhat 7.2

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
-rw-r--r-- 1 root root   560184 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 root root   394472 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 root root   737516 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so
-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.3
-rw-r--r-- 1 root root 59715990 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn_static.a

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`): f98c5ded31d7da0c2d127c28b2c16f0307a368f0
2. The output of `bazel version`: 
WARNING: Output base '/home/rengan/.cache/bazel/_bazel_rengan/2c9ddf610cf302ceda3f2b852669382c' is on NFS. This may lead to surprising failures and undetermined behavior.
.
Build label: 0.3.1- (@non-git)
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Sep 29 22:19:27 2016 (1475187567)
Build timestamp: 1475187567
Build timestamp as int: 1475187567

Thanks in advance.
"
4683,sigmoid_cross_entropy shape compatible check bug (setting n_classes=2 in DNNClassifier),"### Environment info

Operating System: Ubuntu 16.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
-rw-r--r-- 1 root root    558720 Sep 15 07:02 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root        16 Sep 15 07:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root        19 Sep 15 07:05 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rw-r--r-- 1 root root    415432 Sep 15 07:02 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root    775162 Sep 15 07:02 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 xj   users       13 Jul 27 13:55 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 xj   users       17 Jul 27 13:55 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxr-xr-x 1 xj   xj    78065952 Apr 23 03:17 /usr/local/cuda/lib64/libcudnn.so.5.0.5
-rwxrwxr-x 1 xj   users 79337624 Jul 27 13:53 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-rw-r-- 1 xj   users 69756172 Jul 27 13:53 /usr/local/cuda/lib64/libcudnn_static.a

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
   bad7c50b9dc9789ad7dd0a62daca40b7269841ed
2. The output of `bazel version`
   .
   Build label: 0.3.1
   Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
   Build time: Fri Jul 29 09:09:52 2016 (1469783392)
   Build timestamp: 1469783392
   Build timestamp as int: 1469783392
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

from **future** import absolute_import
from **future** import division
from **future** import print_function

import tensorflow as tf
import tensorflow.contrib as contrib
import numpy as np
# Data sets

IRIS_TRAINING = ""iris_training.csv""
IRIS_TEST = ""iris_test.csv""
# Load datasets.

training_set = contrib.learn.datasets.base.load_csv_with_header(filename=IRIS_TRAINING,
                                                                target_dtype=np.int,
                                                                features_dtype=np.float32)
test_set = contrib.learn.datasets.base.load_csv_with_header(filename=IRIS_TEST,
                                                            target_dtype=np.int,
                                                            features_dtype=np.float32)
# Specify that all features have real-value data

feature_columns = [contrib.layers.real_valued_column("""", dimension=4)]
# Build 3 layer DNN with 10, 20, 10 units respectively.

classifier = contrib.learn.DNNClassifier(feature_columns=feature_columns,
                                            hidden_units=[10, 20, 10],
                                            n_classes=2,
                                            model_dir=""/tmp/iris_model"")
# Fit model.

classifier.fit(x=training_set.data,
               y=training_set.target.astype(np.int),
               steps=200)
"
4682,Running tensorboard with vz-projector demo in development mode,"Is it possible to actually test the `vz-projector` element in the tensorboard components ? 
I know it's a bleeding edge feature but I'd like to give it a try to see how it works but I get the following error while running `gulp`

```
TypeScript error: components/vz-projector/data.ts(170,24): Error TS2345: Argument of type '{ metadata: { [key: string]: number | string; }; index: number; vector: number[]; projectedPoint:...' is not assignable to parameter of type 'DataPoint[]'.
  Type '{ metadata: { [key: string]: number | string; }; index: number; vector: number[]; projectedPoint:...' is not assignable to type 'DataPoint'.
    Types of property 'projections' are incompatible.
      Type '{}' is not assignable to type '{ [key: string]: number; }'.
        Index signature is missing in type '{}'.
TypeScript error: components/vz-projector/vz-projector-bookmark-panel.ts(74,5): Error TS2346: Supplied parameters do not match any signature of call target.
TypeScript error: components/vz-projector/vz-projector-bookmark-panel.ts(88,7): Error TS2339: Property 'download' does not exist on type 'HTMLAnchorElement'.
```
"
4681,restore previous checkpoint and continue training,"i am beginner in tensorflow and i have a problem of restoring my previous checkpoint, after adding a new output for my network. So what i want to do after restoring, add a new label. what is the process to do that ? any idea about that 
"
4680,./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:42: fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory,"### Environment info

Operating System: Ubuntu 14.04

I'm trying to build the pi-examples by building the tensorflow via makefile on Linux(Ubuntu 14.04).

So 1st I've done ""Building on Linux"" successfully from here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#building-on-linux

now at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/pi_examples I'm trying to build camera example, but when I've run `make -f tensorflow/contrib/pi_examples/camera/Makefile` command, the result came out as follows on terminal:

`gcc --std=c++11 -O0 -I/usr/local/include -I. -I/home/tushar/Desktop/tushar/tensorflow/tensorflow/contrib/pi_examples/camera/../../makefile/downloads -I/home/tushar/Desktop/tushar/tensorflow/tensorflow/contrib/pi_examples/camera/../../makefile/downloads/eigen-latest/ -I/home/tushar/Desktop/tushar/tensorflow/tensorflow/contrib/pi_examples/camera/../../makefile/gen/proto/ -I/home/tushar/Desktop/tushar/tensorflow/tensorflow/contrib/pi_examples/camera/../../makefile/gen/proto_text/ -c tensorflow/contrib/pi_examples/camera/camera.cc -o /home/tushar/Desktop/tushar/tensorflow/tensorflow/contrib/pi_examples/camera/gen/obj/tensorflow/contrib/pi_examples/camera/camera.o
In file included from ./tensorflow/core/framework/tensor.h:19:0,
                 from tensorflow/contrib/pi_examples/camera/camera.cc:33:
./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:42: fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory
 #include ""unsupported/Eigen/CXX11/Tensor""
                                          ^
compilation terminated.
make: *** [/home/tushar/Desktop/tushar/tensorflow/tensorflow/contrib/pi_examples/camera/gen/obj/tensorflow/contrib/pi_examples/camera/camera.o] Error 1
`

Why? could anyone confirm me that third-party library specially (eigen3) is complete or something missing.

I've also tried with replacing it's third-party eigen3 library by separately downloaded library, but that create new error, but similar to this (i.e. XYZ...   No such file or directory)
"
4677,strided_slice unexpected behaviour,"On commit `931ff427f3a55a9c6c734a4c325d6af1b53665c3`, the following happens when trying to slice a tensor with an invalid slice.

``` python
t = tf.ones([2, 3])
t[4, 0]
# <tf.Tensor 'strided_slice_10:0' shape=() dtype=float32>
t[4, 0].eval()
#0.0

# Also when calling the function directly:
tf.strided_slice(t, [4, 0], [5, 1], [1, 1], shrink_axis_mask=0b11).eval()
#0.0

# Note this is different to the numpy behaviour
a = np.ones([2, 3])
a[4, 0]
# IndexError: index 4 is out of bounds for axis 0 with size 2
```

Also, calling `tf.strided_slice` directly sometimes results in core dumps when using it with nonsensical parameters:

``` python
t = tf.ones([2, 3])
tf.strided_slice(t, [0, 0], [100, 100], [1, 1], shrink_axis_mask=0b11)
# <tf.Tensor 'StridedSlice_1:0' shape=() dtype=float32>
tf.strided_slice(t, [0, 0], [100, 100], [1, 1], shrink_axis_mask=0b11).eval()
# F tensorflow/core/kernels/strided_slice_op.cc:307] Check failed: tmp.CopyFrom(input, final_shape)
# Aborted (core dumped)
```
### Environment info

Operating System:
Ubuntu 14.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

```
-rw-r--r-- 1 root root   322936 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root       19 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root   383336 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18
-rw-r--r-- 1 root root   720192 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 May 23 17:04 /usr/local/cuda-7.5/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       17 May 23 17:03 /usr/local/cuda-7.5/lib64/libcudnn.so.5 -> libcudnn.so.5.0.5
-rwxr-xr-x 1 root root 59909104 May 23 17:02 /usr/local/cuda-7.5/lib64/libcudnn.so.5.0.5
-rw-r--r-- 1 root root 58775484 May 23 17:02 /usr/local/cuda-7.5/lib64/libcudnn_static.a
```

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
   `931ff427f3a55a9c6c734a4c325d6af1b53665c3`
2. The output of `bazel version`

```
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Jan 01 00:00:00 1970 (0)
Build timestamp: Thu Jan 01 00:00:00 1970 (0)
Build timestamp as int: 0
```
"
4675,LinearClassifier feature_columns overwritten in LinearClassifier.fit,"`tensorflow.contrib.learn.python.learn.estimators.linear.LinearClassifier.fit`
effectively returns
`tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator.fit`
`Estimator.fit` calls:
`tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator._train_model`
which has these lines:

```
      features, targets = input_fn()
      self._check_inputs(features, targets)
      train_op, loss_op = self._get_train_ops(features, targets)
```

`tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator._check_inputs`

```
    if self._features_info is not None:
      ...
    else:
      self._features_info = tensor_signature.create_signatures(features)
```

So we get to a point where features, as derived from the input_fn, is treated as our feature columns set.

In pseudo code:

```
def input_function()
    return [foo, bar, baz], quux

lc=LinearClassifier(feature_columns=[foo]) # We pass in what we want our feature column to be
lc.fit(input_function) # run fit on out input function
lc._feature_columns # repr _feature_columns on the instantiated classifier
    [foo]
lc.estimator._features_info # _features_info on the instantiated classifiers instantiated estimator
    [foo, bar, baz]
```
### The issue is:

Although this line appears to indicate that we will be making an estimation based on the feature columns supplied:
`lc=LinearClassifier(feature_columns=[foo]) # We pass in what we want our feature column to be`
What happens is that the passed in `feature_columns` is unused and instead the return from the input_function supplied to fit are used.

Am I correct in thinking that if the `feature_columns` arg is supplied that only those columns should be used by the classifiers estimator?
That when we instantiate the classifier we are setting the feature_columns we expect to be used?

The work around for this is simply to only return the columns you need from your input function however I found this misleading.

Point in the tutorial:
Either the code or the tutorial need to be changed.
https://www.tensorflow.org/versions/r0.10/tutorials/wide/index.html#defining-the-logistic-regression-model

Error raised:
`WARNING:tensorflow:Setting feature info to`
as per tensorflow/contrib/learn/python/learn/estimators/estimator.py:613
"
4674,GPU pip wheel cannot be used on a system without the GPU libraries installed,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

I found some issues like #2653 but it wasn't really helpful.
### Environment info

Operating System: Ubuntu 16.04.1

Installed version of CUDA and cuDNN: CUDA 8.0 / cuDNN 5.1.5
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

```
$ ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root   558720 Sep 30 15:48 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Sep 30 15:48 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Sep 30 15:48 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rwxr-xr-x 1 root root   415432 Sep 30 15:48 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 Sep 30 15:48 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 Sep 30 15:50 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       17 Sep 30 15:50 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxr-xr-x 1 root root 79337624 Sep 30 15:50 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Sep 30 15:50 /usr/local/cuda/lib64/libcudnn_static.a
```

If installed from binary pip package, provide:
1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

I've built TF from the source (latest r0.10 branch 931ff427f with cherry-picked 6d57860e3 (for avx2 issue)) with CUDA enabled.

```
$ bazel version
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Jan 01 00:00:00 1970 (0)
Build timestamp: Thu Jan 01 00:00:00 1970 (0)
Build timestamp as int: 0
```
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

`python3 -c ""import tensorflow; print(tensorflow.__version__)""` would normally work with GPU.

```
$ python3 -c ""import tensorflow as tf; print(tf.__version__)""
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.8.0 locally
0.10.0
$ env | grep cuda
CUDA_HOME=/usr/local/cuda
LD_LIBRARY_PATH=:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
```

However, if I unset CUDA-related variables and SET `CUDA_VISIBLE_DEVICES=-1`(for using CPU Only), TF won't work.

```
$ unset LD_LIBRARY_PATH             
$ unset CUDA_HOME           
$ CUDA_VISIBLE_DEVICES=-1 python3 -c ""import tensorflow as tf; print(tf.__version__)""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/leebc/.local/lib/python3.5/site-packages/tensorflow/__init__.py"", line 23, in <module>
    from tensorflow.python import *
  File ""/home/leebc/.local/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 48, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/leebc/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/home/leebc/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory
```
### What other attempted solutions have you tried?
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
"
4673,InvalidArgument error for space_to_batch function using half-float (fp16),"I am trying to execute atrous_conv2d operation using half-float (fp16) as input data type (float32 dataType works fine) but I get the following InvalidArgumentError:

**tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'Tower_0/conv2/atrous_conv2d/SpaceToBatch': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available**

Here is the software configuration:
OS: SLES 12
Tensor Flow: 0.10 (compiled from sources using Cuda 8.0.27 and cuDNN 5.1.5)
Python: 3.5.2
Bazel : 0.3.1

Is this a known issue? If so, in which Tensor Flow version will it be fixed?
"
4672,Can't visualize plots from multiple log directories in Tensorboard after refreshing browser. Must close reopen tab.,"### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
1. Run tensorboard for one log directory such as:
   tensorboard --logdir=some/directory1
2. View it on a browser
3. Run tensorboard for two log directories such as:
   tensorboard --logdir=somelabel1:some/directory1,somelabel2:some/directory2
4. Refresh the browser tab

Expected: two plots
Actual: only one plot
1. If you close the browser tab and then navigate to tensorboard again the two plots will be visible.
### What other attempted solutions have you tried?

Close reopen browser tab.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
"
4668,Implement tf.nn.atrous_conv2d_transpose(),"Can we have a `atrous_conv2d_transpose()` function, just like the existing `conv2d_transpose()` function? Or is there some simple way to get what I am looking for using other existing functions?

I had a look at the `conv2d_transpose()` code, and it seems shouldn't be too difficult to adapt it to get a `atrous_conv2d_transpose()`.

Thanks.
"
4664,ImportError: libcudart.so.7.5: cannot open shared object file: No such file or directory,"I'm a new user of tensorflow.I install tensorflow like this.
1.export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl
2.sudo pip install --upgrade $TF_BINARY_URL

and I succeed install tensorflow ,when I import tensorflow ,It happens that ""ImportError: libcudart.so.7.5: cannot open shared object file: No such file or directory"".It's crazy,I find some solution ,that my shell environment variables should do like this:
1.export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/local/cuda/lib64""
2.export CUDA_HOME=/usr/local/cuda
but it not works.
I find there is not libcudart.so.7.5 in ""/usr/local/cuda/lib64"",it's only ""libcudart so.8.0""

os:ubuntu 16.04 LTS,cuda:8.0
"
4663,Control dependency on identity containing assign not working,"I'm running Tensorflow 0.10.

The following code 

``` python
import tensorflow as tf

x = tf.Variable(0, dtype=tf.int32)

old_val = tf.identity(x)
with tf.control_dependencies([old_val]):
    new_val = tf.assign(x, x + 1)

with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())

    for i in xrange(3):
        print sess.run([old_val, new_val, x])
```

outputs

```
[1, 1, 1]
[2, 2, 2]
[3, 3, 3]
```

From reading the docs on `control_dependencies` and `identity` as well as StackOverflow, I expected output

```
[0, 1, ?]
[1, 2, ?]
[2, 3, ?]
```

where `?` indicates that the variable value is unspecified.

Is this a bug? If this is not a bug, what is the correct way to refer to the value of variable before and after assignment in a single graph?
"
4662,./configure should fail if the bazel commands fail,"The configure script now calls bazel clean and bazel fetch (as of ~Aug 2016).  If they fail, the script should exit with a nonzero exit status rather than continuing onwards.  This is important for automated build systems to detect failed configure calls.
"
4661,Defun grad_func should accept the outputs of the function being differentiated,"When dealing with discrete stochastic units, one often wants to define custom gradients. It seems like the most legitimate way to it in Tensorflow is to use `grad_func` keyword argument of `function.Defun` from `tensorflow.python.framework`.  However, the gradient function is only given the inputs of the function defined and the gradients, whereas it would be often very convenient to also use the outputs. 

For example, see the code below.

``` python
# In current Tensorflow I have to do like this:

@function.Defun(*(3 * [tf.float32]))
def oneovertwo_unit_bprop(probs, noise, grads):
  outcome = tf.to_float(tf.less(noise, probs))
  outcome_probs = outcome * probs + (1 - outcome) * (1 - probs)
  return grads / 2. / outcome_probs, tf.zeros_like(noise)


@function.Defun(*(2 * [tf.float32]), grad_func=oneovertwo_unit_bprop)
def oneovertwo_unit_fprop(probs, noise):
  return tf.to_float(tf.less(noise, probs))


def oneovertwo_unit(probs):
  return oneovertwo_unit_fprop(probs, tf.random_uniform(tf.shape(probs)))

# I would like to do like this

def oneovertwo_unit_bprop(probs, outcome, grads):
  outcome_probs = outcome * probs + (1 - outcome) * (1 - probs)
  return grads / 2. / outcome_probs


@function.Defun(*(2 * [tf.float32]), grad_func=oneovertwo_unit_bprop)
def oneovertwo_unit(probs):
  return tf.to_float(tf.less(tf.random_uniform(probs.get_shape()), probs))
```

FYI, this an implementation of the gradient estimator from the DARN paper (https://arxiv.org/pdf/1310.8499v2.pdf).
"
4659,"Missing python packages from Dockerfile, scikit-learn, pyreadline","I am using the Dockerfile provided in this project to do the Udacity assignments. 

The first example of the Udacity DL fails at this line:
   ""from sklearn.linear_model import LogisticRegression""

The iPython notebook also doesn't support tab-autocomplete. 

The fix is to add the two missing python packages in the Dockerfile, line 32, 33:

26 RUN pip --no-cache-dir install \
 27         ipykernel \
 28         jupyter \
 29         matplotlib \
 30         numpy \
 31         scipy \
 32         scikit-learn \
 33         pyreadline \
 34         && \
 35     python -m ipykernel.kernelspec
### Environment info

Operating System:
Mac OS 10.11.6
"
4658,Change variable name  not take effect in checkpoints,"I want to load into one session different checkpoints of the same model. Like take step 10 and step 100
In order to avoid conflict name, I try to modify the variable name first, like change from 
show_and_tell/model_init/emb:0 to show_and_tell_1/model_init/emb:0,
my input checkpoint with all varaibles starts with show_and_tell, and I want to modify it to show_and_tell_1
so I can load the two models at once.
Code like below, but the savec checkpoint.meta shows still show_and_tell/model_init/emb:0, what's wrong?

def reset_model_top_scope():
    sess = tf.InteractiveSession()
    meta_filename = ""."".join([input_checkpoint, ""meta""])
    saver = tf.train.import_meta_graph(meta_filename)
    saver.restore(sess, input_checkpoint)
    scope = FLAGS.scope
    out_scope = FLAGS.out_scope if FLAGS.out_scope else '%s_%d'%(scope, FLAGS.out_index)
    output_checkpoint = FLAGS.output_checkpoint 
    with tf.variable_scope(scope) as topscope:
        src_vars =[v for v in tf.all_variables() if v.name.startswith(topscope.name)]
    out_vars = {out_scope + v.name[len(scope):v.name.rfind(':')]:v for v in src_vars}
    tf.train.Saver(var_list=out_vars).save(sess, output_checkpoint)
    sess.close()
"
4657,arg_scope overwrite fails for positional arguments,"The code

```
x = tf.placeholder(tf.float32, [None, 10])
with slim.arg_scope([slim.fully_connected], num_outputs=20):
    y = slim.fully_connected(x, 30)
```

raises

```
TypeError: fully_connected() got multiple values for argument 'num_outputs'
```

There is an easy workaround, namely changing `30` to `num_outputs=30` seems to work fine.  I'm not sure if this is easy to fix or was even intended to work.
"
4656,Missing documentation,"Hi,

The documentation for tf.contrib.learn.LinearRegressor.weights_ is missing. Can you update what the values returned by the above param means?

Thank,
Sumit
"
4655,expend the output of CNN,"I am trying to expend the outputs of my network from 11 to 12 outputs, i have restored the previous checkpoint that is already retrained in 11 outputs. I got the answer from  here http://stackoverflow.com/questions/34913762/how-to-expand-a-tensorflow-variable in This question told me how to change the shape of the variable, to expand it to fit another row of weights, but I don't know if i initialize the weight and biases in the right way. Actually i don't have error but the the test accuracy is decreased from 95% to 9%. may be there is somewhere wrong issue in the code. that's the code:

```
 w_b_not = {
  'weight_4': tf.Variable(tf.random_normal([num_hidden, num_labels], stddev=0.1)),
  'bias_4'  : tf.Variable(tf.constant(1.0, shape=[num_labels])),}

 w_b = {
  'wc1_0': tf.Variable(tf.random_normal([patch_size_1, patch_size_1, num_channels, depth],stddev=0.1)),
   .....
  'bc1_0' : tf.Variable(tf.zeros([depth]))}

 .... #here is the networks model

 num_steps = 1001 
 with tf.Session(graph=graph) as sess:
    ckpt = ('path_of_checkpoint.ckpt')
    if os.path.isfile(ckpt) :
       layer6_weights = tf.Variable(tf.random_normal([num_hidden, num_labels], stddev=0.1))
       layer6_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))

  n_w_b = {
  'new_layer_weights' : tf.concat(0,[w_b_not['weight_4'], layer6_weights]),
  'new_layer_biases' : tf.concat(0,[w_b_not['bias_4'], layer6_biases])}
  resize_var_1 = tf.assign(w_b_not['weight_4'], n_w_b['new_layer_weights'], validate_shape=False)
  resize_var_2 = tf.assign(w_b_not['bias_4'], n_w_b['new_layer_biases'], validate_shape=False)
  logits = tf.get_collection('logits')[0]
  w_b_new_saver = tf.train.Saver()
  init_op = tf.initialize_all_variables()        
  w_b_saver.restore(sess, ckpt)
  print(""restore complete"")
  for step in xrange(num_steps):
    sess.run(init_op)
  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval() , test_labels,  force = False ))  
```
"
4654,expend the output of CNN,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
### Environment info

Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:
1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
### What other attempted solutions have you tried?
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
"
4653,sysmalloc in session.run(tf.initialize_all_variables()) - cuda 8.0 + cudnn 5/5.1 + tensorflow 0.9/0.10,"Hi all
I recently bought a new Titan X Pascal, and needed to upgrade cuda from 7 to 8.
After updating cuda 7 + cudnn 4 + tensorflow 0.9 -> cuda 8 + cudnn 5 + tensorflow 0.10, my code which used to work fine now crashes with a segfault : 
- with my GPUs on : 

```
/usr/bin/python2.7 /work/repos/qs-lab-cornerdetection/model_noclass.py
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:03:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x30882d0
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:04:00.0
Total memory: 11.92GiB
Free memory: 11.81GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1
I tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:04:00.0)
python2.7: malloc.c:2372: sysmalloc: Assertion `(old_top == (((mbinptr) (((char *) &((av)->bins[((1) - 1) * 2])) - __builtin_offsetof (struct malloc_chunk, fd)))) && old_size == 0) || ((unsigned long) (old_size) >= (unsigned long)((((__builtin_offsetof (struct malloc_chunk, fd_nextsize))+((2 *(sizeof(size_t))) - 1)) & ~((2 *(sizeof(size_t))) - 1))) && ((old_top)->size & 0x1) && ((unsigned long) old_end & pagemask) == 0)' failed.

Process finished with exit code 134 (interrupted by signal 6: SIGABRT)
```
- if I restrict to 1 detected device with `os.environ['CUDA_VISIBLE_DEVICES'] = '1'`, it still crashes (with any of my 2 gpus) but with a different signal code: 

```
/usr/bin/python2.7 /work/repos/qs-lab-cornerdetection/model_noclass.py
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:04:00.0
Total memory: 11.92GiB
Free memory: 11.81GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:04:00.0)

Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)
```
- If finally I restrict to CPU only, it works fine.

I verified, both host and docker have the same driver version (367.48 ), cuda is installed correctly (on my host AND in my docker), nvdia-smi correctly finds the 2 GPUs, deviceQuery runs fine (also on the host and inside the docker), cudnn libraries are there, and moreover caffe works just fine on the gpus, which would indicate that my install is indeed correct.

The problem persists with cudnn 5.1, and on both version of cudnn if i rollback to tensorflow 0.9
### Environment info

Operating System:
Docker ubuntu 14.04

Installed version of CUDA and cuDNN: 

```
-rw-r--r-- 1 root root   558720 Sep 29 09:18 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Sep 29 09:18 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       38 Sep 29 10:01 /usr/local/cuda/lib64/libcudart.so.7.5 -> /usr/local/cuda/lib64/libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Sep 29 09:18 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rwxr-xr-x 1 root root   415432 Sep 29 09:18 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 Sep 29 09:18 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 1000 1000       13 Apr 25 01:19 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 1000 1000       17 Apr 25 01:19 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.0.5
-rwxr-xr-x 1 1000 1000 78065952 Apr 22 19:17 /usr/local/cuda/lib64/libcudnn.so.5.0.5
```

But the problem remains with cudnn 5.1

If installed from binary pip package, provide:
1. A link to the pip package you installed:
   `http://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl`

(but the problem reproduces with tensorflow 0.9)
1. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

```
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
0.10.0
```
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Complicated at this point, but I'm not sure its relevant...
### What other attempted solutions have you tried?

Tried with cudnn-5 and cudnn 5.1, and tensorflow 0.9 / 0.10
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
"
4652,tensorflow running principle ,"1. In tensorflow/python/ops/seq2seq_model.py  line 236 , we see the output_feed cosists of  self.updates and self.gradient_norms.
   And the self.updates is not used in anywhere, I wonder if I can delete this from output_feed ?
2. How does the tensorflow run:  If a variable is not in its output_feed list, does tensorflow count this variable ?
"
4651,Distributed tensorflow hangs ,"I set up a distributed tensorflow according to the inception example(https://github.com/tensorflow/models/tree/master/inception/inception), with a very deep network.
During training(minutes or hours), it must run into hanging. No error occurs in ps or workers, but the cpu/gpu usage falls to zero and no any further steps are made.
If I replace the network with a much simpler one(much less variables), all things work fine.
Is there any limit on the amount of vars placed on ps? or any suggestions to deal with the problem?

Setup: tensorflow r0.10 build on cuda 7.5 and cudnn v5 
"
4650,Could not create TensorFlow Graph: Not found: ~/graphs/inception/tensorflow_inception_graph.pb,"### Environment info

Operating System: Ubuntu 14.04

**URL:** https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#building-on-linux

I'm following the ""Building on Linux"", however all commands run fine, but when I've run this following command
`tensorflow/contrib/makefile/gen/bin/benchmark \
 --graph=~/graphs/inception/tensorflow_inception_graph.pb`

**Output** on terminal is as following:

`I tensorflow/tools/benchmark/benchmark_model.cc:202] Graph: [~/graphs/inception/tensorflow_inception_graph.pb]
 I tensorflow/tools/benchmark/benchmark_model.cc:203] Input layer: [input:0]
 I tensorflow/tools/benchmark/benchmark_model.cc:204] Input shape: [1,224,224,3]
 I tensorflow/tools/benchmark/benchmark_model.cc:205] Input type: [float]
 I tensorflow/tools/benchmark/benchmark_model.cc:206] Output layer: [output:0]
 I tensorflow/tools/benchmark/benchmark_model.cc:207] Num runs: [50]
 I tensorflow/tools/benchmark/benchmark_model.cc:208] Inter-run delay (seconds): [-1.0]
 I tensorflow/tools/benchmark/benchmark_model.cc:209] Num threads: [-1]
 I tensorflow/tools/benchmark/benchmark_model.cc:210] Benchmark name: []
 I tensorflow/tools/benchmark/benchmark_model.cc:211] Output prefix: []
 I tensorflow/tools/benchmark/benchmark_model.cc:212] Show sizes: [0]
 I tensorflow/tools/benchmark/benchmark_model.cc:51] Loading TensorFlow.
 I tensorflow/tools/benchmark/benchmark_model.cc:58] Got config, 0 devices
 E tensorflow/tools/benchmark/benchmark_model.cc:64] Could not create TensorFlow Graph: Not found: ~/graphs/inception/tensorflow_inception_graph.pb`

Why at last TensorFlow Graph is not created or found at ~/graphs/inception/tensorflow_inception_graph.pb even that file is there. 
"
4649,build failed,"Build failed with 

```
bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```

,but

```
bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
```

 works. Any suggestions? Thanks

Here are the error messages:

```
ERROR: /data1/wenhaojiang/.cache/bazel/_bazel_wenhaojiang/4d6dbdb415b7bb548255a6ccf48e22e6/external/protobuf/BUILD:73:1: undeclared inclusion(s) in rule '@protobuf//:protobuf_lite':
this rule is missing dependency declarations for the following files included by 'external/protobuf/src/google/protobuf/stubs/once.cc':
  '/include/c++/4.8.2/string'
  '/include/c++/4.8.2/x86_64-redhat-linux/bits/c++config.h'
  '/include/c++/4.8.2/x86_64-redhat-linux/bits/os_defines.h'
  '/include/c++/4.8.2/x86_64-redhat-linux/bits/cpu_defines.h'
  '/include/c++/4.8.2/bits/stringfwd.h'
  '/include/c++/4.8.2/bits/memoryfwd.h'
  '/include/c++/4.8.2/bits/char_traits.h'
  '/include/c++/4.8.2/bits/stl_algobase.h'
  '/include/c++/4.8.2/bits/functexcept.h'
  '/include/c++/4.8.2/bits/exception_defines.h'
  '/include/c++/4.8.2/bits/cpp_type_traits.h'
  '/include/c++/4.8.2/ext/type_traits.h'
  '/include/c++/4.8.2/ext/numeric_traits.h'
  '/include/c++/4.8.2/bits/stl_pair.h'
  '/include/c++/4.8.2/bits/move.h'
  '/include/c++/4.8.2/bits/concept_check.h'
  '/include/c++/4.8.2/type_traits'
  '/include/c++/4.8.2/bits/stl_iterator_base_types.h'
  '/include/c++/4.8.2/bits/stl_iterator_base_funcs.h'
  '/include/c++/4.8.2/debug/debug.h'
  '/include/c++/4.8.2/bits/stl_iterator.h'
  '/include/c++/4.8.2/bits/postypes.h'
  '/include/c++/4.8.2/cwchar'
  '/lib/gcc/x86_64-redhat-linux/4.8.2/include/stdarg.h'
  '/lib/gcc/x86_64-redhat-linux/4.8.2/include/stddef.h'
  '/include/c++/4.8.2/cstdint'
  '/lib/gcc/x86_64-redhat-linux/4.8.2/include/stdint.h'
  '/include/c++/4.8.2/bits/allocator.h'
  '/include/c++/4.8.2/x86_64-redhat-linux/bits/c++allocator.h'
  '/include/c++/4.8.2/ext/new_allocator.h'
  '/include/c++/4.8.2/new'
  '/include/c++/4.8.2/exception'
  '/include/c++/4.8.2/bits/atomic_lockfree_defines.h'
  '/include/c++/4.8.2/bits/exception_ptr.h'
  '/include/c++/4.8.2/bits/nested_exception.h'
  '/include/c++/4.8.2/bits/localefwd.h'
  '/include/c++/4.8.2/x86_64-redhat-linux/bits/c++locale.h'
  '/include/c++/4.8.2/clocale'
  '/include/c++/4.8.2/iosfwd'
  '/include/c++/4.8.2/cctype'
  '/include/c++/4.8.2/bits/ostream_insert.h'
  '/include/c++/4.8.2/bits/cxxabi_forced.h'
  '/include/c++/4.8.2/bits/stl_function.h'
  '/include/c++/4.8.2/backward/binders.h'
  '/include/c++/4.8.2/bits/range_access.h'
  '/include/c++/4.8.2/bits/basic_string.h'
  '/include/c++/4.8.2/ext/atomicity.h'
  '/include/c++/4.8.2/x86_64-redhat-linux/bits/gthr.h'
  '/include/c++/4.8.2/x86_64-redhat-linux/bits/gthr-default.h'
  '/include/c++/4.8.2/x86_64-redhat-linux/bits/atomic_word.h'
  '/include/c++/4.8.2/initializer_list'
  '/include/c++/4.8.2/ext/string_conversions.h'
  '/include/c++/4.8.2/cstdlib'
  '/include/c++/4.8.2/cstdio'
  '/include/c++/4.8.2/cerrno'
  '/include/c++/4.8.2/bits/functional_hash.h'
  '/include/c++/4.8.2/bits/hash_bytes.h'
  '/include/c++/4.8.2/bits/basic_string.tcc'
  '/include/c++/4.8.2/cstddef'
  '/lib/gcc/x86_64-redhat-linux/4.8.2/include/limits.h'
  '/lib/gcc/x86_64-redhat-linux/4.8.2/include/syslimits.h'
  '/include/c++/4.8.2/utility'
  '/include/c++/4.8.2/bits/stl_relops.h'.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 3.151s, Critical Path: 0.87s
```

CUDA 7.5
gcc -v

```
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/4.8.2/lto-wrapper
Target: x86_64-redhat-linux
Configured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-languages=c,c++,objc,obj-c++,java,fortran,ada,go,lto --enable-plugin --enable-initfini-array --disable-libgcj --with-isl=/builddir/build/BUILD/gcc-4.8.2-20140120/obj-x86_64-redhat-linux/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.8.2-20140120/obj-x86_64-redhat-linux/cloog-install --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linux
Thread model: posix
gcc version 4.8.2 20140120 (Red Hat 4.8.2-16) (GCC)
```

bazel version

```
Build label: 0.3.1-jdk7
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jul 29 09:07:03 2016 (1469783223)
Build timestamp: 1469783223
```

output of `echo | gcc -E -xc++ - -v`:

```
Using built-in specs.
COLLECT_GCC=gcc
Target: x86_64-redhat-linux
Configured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-languages=c,c++,objc,obj-c++,java,fortran,ada,go,lto --enable-plugin --enable-initfini-array --disable-libgcj --with-isl=/builddir/build/BUILD/gcc-4.8.2-20140120/obj-x86_64-redhat-linux/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.8.2-20140120/obj-x86_64-redhat-linux/cloog-install --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linux
Thread model: posix
gcc version 4.8.2 20140120 (Red Hat 4.8.2-16) (GCC)
COLLECT_GCC_OPTIONS='-E' '-v' '-mtune=generic' '-march=x86-64'
 /usr/libexec/gcc/x86_64-redhat-linux/4.8.2/cc1plus -E -quiet -v -D_GNU_SOURCE - -mtune=generic -march=x86-64
ignoring nonexistent directory ""/usr/lib/gcc/x86_64-redhat-linux/4.8.2/include-fixed""
ignoring nonexistent directory ""/usr/lib/gcc/x86_64-redhat-linux/4.8.2/../../../../x86_64-redhat-linux/include""
#include ""..."" search starts here:
#include <...> search starts here:
 /usr/lib/gcc/x86_64-redhat-linux/4.8.2/../../../../include/c++/4.8.2
 /usr/lib/gcc/x86_64-redhat-linux/4.8.2/../../../../include/c++/4.8.2/x86_64-redhat-linux
 /usr/lib/gcc/x86_64-redhat-linux/4.8.2/../../../../include/c++/4.8.2/backward
 /usr/lib/gcc/x86_64-redhat-linux/4.8.2/include
 /usr/local/include
 /usr/include
End of search list.
#1 ""<stdin>""
#1 ""<command-line>""
#1 ""/usr/include/stdc-predef.h"" 1 3 4
#1 ""<command-line>"" 2
#1 ""<stdin>""
COMPILER_PATH=/usr/libexec/gcc/x86_64-redhat-linux/4.8.2/:/usr/libexec/gcc/x86_64-redhat-linux/4.8.2/:/usr/libexec/gcc/x86_64-redhat-linux/:/usr/lib/gcc/x86_64-redhat-linux/4.8.2/:/usr/lib/gcc/x86_64-redhat-linux/
LIBRARY_PATH=/usr/lib/gcc/x86_64-redhat-linux/4.8.2/:/usr/lib/gcc/x86_64-redhat-linux/4.8.2/../../../../lib64/:/lib/../lib64/:/usr/lib/../lib64/:/usr/lib/gcc/x86_64-redhat-linux/4.8.2/../../../:/lib/:/usr/lib/
COLLECT_GCC_OPTIONS='-E' '-v' '-mtune=generic' '-march=x86-64'
```
"
4648,tf.contrib.learn.estimator each call to predict() reloads the model,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

None
### Environment info

Operating System:
OSX & Ubuntu 14.04
### If possible, provide a minimal reproducible example - Explanation

Each call to estimator.predict() completely reloads the model, which is very slow. Is there any mechanism to maintain the model in GPU/memory for consecutive calls? 

I understand that large arrays can be passed to the function for simultaneous predictions, but my use case is for slowly arriving data which must be processed quickly. Is there a mechanism for decreasing the 'start-up' time of this function?
### What other attempted solutions have you tried?

Looked through API docs & source code for alternative solutions
###### 
"
4643,Inf/Nan in MSE/CE with very simple demo,"Hi there,

I would use a very simple LSTM demo with only forward without any BP tuning to clear the question. I pass one random batch of size (2,2,3) to a rnn_cell.LSTMcell, using 100 hidden layer, and output the mse/ce between the output and the last time step (:, -1, :). What strike me is the both the SE/CE are very strange (too large or even inf).

Below is my code.

```
import tensorflow as tf
import numpy as np


n_steps = 2
n_visible = 3
n_hidden = 100

x = tf.placeholder(tf.float64, [None, n_steps, n_visible])
x_ = tf.placeholder(tf.float64, [None, n_steps, n_visible])
batch_size = tf.placeholder(tf.int32)

low = - 4 * np.sqrt(6. / (n_hidden + n_visible))
high = 4 * np.sqrt(6. / (n_hidden + n_visible))


W2 = tf.Variable(tf.random_uniform([n_hidden ,n_visible], minval=low, maxval=high, dtype=tf.float64))

b2 = tf.Variable(tf.zeros(n_visible, dtype = tf.float64))


lstm_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, state_is_tuple = True, forget_bias=1.)
initial_state = lstm_cell.zero_state(batch_size, tf.float64)
state = initial_state

inputs = [tf.squeeze(input_, [1])  for input_ in tf.split(1, n_steps, x)]
encoder = tf.nn.rnn(lstm_cell, inputs, initial_state=initial_state)
decoder = tf.nn.sigmoid(tf.matmul(encoder[-1][0], W2) + b2)

ce = -x[:,-1,:] * tf.log(decoder)+(1-x[:,-1,:])*tf.log(1 - decoder)
se = (x[:,-1,:] - decoder)**2
mse = tf.reduce_mean(se)
rmse = tf.sqrt(mse)


sess = tf.Session()
sess.run(tf.initialize_all_variables())
x_train_batch = np.random.rand(2,2,3)
feed_dict = {x: x_train_batch, x_:x_train_batch, batch_size: 2}
encoder_, decoder_, se_, mse_, rmse_, ce_=  sess.run([encoder, decoder, se, mse, rmse, ce], feed_dict=feed_dict)

```

The outputs are:

```
ce_
Out[2]: 
array([[ -9.30480083e-001,  -7.45718134e-001,   1.65488117e+243],
       [ -8.92276217e-001,  -7.66764460e-001,   5.43823895e+064]])

se_
Out[3]: 
array([[  3.66794556e-001,   2.76262193e-001,               inf],
       [  3.48428030e-001,   2.86745693e-001,   1.23185516e+129]])

decoder_
Out[4]: 
array([[ 0.60563566,  0.5256065 ,  0.62439037],
       [ 0.59027793,  0.53548641,  0.69399984]])

x_train_batch[:,-1,:]
Out[5]: 
array([[ 0.93217308,  0.28895401,  0.31784797],
       [ 0.57656244,  0.73494513,  0.57125778]])

```

The value of decoder and x seems well, but why ce_ and se_ has inf and some weird value like 1.23E+129?

It is running on a Mac OS EI Captain 64 bit without GPU support.

```
tf.__version__
Out[6]: '0.10.0'
```
"
4641,"bazel build succeeds, but bazel-mobile install fails","The build succeeds on OS 10.11.6, with bazel version 0.3.1-homebrew. However, the bazel mobile-install fails. This is not a TensorFlow capitalization problem or a camera permissions issue! 
Thanks || 

bazel mobile-install //tensorflow/examples/android:tensorflow_demo --verbose_failures
## The main error is listed below:

ERROR: /private/var/tmp/_bazel_kg/16cb3f28d531847b42ceb660896dbffa/external/bazel_tools/tools/android/BUILD:132:1: Converting to Python 3: external/bazel_tools/tools/android/incremental_install.py failed: 2to3 failed: error executing command 
  (cd /private/var/tmp/_bazel_kg/16cb3f28d531847b42ceb660896dbffa/execroot/tensorflow && \
  exec env - \
  bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/host/genfiles/python3/external/bazel_tools/tools/android --write-unchanged-files external/bazel_tools/tools/android/incremental_install.py): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
Target //tensorflow/examples/android:tensorflow_demo failed to build
## INFO: Elapsed time: 0.396s, Critical Path: 0.15s

WARNING: Bazel Android NDK crosstools are based on Android NDK revision 11. The revision of the Android NDK given in android_ndk_repository rule 'androidndk' is '12.1.2977051'.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/debug:debug_graph_utils.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/debug:debug_graph_utils.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_patch_3d.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/kg/tensorflow/tensorflow/examples/android/BUILD:56:12: in srcs attribute of android_binary rule //tensorflow/examples/android:tensorflow_demo: please do not import '//tensorflow/contrib/android:java/org/tensorflow/contrib/android/TensorFlowInferenceInterface.java' directly. You should either move the file to this package or depend on an appropriate rule there.
INFO: Found 1 target...
ERROR: /private/var/tmp/_bazel_kg/16cb3f28d531847b42ceb660896dbffa/external/bazel_tools/tools/android/BUILD:132:1: Converting to Python 3: external/bazel_tools/tools/android/incremental_install.py failed: 2to3 failed: error executing command 
  (cd /private/var/tmp/_bazel_kg/16cb3f28d531847b42ceb660896dbffa/execroot/tensorflow && \
  exec env - \
  bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/host/genfiles/python3/external/bazel_tools/tools/android --write-unchanged-files external/bazel_tools/tools/android/incremental_install.py): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
Target //tensorflow/examples/android:tensorflow_demo failed to build
INFO: Elapsed time: 0.396s, Critical Path: 0.15s

NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
### Environment info

Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:
1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
### What other attempted solutions have you tried?
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
"
4640,Building TensorFlow for iOS failed on macOS Sierra and xCode 8.0,"When running `compile_ios_protobuf.sh`, it says: 
checking whether we are cross compiling... configure: error: in '.../tensorflow/tensorflow/contrib/makefile/downloads/protobuf':
configure: error: cannot run C compiled programs.
If you meant to cross compile, use `--host'.
See`config.log' for more details
"
4639,Implement NumPy style boolean indexing,"See NumPy's documentation
http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#boolean-array-indexing

We currently support basic indexing http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#basic-slicing-and-indexing.

Broken off of #206.
"
4638,Implement advanced indexing (and mixed basic/advanced),"NumPy style advanced indexing is documented here http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing

We currently support basic indexing http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#basic-slicing-and-indexing
using StridedSlice.

e.g.

foo = Some tensor
idx1=[1,2,3]
idx2=[3,4,5]
foo[idx1, idx2, 3]
"
4636,tf.contrib.learn Quickstart: float64 Warning,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
### Environment info

Operating System:

Fedora release 23 (Twenty Three)

Installed version of CUDA and cuDNN: 

GPU does not support computation :. no CUDA
NVIDIA Corporation GM107 [GeForce GTX 745]

(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

ls: No match.

If installed from binary pip package, provide:
1. A link to the pip package you installed:
   Linux CPU Python 2.7
   https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
   0.10.0

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
1. Copy Past code from tf.contrib.learn Quickstart
   https://www.tensorflow.org/versions/r0.10/tutorials/tflearn/index.html#tf-contrib-learn-quickstart
2. Run the Code
3. observe the float64 Warnings.  
   Note:  replace 'load_csv()' with 'load_csv_with_header()' produces the correct Prediction. but float64 warnings remain.

Is there a way to work around this and remove the warnings?
### What other attempted solutions have you tried?

Tried 
in 'training_set' and 'test_set'
target_dtype = np.int32 ; np.float32; tf.int32; tf.float32

in 'new_samples'
dtype = np.int32 ; np.float32; tf.int32; tf.float32

feature_columns = tf.cast(feature_columns, tf.float32)
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).

$ python confErr.py
WARNING:tensorflow:load_csv (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed after 2016-09-15.
Instructions for updating:
Please use load_csv_{with|without}_header instead.
WARNING:tensorflow:load_csv (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed after 2016-09-15.
Instructions for updating:
Please use load_csv_{with|without}_header instead.
WARNING:tensorflow:Using default config.
WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.
WARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float64, shape=TensorShape([Dimension(None), Dimension(4)]), is_sparse=False)
WARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False)
WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.
WARNING:tensorflow:Given features: Tensor(""input:0"", shape=(?, 4), dtype=float64), required signatures: TensorSignature(dtype=tf.float64, shape=TensorShape([Dimension(None), Dimension(4)]), is_sparse=False).
WARNING:tensorflow:Given targets: Tensor(""output:0"", shape=(?,), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False).
Accuracy: 0.966667
WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.
Predictions: [1 1]
"
4635,Performance and memory usage improvements,"Is the tensorflow computational graph a good working unit for scaling the performance? Do we need an another layer of Data Flow that can help do the resource management efficiently?
"
4632,Random Uniform distribution cannot have Batch as a dimension,"`tf.random_uniform((None, 10))`
Results in an error 
TypeError: Expected binary or unicode string, got (None, 10)

Likewise
`tf.random_uniform((-1, 10))`
Results in an error
ValueError: Dimension -1 must be >= 0

Although you could pass the random matrix in as a placeholder, this is awkward..
"
4629,C++ ShapeInference / ShapeRefiner use without PlaceHolder/ConstantOp support,"The ShapeInference for C++ engine doesn't support the PlaceholderOp.  Is this because it isn't finished, or am I missing something?

It seems to be that you can't add a node to the shape inference engine without having added its dependencies.  Since ConsantOp and PlaceholderOp do not have a shape inference functions, I don't understand how the ShapeRefiner can have any nodes added to it.
"
4628,Compiled static library has different name than given in README.md,"At url: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile in Building on Linux section of README.md file, when we run `tensorflow/contrib/makefile/build_all_linux.sh` command, then the build file name written as following :
""This should compile a static library in `tensorflow/contrib/makefile/gen/lib/tf_lib.a` ""

But when I tried to run `tensorflow/contrib/makefile/build_all_linux.sh` command, then it's generated file have following name:
`/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a`

Could anyone confirm me that why they are different? does docs have not updated name or something else.
"
4626,TensorFlow 0.10.0rc fails to detects CUDA device on my Macbook Pro (with nvidia 750M graphic card),"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
- This one is related, but it is for Ubuntu
  https://github.com/tensorflow/tensorflow/issues/2882
- This one is related also
  https://github.com/tensorflow/tensorflow/issues/2940#issuecomment-238952433
### Environment info

Operating System: mac os 10.12

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

```
ls -l /usr/local/cuda/lib/libcud*
-rwxr-xr-x@ 1 root   admin   8.1K Sep 28 14:39 /usr/local/cuda/lib/libcuda.1.dylib*
-rwxr-xr-x@ 1 root   wheel   8.1K Apr 13 08:02 /usr/local/cuda/lib/libcuda.dylib*
lrwxr-xr-x@ 1 root   wheel    45B Apr 13 08:03 /usr/local/cuda/lib/libcudadevrt.a@ -> /Developer/NVIDIA/CUDA-7.5/lib/libcudadevrt.a
lrwxr-xr-x@ 1 root   wheel    50B Apr 13 08:03 /usr/local/cuda/lib/libcudart.7.5.dylib@ -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart.7.5.dylib
lrwxr-xr-x@ 1 root   wheel    46B Apr 13 08:03 /usr/local/cuda/lib/libcudart.dylib@ -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart.dylib
lrwxr-xr-x@ 1 root   wheel    49B Apr 13 08:03 /usr/local/cuda/lib/libcudart_static.a@ -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart_static.a
-rwxr-xr-x@ 1 qdang  staff    56M Apr 23 02:19 /usr/local/cuda/lib/libcudnn.5.dylib*
lrwxr-xr-x@ 1 qdang  staff    16B Apr 23 04:10 /usr/local/cuda/lib/libcudnn.dylib@ -> libcudnn.5.dylib
-rw-r--r--@ 1 qdang  staff    53M Apr 23 02:19 /usr/local/cuda/lib/libcudnn_static.a
```

If installed from binary pip package, provide:
1. A link to the pip package you installed:
   `TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow-0.10.0-py2-none-any.whl`
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

```
python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.dylib locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.dylib locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.dylib locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.1.dylib locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.dylib locally
0.10.0
```

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

I just run the `iris_monitor.py` example at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/monitors and got the error

```
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.dylib locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.dylib locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.dylib locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.1.dylib locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.dylib locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_NO_DEVICE
```
### What other attempted solutions have you tried?

I ran torch on my GPU successfully before, so I could confirm that GPU computing is possible on my computer.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
"
4624,Meta Graph Import not working when used with outputs_collections,"### OS info

```
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=15.10
DISTRIB_CODENAME=wily
DISTRIB_DESCRIPTION=""Ubuntu 15.10""
NAME=""Ubuntu""
VERSION=""15.10 (Wily Werewolf)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 15.10""
VERSION_ID=""15.10""

KERNEL: Linux 4.2.0-16-generic x86_64
```
### Cuda Version:

```
-rw-r--r-- 1 root root 189170 Nov 30  2015 /usr/local/cuda/lib/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Nov 30  2015 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19 Nov 30  2015 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 311596 Nov 30  2015 /usr/local/cuda/lib/libcudart.so.7.5.18
-rw-r--r-- 1 root root 558020 Nov 30  2015 /usr/local/cuda/lib/libcudart_static.a
```
### Tensorflow pip version:

https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp34-cp34m-linux_x86_64.whl
### Python `tensorflow.__version__` output

```
0.10.0
```
### Minimal Reproducible code

```
import tensorflow as tf
from tensorflow.contrib import slim

tf_chk = ""model_ckpt-0""
tf_meta = ""model_ckpt-0.meta""

images = tf.constant(1.2, tf.float32, shape=[100, 100])
with slim.arg_scope(
        [slim.fully_connected],
        outputs_collections=[tf.GraphKeys.ACTIVATIONS]): ## NOT working
        # outputs_collections=[]): ## This works
    logits = slim.fully_connected(images, 100, scope='logits')

init = tf.initialize_all_variables()

with tf.Session() as sess:
    sess.run(init)
    saver = tf.train.Saver()
    saver.save(sess, tf_chk)

with tf.Graph().as_default():
    with tf.Session() as sess:
        new_saver = tf.train.import_meta_graph(tf_meta)
        new_saver.restore(sess, tf_chk)
```
### Description

Code above will crash the program with the following stack trace:

```
Traceback (most recent call last):
  File ""test.py"", line 24, in <module>
    new_saver = tf.train.import_meta_graph(tf_meta)
  File ""/home/x/.local/lib/python3.4/site-packages/tensorflow/python/training/saver.py"", line 1458, in import_m
eta_graph
    return _import_meta_graph_def(read_meta_graph_file(meta_graph_or_file))
  File ""/home/x/.local/lib/python3.4/site-packages/tensorflow/python/training/saver.py"", line 1369, in _import_
meta_graph_def
    col_op = ops.get_default_graph().as_graph_element(value)
  File ""/home/x/.local/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 2392, in as_graph_
element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/home/x/.local/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 2452, in _as_graph
_element_locked
    ""graph."" % repr(name))
KeyError: ""The name 'logits' refers to an Operation not in the graph.""
```

However, if I create _outputs_collections_ with the empty list, the code will
work. Also, the code will work if I don't specify the _outputs_collections_
argument.
"
4623,examples/image_retraining examples/android,"hi guys:
i want to use a retraining model on the android device, and i moved the model.pb(retraining Inception v3 model ) and label.txt(containing two labels) to the examples/android/assets, but i got some problem as the figure.

(09-28 18:37:11.572 21326-21351/org.tensorflow.demo E/native: tensorflow_inference_jni.cc:160 Output [output:0] not found, aborting!)

Is there any document which explains detail about how to predict with a model in .cc file and how should i rewrite tensorflow_inference_jni.cc file to load the model.
<img width=""1143"" alt=""2016-09-28 6 38 03"" src=""https://cloud.githubusercontent.com/assets/12976847/18910659/975ef168-85ab-11e6-8fe0-4f6eaec3ba9a.png"">

thanks for your time
"
4622,Contributing to TF,"My apology if I am posting this in a wrong place.
I was wondering if I have a valid bug fix or improvement to TF, and I want to test it before submitting a PR, what's the fastest way of doing it?
I usually use

```
bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
```

to re-set up TF but it always takes too much time.
Thanks
"
4620,new feature in cuda8.0 release,"```
New in CUDA 8
Pascal Architecture Support
Enhance performance out-of-the-box on Pascal GPUs
Simplify programming using Unified Memory including support for large datasets, concurrent data access and atomics
Optimize Unified Memory performance using new data migration APIs
Increase throughput at ultra-fast speeds using NVLINK, new high-speed interconnect
Developer Tools
Identify latent system-level bottlenecks using critical path analysis
Improve productivity up to 2x with NVCC
Tune OpenACC applications and overall host code using new profiling extensions
Libraries
Accelerate graph analytics algorithms with nvGRAPH
Speed-up Deep Learning applications using native support for FP16 and INT8, support for batch operation in cuBLAS
```

Do we have anything to improve in these places
1.Simplify programming using Unified Memory including support for large datasets, concurrent data access and atomics
2.Optimize Unified Memory performance using new data migration APIs
3.Increase throughput at ultra-fast speeds using NVLINK, new high-speed interconnect
4.Speed-up Deep Learning applications using native support for FP16 and INT8, support for batch operation in cuBLAS
"
4617,sparse lookup tables for tf.nn.embedding_lookup_sparse,"Hope that â€œ**params**â€ of **tf.nn.embedding_lookup_sparse** support **SparseTensor**!!!

[tf.nn.embedding_lookup_sparse(params, sp_ids, sp_weights, partition_strategy='mod', name=None, combiner='mean')](https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#embedding_lookup_sparse)

The efficiency of sparse lookup tables is low,  but it is very useful for large-scale linear regression system, especially when designing a prototype. 
For example, the wide part of ""wide and deep learning"" use sparse features.
Although I don't know when the hashing function is applied, using sparse lookup table is more accurate than hashing.  The hashing function may merge two features into one bin (share the same weight), or discard new features when the lookup table is full.
"
4616,Commit broke keras.,"I'm running `Ubuntu 14.04.5 LTS` with `tensorflow==0.10.0` and `keras==1.1.0`.

```
ls -la /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root 322936 Aug  3 10:55 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Aug  3 10:55 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19 Aug  3 10:55 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 383336 Aug  3 10:55 /usr/local/cuda/lib64/libcudart.so.7.5.18
-rw-r--r-- 1 root root 720192 Aug  3 10:55 /usr/local/cuda/lib64/libcudart_static.a
```

After installing tensorflow from source on an EC2 machine and attempting to import keras, I recieved an `AttributeError: module 'tensorflow.python' has no attribute 'control_flow_ops'` error. The same error mentioned in: https://github.com/fchollet/keras/issues/3857. I noticed that after this tensorflow commit: https://github.com/tensorflow/tensorflow/commit/5563229b5b45917cd8155bc47a8356af90fb20a2#diff-6ad579dc34adb7a581b8c0bcb1a4dd79

that `tensorflow.python.control_flow_ops` was no longer accessible and thus is what is breaking the code. I got around it by doing `from tensorflow.python.ops import control_flow_ops`.

Should I submit a pull request to keras or does tensorflow have a plan to fix this issue?
"
4614,It might be better to have one more return value in function `tf.unique()` showing the indexes in the original array,"The function `tf.unique(x, name=None)` finds the unique elements in a 1-D tensor. And it now returns two value: `y` and `idx`. `y` contains all of the unique elements of `x` sorted inthe same order that they occur in `x`. `idx` contains the index of each value of `x` in the unique output `y`.

```
# tensor 'x' is [1, 1, 2, 3, 3, 3, 7, 8, 8]
y, idx = tf.unique(x)
y ==> [1, 2, 3, 7, 8]
idx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]
```

BUT i think a third return value which contains the first index of each value of `y` in the original tensor `x` is also needed. It might work like this:

```
# tensor 'x' is [1, 1, 2, 3, 3, 3, 7, 8, 8]
y, idx, idx_ori  = tf.unique(x)
y ==> [1, 2, 3, 7, 8]
idx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]
idx_ori ==> [0, 2, 3, 6, 7]
```

Just like its equivalent in Numpy does:

```
array 'x' is [1, 1, 2, 3, 3, 3, 7, 8, 8]
y, idx_ori = np.unique(x, return_index=True)
y ==> [1, 2, 3, 7, 8]
idx_ori ==> [0, 2, 3, 6, 7]
```
"
4613,Maybe there should be an argument 'unique' in the function `tf.where()`,"There IS the case where we wanna find the first position of the `True` element in a tensor.
suppose there is a tensor `a` like this:

```
[array([[[ True, False],
        [False, False]],

       [[False,  True],
        [ True, False]],

       [[False, False],
        [False,  True]],

       [[False, False],
        [ True, False]]], dtype=bool)
```

and u run `tf.where(a)`, u will get:

```
 array([[0, 0, 0],
       [1, 0, 1],
       [1, 1, 0],
       [2, 1, 1],
       [3, 1, 0]])]
```

But we sometimes want a result like this:

```
 array([[0, 0, 0],
       [1, 0, 1],
       [2, 1, 1],
       [3, 1, 0]])]

```

which show us the index of first `True` element instead of all `True` elements.

I suppose the function `tf.where()` might as well add an argument `unique` , indicating whether u wanna show all true element positions or the first one. And the default value of `unique` should be False. 
the prototype might be like this:

```
tf.where(input, unique=False, name=None)
```

and for the requirements shown above, i might run something like this:

```
tf.where(a, unique = True)
```
"
4611,Documentation for Allocator,"Feature request: Documentation for Allocator on the web site (referenced by https://www.tensorflow.org/versions/r0.10/api_docs/cc/ClassTensor.html).
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

There is no documentation for Allocator or references to it except a few SO threads about interpreting PoolAllocator log messages. Information about it does not seem to exist outside of the source itself.
"
4609,"User ops example ""Fact"" can't work.","I build tensorflow from sources(master). When i try to use the user_ops example ""Fact"", I got the follow errors. I try to change to branch r0.10, the error does't exists. It may be a bug in master branch?

```
In [11]: tf.user_ops.my_fact()

AttributeError                            Traceback (most recent call last)
<ipython-input-11-d6bdebf36140> in <module>()
----> 1 tf.user_ops.my_fact()

/path/to/tensorflow/_python_build/tensorflow/python/user_ops/user_ops.py in my_fact()
     26 def my_fact():
     27   """"""Example of overriding the generated code for an Op.""""""
---> 28   return gen_user_ops._fact()

AttributeError: 'module' object has no attribute '_fact'
```
"
4607,dtype argument for foldl/foldr,"Why doesn't foldl/foldr have a dtype argument as in map_fn, so that the output dtype can be different from the input dtype?

Use case: 
input is a list of indices (int32),  output is sum of embeddings of the indices (float32).

Thanks!
"
4604,Using negative index slice on tf.scan accesses uninitialized memory,"Accessing last element of array produced by scan by using array[-1] gives random results. IE

```
import tensorflow as tf
out = tf.scan(lambda p, c: p * c, tf.fill([2], 2), initializer=tf.constant(1)) 
sess = tf.Session()
print sess.run(out)
print sess.run(out[1])
print sess.run(out[-1])

```

This generates

```
[2 4]
4
181075993
```

The last number is different on reruns, `tf.__version__` is 0.10
"
4603,import_meta_graph: ValueError: At least two variables have the same name: Variable_1,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

None
### Environment info

Operating System: Ubuntu 16.04.1 LTS
Installed version of CUDA and cuDNN: none
pip package: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl
$ python -c ""import tensorflow; print(tensorflow.**version**)""
0.10.0
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Using commands from https://www.tensorflow.org/versions/r0.9/get_started/index.html to create a model:

```
import tensorflow as tf
import numpy as np
x_data = np.random.rand(100).astype(np.float32)
y_data = x_data * 0.1 + 0.3
W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))
b = tf.Variable(tf.zeros([1]))
y = W * x_data + b
loss = tf.reduce_mean(tf.square(y - y_data))
optimizer = tf.train.GradientDescentOptimizer(0.5)
train = optimizer.minimize(loss)
init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)
sess.run(train)
```

Then this works:

```
saver = tf.train.Saver()
saver.save(sess, 'my-model')
sess = tf.Session()
new_saver = tf.train.import_meta_graph('my-model.meta')
```

But this doesn't:

```
tf.train.export_meta_graph(filename='my-model.meta')
new_saver = tf.train.import_meta_graph('my-model.meta')
```

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1458, in import_meta_graph
    return _import_meta_graph_def(read_meta_graph_file(meta_graph_or_file))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1386, in _import_meta_graph_def
    return Saver()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 861, in __init__
    restore_sequentially=restore_sequentially)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 502, in build
    vars_to_save = self._ValidateAndSliceInputs(names_to_variables)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 399, in _ValidateAndSliceInputs
    names_to_variables = self._VarListToDict(names_to_variables)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 377, in _VarListToDict
    name)
ValueError: At least two variables have the same name: Variable_1

What I actually want to use is this, which also does not work (same error):

```
meta_graph_def = tf.train.export_meta_graph()
saver = tf.train.import_meta_graph(meta_graph_def)
```

I need a solution that does not rely on the file system.
Am I doing something wrong?
### What other attempted solutions have you tried?

I tried to look at the files to see where these duplicated names occur but did not find any.
"
4601,convert Inception v1 model .pb file into 8bit precision fail,"I use the guide below to download a v3 model and convert it to 8 bit precision without no issue.

https://www.tensorflow.org/versions/master/how_tos/quantization/index.html

curl http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz -o /tmp/inceptionv3.tgz
tar xzf /tmp/inceptionv3.tgz -C /tmp/
bazel build tensorflow/contrib/quantization/tools:quantize_graph
**bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph \
--input=/tmp/classify_image_graph_def.pb \
--output_node_names=""softmax"" --output=/tmp/quantized_graph.pb \
--mode=eightbit**

However, i download a v1 model from below and try to convert to 8 bit precision fail.

https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples/

https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip

the model file is: ""tensorflow_inception_graph.pb"" 
## I get error:

File ""/bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph.runfiles/org_tensorflow/tensorflow/contrib/quantization/tools/quantize_graph.py"", line 319, in rewrite
    for output_node_name in output_node_names]
## KeyError: 'softmax'

it says key error: softmax. is this caused by v1 model use something other than softmax or it has different name? What should i input for --output_node_names parameter when I convert v1? Or how can i get the output node name? If i try to convert other model .pb file how to input this parameter?

Thanks
"
4600,fold/scan gradient high memory usage,"It seems that the gradients for higher order functions (foldl/foldr/scan) require a very large amount of memory.

The following code block will easily compute `res` (even with `swap_memory` disabled) but will fail when computing `grad`:

``` python
v = tf.Variable([1.0])
def foo(aggregate, arr):
    arr = v * tf.tile(arr, [10000])
    return aggregate + arr[0, 0]

arr = np.empty([10000, 1000], dtype=np.float32)
res = tf.foldl(foo, arr, 0.0, 1, swap_memory=True)
opt = tf.train.GradientDescentOptimizer(0.001)
grad = opt.compute_gradients(res)

sess = tf.Session()
sess.run(tf.initialize_all_variables())
sess.run(res) #This will run
sess.run(grad) #This will cause an OOM error
```

This was tested on a machine with a GTX1070 and 24GB RAM. 

I'm guessing that what's happening is that tensorflow is only keeping track of data necessary for one iteration of `foldl` at a time when computing `res` (~40MB) but has to keep track of all the memory in all iterations of `foldl` when computing `grad` (~400GB).

In fact, reducing `arr` to be of size 1000x1000, still causes an OOM error, but reducing the size of `arr` to be 500x1000 ends up with the code block succeeding. This makes sense given that setting `arr` to these two sizes requires ~40GB and ~20GB respectively, the latter of which just fits on my machine's memory.

From what I understand of automatic differentiation, tensorflow shouldn't need to simultaneously keep track of the data needed across all iterations of `foldl` when computing gradients. Or am I missing something?
"
4596,Tensorboard generating javascript errors: Failed to load resource: net::ERR_CONNECTION_RESET,"I'm running tensorboard on the Windows 10 Bash on Ubuntu shell. After bringing up the tensorboard webserver (localhost:6006) with `tensorboard --logdir=/mnt/d/Dropbox/Andor/Summaries --debug` and connecting to it with my Chrome browser, I don't see anything on the screen. The JS console gives me a couple of the following ERRORS:

```
Failed to load resource: net::ERR_CONNECTION_RESET http://localhost:6006/external/plottable/plottable.min.js
Failed to load resource: net::ERR_CONNECTION_RESET http://localhost:6006/external/polymer/polymer.html
Failed to load resource: net::ERR_CONNECTION_RESET http://localhost:6006/dist/tf-tensorboard.html
Failed to load resource: net::ERR_CONNECTION_RESET http://localhost:6006/external/iron-icons/iron-icons.html
```

Except for http://localhost:6006/dist/tf-tensorboard.html, the other three missing resources are all there in the correct directories (/usr/local/lib/python2.7/dist-packages/external/...). The /usr/local/lib/python2.7/dist-packages/dist/ directory does not exist in my installation.

After these errors in the JS console come hundreds of `Uncaught ReferenceError: Polymer is not defined`

I have searched on the web for related issues but only found [this one issue](https://github.com/tensorflow/tensorflow/issues/1421) which I believe is unrelated as I don't get any of the described errors in my case. In fact, there are no errors on the server's console output, no warnings, all ok (I'm running the server in debug mode).

Other parameters of my system:
- I did a pip install of tensorflow

Also:

```
python -c ""import tensorflow; print(tensorflow.__version__)
0.8.0
```

A small event file is attached here:
[events.out.tfevents.1474905790.zip](https://github.com/tensorflow/tensorflow/files/494789/events.out.tfevents.1474905790.zip)

Also, the server's console output is here (after starting the server AND making a request in the browser http://localhost:6006):

```
User@PC_001:/mnt/d/Dropbox/Andor$ tensorboard --logdir=/mnt/d/Dropbox/Andor/Summaries --debug
INFO:tensorflow:TensorBoard is in debug mode.
INFO:tensorflow:Starting TensorBoard in directory /mnt/d/Dropbox/Andor
INFO:tensorflow:TensorBoard path_to_run is: {'/mnt/d/Dropbox/Andor/Summaries': None}
INFO:tensorflow:Adding events from directory /mnt/d/Dropbox/Andor/Summaries/train
INFO:tensorflow:Constructing EventAccumulator for /mnt/d/Dropbox/Andor/Summaries/train
DEBUG:tensorflow:Opening a record reader pointing at /mnt/d/Dropbox/Andor/Summaries/train/events.out.tfevents.1474905790.DUCANDUGMBH_001
DEBUG:tensorflow:No more events in /mnt/d/Dropbox/Andor/Summaries/train/events.out.tfevents.1474905790.DUCANDUGMBH_001
INFO:tensorflow:No path found after /mnt/d/Dropbox/Andor/Summaries/train/events.out.tfevents.1474905790.DUCANDUGMBH_001
DEBUG:tensorflow:No more events in /mnt/d/Dropbox/Andor/Summaries/train/events.out.tfevents.1474905790.DUCANDUGMBH_001
INFO:tensorflow:No path found after /mnt/d/Dropbox/Andor/Summaries/train/events.out.tfevents.1474905790.DUCANDUGMBH_001
INFO:tensorflow:Multiplexer done loading. Load took 0.0 secs
INFO:tensorflow:TensorBoard is tag: 16
Starting TensorBoard 16 on port 6006
(You can navigate to http://0.0.0.0:6006)
127.0.0.1 - - [27/Sep/2016 09:12:48] ""GET / HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /lib/css/global.css HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /external/lodash/lodash.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /external/plottable/plottable.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /external/d3/d3.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /external/plottable/plottable.css HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /external/graphlib/dist/graphlib.core.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /external/dagre/dist/dagre.core.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /external/polymer/polymer.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /external/webcomponentsjs/webcomponents-lite.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /external/iron-ajax/iron-ajax.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /external/iron-collapse/iron-collapse.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /external/iron-list/iron-list.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /external/paper-button/paper-button.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /external/paper-checkbox/paper-checkbox.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /external/paper-dropdown-menu/paper-dropdown-menu.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /external/paper-icon-button/paper-icon-button.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /external/paper-header-panel/paper-header-panel.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /external/paper-input/paper-input.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /external/paper-item/paper-item.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /external/paper-menu/paper-menu.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:49] ""GET /external/paper-progress/paper-progress.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:50] ""GET /external/paper-radio-button/paper-radio-button.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:50] ""GET /external/paper-radio-group/paper-radio-group.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:50] ""GET /external/paper-slider/paper-slider.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:50] ""GET /external/paper-styles/paper-styles.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:50] ""GET /external/paper-toggle-button/paper-toggle-button.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:50] ""GET /external/paper-toolbar/paper-toolbar.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:50] ""GET /external/paper-tabs/paper-tabs.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:50] ""GET /dist/tf-tensorboard.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:50] ""GET /external/iron-resizable-behavior/iron-resizable-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:50] ""GET /external/iron-ajax/iron-request.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:50] ""GET /external/paper-material/paper-material.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:50] ""GET /external/paper-ripple/paper-ripple.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:50] ""GET /external/paper-behaviors/paper-button-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:50] ""GET /external/iron-flex-layout/iron-flex-layout.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:50] ""GET /external/paper-styles/default-theme.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:50] ""GET /external/paper-behaviors/paper-checked-element-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:50] ""GET /external/paper-menu-button/paper-menu-button.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:50] ""GET /external/iron-a11y-keys-behavior/iron-a11y-keys-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:50] ""GET /external/iron-behaviors/iron-control-state.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:50] ""GET /external/iron-behaviors/iron-button-state.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:51] ""GET /external/iron-icons/iron-icons.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:51] ""GET /external/iron-icon/iron-icon.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:51] ""GET /external/iron-form-element-behavior/iron-form-element-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:51] ""GET /external/iron-validatable-behavior/iron-validatable-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:51] ""GET /external/paper-behaviors/paper-inky-focus-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:51] ""GET /external/iron-input/iron-input.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:51] ""GET /external/paper-input/paper-input-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:51] ""GET /external/paper-input/paper-input-char-counter.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:51] ""GET /external/paper-input/paper-input-container.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:51] ""GET /external/paper-input/paper-input-error.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:51] ""GET /external/paper-item/paper-item-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:51] ""GET /external/paper-item/paper-item-shared-styles.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:51] ""GET /external/iron-menu-behavior/iron-menu-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:51] ""GET /external/paper-styles/color.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:51] ""GET /external/paper-menu/paper-menu-shared-styles.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:51] ""GET /external/iron-range-behavior/iron-range-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:51] ""GET /external/iron-selector/iron-selectable.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:51] ""GET /external/iron-flex-layout/classes/iron-flex-layout.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:52] ""GET /external/paper-styles/shadow.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:52] ""GET /external/paper-styles/typography.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:52] ""GET /external/iron-menu-behavior/iron-menubar-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:52] ""GET /external/paper-tabs/paper-tabs-icons.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:52] ""GET /external/paper-tabs/paper-tab.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:52] ""GET /external/promise-polyfill/promise-polyfill-lite.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:52] ""GET /external/paper-material/paper-material-shared-styles.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:52] ""GET /external/paper-behaviors/paper-ripple-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:52] ""GET /external/iron-checked-element-behavior/iron-checked-element-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:52] ""GET /external/iron-dropdown/iron-dropdown.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:52] ""GET /external/neon-animation/animations/fade-in-animation.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:52] ""GET /external/neon-animation/animations/fade-out-animation.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:52] ""GET /external/paper-menu-button/paper-menu-button-animations.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:52] ""GET /external/iron-meta/iron-meta.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:52] ""GET /external/iron-a11y-announcer/iron-a11y-announcer.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:52] ""GET /external/paper-input/paper-input-addon-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:52] ""GET /external/iron-selector/iron-selection.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:52] ""GET /external/iron-selector/iron-multi-selectable.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:53] ""GET /external/iron-flex-layout/classes/iron-shadow-flex-layout.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:53] ""GET /external/font-roboto/roboto.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:53] ""GET /external/iron-iconset-svg/iron-iconset-svg.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:53] ""GET /external/promise-polyfill/Promise.js HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:53] ""GET /external/iron-overlay-behavior/iron-overlay-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:53] ""GET /external/neon-animation/neon-animation-runner-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:53] ""GET /external/neon-animation/animations/opaque-animation.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:53] ""GET /external/iron-dropdown/iron-dropdown-scroll-manager.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:53] ""GET /external/neon-animation/neon-animation-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:53] ""GET /external/neon-animation/web-animations.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:53] ""GET /external/iron-fit-behavior/iron-fit-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:53] ""GET /external/iron-overlay-behavior/iron-overlay-backdrop.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:53] ""GET /external/iron-overlay-behavior/iron-overlay-manager.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:53] ""GET /external/neon-animation/neon-animatable-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:53] ""GET /external/web-animations-js/web-animations-next-lite.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [27/Sep/2016 09:12:54] ""GET /external/web-animations-js/web-animations-next-lite.min.js.map HTTP/1.1"" 200 -

```
"
4595,Rename TF_Attr_Type and TF_Attr_Metadata,"The names of the `TF_Attr_Type` enum and `TF_Attr_Metadata` struct seem to be unlike any other names in the C API. Contrast them with `TF_OperationDescription`, `TF_SessionOptions`, `TF_SessionWithGraph`, and `TF_DataType`, which are also composed of multiple words. Since 0.11 hasnâ€™t been released yet, perhaps itâ€™s not too late to unify the naming scheme.

Regards,
Ivan
"
4593,tf.cumsum returns wrong value on reverse,"To reproduce the issue:

``` python
v = [ 10.52005689  10.9425797   10.60515152   9.59574335   8.4282654
   9.97478889   9.33102282  10.68171879  10.85262443   9.06804822]
w = tf.cumsum(tf.reverse(v,[True]))
q = tf.cumsum(tf.reverse(v,[True]),reverse=True)
with tf.Session() as sess:
    res = sess.run([w,q])
    print res[0]
    print res[1]
```

returns

``` python
[   9.06804822   19.92067265   30.60239143   39.93341425   49.90820314
   58.33646854   67.93221189   78.5373634    89.47994311  100.        ]
[ 100.           90.93195178   80.07932735   69.39760857   60.06658575
   50.09179686   41.66353146   32.06778811   21.4626366    10.52005689]
```

which are NOT the reverse of each other.  Note, the value for w is correct.  The q value should be the reverse of w, but is not.  The cumsum values are also incorrect...
"
4592,Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary,"when i  change 'max_step' from 1000 to 10000 in mnist_with_summaries.py ,after 8550 iters occur these error:
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary
     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]
Traceback (most recent call last):
"
4590,Better shape inference for tf.slice and tf.strided_slice,"Currently, if any of the values in the `size` argument is not a constant, the output shape is completely unknown (but the correct rank):

``` python
>>> z = tf.zeros((1, 2, 3))
>>> z.get_shape().as_list()
[1, 2, 3]
>>> m = tf.slice(z, [0, 0, 0], [-1, -1, -1])
>>> m.get_shape().as_list()
[1, 2, 3]
>>> m = tf.slice(z, [0, 0, 0], [tf.constant(1) + 0, -1, -1])
>>> m.get_shape().as_list()
[None, None, None]
```

The desired behaviour would instead treat the second and third dimensions correctly:

``` python
>>> m = tf.slice(z, [0, 0, 0], [tf.constant(1) + 0, -1, -1])
>>> m.get_shape().as_list()
[None, 2, 3]
```

Looking briefly at the code, this would requite being a bit more clever in terms of how constant values are computed; right now if anything in a `Pack`-ed array is unknown at graph construction time, the entire array is unknown (see `_ConstantValue`'s `Pack` case).

I guess this request ends up being just a request for a better constant propagation system which supports partially-known tensors. Perhaps you already have other use cases for such a feature, in which case view this as just another request that such a feature would enable.
"
4589,Support for native half-float computation (float16/fp16),"https://github.com/tensorflow/tensorflow/issues/1300 added support for fp16 storage, but there is currently no support for native fp16 computation, which is available on some hardware such as Pascal GPUs.

In particular, the conv2d and matmul ops could take a new parameter along the lines of ""compute_dtype"", which would be plumbed through to CUDNN (convolution descriptor) and CUBLAS (Hgemm) in the backend, with the potential for up to a 2x speedup.

Related issues:
https://github.com/tensorflow/tensorflow/issues/1300
https://github.com/tensorflow/tensorflow/issues/4314
https://github.com/tensorflow/tensorflow/issues/851#issuecomment-230923665
"
4588,dynamic_rnn broken on master?,"The following code works on TF 0.10 but fails on `master`:

```
import tensorflow as tf
i = tf.placeholder(tf.float32, shape=[1, None, 20])
cell = tf.nn.rnn_cell.GRUCell(30)
o = tf.nn.dynamic_rnn(cell, i, dtype=tf.float32)
```

The autogenerated documentation for `master` [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard8/tf.nn.dynamic_rnn.md) does not mention anything which could explain this.

The failure is the following exception:

```
ValueError: Time steps is not the same for all the elements in the input in a batch.
```

raised on line `915` of `rnn.py`

I could not find any issues in the issue tracker that address this.
"
4586,tensorflow/examples/android,"hi all

This example seems like a new version and i got some errors when i want to build the APK on android studio.
Dose any have same issue?

thanks for your time

<img width=""1280"" alt=""2016-09-27 1 16 48"" src=""https://cloud.githubusercontent.com/assets/12976847/18844583/6ab64f50-8450-11e6-8099-b7af5ca8ef20.png"">
"
4584,A strange problem about gfile,"<img width=""848"" alt=""2016-09-26 20 46 30"" src=""https://cloud.githubusercontent.com/assets/11813885/18834822/9ee71bea-842a-11e6-95d3-2af68449343d.png"">

I use Tensorflow of gpu's version and Python3 on Linux.As you can see, I read a file by Gfile with mode ""r"",but it seems to return a byte object rather than a string.Can anyone explain this issue?
"
4583,Bazel can't fetch numeric-1.2.6.min.js because numericjs.com has expired,"INFO: Downloading from http://www.numericjs.com/lib/numeric-1.2.6.min.js: 0B

---

When i opened it on my broswer, it prompted that numericjs.com has expired
"
4582,Tensorflow model as executable file,"I want to share my model as an executable file (without sharing my source code ) . I am using Tensorflow on Ubutun 16.04.1 LTS .I tried pyinstaller and cx_freeze but it give me alot of errors

**pyinstaller error** 
![jvssz](https://cloud.githubusercontent.com/assets/15180702/18830940/609bb61c-83e3-11e6-939d-3d7b8a7cf7a0.png)

**cx_freeze error** 
![ikzr5](https://cloud.githubusercontent.com/assets/15180702/18830975/8c4a345a-83e3-11e6-854e-649b1a269d94.png)

I installed pyqt4 and I got this error 
` ` `
Error loading Python lib '/root/build/test/libpython2.7.so.1.0': /root/build/test/libpython2.7.so.1.0: cannot open shared object file: No such file or directory 
` ` ` 
"
4581,How can user control the communication in distributed tensorflow,"sorry to trouble.
When I try to train model with ps and worker. I found that TensorFlow passed the whole gradients of each op between ps and each worker. 
If I explicitly calculate the gradient of inputs, which is defined as placeholder, TensorFlow passed these gradients. While the batch size of the inputs is too big, it makes the time of communication too long.
Is there any way to choose the parameters which user want to pass between ps and worker.
"
4579,A Convolutional Neural Network Model Could not be loaded,"I have saved the model using Keras' model.save('my_model.h5') it's working. However, when I try to load the model in a different project I get this error message: ValueError: Tensor(""cond/pred_id:0"", dtype=bool) must be from the same graph as Tensor(""dropout_1/mul_1:0"", shape=(?, 1, 256), dtype=float32).
Could this be a bug? Any idea?
"
4578,Default weights_initializer for tf.contrib.layers.convolution2d should be xavier_initializer_conv2d,"Hi, 

I notice that in [tf.contrib.layers.convolution2d](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L354), [tf.contrib.layers.convolution2d_in_plane](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L469), [tf.contrib.layers.convolution2d_transpose](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L573), [tf.contrib.layers.separable_convolution2d](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L1097), the default weights_initializer is tf.contrib.layers.xavier_initializer, rather than tf.contrib.layers.xavier_initializer_conv2d. Is it better to use tf.contrib.layers.xavier_initializer_conv2d?

Thanks!
"
4577,Unable to run example trainer using GPU,"Hi experts,

I am building tensorflow from source and using virtualenv. I followed all the steps in 
https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#installing-from-sources
but when i try to run the example trainer, i get a Floating point exception core dump error.
Please let me know if you need further information and how I can get it for you.

Much appreciated!
- Macchakaran

(tensorflow) macchakaran@macchakaran-pc:~/Work/tensorflow$ bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5.1.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.683
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 7.47GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
Floating point exception (core dumped)
"
4576,"python/ops/variable_scope.py, variable_scope() is not backward compatible","https://github.com/tensorflow/tensorflow/commit/3d1ee95e612b1987e664ca46a7c584872d36dde9#diff-57dece591b2185b1c01f712d53baef96L1089

It used to accept """" as name_of_scope, newer version TF will produce an error.

```
if default_name is None and not name_or_scope:
      raise TypeError(""If default_name is None then name_or_scope is required"")     
```

By reading the newer code, I believe disallowing """" as scope name wasn't your team's intension.
A simple fix could make this function completely backward compatible.
"
4574,translate.py in the seq2seq model is missing in sitepackages for python3.5 cpu only release,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

searched github issues for ""translate.py"" no hits
### Environment info

Operating System:
Ubuntu 16.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
NA, CPU only

If installed from binary pip package, provide:
1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
   0.10.0
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

NA
### What other attempted solutions have you tried?

NA
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
"
4573,ERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support.,"My CUDA version is 8.0 and cuDNN is 5.1. I have run many cuda examples without any problem.

But I stuck in following problem for two days.

First I run the configure as below
`devymex@DL-LAB:~/Software/tensorflow$ ./configure 
~/Software/tensorflow ~/Software/tensorflow
Please specify the location of python. [Default is /usr/bin/python]: 
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] N
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] N
No Hadoop File System support will be enabled for TensorFlow
Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]

/usr/local/lib/python2.7/dist-packages
Do you wish to build TensorFlow with GPU support? [y/N] y
GPU support will be enabled for TensorFlow
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 5
Please specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 
WARNING: Running Bazel server needs to be killed, because the startup options are different.
Sending SIGTERM to previous Bazel server (pid=5498)... done.
.
INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
.
INFO: All external dependencies fetched successfully.
Configuration finished
`

Then build tensorflow by bazel:

`bazel build -c opt --config=cuda
WARNING: Running Bazel server needs to be killed, because the startup options are different.
Sending SIGTERM to previous Bazel server (pid=20618)... done.
.
ERROR: /home/devymex/.cache/bazel/_bazel_devymex/05af4cc48fb50d1cc8f7e879f4c1ce83/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):
    File ""/home/devymex/.cache/bazel/_bazel_devymex/05af4cc48fb50d1cc8f7e879f4c1ce83/external/local_config_cuda/crosstool/BUILD"", line 4
        error_gpu_disabled()
    File ""/home/devymex/.cache/bazel/_bazel_devymex/05af4cc48fb50d1cc8f7e879f4c1ce83/external/local_config_cuda/crosstool/error_gpu_disabled.bzl"", line 3, in error_gpu_disabled
        fail(""ERROR: Building with --config=c..."")
ERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.
Unhandled exception thrown during build; message: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@e06068b3' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@de03db55', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@2228d81e', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@2342a6cb')
INFO: Elapsed time: 0.539s
java.lang.RuntimeException: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@e06068b3' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@de03db55', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@2228d81e', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@2342a6cb')
    at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1070)
    at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:474)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: com.google.devtools.build.lib.packages.NoSuchTargetException: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/devymex/.cache/bazel/_bazel_devymex/05af4cc48fb50d1cc8f7e879f4c1ce83/external/local_config_cuda/crosstool/BUILD
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.getCrosstoolProtofromBuildFile(CrosstoolConfigurationLoader.java:179)
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.findCrosstoolConfiguration(CrosstoolConfigurationLoader.java:239)
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.readCrosstool(CrosstoolConfigurationLoader.java:281)
    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.createParameters(CppConfigurationLoader.java:128)
    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.create(CppConfigurationLoader.java:73)
    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.create(CppConfigurationLoader.java:48)
    at com.google.devtools.build.lib.skyframe.ConfigurationFragmentFunction.compute(ConfigurationFragmentFunction.java:78)
    at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1016)
    ... 4 more
Caused by: com.google.devtools.build.lib.packages.NoSuchTargetException: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/devymex/.cache/bazel/_bazel_devymex/05af4cc48fb50d1cc8f7e879f4c1ce83/external/local_config_cuda/crosstool/BUILD
    at com.google.devtools.build.lib.packages.Package.makeNoSuchTargetException(Package.java:559)
    at com.google.devtools.build.lib.packages.Package.getTarget(Package.java:543)
    at com.google.devtools.build.lib.skyframe.SkyframePackageLoaderWithValueEnvironment.getTarget(SkyframePackageLoaderWithValueEnvironment.java:71)
    at com.google.devtools.build.lib.skyframe.ConfigurationFragmentFunction$ConfigurationBuilderEnvironment.getTarget(ConfigurationFragmentFunction.java:193)
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.getCrosstoolProtofromBuildFile(CrosstoolConfigurationLoader.java:177)
    ... 11 more
java.lang.RuntimeException: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@e06068b3' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@de03db55', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@2228d81e', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@2342a6cb')
    at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1070)
    at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:474)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: com.google.devtools.build.lib.packages.NoSuchTargetException: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/devymex/.cache/bazel/_bazel_devymex/05af4cc48fb50d1cc8f7e879f4c1ce83/external/local_config_cuda/crosstool/BUILD
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.getCrosstoolProtofromBuildFile(CrosstoolConfigurationLoader.java:179)
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.findCrosstoolConfiguration(CrosstoolConfigurationLoader.java:239)
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.readCrosstool(CrosstoolConfigurationLoader.java:281)
    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.createParameters(CppConfigurationLoader.java:128)
    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.create(CppConfigurationLoader.java:73)
    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.create(CppConfigurationLoader.java:48)
    at com.google.devtools.build.lib.skyframe.ConfigurationFragmentFunction.compute(ConfigurationFragmentFunction.java:78)
    at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1016)
    ... 4 more
Caused by: com.google.devtools.build.lib.packages.NoSuchTargetException: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/devymex/.cache/bazel/_bazel_devymex/05af4cc48fb50d1cc8f7e879f4c1ce83/external/local_config_cuda/crosstool/BUILD
    at com.google.devtools.build.lib.packages.Package.makeNoSuchTargetException(Package.java:559)
    at com.google.devtools.build.lib.packages.Package.getTarget(Package.java:543)
    at com.google.devtools.build.lib.skyframe.SkyframePackageLoaderWithValueEnvironment.getTarget(SkyframePackageLoaderWithValueEnvironment.java:71)
    at com.google.devtools.build.lib.skyframe.ConfigurationFragmentFunction$ConfigurationBuilderEnvironment.getTarget(ConfigurationFragmentFunction.java:193)
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.getCrosstoolProtofromBuildFile(CrosstoolConfigurationLoader.java:177)
    ... 11 more
`

Please help me to get rid of the annoying problem, thanks!
"
4571,"Tensorflow website stating CUDNN v5 required, but my version working only on v5.1","https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#requirements
states that cuDNN v5 is required. So I tried to compile the basic example. This error was shown.
_Loaded runtime CuDNN library: 5005 (compatibility version 5000) but source was compiled with 5103 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration._

It worked when I upgraded cuDNN to v5.1. Please update the said webpage if possible. 

Operating System: Ubuntu Linux 14.04 LTS

Installed version of CUDA and cuDNN: 
-rw-r--r-- 1 root root   322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root       19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root   383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18
-rw-r--r-- 1 root root   720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 Sep 24 20:03 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       17 Sep 24 20:03 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.3
-rwxr-xr-x 1 root root 59909104 Sep 24 19:10 /usr/local/cuda/lib64/libcudnn.so.5.0.5
-rwxr-xr-x 1 root root 60696704 Jun 10 13:48 /usr/local/cuda/lib64/libcudnn.so.5.1.3
-rw-r--r-- 1 root root 59715990 Jun 10 13:48 /usr/local/cuda/lib64/libcudnn_static.a

**Tensorflow version:**
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
0.10.0
"
4568,Download dependencies to install tensorflow from source on Mac,"### Environment info

Operating System: OS X 10.11.6

Installed version of CUDA and cuDNN: 
No

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`) :
   6218ac2be3cc530da866ec32da4cb86c6ac5bb85
2. The output of `bazel version`
   Build label: 0.3.1-homebrew
   Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
   Build time: Thu Aug 4 09:58:27 2016 (1470304707)
   Build timestamp: 1470304707
   Build timestamp as int: 1470304707
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

tensorflow/contrib/makefile/download_dependencies.sh
### What other attempted solutions have you tried?

I thought it was some problem that I havent install the tar in my Mac, but it didnt help
### Logs or other output that would be helpful

When i run the command line like above, I got some error which indicate it failed to download eigen.

downloading http://bitbucket.org/eigen/eigen/get/"" + eigen_version + "".tar.gz
tar: Unrecognized archive format
tar: Error exit delayed from previous errors.

Can you help me?  I am new to tensorflow
"
4567,tensorflow/contrib/makefile/download_dependencies.sh fails to parse workspace.bzl,"Hi, 

In download_dependencies.sh we grep tensorflow/workspace.bzl in order to determine which Eigen to download. We do this using the command `grep -o 'http.*bitbucket.org/eigen/eigen/.*tar\.gz' tensorflow/workspace.bzl` which we then proceed to try and curl and untar (ln22).  

Due to the new format of workspace.bzl, that grep returns `http://bitbucket.org/eigen/eigen/get/"" + eigen_version + "".tar.gz` which will download a 404 from bitbucket. Can we please update the makefile to account for this?

Thanks,
Tomas
"
4566,Allow user to control amount of GPU memory consumed,"Environment:
~tensorflow 0.10 
~NVIDIA K80 GPU server. 
~ubuntu 14.04

I do not change the code which is download from this repository.
When I run ptb_word_lm.py, it takes up all my GPU memory (from device gpu:0 to device gpu:15). Is it a bug? How could I fix it?

Thanks a lot in advance!
Swind
"
4561,protobuf needs to be updated,"@kamal94 noticed in https://github.com/tensorflow/tensorflow/issues/4371#issuecomment-249340251 that the version of protobuf we're referencing contains HOST_CFG in its protobuf.bzl file. We need to upgrade that.
"
4557,using GraphDef in C API?,"I'm looking over the C API to turn a GraphDef into a TF_Graph for use with TF_NewSessionWithGraph, but I can't figure out how that's supposed to be done.

It kind of seems like you can't unless you use the deprecated TF_ExtendGraph API? But that's deprecated, so I'm not sure what to do.
"
4554,TensorFlow Binary Segmentation,"I've written the following [segmentor](https://docs.google.com/document/d/1Mnfrcu5AU47rRoPwweQv8kRGzTPJaXwJf7zd3pksXK4/edit?usp=sharing) and I can't get the accuracy to work. In fact I'm always getting accuracy of 0.0 whatever the size of my sample.

I think the problem is at the sigmoid layer at the end of U() function where a tensor of continuous elements between 0 and 1 (conv10) is further compared to a binary tensor and therefore there's no chance of getting any equality between the two.
"
4553,I got a problem when I ran the flower demo of slim,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
### Environment info

Operating System:
Ubuntu 14.04
Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
Cuda 7.5   cuDNN 5.0
If installed from binary pip package, provide:
No, we installed tensorflow with the source code
1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
   754048a0453a04a761e112ae5d99c149eb9910dd
2. The output of `bazel version`
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

We ran the flower demo alone, everything is ok, however, with the slim I got the error like this:
![image](https://cloud.githubusercontent.com/assets/10766005/18788663/5d1d63ce-81da-11e6-91fa-0a34ae52e130.png)
 I think something wrong with this file /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/image_ops.py
### What other attempted solutions have you tried?
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
think you!
"
4552,Feature request: better handling of concurrent execution to prevent unnecessary out of memory failures ,"I have a model in which one operation happens to be very memory intensive and cannot be executed at once. I thought that by splitting the operation manually and executing parts of it individually (all on the GPU), I would get around the OOM problem. Unfortunately, it appears that TF aggressively tries to schedule as many ops to run in parallel as possible, even if that results in an OOM failure. I describe my specific issue further in this [Stack Overflow post](http://stackoverflow.com/questions/39643250/best-way-to-split-computation-too-large-to-fit-into-memory).

It seems to me that this problem should be reasonably easy to solve, and is rather a serious current limitation of TF. The basic principle is that a computation should not fail if it's possible, in some sequential order, to execute it in memory. I'm not talking about situations in which TF has to be clever about splitting a seemingly atomic operations in parts. That I understand can be quite difficult and requires ""art"" on the part of the programmer. What I'm describing is a situation in which the computation graph, _as described_, can in fact be executed without running out of memory, but because TF is (overly) aggressive in scheduling independent ops concurrently, it runs out of memory.

The solution seems rather straight-forward. Before putting yet another op onto the GPU (or whatever hardware), TF should check if that would cause the memory to run out. If so, it should check if there's a cyclic dependency on the current op getting executed, i.e. it would be impossible for it to move forward unless the current op is executed. If so then that's a legitimate problem and it would fail with an OOM error. On the other hand, if it's possible for it to wait until other ops are executed before trying again, then it should simply wait until more memory is available. I.e. it should have a way to run independent operations sequentially.

I'm not familiar enough with the internals of TF to know how difficult a change like this would be, but as it currently stands, it seems like it's preventing a large swath of models from getting executed that otherwise could get executed.

On a related note, the dynamic_rnn op already seems to do this, where it's able to shuffle memory back and forth until the computation is done, and so perhaps there's an existing partial solution that I'm not aware of?
"
4551,Feature request: add TensorFlow Serving to conda-forge as well,"[TensorFlow made it into conda-forge](https://github.com/conda-forge/tensorflow-feedstock) which is awesome, but could TensorFlow Serving be provided as well for easier CI? Installing bazel just for building Serving during a CI run is a bit excessive.
"
4550,Multilabel image classification with sparse labels?,"I want to perform a multilabel image classification task for n classes.
I've got sparse label vectors for each image and each dimension of each label vector is currently encoded in this way:
-   1.0 ->Label true / Image belongs to this class
- -1.0 ->Label false / Image does not contain to this class.
-  0.0 ->missing value/label

E.g.: V= {1.0,-1.0,1.0, 0.0}

For this example V the model should learn, that the corresponding image should be classified in the first and third class.

My problem is currently how to handle the missing values/labels. I've searched through the issues and found this issue:
[https://github.com/tensorflow/skflow/issues/113](https://github.com/tensorflow/skflow/issues/113)

So could do multilable image classification with:
[tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=None)](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard5/tf.nn.sigmoid_cross_entropy_with_logits.md)

but TensorFlow has this error function for sparse softmax, which is used for exclusive classification:
[tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name=None)](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard4/tf.nn.sparse_softmax_cross_entropy_with_logits.md)

So is there something like sparse sigmoid cross entropy? (Couldn't find something) or any suggestions how can I handle my multilabel classification problem with sparse labels.
"
4549,tensorboard can not work,"when i run tensorboard demo:
1. python mnist_with_summaries.py   OK
2. tensorboard --logdir=/tmp/mnist_logs/  
print like this 
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/webcomponentsjs/webcomponents-lite.min.js' on path /usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/webcomponentsjs/webcomponents-lite.min.js
127.0.0.1 - - [23/Sep/2016 16:55:12] ""GET /lib/css/global.css HTTP/1.1"" 200 -
127.0.0.1 - - [23/Sep/2016 16:55:12] ""GET /webcomponentsjs/webcomponents-lite.min.js HTTP/1.1"" 200 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/dist/bazel-html-imports.html' on path /usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/dist/bazel-html-imports.html
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-packages/external/dist/bazel-html-imports.html' on path /usr/local/lib/python2.7/dist-packages/external/dist/bazel-html-imports.html
127.0.0.1 - - [23/Sep/2016 16:55:12] code 404, message Not Found
127.0.0.1 - - [23/Sep/2016 16:55:12] ""GET /dist/bazel-html-imports.html HTTP/1.1"" 404 -
127.0.0.1 - - [23/Sep/2016 16:55:12] ""GET /dist/tf-tensorboard.html HTTP/1.1"" 200 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/polymer/polymer.html' on path /usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/polymer/polymer.html
127.0.0.1 - - [23/Sep/2016 16:55:12] ""GET /polymer/polymer.html HTTP/1.1"" 200 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/iron-icons/iron-icons.html' on path /usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/iron-icons/iron-icons.html
.......

No such file or directory error 

how can i install these file ?
"
4548,r0.10: ImportError: No module named tensorflow,"My code runs well on r0.9, and after I upgrade to r0.10, and it runs failed. And my TF r0.10 is built from source also.
"
4547,import tensorflow on apache2 cgi module,"I try to run tensorflow on web server python cgi
I succeed in running python in apache2 web server

but I stoped by environment variables :LD_LIBRARY_PATH set in Python Code

I run os.environ:function and result print check
This result is correct value
I don't know what can i do

I need you help

**my code is** 

```

import cgi
import os
import cgitb
cgitb.enable()

import requests

print(""Content-type: text/html;charset=utf-8\r\n"")

print(""<html>"")
print(""<head>"")
print(""\t<title>Python CGI Test</title>"")
print(""</head>"")
print(""<body>"")
print(""\t<h1>Hello, Python CGI!</h1>"")
print(""</body>"")
print(""</html>"")


os.environ[""LD_LIBRARY_PATH""] = '$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64'

print os.environ
print ("""")


import tensorflow as tf
hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
print(sess.run(hello))
```

**and result**

> Hello, Python CGI!
> 
> {'CONTEXT_DOCUMENT_ROOT': '/var/www/html', 'SERVER_SOFTWARE': 'Apache/2.4.18 (Ubuntu)', 'CONTEXT_PREFIX': '', 'SERVER_SIGNATURE': '
> Apache/2.4.18 (Ubuntu) Server at 192.168.0.115 Port 80
> \n', 'REQUEST_METHOD': 'GET', 'SERVER_PROTOCOL': 'HTTP/1.1', 'QUERY_STRING': '', 'PATH': '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'LD_LIBRARY_PATH': '$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64', 'HTTP_USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36', 'HTTP_CONNECTION': 'keep-alive', 'SERVER_NAME': '192.168.0.115', 'REMOTE_PORT': '57930', 'SERVER_PORT': '80', 'SERVER_ADDR': '192.168.0.115', 'DOCUMENT_ROOT': '/var/www/html', 'SCRIPT_FILENAME': '/var/www/html/123.py', 'SERVER_ADMIN': 'webmaster@localhost', 'HTTP_HOST': '192.168.0.115', 'SCRIPT_NAME': '/123.py', 'HTTP_UPGRADE_INSECURE_REQUESTS': '1', 'HTTP_CACHE_CONTROL': 'max-age=0', 'REQUEST_URI': '/123.py', 'HTTP_ACCEPT': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,_/_;q=0.8', 'GATEWAY_INTERFACE': 'CGI/1.1', 'REMOTE_ADDR': '192.168.0.115', 'HTTP_ACCEPT_LANGUAGE': 'ko-KR,ko;q=0.8,en-US;q=0.6,en;q=0.4', 'REQUEST_SCHEME': 'http', 'HTTP_ACCEPT_ENCODING': 'gzip, deflate, sdch'} --> -->
> 
> <type 'exceptions.ImportError'>   Python 2.7.12: /home/mutation/anaconda2/bin/python2.7
> Fri Sep 23 15:28:53 2016
> A problem occurred in a Python script. Here is the sequence of function calls leading up to the error, in the order they occurred.
> 
>  /var/www/html/123.py in ()
>      41 
>      42 
> =>   43 import tensorflow as tf
>      44 hello = tf.constant('Hello, TensorFlow!')
>      45 sess = tf.Session()
> tensorflow undefined, tf undefined
>  /home/mutation/anaconda2/lib/python2.7/site-packages/tensorflow/**init**.py in ()
>      19 from **future** import absolute_import
>      20 from **future** import division
>      21 from **future** import print_function
>      22 
> =>   23 from tensorflow.python import *
> tensorflow undefined
>  /home/mutation/anaconda2/lib/python2.7/site-packages/tensorflow/python/**init**.py in ()
>      46 _default_dlopen_flags = sys.getdlopenflags()
>      47 sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_GLOBAL)
> =>   48 from tensorflow.python import pywrap_tensorflow
>      49 sys.setdlopenflags(_default_dlopen_flags)
>      50 
> tensorflow undefined, pywrap_tensorflow undefined
>  /home/mutation/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py in ()
>      26                 fp.close()
>      27             return _mod
> =>   28     _pywrap_tensorflow = swig_import_helper()
>      29     del swig_import_helper
>      30 else:
> _pywrap_tensorflow undefined, swig_import_helper = None
>  /home/mutation/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py in swig_import_helper()
>      22         if fp is not None:
>      23             try:
> =>   24                 _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
>      25             finally:
>      26                 fp.close()
> _mod undefined, imp = <module 'imp' (built-in)>, imp.load_module = <built-in function load_module>, fp = <closed file '/home/mutation/anaconda2/lib/pytho...sorflow/python/_pywrap_tensorflow.so', mode 'rb'>, pathname = '/home/mutation/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so', description = ('.so', 'rb', 3)
> <type 'exceptions.ImportError'>: libcudart.so.8.0: cannot open shared object file: No such file or directory 
>       args = ('libcudart.so.8.0: cannot open shared object file: No such file or directory',) 
>       message = 'libcudart.so.8.0: cannot open shared object file: No such file or directory'
"
4545,TypeError: Expected int for argument 'num' not <tf.Tensor 'Squeeze:0' shape=() dtype=int32>.,"I run the following code, but it has one error at    ""tf.unpack(inputs,self.bathsize)"",the error is as follows:
TypeError: Expected int for argument 'num' not <tf.Tensor 'Squeeze:0' shape=() dtype=int32>.

```
 def build(n_dict, word_dim):
       self.word_ids = tf.placeholder(tf.int32, shape=(None, None, 7), name='word_ids')
        self.bathsize = tf.shape(self.word_ids)[0]
        self.word_len=tf.shape(self.word_ids)[1]
        with tf.device(""/cpu:0""):
            self.embedding = tf.Variable(tf.random_uniform([n_dict, word_dim], -1.0, 1.0), name='embedding', dtype='float32')
            self.embedded_input = tf.nn.embedding_lookup(self.embedding, self.word_ids)
        inputs = tf.reshape(self.embedded_input, [self.bathsize, self.word_len, -1])
        # inputs=tf.transpose(inputs,[1,0,2])
        inputs = tf.nn.dropout(inputs, self.dropout)
        self.inputs = inputs

        self.input_length = tf.fill([self.word_len], self.bathsize)
        self.input_length = tf.cast(self.input_length, dtype=tf.int64)
        with tf.variable_scope('forward'):
            self.lstm_fw_cell = rnn_cell.BasicLSTMCell(n_hidden)
        with tf.variable_scope('backward'):
            self.lstm_bw_cell = rnn_cell.BasicLSTMCell(n_hidden)
        lstm_state_fw = self.lstm_fw_cell.zero_state(self.word_len, tf.float32)
        lstm_state_bw = self.lstm_bw_cell.zero_state(self.word_len, tf.float32)
        # sequence_length=self.input_length

        outputs, output_state_fw, output_state_bw = rnn.bidirectional_rnn(
            self.lstm_fw_cell, self.lstm_bw_cell,
            tf.unpack(inputs,self.bathsize),
            initial_state_fw=lstm_state_fw,
            initial_state_bw=lstm_state_bw,
            sequence_length=self.input_length
        )
```

As I know, tf.unpack have one parameter num, when the code run to ""tf.unpack(inputs,self.bathsize)"", num=None.  When the bachsize if fixed, it succeed to run. But  I want the model inputs have variable bacthsize, do you have any idea to solve this?@Geoffrey Irving
"
4544,"A problem when indexing tensor, I'm not sure if it is related to gather_nd issue","### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Might be related to https://github.com/tensorflow/tensorflow/issues/206
### Environment info

cuda-7.5
tensorflow 0.10.0rc0
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```
# logit_t is tensor of shape (batch_size, number_of_label)
# correct_t is targets, of size  (batch_size)

logit_corrects = []
# I loop through batch_size because gather_nd doesn't support gradient
for i in xrange(batch_size):
    logit_correct = tf.gather( logit_t[ i , : ], correct_t[ i ] ) 
    logit_corrects.append( logit_correct )

logit_corrects = tf.pack( logit_corrects )
self._cost = cost = tf.reduce_mean( -logit_correct )
```

However, it complained that there is no supported kernel for GPU device. 
### What other attempted solutions have you tried?
### Logs or other output that would be helpful

```
E tensorflow/core/client/tensor_c_api.cc:485] Cannot assign a device to node 'model/gradients/model/Squeeze_159_grad/Res
hape/tensor': Could not satisfy explicit device specification '/device:GPU:3' because no supported kernel for GPU device
s is available.
         [[Node: model/gradients/model/Squeeze_159_grad/Reshape/tensor = UnsortedSegmentSum[T=DT_FLOAT, Tindices=DT_INT3
2, _device=""/device:GPU:3""](model/gradients/model/Gather_39_grad/Reshape, model/gradients/model/Gather_39_grad/Reshape_1
, model/gradients/model/Squeeze_159_grad/Reshape/Squeeze)]]
Traceback (most recent call last):
  File ""learning3.py"", line 976, in <module>
    tf.initialize_all_variables().run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1553, in run
    _run_using_default_session(self, feed_dict, self.graph, session)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3684, in _run_using_default_ses
sion
    session.run(operation, feed_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 382, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 655, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 723, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 743, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'model/gradients/model/Squeeze_1
59_grad/Reshape/tensor': Could not satisfy explicit device specification '/device:GPU:3' because no supported kernel for
 GPU devices is available.
         [[Node: model/gradients/model/Squeeze_159_grad/Reshape/tensor = UnsortedSegmentSum[T=DT_FLOAT, Tindices=DT_INT3
2, _device=""/device:GPU:3""](model/gradients/model/Gather_39_grad/Reshape, model/gradients/model/Gather_39_grad/Reshape_1
, model/gradients/model/Squeeze_159_grad/Reshape/Squeeze)]]
Caused by op u'model/gradients/model/Squeeze_159_grad/Reshape/tensor', defined at:
  File ""learning3.py"", line 967, in <module>
    m = Recognizer(is_training=True, config=config)
  File ""learning3.py"", line 501, in __init__
    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py"", line 478, in gradients
    in_grads = _AsList(grad_fn(op, *out_grads))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_grad.py"", line 291, in _SqueezeGrad
    return _ReshapeToInput(op, grad)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_grad.py"", line 281, in _ReshapeToInput
    return array_ops.reshape(grad, array_ops.shape(op.inputs[0]))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 1750, in reshape
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 454, in apply_op
    as_ref=input_arg.is_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 621, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py"", line 95, in _IndexedSlicesToTensor
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 2350, in unsorted_segment_su
m
    num_segments=num_segments, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 703, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2310, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1232, in __init__
    self._traceback = _extract_stack()

...which was originally created as op u'model/Squeeze_159', defined at:
  File ""learning3.py"", line 967, in <module>
    m = Recognizer(is_training=True, config=config)
  File ""learning3.py"", line 480, in __init__
    tf.gather(logit_t[i,:], correct_t[i])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 340, in _SliceHelper
    return squeeze(sliced, squeeze_dims=squeeze_dims)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 2273, in squeeze
    squeeze_dims=squeeze_dims, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 703, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2310, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1232, in __init__
    self._traceback = _extract_stack()
```
"
4543,ValidationMonitor + readers lead to: Coordinator didn't stop cleanly: Attempted to use a closed Session.,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

http://andyljones.tumblr.com/post/133267887103/tensorflow-placeholder-queue-errors

Above not totally relevant, but couldn't find much.
### Environment info

Operating System:

Relatively recently updated Arch.
Linux terrapin 4.6.5-4-ck #1 SMP PREEMPT Mon Aug 8 20:47:19 EDT 2016 x86_64 GNU/Linux

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

```
$ ls -l /usr/local/cuda/lib/libcud*
-rw-r--r-- 1 root root 189170 Sep 17 01:09 /usr/local/cuda/lib/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Sep 17 01:09 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19 Sep 17 01:09 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 311596 Sep 17 01:09 /usr/local/cuda/lib/libcudart.so.7.5.18
-rw-r--r-- 1 root root 558020 Sep 17 01:09 /usr/local/cuda/lib/libcudart_static.a
```

If installed from binary pip package, provide:
1. A link to the pip package you installed:
   https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp35-cp35m-linux_x86_64.whl
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

```
$ python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
0.10.0
```

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Would be a bit of a hassle, and I think I already found the bug so...

I've got a fairly simple model, it is fed by a `string_input_queue()` and then batched up by `shuffle_batch()` for training and `batch()` for evaluation. I'm using an `estimator.Estimator()` for the model and then calling `fit()` on the estimator with a `ValidationMonitor`.

I get a bunch of errors in the output when trying this. Here are the relevant parts:

```
WARNING:tensorflow:Coordinator didn't stop cleanly: Attempted to use a closed Session.
WARNING:tensorflow:Given features: {'clips': <tf.Tensor 'input_test-11/batch:0' shape=(100, 3840) dtype=float32>}, required signatures: {'clips': TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(100), Dimension(3840)]), is_sparse=False)}.
WARNING:tensorflow:Given targets: Tensor(""input_test-11/batch:1"", shape=(100,), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(100)]), is_sparse=False).
I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970, pci bus id: 0000:03:00.0)
W tensorflow/core/kernels/queue_base.cc:294] _9_input_test-11/input_producer: Skipping cancelled enqueue attempt with queue not closed
WARNING:tensorflow:Coordinator didn't stop cleanly: Enqueue operation was cancelled
     [[Node: input_test-11/input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[""loc:@input_test-11/input_producer""], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](input_test-11/input_producer, input_test-11/input_producer/Identity)]]
Caused by op 'input_test-11/input_producer/input_producer_EnqueueMany', defined at:
  File ""./train.py"", line 320, in <module>
    tf.app.run()
```

Without the `ValidationMonitor`, everything is fine, but with it, most of these errors happen on every validate, and sometimes it throws the exception shown.

Of particular relevance is the error `Coordinator didn't stop cleanly: Attempted to use a closed Session.` which pretty clearly points out the problem.

You can see the exact situation the error is complaining about here:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/graph_actions.py#L757
There's even a comment saying that exactly that is about to be done, although I don't know why. If I move the `session.close()` after the `coord.join(...)` call, then all my problems go away.

Seems like a pretty clear problem with a pretty clear fix. Let me know if you need more details.
"
4542,"""Convolution"" across two matrices","Are there any plans to implement ""convolutions"" wherein we have a moving patch over a second matrix that acts as a changing filter?

This would be useful for implementing kernels such as this one:
https://papers.nips.cc/paper/5348-convolutional-kernel-networks.pdf

I asked whether a restricted version of this could be achieved by using already existing tf ops on [stack overflow](http://stackoverflow.com/questions/39632849/dot-product-of-patches-in-tensorflow). After thinking about it for a while though I don't think there's a way to do this in an efficient manner

For a simple convolution with no strides I could do something like:
`filters = tf.extract_image_patches(filter_matrix, [filter_height, filter_width], [1, 1, 1, 1], [1, 1, 1, 1], padding)`
`filters = tf.reshape(filters, [out_rows * out_cols, filter_height, filter_width, n_channels])`
`filters = tf.transpose(filters, [1, 2, 3, 0])`
`tf.conv2d(image, filters, [1, 1, 1, 1], padding)`
but it seems inefficient since I'd be generating an intermediate tensor that is going to be about filter_height \* filter_width larger than filter_matrix.

If there is a need for such an op but no plans to implement it in the immediate future, I'm happy to look into adding it.
"
4540,error when using embedding_lookup with random initial embeddings,"Hi all. I'm working on variable-length sequence data. eg. a sequence `data = np.array([4,2,3,2,0,-1,-1])`(padded to length 7, the real sequence length is 5). They are word ids. So I can get word embeddings through `tf.nn.embedding_lookup`. 

```
import tensorflow as tf
import numpy as np
import os
import cPickle


vocab_size = 105374
embed_dim = 200
input_x = tf.placeholder(dtype=tf.int32, shape=[7], name=""x"")
embedding_matrix = tf.placeholder(dtype=tf.float32, shape=[vocab_size, embed_dim], name=""embeddings"")
with tf.device(""/cpu:0""), tf.variable_scope(""embedding""):
    embedded_inputs = tf.nn.embedding_lookup(embedding_matrix, input_x)


## Test
data = np.array([4,2,3,2,0,-1,-1])
embedding_path = os.path.join(""/home/lan/data/dataset-TSU"", ""IMDB"", ""embinit.save"")
with open(embedding_path, 'rb') as f:
    embed_init = cPickle.load(f)
with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())
    print sess.run(embedded_inputs, feed_dict={input_x:data, embedding_matrix:embed_init})
```

If I use `placeholder` to hold my pre-trained word vectors `embinit.save`, `id = -1` is ok to work.
I got result from the code above like that. As I expected, the last two ids are both -1, which cannot be found in word embedding matrix. So it returned all zeros.

```
[[ 0.26877201  0.239695   -0.08297    ..., -0.030947    0.199618
  -0.24129499]
 [ 0.115232    0.123859   -0.055312   ..., -0.040216    0.176268   -0.259417  ]
 [ 0.132403    0.134068   -0.059557   ..., -0.010268    0.16242801
  -0.240173  ]
 ..., 
 [ 0.20500401  0.125486   -0.116052   ...,  0.066659    0.15658499
  -0.227872  ]
 [ 0.          0.          0.         ...,  0.          0.          0.        ]
 [ 0.          0.          0.         ...,  0.          0.          0.        ]]
```

But if I randomly initialized word embedding matrix, like that 

```
vocab_size = 105374
embed_dim = 200
input_x = tf.placeholder(dtype=tf.int32, shape=[7], name=""x"")
with tf.device(""/cpu:0""), tf.variable_scope(""embedding""):
    embedding_matrix = tf.get_variable(name=""embedding_matrix"",
                                            dtype=tf.float32,
                                            shape=[vocab_size,embed_dim],
                                            initializer=tf.random_uniform_initializer(-1.0, 1.0))
    embedded_inputs = tf.nn.embedding_lookup(embedding_matrix, input_x)
```

I got such an error:

```
tensorflow.python.framework.errors.InvalidArgumentError: indices[5] = -1 is not in [0, 105374)
```

That's to say I cannot use id=-1 in this way. Why?? Is there any solution?
"
4539,Clarification about embedding_attention_seq2seq,"This would qualify as a bug depending on what was the intention behind writing the code, the way it was written and whether I understand the workings of Tensorflow well enough.

The [`cell` input](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L755) given to
`embedding_attention_seq2seq` function in [seq2seq.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py)  seems to be used both by [the encoder](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L811) and by [the decoder](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L832). Internally, the EmbeddingWrapper class just does [`self._cell = cell`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L717)  which means that it does not create a copy of the cell. Doesn't this mean that the decoder and encoder end up sharing the parameters of their respective LSTMs? is this intentional?

EDIT: realized why they're different cells: because of the scopes.
"
4536,Convolutional RNN/LSTM,"Hi all,

This is a feature request for Convolutional RNNs such as [Convolutional LSTMS](http://arxiv.org/pdf/1506.04214v2.pdf). Is there any plan to support them in tensorflow?

I have an implementation working in [here](https://github.com/loliverhennigh/Convolutional-LSTM-in-Tensorflow) and some basic results indicating how well they do. I went about implementing them by making a new class called `ConvRNNCell` and `BasicVONVLSTMCell` and following what is seen in the tensorflow `rnn_cell.py` file. It seems I could redo the whole `rnn_cell.py` file like this. 

If this is of interest I would be more then happy to implement it for all the rnn functions.
"
4535,"Feature: Ability to restart, reopen or reset a queue.","Taking into account what @mrry said on http://stackoverflow.com/questions/39204335/can-a-tensorflow-queue-be-reopened-after-it-is-closed/39209186#39209186 ;

It's not possible to reset a queue unless you have multiple sessions. How about resetting it when you have a single session?

**Use case:**
In a single session, if one would want to run _exactly_ one epoch (where epoch_size%batch_size!=0) of a model _x_ times and report it's result exactly after one full and single epoch. It would be convenient if one could restart the queues.
"
4534,Enhancement: Locally Connected layers ,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

http://stackoverflow.com/questions/34858459/tensorflow-not-sharing-variables
https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/S5_xohNLwgs

I need to implement a network with locally connected layers (convolution without weight sharing) with 2D inputs (images).

I was wondering if the implementation of this feature is on the roadmap or if there exists some clever hacks using existing TF functions to do it.

**EDIT:** It seems this kind of layer is often used for face classification for instance in [DeepFace](https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf). 
"
4531,Error while updating tensorflow installed from source,"### Environment info

Operating System: Ubuntu 14.04.4

Installed version of CUDA and cuDNN: CUDA 8.0, cuDNN 5
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

```
-rw-r--r-- 1 root root   560184  7ì›” 29 09:12 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16  7ì›” 29 09:12 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19  7ì›” 29 09:12 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 root root   394472  7ì›” 29 09:12 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 root root   737516  7ì›” 29 09:12 /usr/local/cuda-8.0/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 78065952  7ì›” 29 09:43 /usr/local/cuda-8.0/lib64/libcudnn.so
-rwxr-xr-x 1 root root 78065952  7ì›” 29 09:43 /usr/local/cuda-8.0/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 78065952  7ì›” 29 09:43 /usr/local/cuda-8.0/lib64/libcudnn.so.5.0.5
-rw-r--r-- 1 root root 68709594  7ì›” 29 09:43 /usr/local/cuda-8.0/lib64/libcudnn_static.a
```

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)

`ca1aa4ad2d4e011e8479319e10d73281a50f7560`
1. The output of `bazel version`
   
    Build label: 0.2.3
    Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
    Build time: Tue May 17 14:21:13 2016 (1463494873)
    Build timestamp: 1463494873
    Build timestamp as int: 1463494873
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

I installed tensorflow around two monthes ago and I've used it.
Then, I tried to update by following [Stackoverflow](http://stackoverflow.com/questions/34239537/how-to-update-tensorflow-from-source).

I first tried 'git pull', it could not be merged since I have some changes in source.
So I reset the source by

```
git fetch --all
git reset --hard origin/master
```

Then I did `./configure`. But it gives errors. I attached the output of the configure below.
It's weird because I successfully update with this way before.
Actually, I also tried same way in another system (Cuda 7.0 cuDNN 4), but failed with similar error message...
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).

```
Please specify the location of python. [Default is /home/jinhyung/anaconda2/bin/python]: 
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] 
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] 
No Hadoop File System support will be enabled for TensorFlow
Found possible Python library paths:
  /home/jinhyung/anaconda2/lib/python2.7/site-packages
Please input the desired Python library path to use.  Default is [/home/jinhyung/anaconda2/lib/python2.7/site-packages]

/home/jinhyung/anaconda2/lib/python2.7/site-packages
Do you wish to build TensorFlow with GPU support? [y/N] y
GPU support will be enabled for TensorFlow
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 
Please specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 
Please specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.

INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
.
ERROR: /home/jinhyung/tensorflow/tensorflow/tensorflow.bzl:567:26: Traceback (most recent call last):
    File ""/home/jinhyung/tensorflow/tensorflow/tensorflow.bzl"", line 561
        rule(attrs = {""srcs"": attr.label_list...""), <3 more arguments>)}, <2 more arguments>)
    File ""/home/jinhyung/tensorflow/tensorflow/tensorflow.bzl"", line 567, in rule
        attr.label_list(cfg = ""data"", allow_files = True)
expected ConfigurationTransition or NoneType for 'cfg' while calling label_list but got string instead:     data.
ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension file 'tensorflow/tensorflow.bzl' has errors.
Configuration finished
```
"
4530,distribute tensorflow chief node OOM,"sorry to trouble.
my model has 6 conv layer and a softmax layer. kernel size is 3*3, channel is 192, num class is 300.
I run a distribute model with 20 Tesla K40m.
when I run one ps job and 5 worker job, everything is fine. But when I run a ps job and 19 worker jobs in 20 PC, when the chief node Initialize for node 15, it gets the OOM error.
I don't know how to deal with it.
Anyone can help me?
"
4529,can't bazel build benchmark_model with quantization deps due to -lpthread linking error,"Trying to build //tensorflow/tools/benchmark:benchmark_model per instructions at this stackoveflow post http://stackoverflow.com/questions/37953369/tensorflow-quantized-graph-for-android. Am getting error that cannot find -lpthread despite my belief that lib is unneeded and best efforts to remove it.

```
external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/../lib/gcc/arm-linux-androideabi/4.9/../../../../arm-linux-androideabi/bin/ld: error: cannot find -lpthread
collect2: error: ld returned 1 exit status
```

//tensorflow/tools/benchmark/BUILD cc_binary

```
deps = ["":benchmark_model_lib"",
        ""//tensorflow/contrib/quantization/kernels:quantized_ops"",
            ],
```

//tensorflow/contrib/quantization/kernels/BUILD:

```
deps = [
    ""//tensorflow/contrib/quantization:cc_array_ops"",
    ""//tensorflow/contrib/quantization:cc_math_ops"",
    ""//tensorflow/contrib/quantization:cc_nn_ops"",
    #""//tensorflow/core"",
    #""//tensorflow/core:framework"",
    #""//tensorflow/core:lib"",
    #""//tensorflow/core/kernels:concat_lib_hdrs"",
    #""//tensorflow/core/kernels:conv_ops"",
    #""//tensorflow/core/kernels:eigen_helpers"",
    #""//tensorflow/core/kernels:ops_util"",
    #""//tensorflow/core/kernels:pooling_ops"",
    ""//third_party/eigen3"",
    ""@gemmlowp//:eight_bit_int_gemm"",
],
```

Then run:
`bazel build -c opt --cxxopt='-std=gnu++11'--crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain tensorflow/tools/benchmark:benchmark_model --verbose_failures`

Which (with following all other instructions in linked post) succeeds with the exception that it fails to link against pthread.

I've tried removing -lpthread in `tensorflow/tensorflow.bzl`,  `tensorflow/tools/proto_text/BUILD`, `tensorflow/tools/proto_text/BUILD`, and `tensorflow/cc/BUILD` but this still results in -lpthread being linked to in final compilation. 

I've also looked at the fixes such as this one https://github.com/tensorflow/tensorflow/issues/419 which either remove -lpthread from google protobuf or link to the dummy lib but these seem to only apply to earlier versions of TensorFlow.

Apologies if this is a basic config error, I'm new to Bazel and am pretty stuck. benchmark_model and quantize_graph both compile well independently but not yet for me when combined.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

http://stackoverflow.com/questions/37953369/tensorflow-quantized-graph-for-android
https://github.com/tensorflow/tensorflow/issues/419
https://github.com/google/protobuf/issues/1373
### Environment info

Operating System: Ubuntu 14.04

Installed version of CUDA and cuDNN: 
(ml) socialh@socialh:~$ echo $LD_LIBRARY_PATH
/home/socialh/cuda/lib64:/usr/local/cuda/lib64:

(ml) socialh@socialh:~/otf$ ll /usr/local/cuda/lib/libcud*
-rw-r--r-- 1 root root 189170 Aug 31 15:22 /usr/local/cuda/lib/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Aug 31 15:22 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5*
lrwxrwxrwx 1 root root     19 Aug 31 15:22 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18*
-rwxr-xr-x 1 root root 311596 Aug 31 15:22 /usr/local/cuda/lib/libcudart.so.7.5.18*
-rw-r--r-- 1 root root 558020 Aug 31 15:22 /usr/local/cuda/lib/libcudart_static.a

(ml) socialh@socialh:~$ ll /home/socialh/cuda/lib64/
lrwxrwxrwx 1 socialh socialh       13 Jun 10 01:20 libcudnn.so -> libcudnn.so.5*
lrwxrwxrwx 1 socialh socialh       17 Jun 10 01:20 libcudnn.so.5 -> libcudnn.so.5.1.3*
-rwxr-xr-x 1 socialh socialh 60696704 Jun 10 01:18 libcudnn.so.5.1.3*
-rw-r--r-- 1 socialh socialh 59715990 Jun 10 01:18 libcudnn_static.a

If installed from source, provide 
`git rev-parse head`
640353d5d1db50d6601f9410b9d06462a8a71ce4
`bazel version`
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Jan 01 00:00:00 1970 (0)
Build timestamp: Thu Jan 01 00:00:00 1970 (0)
Build timestamp as int: 0
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

`bazel build -c opt --cxxopt '-std=gnu++11' --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain   tensorflow/tools/benchmark:benchmark_model --verbose_failures
`
with other steps taken in first StackOverflow example
### What other attempted solutions have you tried?

Tried removing all -lpthreads references, also tried removing lpthreads in bazel-out files but did not help. 
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
"
4528,How could wide_n_deep tutorial apply for muti-class categoring task?,"### Environment info

Operating System: Linux
### What other attempted solutions have you tried?

delete the lines: 

```
df_train[LABEL_COLUMN] = (df_train[""label""].apply(lambda x: '>50K' in x)).astype(int)
df_test[LABEL_COLUMN] = (df_test[""label""].apply(lambda x: '>50K' in x)).astype(int)
```

which caused 'tensorflow.contrib.learn.python.learn.monitors.NanLossDuringTrainingError: NaN loss during training'. 
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
"
4527,Error occurs when a client accesses a server on different machine,"I just get started to use distributed version of tensorflow and start by simply run a server on comp1:2222:

""
import tensorflow as tf
worker1 = ""192.168.217.227:2222""
worker_hosts = [worker1]
cluster_spec = tf.train.ClusterSpec({""worker"" : worker_hosts})
server = tf.train.Server(cluster_spec, job_name=""worker"", task_index=0)
server.join()
""

and run a client (""192.168.217.205"") :

""
import tensorflow as tf
c = tf.constant(""hello"")
with tf.Session(""grpc://192.168.217.227:2222"") as sess:
    print(sess.run(c))
""
Always got the below error:
""
UnimplementedError                        Traceback (most recent call last)
/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, _args)
    729     try:
--> 730       return fn(_args)
    731     except errors.OpError as e:

/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
    707       # Ensure any changes to the graph are reflected in the runtime.
--> 708       self._extend_graph()
    709       with errors.raise_exception_on_not_ok_status() as status:

/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _extend_graph(self)
    756           tf_session.TF_ExtendGraph(
--> 757               self._session, graph_def.SerializeToString(), status)
    758         self._opened = True

/usr/lib/python3.5/contextlib.py in **exit**(self, type, value, traceback)
     65             try:
---> 66                 next(self.gen)
     67             except StopIteration:

/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors.py in raise_exception_on_not_ok_status()
    449           compat.as_text(pywrap_tensorflow.TF_Message(status)),
--> 450           pywrap_tensorflow.TF_GetCode(status))
    451   finally:

UnimplementedError: 
""

If run the client code in the same machine as the server, I can see the correct result.
### Environment info

Operating System: Ubuntu 16.04

Installed version of CUDA and cuDNN: no
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:
1. A link to the pip package you installed:
   pip 8.1.2 from /home/***/.local/lib/python3.5/site-packages (python 3.5)
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
   $ python3 -c ""import tensorflow; print(tensorflow.**version**)""
   0.10.0rc0
"
4526,GPU-resident queue for prefetching over PCIe,"It would improve performance in some cases to be able to asynchronously prefetch data over the PCIe bus while GPU computation is taking place. A GPU-resident queue seems like the natural way to achieve this.

In the SO thread below, @yaroslavvb mentions using Variables pinned to the GPU to achieve the same effect, but I was unable to find a way to get this to work.
### Related threads:

https://stackoverflow.com/questions/38751736/understanding-tensorflow-queues-and-cpu-gpu-transfer
https://github.com/tensorflow/tensorflow/issues/3009#issuecomment-242235654
https://github.com/tensorflow/tensorflow/issues/3377#issuecomment-239966932
"
4525,Python Exceptions lead to CUDA Memory Leak,"I've been using hyperopt in it's MondoTrials configuration to do a grid search of my hyperparameters and the process has been slow going since every time my python program crashes for whatever reason, my CUDA memory is not freed

Here is the output i see from nvidia-smi:

```
$ nvidia-smi
Wed Sep 21 20:09:42 2016       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 367.44                 Driver Version: 367.44                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX TIT...  Off  | 0000:03:00.0      On |                  N/A |
| 22%   32C    P8    15W / 250W |  11848MiB / 12198MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      3475    G   /usr/lib/xorg/Xorg                             207MiB |
|    0      4579    G   compiz                                          52MiB |
|    0      5258    G   /opt/google/chrom                               86MiB |
|    0      6144    G   ...s-passed-by-fd --v8-snapshot-passed-by-fd    37MiB |
+-----------------------------------------------------------------------------+
```
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Lots of people asking questions, no-one with answers.
### Environment info

Operating System: Ubuntu 16.04

Installed version of CUDA and cuDNN: CUDA 7.5 cuDNN 5.1

If installed from binary pip package, provide:
1. https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl
2. 0.10.0
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

I don't have a minimal sample yet.
### What other attempted solutions have you tried?

Rebooting works, but is painful
### Logs or other output that would be helpful

Please let me know what logs would be useful.
"
4524,bidirectional_rnn not taking sequence_length [batch_size],"Trying to do variable sequence length with brinn and not able to use placeholder of batch_size as sequence length, instead wants num_steps size even though function description says [batch_size].  

In the code below setting up the graph gives an dimension error. (I am using the latest tensorflow version 0.10.0.)

```
import tensorflow as tf
flags = tf.flags
FLAGS = flags.FLAGS
flags.DEFINE_bool(""use_fp16"", False,
                  ""Train using 16-bit floats instead of 32bit floats"")

def data_type():
    return tf.float16 if FLAGS.use_fp16 else tf.float32

batch_size = 20
num_steps = 40
hidden_size = 64
num_layers = 2
vocab_size = 1000000

embedding_input = tf.placeholder(tf.int32, [batch_size, num_steps])
sequence_length = tf.placeholder(tf.int32, [batch_size])
embeddings = tf.get_variable(""embedding"",
                             [vocab_size, 300],dtype=data_type(),trainable=False)

embedding_input = tf.nn.embedding_lookup(embeddings, embedding_input)


initializer = tf.random_uniform_initializer(-1,1)

lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_size,
                                       initializer=initializer)
lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_size,
                                       initializer=initializer)

lstm_fw_cell = tf.nn.rnn_cell.MultiRNNCell([lstm_fw_cell] * num_layers)
lstm_bw_cell = tf.nn.rnn_cell.MultiRNNCell([lstm_bw_cell] * num_layers)

lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=0.9)
lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_bw_cell, output_keep_prob=0.9)
inputs = tf.nn.dropout(embedding_input, 0.9, noise_shape=None, seed=None)

inputs = [tf.squeeze(x) for x in tf.split(0, batch_size, inputs)]
output, _, _ = tf.nn.bidirectional_rnn(lstm_fw_cell,
                                       lstm_bw_cell,inputs,
                                       sequence_length=sequence_length,
                                       dtype=tf.float32)
```

Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 566, in merge_with
    new_dims.append(dim.merge_with(other[i]))
  File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 133, in merge_with
    self.assert_is_compatible_with(other)
  File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 108, in assert_is_compatible_with
    % (self, other))
ValueError: Dimensions 20 and 40 are not compatible
"
4521,Floating Point Exception with Conv3d ,"I am getting a floating point exception with the following example:

``` python
import numpy as np
import tensorflow as tf

inputShape = (16, 3, 64, 256, 6)
weightShape = (2, 16, 32, 6, 1028)

np_in = np.random.rand(*inputShape).astype(np.float32)
np_w = np.random.rand(*weightShape).astype(np.float32)

tf_in = tf.Variable(np_in)
tf_w = tf.Variable(np_w)

tf_out = tf.nn.conv3d(tf_in, tf_w, [1, 1, 1, 1, 1], padding=""SAME"")

init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)

np_out = sess.run(tf_out)
```

I'm currently using Cuda 8.0.27 and CUDNN 5.1.5, running on Ubuntu 16.04.1 LTS
TensorFlow built from source, commit a0d929df36ca7d6f72184371f6c3d6d877dded3e
Bazel version 0.3.1

The code does not error out if you change the kernel height (currently 16) to other numbers (I've tried 14, 15, 17, and 18).

Thanks in advanced.
"
4519,raw_rnn,"For @ebrevdo on his new [raw_rnn implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L1018).

In your [example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L1064)

When you define `next_input` as the following:

``` python
elements_finished = (time >= sequence_length)
...
next_input = tf.cond(
    finished,
    lambda: tf.zeros([batch_size, input_depth], dtype=tf.float32),
    lambda: inputs_ta.read(time))
```

Why make it return an zero input? Will this not cause the last state to be computed incorrectly (with zeros as input)?

Alternatively the zero input can be removed, by rewriting it the following way

``` python
elements_finished = (time >= (sequence_length-1))
...
next_input = inputs_ta.read(time)
```

Thanks
"
4518,GPU kernel for MatrixTriangularSolve ,"A feature request. This should be possible without too much disruption because it relies on a CUBLAS call and therefore would not require changes to stream_executor, see section 3.4.6 of the linked document. 

https://developer.nvidia.com/sites/default/files/akamai/cuda/files/Misc/mygpu.pdf

The requisite blas function is called _cublasStrsm_ and seems to be already in stream executor in this file

https://github.com/tensorflow/tensorflow/blob/6d04d601e9e8758ec4642fa9d548b7321d804d63/tensorflow/stream_executor/cuda/cuda_blas.cc

I know that @rmlarsen has been interested in this sort of area in the past. 

I don't think it is possible to do this as a user op added at run time because stream executor is not part of the tf includes in the binary, but I'd be happy to be wrong. 
"
4517,Documentation: add warning to GPU setup instructions that Intel not supported,"This is obvious in retrospect and isn't TF specific, but it might save some people some time to say something like ""Tensorflow only supports CUDA, which is proprietary to NVIDIA, and not all Macs have an NVIDIA GPU"".
"
4516,Interface improvement suggestions for tf.tranpose and tf.slice,"I would like to suggest the following improvements for the interfaces of `tf.transpose` and `tf.slice`.
1. `tf.transpose` currently accepts a boolean mask that selects the dimensions to be reversed. AFAIK this is the only op in Tensorflow that uses a mask for this purpose, whereas other, e.g. `tf.reduce_***`, take an axis number or a list of axis numbers. I think `tf.tranpose` could also be changed accordingly.
2. `tf.slice` signature is `tf.slice(input_, begin, size, name=None)`, and it is very unusual for Python that the slices are defined by their size, not by their end. Also, there is a `slice` object in Python, which would be a very natural argument for `tf.slice`. 
"
4514,a small bug in docstring of tensorflow.zeros:,"a small bug in docstring of tensorflow.zeros:
`print tf.zeros.__doc__`
it give with example:

```
  tf.zeros([3, 4], int32) ==> [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]
```

but it should be:

```
  tf.zeros([3, 4], 'int32') ==> [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]
```

![small bug](https://cloud.githubusercontent.com/assets/3587832/18719636/bee6f7f4-805a-11e6-8dca-bbe7953b9825.png)
"
4513,text summarization ,"hello everone,
any one tried to use tensorflow for text summarization ? (javascript)
any do/not do ..etc ?
"
4512,"Indexing the last N-1 elements of  a 1D tensor raises a ""stop condition"" error.","### Environment info

Operating System:
OSX 10.11.6
1. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

0.10.0rc0
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

I'm trying to select the last N-1 element of a 1D tensor with N elements, for the array [0,1,2,3,4] I'm trying to extract [1,2,3,4]

```
x = tf.convert_to_tensor([0,1,2,3,4])
b = x[:tf.shape(x)[0]-1]
```
### Logs or other output that would be helpful

  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 313, in _SliceHelper
    if s.stop is not None and s.stop < 0:
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 529, in __nonzero__
    raise TypeError(""Using a `tf.Tensor` as a Python `bool` is not allowed. ""
TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use the logical TensorFlow ops to test the value of a tensor.
(If logs are large, please upload as attachment or provide link).
"
4510,No module named 'contrib.ctc',"I am trying to run an example provided by _keras_ documentation **'imdb.py**' and this error occurs 
*\* import tensorflow.contrib.ctc as ctc
ImportError: No module named contrib.ctc **

Is this a bug or Am I missing any file?
"
4509,"fail to run py_func in parameter server, which implements as a gradient function","W tensorflow/core/framework/op_kernel.cc:968] Internal: Failed to run py callback pyfunc_2: see error log.
"
4507,Installing tensorflow on Anaconda,"I have a linux machine to which i installed Anaconda. I am following:

https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html

pip instaltion part.

To be more specific:

which python
gives

/home/user/anaconda2/bin/python

After which i entered:

export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl
And after:

sudo pip install --upgrade $TF_BINARY_URL
However, while trying:

python -c ""import tensorflow""
I get an import error:

ImportError: No module named tensorflow
"
4506,"Compute the ""local gradients""?","Hi all, 
I wonder if the tensorflow can do the backprop given the gradient from the loss?
eg.
based on the chain rule, for y=f(x)
dloss/dx = dloss/dy \* dy/dx
Can I do the backprop given the dloss/dy?
"
4505,Pack error for SparseTensor,"Hello, 
I'm using the git version of tensorflow (a6c5f8e4e013e54fed8dfcf49fb6de365f018022) without CUDA
I'm on Ubuntu 16.04

Does SparseTensor are considered as Tensor, because when i try to pack some of them, there is an error of conversion.

```
File ""/home/siniac/lplaton/Project/RNApred/src/python/BMKSOM.py"", line 31, in __init__
    self.mksom_N = tf.pack([self.filter_op() for _ in range(self.b)])
  File ""/home/siniac/lplaton/.virtualenvs/tensorflow_py3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py"", line 655, in pack
    value_shape = ops.convert_to_tensor(values[0], name=name).get_shape()
  File ""/home/siniac/lplaton/.virtualenvs/tensorflow_py3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 657, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/siniac/lplaton/.virtualenvs/tensorflow_py3/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 180, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/siniac/lplaton/.virtualenvs/tensorflow_py3/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 163, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))
  File ""/home/siniac/lplaton/.virtualenvs/tensorflow_py3/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py"", line 422, in make_tensor_proto
    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])
  File ""/home/siniac/lplaton/.virtualenvs/tensorflow_py3/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py"", line 422, in <listcomp>
    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])
  File ""/home/siniac/lplaton/.virtualenvs/tensorflow_py3/lib/python3.5/site-packages/tensorflow/python/util/compat.py"", line 45, in as_bytes
    (bytes_or_text,))
TypeError: Expected binary or unicode string, got <tensorflow.python.framework.ops.SparseTensor object at 0x7f6de596a390>
```

NB: the filter_op() method product SparseTensor 
"
4502,segmentarion fault(core dumped),"when i try to ""import tensorflow"", it often occurs that ""segmentarion fault(core dumped)"" and quits. It does not happen all the time. Maybe it can function well sometimes. I dont know why.
"
4500,Attempting to use uninitialized value lstm/LSTMCell/W_0 on Distributed TensorFlow,"On a cluster of Tensorflow, which has one ps server, and one worker:

``` python
def inference(self, inds, early_stops):

            ...

            with tf.variable_scope('lstm_encoder') as scope:
                initializer = tf.random_uniform_initializer(-0.08, 0.08)
                lstm_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_units, initializer = initializer, state_is_tuple = True)
                if self.is_training and self.keep_prob < 1:
                    lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob = self.keep_prob)

       ...
```

``` python
 with tf.device(tf.train.replica_device_setter(worker_device =
                                                          '/job:worker/task:%d' % int(os.environ['task_index']), cluster = cluster)):
                train_op, loss = inference()
```

it turns out that:

```
tensorflow.python.framework.errors.FailedPreconditionError: Attempting to use uninitialized value sentiment/lstm_encoder/RNN/LSTMCell/W_0
     [[Node: sentiment/lstm_encoder/RNN/LSTMCell/W_0_S8 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=""/job:worker/replica:0/task:0/gpu:0"", send_device=""/job:ps/replica:0/task:0/gpu:0"", send_device_incarnation=-5768356229269645157, tensor_name=""edge_226_sentiment/lstm_encoder/RNN/LSTMCell/W_0"", _device=""/job:ps/replica:0/task:0/gpu:0""](sentiment/lstm_encoder/RNN/LSTMCell/W_0)]]
     [[Node: sentiment/lstm_encoder/RNN/LSTMCell/B/Assign_S4 = _Recv[client_terminated=false, recv_device=""/job:worker/replica:0/task:0/gpu:0"", send_device=""/job:ps/replica:0/task:0/gpu:0"", send_device_incarnation=-5768356229269645157, tensor_name=""edge_214_sentiment/lstm_encoder/RNN/LSTMCell/B/Assign"", tensor_type=DT_FLOAT, _device=""/job:worker/replica:0/task:0/gpu:0""]()]]
```

I try to place variables on cpu or gpu, both of them did not work.
"
4499,tf distributed mnist between graph sync example hang,"when i use distributed mnist between graph sync example hang

example code:
[distributed_between_sync.py.txt](https://github.com/tensorflow/tensorflow/files/484050/distributed_between_sync.py.txt)

who can tell me ,,why it hang???

use tensorflow 0.10 release version

1.start PS:
   nohup python distributed_between_sync.py --job_name=""ps"" --task_index=0 > ps_0.log &
  nohup python distributed_between_sync.py --job_name=""ps"" --task_index=1 > ps_1.log & 

2.start worker0:
  nohup python distributed_between_sync.py --job_name=""worker"" --task_index=0 --max_steps=10000 > worker_0.log &

3.start worker1:
  nohup python distributed_between_sync.py --job_name=""worker"" --task_index=1 --max_steps=10000 > worker_1.log &

worker log 
[worker_0.log.txt](https://github.com/tensorflow/tensorflow/files/484048/worker_0.log.txt)
[worker_1.log.txt](https://github.com/tensorflow/tensorflow/files/484049/worker_1.log.txt)
"
4498,TF Distributed not using the full network bandwith,"Hi,
I have a one ps, 4 worker setup (2 workers are in the same machine as ps), I found no matter what batch size I choose, the network usage is always around 1.3 Gbit/s, which leads to very slow performance with small batch size. However, I do have 40 Gbit network, and when I run iperf -s/c between these two machines, I can usually get over 30 Gbit/s speed. 

I am not sure if the issue is in TF implementation or my network setup. but since iperf proved, I doubt it's my infrastructure problem.  

Below is the configuration 
ps machine: Ubuntu 14.04, TF v0.10.0
worker machine: CentOS 7, TF v0.10.0 
Python 2.7
GPU is Tesla M40 
"
4497,bidirectional_dynamic_rnn: Negative indices are currently unsupported,"I am getting the 'negaive indexing' error since I switched from using `bidirectional_rnn` to `bidirectional_dynamic_rnn`:

```
            (output_fw, output_bw), output_states  = tf.nn.bidirectional_dynamic_rnn(
                cell_fw=lstm_fw_cell,
                cell_bw=lstm_bw_cell,
                inputs=x,
                dtype=tf.float32,
                sequence_length=seq_len
                )
            output_fw = tf.transpose(output_fw, perm=[1,0,2])
            output_bw = tf.transpose(output_bw, perm=[1,0,2])
            rnn_outputs = tf.concat(2, [output_fw, output_bw])

        with tf.variable_scope('output', reuse=reuse):
            with tf.variable_scope('softmax'):
                W = tf.get_variable('W', [self.num_hidden*2, self.num_classes],
                                    initializer=tf.truncated_normal_initializer(stddev=0.1))
                b = tf.get_variable('b', [self.num_classes], initializer=tf.constant_initializer(0.1))
            logits = tf.matmul(rnn_outputs[-1], W) + b
```

```
Traceback (most recent call last):
  File ""train_lstm.py"", line 62, in <module>
    logits = model.inference()
  File ""/home/ccrmad/Code/TDLSTM/models/rnn_classifier.py"", line 70, in inference
    logits = tf.matmul(rnn_outputs[-1], W) + b
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 334, in _SliceHelper
    raise NotImplementedError(""Negative indices are currently unsupported"")
NotImplementedError: Negative indices are currently unsupported
```

How come I could negative-index the output from `bidirectional_rnn` but not `bidirectional_dynamic_rnn` this way? Thanks.
"
4496,Chit,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
### Environment info

Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:
1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
### What other attempted solutions have you tried?
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
"
4495,Documentation on how reusable variables sync between CPU and GPU,"I haven't been able to find anything online that documents how the sync of reusable variables happens between CPU and GPU. Specifically, I'm interested in if a CPU pinned variable is used multiple times in a GPU computation without being updated, does it sync only once (i.e. use a dirty flag), or will it try sync again each time?
"
4493,Tensorflow Bazel build error,"Hello,

I was pointed here from StackOverflow, hopefully I am in the right place.

I have been trying to setup tensorflow on my Raspberry Pi 3B using the guide [here](https://github.com/samjabrahams/tensorflow-on-raspberry-pi/blob/master/GUIDE.md). However I run into an error when executing the command:

`bazel build -c opt --copt=""-mfpu=neon"" --local_resources 1024,1.0,1.0 --verbose_failures tensorflow/tools/pip_package:build_pip_package`

Below is the error:

```
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
ERROR: /home/pi/makevoicedemo/tf/tensorflow/tensorflow/tensorflow.bzl:571:26: Traceback (most recent call last):
        File ""/home/pi/makevoicedemo/tf/tensorflow/tensorflow/tensorflow.bzl"", line 565
                rule(attrs = {""srcs"": attr.label_list...""), <3 more arguments>)}, <2 more arguments>)
        File ""/home/pi/makevoicedemo/tf/tensorflow/tensorflow/tensorflow.bzl"", line 571, in rule
                attr.label_list(cfg = ""data"", allow_files = True)
expected ConfigurationTransition or NoneType for 'cfg' while calling label_list but got string instead: data.
ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension file 'tensorflow/tensorflow.bzl' has errors.
INFO: Elapsed time: 0.337s
```

I was told that this is possibly due to a bug.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Error report #4319 however the thread seemed to stray from the original topic and I wasn't able to pick out if a fix had been found for the initial issue.  
### Environment info

Operating System:
Raspbian Jessie

Installed version of CUDA and cuDNN: 
N/A
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
`ls: cannot access /path/to/cuda/lib/libcud*: No such file or directory`

Commit hash (`git rev-parse HEAD`):
7df9c6860e00b91eda0e550b11d9be52d9341d85

Output of `bazel version`:

```
Build label: 0.2.1-2016-09-15 (@e7a95e5)
Build target: bazel-out/local_linux-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Sep 16 00:46:39 2016 (1473986799)
Build timestamp: 1473986799
Build timestamp as int: 1473986799

```
### What other attempted solutions have you tried?

Attempted to install from pip, both attempts (python 2.7, and 3.3+) failed.
"
4492,Missing C++ headers for Session API in PIP packages.,"C++ headers for the session APIs, such as tensorflow/core/public/<session.h session_options.h tensor_c_api.h> are not included in the official PIP packages. Could we have them included in the list of headers to export in the framework_headers target in tensorflow/core/BUILD or is there a reason to exclude them?
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

https://github.com/tensorflow/tensorflow/issues/3536
https://github.com/tensorflow/tensorflow/issues/720
### Environment info

Operating System: Ubuntu 16.04 (Xenial)

Installed version of CUDA and cuDNN: n/a

If installed from binary pip package, provide:
1. A link to the pip package you installed:
http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl
1. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
   0.9.0
"
4487,config parameter in tf.contrib.learn.Estimator is not working!,"I'm using TensorFlow 0.10 on Ubuntu (GPU). I have an estimator as follow:

```
  estimator = tf.contrib.learn.Estimator(
      model_fn=model_fn,
      model_dir=MODEL_DIR,
      config=tf.contrib.learn.RunConfig(
          save_checkpoints_secs=1))
```

I want to save checkpoints in every second. 
The problem is that after running, checkpoints are saved every 300 iterations and `config` parameter does not work!
"
4486,why GrpcSession not support RunMetadata in r0.10,
4485,Unable to retrieve flower_photos.tgz using curl command,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
### Environment info

Operating System:Ubuntu 12.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

I have installed TensorFlow using docker by running the following command $ docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow

If installed from binary pip package, provide:
1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
### What other attempted solutions have you tried?
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
I am trying to retrieve the set of images for TensorFlow for Poets using the following command curl -O http://download.tensorflow.org/example_images/flower_photos.tgz but the below mentioned error is being thrown

~/tf_files$ curl http://download.tensorflow.org/example_images/flower_photo.tgz
<?xml version='1.0' encoding='UTF-8'?><Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message></Error>
"
4483,source install transorflow  configure  error,"i install Tensorflow on Ubuntu16.04 GTX1080  CUDA 8.0 CUDNN 5.0
when git Tensorflow configure occure this issue:

chenl@FS-S07:~/tensorflow$ ./configure 
~/tensorflow ~/tensorflow
Please specify the location of python. [Default is /usr/bin/python]: 
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] n
No Google Cloud Platform support will be enabled for TensorFlow
Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]

/usr/local/lib/python2.7/dist-packages
Do you wish to build TensorFlow with GPU support? [y/N] y
GPU support will be enabled for TensorFlow
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 5
Please specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 
.
INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
.
ERROR: /home/chenl/tensorflow/tensorflow/tensorflow.bzl:571:26: Traceback (most recent call last):
    File ""/home/chenl/tensorflow/tensorflow/tensorflow.bzl"", line 565
        rule(attrs = {""srcs"": attr.label_list...""), <3 more arguments>)}, <2 more arguments>)
    File ""/home/chenl/tensorflow/tensorflow/tensorflow.bzl"", line 571, in rule
        attr.label_list(cfg = ""data"", allow_files = True)
expected ConfigurationTransition or NoneType for 'cfg' while calling label_list but got string instead: data.
ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension file 'tensorflow/tensorflow.bzl' has errors.

who can help me > Thank you very much 
"
4481,GradOutput in Torch,"Hi all, 
I wonder that is there any function that return the gradOutput (gradient w.r.t. the output of the module) of a specific layer like Torch? 
Ex: 
assume f,g are simple linear functions
f(x)=x'
g(x')=y
I want to directly get the value dy/df(x), the gradient of output
instead of the gradient of variable in f
"
4480,`UnimplementedError: File system scheme` in sess.restore() with the path including colons.,"I found that we get

```
tensorflow.python.framework.errors.UnimplementedError: File system scheme {{ part of path }} not implemented
```

When we try to restore the model from the directory which has colons in its path.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
- https://github.com/ibab/tensorflow-wavenet/pull/28
- https://github.com/carpedm20/variational-text-tensorflow/commit/4dee0ce68814f212d2ece7a9b289c41cc4c35cd6
- I have searched here tensorflow's issues with the words such as ""colon"", ""File system scheme"", or ""UnimplementedError"" but I couldn't find someone note this.
### Environment info
- Mac OS X 10.11.6
- Python 3.5.2
- Tensorflow 0.10.0 (from pip, https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.10.0-py3-none-any.whl )
- Installed version of CUDA and cuDNN: None
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

``` python
import tensorflow as tf

s = tf.Session()
saver = tf.train.Saver()
saver.restore(sess, ""path/to/somewhere/with:colon/model.ckpt-0"") # where the path is exist with valid checkpoint.
```
### What other attempted solutions have you tried?
- Renamed the directory containing the checkpoint => fixed.
### Logs or other output that would be helpful

```
Traceback (most recent call last):
  File ""./generate.py"", line 94, in <module>
    main()
  File ""./generate.py"", line 57, in main
    saver.restore(sess, args.checkpoint)
  File ""/Users/***/.virtualenvs/***/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 1126, in restore
    if not file_io.get_matching_files(save_path):
  File ""/Users/***/.virtualenvs/***/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py"", line 53, in get_matching_files
    return pywrap_tensorflow.GetMatchingFiles(compat.as_bytes(filename), status)
  File ""/usr/local/Cellar/python3/3.5.2/Frameworks/Python.framework/Versions/3.5/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/Users/***/.virtualenvs/***/lib/python3.5/site-packages/tensorflow/python/framework/errors.py"", line 450, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors.UnimplementedError: File system scheme {{ path/to/somewhere/with }} not implemented
```

Note `.generate.py` is the user code and {{ path/to/somewhere/with }} was actual path of the model, but terminated before the colon.
"
4479,the nightly binaries of TF is 404 error,"hello, in the Installation , when i try to install the nightly build ""Linux GPU: Python 2"" , it tells ""404 error"". how should i do ?
"
4478,Frame of the Variable,"Hello

I am using tf.scan for implementing memory augmented network and when trying to run tf.initialize_all_variables() getting the error about frame of the variables:

```
All inputs to node scan_1/while/Variable_13/Assign must be from the same frame.
```

As I understood this frame is some inner identification of the variables, so what should I do to get rid of this error? Scan works if I do not define any variables outside the step, but I need at least weights-biases and loss-accuracy variables.
"
4476,"build errors on missing ""//base""","When running ./configure on OS X with CUDA support enabled with bazel 0.3.1-homebrew, these errors occur:

```
ERROR: /Users/jmhodges/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:692:1: no such package 'base': BUILD file not found on package path and referenced by '//tensorflow/core:ios_tensorflow_test_lib'.
ERROR: /Users/jmhodges/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such package 'base': BUILD file not found on package path and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.
ERROR: /Users/jmhodges/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such target '//tensorflow/core:android_tensorflow_lib_lite_no_rtti_lite_runtime': target 'android_tensorflow_lib_lite_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /Users/jmhodges/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.
ERROR: /Users/jmhodges/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such target '//tensorflow/core:android_proto_lib_no_rtti_lite_runtime': target 'android_proto_lib_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /Users/jmhodges/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.
ERROR: Evaluation of query ""deps((//... union @bazel_tools//tools/jdk:toolchain))"" failed: errors were encountered while computing transitive closure.
```

This seems to be because of targets like ""ios_tensorflow_test_lib"" and, so on, depending on a `//base` and `//tensorflow/core:android_tensorflow_lib_lite_no_rtti_lite_runtime` that do not exist.

Off hand, I'm not able to figure out what this was trying to refer to. Perhaps it used to exist?
"
4474,Feature request: Patch extracting given location implementation needed.,"Hello there,
## Feature

I consider a lot but finally decide put this request here.
I know there are some matrix operation and image operations like `image_patch_extract` and `image.extract_glimpse()`.
But what I want is extract the patches given several locations.

Input Tensor: 

``` python
         batch image: [batch_size, image_height, image_width, image_channel]
         batch locations: [batch_size, num_patches, 2]
```

Output Tensor: 

``` python
        [batch_size, num_patches, patch_height, patch_width, image_channel]
or 
        [batch_size * num_patches, patch_height, patch_width, image_channel]
```
## Reference:

Georgis have done the similar thing and he made the a patch in v0.8.0 and only cpu supports.

```
https://github.com/trigeorgis/tensorflow
```

But when I use it in v0.10.0, it requires me to define a shape function. 
I really want to use it in the future, so I am glad it can be added as a new feature.
## Other solution I tried

I have tried use `extract_glimpse()` instead.

``` python
output_list = [[] for _ in range(batch_size)]   # create n_patch list
locations = locations/image_size  # normalize the location
for j in range(n_patch):
    patch_one = tf.image.glimpse(batch_image, tf.constant([20, 20]), locations[:, j, :], centered=False)
    for i in range(batch_size):
          output_list[i].append(patch_one[j])  # add tensor to each list
patches = tf.pack(output_list)  # pack the list to tensor, size = [batch_size, n_patch, patch_size, patch_height, image_channel]
```
"
4473,Make the gradient computation available in the C api,"When writing TensorFlow bindings for other languages, a major difficulty is that the C api does not give a simple way to add gradient computations to a graph. In the case of the [tensorflow-ocaml](https://github.com/LaurentMazare/tensorflow-ocaml) bindings, we ended up re-implementing back-propagation and having to register gradients for many operators. Providing support for this in the C api would allow us to remove all this code.

There is already a [todo](https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/c/c_api.h#L505) for this in the code but having a proper issue would make this easier to track.
"
4471,Add `tf.write_file` op,"Tensorflow currently has a `tf.read_file` op, as documented [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard1/tf.read_file.md).

Can we have a `tf.write_file` as well?

It could be defined as something like:

``` python
def write_file(tensor, filename, name=None):
    def write_data(data):
        with open(filename, ""wb"") as handle:
            handle.write(data)

    with tf.name_scope(""WriteFile"" or name):
        tensor = tf.convert_to_tensor(tensor, name=""Values"")
        return tf.py_func(write_data, tensor)
```

There are some usecases where this would be helpful, such as generating images and audio and then writing them to disk. Presumably, this would be done after an op such as `encode_jpeg` or `encode_audio`.

If this already exists in some form, please close this â€“ my apologies, but some googling as well as searching on Github did not find it.
"
4469,Mac GPU installation,"After I run a python3 script I get the following statements and do not know where the 3 errors are coming from. I am using cudnn v5.0 but obviously I have gone wrong somewhere along the installation pipeline. Any help would be fantastic.

```
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.dylib locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.dylib locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.dylib locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.1.dylib locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.dylib locally
number of elements at final reshape = %d. 61440
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] OS X does not support NUMA - returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:
name: GeForce GT 750M
major: 3 minor: 0 memoryClockRate (GHz) 0.9255
pciBusID 0000:01:00.0
Total memory: 2.00GiB
Free memory: 1.28GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)
E tensorflow/stream_executor/cuda/cuda_dnn.cc:354] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
E tensorflow/stream_executor/cuda/cuda_dnn.cc:361] error retrieving driver version: Invalid argument: expected %d.%d or %d.%d.%d form for driver version; got """"
E tensorflow/stream_executor/cuda/cuda_dnn.cc:321] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
F tensorflow/core/kernels/conv_ops.cc:457] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)
Abort trap: 6
```
"
4468,Allow crop_to_bounding_box to work batchwise,"Right now `crop_to_bounding_box` only works with single images. 
"
4467, Why 'tf.python_io.TFRecordWriter' is so SLOW and STORAGE-CONSUMING in TensorFlow?,"I'm going to write to TFRecord file using this [code](https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/examples/how_tos/reading_data/convert_to_records.py#L68) The problem is that this process is very slow, such that it's not feasible to write a large dataset even in days! It's just a writer that serialize to disk. Why it's so slow?! Another problem is that the size of the output file is 10 times greater than the original file!

does anyone know any way to speed up the process of TFRecordWriter and compress the result?
"
4466,Android: Why is RandomShuffleQueue needed for testing?,"Hello, 

I want to test a learnt model under Python on android, I'm on Tensorflow 0.9.
To do so, I freezed my graph to have a single pb file with the graph and weight. I used Queues to manage my learning batches.

When running my session on Android, I specify the input tensor by its name ""input_node"", which is the data layer as input in my network. 
X = tf.reshape(X, [-1, W, H, 1], name=""input_node"")
and call the ""output_node"" layer : 
output = tf.reshape(h_fc11, shape=[-1, 8], name=""output_node"")

The batch generation is done before, so it should not been used when testing.
But I have the following error:
tensorflow_jni.cc:312 Error during inference: Invalid argument: No OpKernel was registered to support Op 'RandomShuffleQueue' with these attrs
[[Node: shuffle_batch/random_shuffle_queue = RandomShuffleQueue[capacity=10750, component_types=[DT_FLOAT, DT_FLOAT], container="""", min_after_dequeue=10000, seed=0, seed2=0, shapes=[[10000], [8]], shared_name=""""]()]]

It seems that the batch generation layer is called (my images are 100x100 and I have 8 outputs), but I don't know why.

When testing the same model with the same input/output layers though the image_labelling.cc directly on Mac (building with Bazel), I don't have the error.

I do not understand why the RandomShuffleQueue is needed when testing. Am I missing something to specify the part of the graph I want to use? Are all the layers of the graph verified even if not used?

Thanks.
"
4465,Distributed training optimization,"After playing with the current distributed training implementation for a while, I think it views each GPU as a separate worker.However, It is common now to have 2~4 GPUs in one box. Isn't it better to adopt the single box multi-GPU methodology to compute average gradients in single box first and then sync up across multiple nodes? This way it ease the I/O traffic a lot, which is always the bottleneck in data parallelism.  
"
4463,Installation links in README.md is wrong,"https://github.com/tensorflow/tensorflow#installation

The links to the nightly images are invalid and returns 404.
"
4461,Tutorials documentation spelling mistake,"At url: https://www.tensorflow.org/versions/r0.10/tutorials/image_recognition/index.html

I've found that In following paragraph '299 pixel **high**' is written, which I think should be '299 pixel **hight**'

The whole paragraph is following in 'Usage with the C++ API' section:

""This gives us a vector of Tensor objects, which in this case we know will only be a single object long. You can think of a Tensor as a multi-dimensional array in this context, and it holds a 299 pixel **high**, 299 pixel width, 3 channel image as float values. If you have your own image-processing framework in your product already, you should be able to use that instead, as long as you apply the same transformations before you feed images into the main graph.""
"
4459,(#4380) TensorFlow master build failing: error: invalid initialization of reference  ,"Created a new issue as I cannot re-open #4380 .

@poxvoculi @girving Do you have any idea on #4380 ?
"
4458,bazel build tensorflow/python/tools:strip_unused throws linker error,"I am trying to solve the problem of running a retrained model on iOS described in issue [2883](https://github.com/tensorflow/tensorflow/issues/2883). However when I call this command I run into linker error:

> bazel build tensorflow/python/tools:strip_unused

The linker error says:

> bazel-out/local-fastbuild/bin/tensorflow/core/libversion_lib.a(version_info.pic.o): In function `tf_git_version()':
> version_info.cc:(.text+0x0): multiple definition of`tf_git_version()'
> bazel-out/local-fastbuild/bin/tensorflow/core/libframework_internal.lo(version_info.pic.o):version_info.cc:(.text+0x0): first defined here
> bazel-out/local-fastbuild/bin/tensorflow/core/libversion_lib.a(version_info.pic.o): In function `tf_compiler_version()':
> version_info.cc:(.text+0xd): multiple definition of`tf_compiler_version()'
> bazel-out/local-fastbuild/bin/tensorflow/core/libframework_internal.lo(version_info.pic.o):version_info.cc:(.text+0xd): first defined here
> collect2: error: ld returned 1 exit status
> Target //tensorflow/python/tools:strip_unused failed to build

I tried it on both Mac and Ubuntu Linux with the same error. I am operating on git commit a6c5f8e. Can someone help me how to build this tool?
"
4456,when will beam search be implemented,"The current version of tensorflow still use a greedy algorithm for seq2seq decoder. I wonder when will beam search be implemented in future versions since this has already been discussed in other issues.
"
4455,Distributed TF managed session: inter_op_parallelism_threads and intra_op_parallelism_threads has no effect,"We are using:

``` .python
with sv.managed_session(
  server.target,
  config=tf.ConfigProto(log_device_placement=True,
                        inter_op_parallelism_threads=1,
                        intra_op_parallelism_threads=1),
) as sess:
```

but TensorFlow still launches a number of threads proportional to the number of cores on each process. I counted +50 threads used by each process on my 24 vcore machine.

What can we do to limit the number of threads in a managed session.

EDIT: @yaroslavvb identified the problem. See the end of the thread about Session vs Server options.
"
4453,Remove state_ops.variable_op()?,"I'm reading the source code, and I find there is a comment saying that 
`# TODO(mrry): Move this to where it is used, so we can get rid of this op wrapper?`
in the [state_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/state_ops.py#L151)

So, I searched and found that only `variables.py` calls it twice times [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variables.py#L305), and [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variables.py#L322).

BTW, some `xx_test.py` also call it, but I think it's easy to change. 
In a word, could I handle the issue and update all the files?@mrry 

I output the file list which call the `variable_op()`:

```
Variables.py
Variables_test.py
Variable_ops_test.py
Tensor_util_test.py
Moving_averages_test.py
Learning_rate_decay_test.py
Graph_util_test.py
Control_flow_ops_py_test.py
```
"
4451,TensorFlow demo app crashes with my own model,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

https://github.com/tensorflow/tensorflow/issues/1269
### Environment info

Operating System: Linux ubuntu

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`) 
   ce3572a08b9ecfa5c8dd94921c2011f37b58e608
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
1. Followin this tutorial: https://www.tensorflow.org/versions/r0.9/how_tos/image_retraining/index.html
   I built a model via transfer learning using custom images. 
2. I edited WORKSPACE and built demo.apk
   App crashed.
3. Then i used inception5h.zip  as referenced in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android.
   The app run without any problem.
### What other attempted solutions have you tried?

Then i looked at issue https://github.com/tensorflow/tensorflow/issues/1269 for guidance.
1.    ---Edited tensorflowImagelistener.java---

``` java
  private static final int NUM_CLASSES = 4; //1001;
  private static final int INPUT_SIZE = 299; //224
  private static final int IMAGE_MEAN = 128; //117
  private static final float IMAGE_STD = 128; //1;
  private static final String INPUT_NAME = ""Mul:0""; //""input:0""; 
  private static final String OUTPUT_NAME = ""final_result:0""; //""output:0"";
```
1. ---I even stripped the graph using strip_unused.py using ----

``` sh
  bazel build tensorflow/python/tools:strip_unused
  bazel-bin/tensorflow/python/tools/strip_unused --input_graph=/tmp/inception.pb   --output_graph=/tmp/stripped_inception.pb --input_node_names=""Mul:0"" --output_node_names=""final_result"" --input_binary=true
```
1. --In strip_unused_lib.py i added if else clause as referenced by @dmirk quick-n-dirty from #1269---

``` python
      if ""jpeg"" in node.op.lower():
        placeholder_node.attr[""dtype""].CopyFrom(tf.AttrValue(
            type=tf.uint8.as_datatype_enum))
      else:
        placeholder_node.attr[""dtype""].CopyFrom(tf.AttrValue(
            type=placeholder_type_enum))
```
1. For some reason i still got this error. See attached 
   [oldlogcat.txt](https://github.com/tensorflow/tensorflow/files/479719/oldlogcat.txt)

``` sh
tensorflow_jni.cc:304 Error during inference: Invalid argument: No OpKernel was registered to support Op 'DecodeJpeg' with these attrs.  Registered kernels:
```
1. As a last resort i did this modification to tensorflow_jni.cc as referenced in #1269. See attached [logcat.txt](https://github.com/tensorflow/tensorflow/files/479718/logcat.txt)

``` cc
      // Copy 3 values
      input_tensor_mapped(0, i, j, 0) =
          (static_cast<float>(src->red) - g_image_mean) / g_image_mean;//g_image_std;
      input_tensor_mapped(0, i, j, 1) =
          (static_cast<float>(src->green) - g_image_mean) / g_image_mean;//g_image_std;
      input_tensor_mapped(0, i, j, 2) =
          (static_cast<float>(src->blue) - g_image_mean) / g_image_mean;//g_image_std;
```

but i think this dint help any much. however i dint find the opkernel error
### Logs or other output that would be helpful
"
4444,Code snippet in the TensorFlow for Poets codelab is confusing,"Based on [this](http://stackoverflow.com/q/39562938/3574081) Stack Overflow question.

In [section 5](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/index.html?index=..%2F..%2Findex#4), the first `python` command has a leading `#`:

``` bash
# python tensorflow/examples/image_retraining/retrain.py \
--bottleneck_dir=/tf_files/bottlenecks \
--how_many_training_steps 500 \
--model_dir=/tf_files/inception \
--output_graph=/tf_files/retrained_graph.pb \
--output_labels=/tf_files/retrained_labels.txt \
--image_dir /tf_files/flower_photos
```

The second one doesn't:

``` bash
python tensorflow/examples/image_retraining/retrain.py \
--bottleneck_dir=/tf_files/bottlenecks \
--model_dir=/tf_files/inception \
--output_graph=/tf_files/retrained_graph.pb \
--output_labels=/tf_files/retrained_labels.txt \
--image_dir /tf_files/flower_photos
```

If you copy and paste the first command, you'll get an unintuitive error like `""--bottleneck_dir=/tf_files/bottlenecks: No such file or directory""`. We should remove the leading `#` or somehow make it clearer (and consistent across both snippets) that this is a shell prompt.
"
4443,Breaking change of `sparse_softmax_cross_entropy_with_logits` in v0.10: bug or feature,"When dealing with sequences of different lengths and RNNs, it is very common to pad ""out-of-range"" part of the input sequence. More than that, labels should also be padded. It is convenient to pad labels with bogus value `-1` and not introducing any new valid label values.

Prior to version 0.10, `sparse_softmax_cross_entropy_with_logits` returned `0.0` for logits that correspond to `-1` labels. These cross-entropy values thus weren't influencing calculation of loss, effectively being ignored. 

Somewhere along the way between v0.9 and v0.10 this behavior has changed, and:
- `nan` is now being returned for logits that correspond to `-1` labels
- gradient calculation results in `nan`

New behavior silently breaks the code that rely on the assumption that zero cross-entropy will be returned and valid gradients will be calculated in case of labels `-1` .

The goal of this issue is to discuss the possibility and practicability of reverting to the old behavior, i.e.:
- returning `0.0` for logits that correspond to `-1` labels
- fixing gradient computation

I would also like to point out that it would be very good to document changes, that could potentially break user code, in the release notes (in the section ""breaking changes""). 

<sub>Sad story: in my case, it took some time to understand what's happening. Current version of documentation says that passing `-1` labels is illegal and leads to incorrect gradient calculations. It was not the case when I've written my code. I couldn't know about this change, as I cannot possibly re-read all the documentation every time I update tensorflow. I spent about 2 weeks exploring NaN loss and thought it's a gradient explosion. I almost threw away a perfectly good model. Advice for future me: if stuck, read the docs AGAIN ;) </sub>

Related:  #1234 
### Code snippet to reproduce the issue

I tried hard to produce a minimum amount of code, but, well, ... it's still huge.
(gist:  [b98a33b2513aac8dca8f70a0a16bc592](https://gist.github.com/ivan-aksamentov/b98a33b2513aac8dca8f70a0a16bc592))

Imagine `x` to be an output of RNN. Notice that  `X_i = 0.0`, `Y_i = -1` and `MASK_i` = 0.0 for pading (indices greater then `LENGTH`)

```
from __future__ import print_function, division
import tensorflow as tf

I = 1  # input size
T = 6  # sequence length (num timesteps)
B = 3  # batch size
C = 7  # number of classes

X = [
    [[0.0], [1.0], [2.0], [3.0], [0.0], [0.0]],
    [[0.0], [1.0], [2.0], [0.0], [0.0], [0.0]],
    [[0.0], [1.0], [2.0], [3.0], [4.0], [5.0]],
]

Y = [
    [0, 1, 2, 3, -1, -1],
    [0, 1, 2, -1, -1, -1],
    [0, 1, 2, 3, 4, 5],
]

MASK = [
    [1, 1, 1, 1, 0, 0],
    [1, 1, 1, 0, 0, 0],
    [1, 1, 1, 1, 1, 1],
]

LENGTHS = [4, 3, 6]

x = tf.placeholder(shape=[B, T, I], dtype=tf.float32, name=""x"")
y = tf.placeholder(shape=[B, T], dtype=tf.int32, name=""y"")
mask = tf.placeholder(shape=[B, T], dtype=tf.bool, name=""mask"")
lengths = tf.placeholder(shape=[B], dtype=tf.int32, name=""lengths"")

x_flat = tf.reshape(x, shape=[B * T, I])
y_flat = tf.reshape(y, shape=[B * T])
mask_flat = tf.reshape(mask, shape=[B * T])

w = tf.Variable(tf.random_normal(mean=0.0, stddev=1e-2, shape=[I, C]), name=""w"")
b = tf.Variable(tf.random_normal(mean=0.0, stddev=1e-2, shape=[C]), name=""b"")

z = tf.nn.xw_plus_b(x_flat, w, b, name=""z"")

xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(z, y_flat)

xentropy_masked = tf.select(
    mask_flat,
    xentropy,
    tf.zeros_like(xentropy)
)

xentropy_masked = tf.reshape(xentropy_masked, [B, T])

xentropy_sum = tf.reduce_sum(xentropy_masked, reduction_indices=1)

xentropy_sum_norm = tf.div(
    xentropy_sum, tf.cast(lengths, dtype=tf.float32)
)

loss = tf.reduce_mean(xentropy_sum_norm)

opt = tf.train.GradientDescentOptimizer(learning_rate=1e-3)
grads_and_vars_op = opt.compute_gradients(loss)

with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())
    grads_ops = [g for (g, v) in grads_and_vars_op]

    fetches = [
        xentropy, xentropy_masked
    ]
    fetches.extend(grads_ops)

    xe, xe_masked, grads, _ = sess.run(fetches, feed_dict={
        x: X,
        y: Y,
        mask: MASK,
        lengths: LENGTHS
    })

    print(""xe:\n"", xe)
    print(""xe_masked:\n"", xe_masked)
    print(""grads:\n"", grads)

print(""tf.__version__: "", tf.__version__)
```
### Desired, old output

with [tensorflow-0.9.0-cp27-none-linux_x86_64.whl](https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl)

```
xe:
 [ 1.94680917  1.95220029  1.97665489  1.90392613  0.          0.
  1.94680917  1.95220029  1.97665489  0.          0.          0.
  1.94680917  1.95220029  1.97665489  1.90392613  1.99289608  1.89933121]
xe_masked:
 [[ 1.94680917  1.95220029  1.97665489  1.90392613  0.          0.        ]
 [ 1.94680917  1.95220029  1.97665489  0.          0.          0.        ]
 [ 1.94680917  1.95220029  1.97665489  1.90392613  1.99289608  1.89933121]]
grads:
 [[ 0.23479398 -0.01371239 -0.27168125 -0.16881087  0.00719513 -0.03152646
   0.24374181]]
tf.__version__:  0.9.0
```
### New output

with [tensorflow-0.10.0-cp27-none-linux_x86_64.whl](https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl)

```
xe:
 [ 1.96044326  1.94155979  1.98353648  1.90060949         nan         nan
  1.96044326  1.94155979  1.98353648         nan         nan         nan
  1.96044326  1.94155979  1.98353648  1.90060949  1.93305326  1.96117282]
xe_masked:
 [[ 1.96044326  1.94155979  1.98353648  1.90060949  0.          0.        ]
 [ 1.96044326  1.94155979  1.98353648  0.          0.          0.        ]
 [ 1.96044326  1.94155979  1.98353648  1.90060949  1.93305326  1.96117282]]
grads:
 [[ nan  nan  nan  nan  nan  nan  nan]]
tf.__version__:  0.10.0
```
"
4442,predict function loads only the first batch from the input_fn,"I have a regular `input_fn_valid` function. 
When using `estimator.evaluate(input_fn=input_fn_valid)` everything is OK and the model is evaluated on the whole dataset. But calling `probs = estimator.predict(input_fn=input_fn_valid)` only returns the predicted values of the first batch (The size of `probs` is equal to `batch_size` parameter). 
I'm using TensorFlow 0.10 and test it on Ubuntu (GPU) and MAC OSX (CPU).
Is it a bug in the predict function?
"
4441,tensorflow read multi-type csv error,"my csv file has 10 column, CONTINUOUS_COLUMNS are  float , and left are string.
my code : 
`parse_fn = lambda example:tf.decode_csv(example, [tf.constant([], dtype=tf.float32) if i in CONTINUOUS_COLUMNS else tf.constant([], dtype=tf.string)  for i in COLUMNS ]  ,field_delim='\t' )`
`inputs = tf.contrib.learn.read_batch_examples([FLAGS.train], 256, tf.TextLineReader,num_epochs=1,queue_capacity=10000, parse_fn=parse_fn)`

error 
`Traceback (most recent call last):
  File ""wide-batch.py"", line 241, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""wide-batch.py"", line 238, in main
    train_and_eval()
  File ""wide-batch.py"", line 232, in train_and_eval
    m.fit(input_fn=lambda: input_fn(stat), steps=FLAGS.train_steps,monitors=[validation_monitor])
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 240, in fit
    max_steps=max_steps)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 548, in _train_model
    features, targets = input_fn()
  File ""wide-batch.py"", line 232, in <lambda>
    m.fit(input_fn=lambda: input_fn(stat), steps=FLAGS.train_steps,monitors=[validation_monitor])
  File ""wide-batch.py"", line 180, in input_fn
    inputs = tf.contrib.learn.read_batch_examples([FLAGS.train], 256, tf.TextLineReader,num_epochs=1,queue_capacity=10000, parse_fn=parse_fn)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/graph_io.py"", line 82, in read_batch_examples
    read_batch_size=read_batch_size, parse_fn=parse_fn, name=name)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/graph_io.py"", line 210, in read_keyed_batch_examples
    allow_smaller_final_batch=allow_smaller_final_batch)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/input.py"", line 899, in shuffle_batch_join
    tensor_list_list = _validate_join(tensor_list_list)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/input.py"", line 421, in _validate_join
    for tl in tensor_list_list]
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 737, in convert_n_to_tensor_or_indexed_slices
    as_ref=as_ref))
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 698, in convert_to_tensor_or_indexed_slices
    return convert_to_tensor(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 621, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 630, in _autopacking_conversion_function
    return _autopacking_helper(v, inferred_dtype, name or ""packed"")
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 572, in _autopacking_helper
    ""%s to %s (Tensor is: %r)"" % (elem.dtype, dtype, elem))
TypeError: Cannot convert a list containing a tensor of dtype <dtype: 'string'> to <dtype: 'float32'> (Tensor is: <tf.Tensor 'read_batch_examples/read/DecodeCSV:1' shape=() dtype=string>)`
"
4440,GradientChecker file not found,"link - https://github.com/tensorflow/tensorflow/tree/r0.10/tensorflow/python/kernel_tests/gradient_checker.py

refered in -
https://www.tensorflow.org/versions/r0.9/how_tos/adding_an_op/index.html#define-the-ops-interface
"
4439,tensorflow add new op : could attr accept  scalar tensor ? ,"In  https://www.tensorflow.org/versions/r0.10/how_tos/adding_an_op/index.html  
, I can't find detail doc about this,   could anyone  give   more detail info?   
"
4438,tf.import_graph_def() requires a non-empty name,"After updating version of tenserflow  there is now an issue when loading with my old models. 

./importer.py"", line 241, in import_graph_def
    raise ValueError('tf.import_graph_def() requires a non-empty `name` '
ValueError: tf.import_graph_def() requires a non-empty `name` if `input_map` is used.

The same is case when i try colorize model colorize-20160110

how to fix this. The code is given below
import tensorflow as tf
import skimage.transform
from skimage.io import imsave, imread

def load_image(path):
    img = imread(path)
    # crop image from center
    short_edge = min(img.shape[:2])
    yy = int((img.shape[0] - short_edge) / 2)
    xx = int((img.shape[1] - short_edge) / 2)
    crop_img = img[yy : yy + short_edge, xx : xx + short_edge]
    # resize to 224, 224
    img = skimage.transform.resize(crop_img, (224, 224))
    # desaturate image
    return (img[:,:,0] + img[:,:,1] + img[:,:,2]) / 3.0

shark_gray = load_image(""shark.jpg"").reshape(1, 224, 224, 1)

with open(""colorize.tfmodel"", mode='rb') as f:
    fileContent = f.read()

graph_def = tf.GraphDef()
graph_def.ParseFromString(fileContent)
grayscale = tf.placeholder(""float"", [1, 224, 224, 1])
tf.import_graph_def(graph_def, input_map={ ""grayscale"": grayscale }, name='')

with tf.Session() as sess:
    inferred_rgb = sess.graph.get_tensor_by_name(""inferred_rgb:0"")
    inferred_batch = sess.run(inferred_rgb, feed_dict={ grayscale: shark_gray })
    imsave(""shark-color.jpg"", inferred_batch[0])
    print (""saved shark-color.jpg"")
"
4437,why there are no documents about tf.nn.seq2seq   ,"the file tensorflow/python/ops/seq2seq contains models like tf.nn.seq2seq.embedding_attention_seq2seq, but there are not any documentation about these models in the online documentation (https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#neural-network)

or there is but i didn't find it?
"
4436,How to restore the model when use distributed  tensorflow ??,"I write a simple distributed tensorflow example, the code is here:
https://github.com/thewintersun/distributeTensorflowExample

I use 2 server as the ps server, 2 server as the worker server. 
**The command is as follow:**
**At ps server:**

CUDA_VISIBLE_DEVICES='' python distribute.py --ps_hosts=192.168.100.42:2222,192.168.100.22:2223 --worker_hosts=192.168.100.30:2224,192.168.100.253:2225 --job_name=ps --task_index=0

CUDA_VISIBLE_DEVICES='' python distribute.py --ps_hosts=192.168.100.42:2222,192.168.100.22:2223 --worker_hosts=192.168.100.30:2224,192.168.100.253:2225 --job_name=ps --task_index=1

**At worker server:**

CUDA_VISIBLE_DEVICES=0 python distribute.py --ps_hosts=192.168.100.42:2222,192.168.100.22:2223 --worker_hosts=192.168.100.30:2224,192.168.100.253:2225 --job_name=worker --task_index=0

CUDA_VISIBLE_DEVICES=0 python distribute.py --ps_hosts=192.168.100.42:2222,192.168.100.22:2223 --worker_hosts=192.168.100.30:2224,192.168.100.253:2225 --job_name=worker --task_index=1

**The train process is ok.
## But when I stop the chief worker process and restart it ,  I expect the program will restore the model from the checkpoint file , but it print the following error and exit :**

I tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0)
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {192.168.100.42:2222, 192.168.100.22:2223}
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {localhost:2224, 192.168.100.253:2225}
I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2224
E tensorflow/core/client/tensor_c_api.cc:485] Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoint/model.ckpt-0
         [[Node: save/restore_slice_2 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=""/job:ps/replica:0/task:1/cpu:0""](_recv_save/Const_0_S1, save/restore_slice_2/tensor_name, save/restore_slice_2/shape_and_slice)]]
Traceback (most recent call last):
  File ""distribute.py"", line 77, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""distribute.py"", line 60, in main
    with sv.managed_session(server.target) as sess:
  File ""/usr/lib/python2.7/contextlib.py"", line 17, in **enter**
    return self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 942, in managed_session
    self.stop(close_summary_writer=close_summary_writer)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 768, in stop
    stop_grace_period_secs=self._stop_grace_secs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 357, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 931, in managed_session
    start_standard_services=start_standard_services)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 680, in prepare_or_wait_for_session
    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 164, in prepare_session
    max_wait_secs=max_wait_secs, config=config)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 224, in recover_session
    saver.restore(sess, ckpt.model_checkpoint_path)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1129, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 382, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 655, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 723, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 743, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoint/model.ckpt-0
         [[Node: save/restore_slice_2 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=""/job:ps/replica:0/task:1/cpu:0""](_recv_save/Const_0_S1, save/restore_slice_2/tensor_name, save/restore_slice_2/shape_and_slice)]]
Caused by op u'save/restore_slice_2', defined at:
  File ""distribute.py"", line 77, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""distribute.py"", line 49, in main
    saver = tf.train.Saver()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 861, in __init__
    restore_sequentially=restore_sequentially)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 519, in build
    filename_tensor, vars_to_save, restore_sequentially, reshape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 272, in _AddRestoreOps
    values = self.restore_op(filename_tensor, vs, preferred_shard)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 187, in restore_op
    preferred_shard=preferred_shard)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/io_ops.py"", line 203, in _restore_slice
    preferred_shard, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 359, in _restore_slice
    preferred_shard=preferred_shard, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 703, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2310, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1232, in __init__
##     self._traceback = _extract_stack()

Is anyone can tell me how to restore the model when use distribute tensorflow??
"
4434,Why is quantized graph inference takes much more time than using the original graph?,"I followed this [tutorial](https://www.tensorflow.org/versions/r0.10/how_tos/quantization/index.html) in order to quantize my graph into 8 bit.I can't share the exact graph here but i can say it's a simple convolutional neural network.

When i run the [benchmark tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark) over the original and quantized networks it's clear that the quantized network is much much slower (100 ms vs. 4.5 ms).

Slowest nodes in original network :

```
time average [ms]   [%] [cdf%]  [Op]    [Name]
1.198   26.54%  26.54%  MatMul  fc10/fc10/MatMul
0.337   7.47%   34.02%  Conv2D  conv2/Conv2D
0.332   7.36%   41.37%  Conv2D  conv4/Conv2D
0.323   7.15%   48.53%  Conv2D  conv3/Conv2D
0.322   7.14%   55.66%  Conv2D  conv5/Conv2D
0.310   6.86%   62.53%  Conv2D  conv1/Conv2D
0.118   2.61%   65.13%  Conv2D  conv2_1/Conv2D
0.105   2.32%   67.45%  MaxPool pool1
```

Slowest nodes in quantized network :

```
time average [ms]   [%] [cdf%]  [Op]    [Name]
8.289   47.67%  47.67%  QuantizedMatMul fc10/fc10/MatMul_eightbit_quantized_bias_add
5.398   5.33%   53.00%  QuantizedConv2D conv5/Conv2D_eightbit_quantized_conv
5.248   5.18%   58.18%  QuantizedConv2D conv4/Conv2D_eightbit_quantized_conv
4.981   4.92%   63.10%  QuantizedConv2D conv2/Conv2D_eightbit_quantized_conv
4.908   4.85%   67.95%  QuantizedConv2D conv3/Conv2D_eightbit_quantized_conv
3.167   3.13%   71.07%  QuantizedConv2D conv5_1/Conv2D_eightbit_quantized_conv
3.049   3.01%   74.08%  QuantizedConv2D conv4_1/Conv2D_eightbit_quantized_conv
2.973   2.94%   77.02%  QuantizedMatMul fc11/MatMul_eightbit_quantized_bias_add
```

What is the reason for that ? 
Is it the expected behavior for quantized network ?
### Environment info

Operating System: Ubuntu 16.04
Installed from source, without GPU support :
1. commit hash = 37256f4
2. bazel version = 
Build label: 0.3.1
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jul 29 09:09:52 2016 (1469783392)
Build timestamp: 1469783392
Build timestamp as int: 1469783392
"
4432,wrong tf.control_dependencies using tf.case  and tf.cond,"add tf.control_dependencies in one branch of tf.case infulunce the other brach:
tf version: rc10

as we can see below, the tfvar is increasing no mather brach f1 or f2 is executed
change tf.case to tf.cond will get the same output.

`def testTF_case_with_control_dependencies():

```
isTraining=tf.placeholder(tf.bool,shape=[])
tfvar=tf.Variable(tf.constant(0),tf.int32);

increase_tfvar_op=tf.assign_add(tfvar.ref(), 1);

sess=tf.Session()
sess.run(tf.initialize_all_variables())

def f1():
    print ('f1')
    with tf.control_dependencies([increase_tfvar_op]):
        return -tfvar;  #if return tfvar directly, no control_dependencies is added

def f2():
    print('f2')
    with tf.control_dependencies([]):
        return tfvar*10;
caseResult = tf.case([(isTraining, f1)], default=f2)

b=True;
for t in range(4):
    b=not b;
    print('\n------')
    print('isTraining: ',end='')
    print(b)
    beforeCase=sess.run(tfvar)
    print('\ttfvar before run case: %d'%beforeCase)
    r=sess.run(caseResult,feed_dict={isTraining:b})
    print('case result:%d'%r)
    aftercase=sess.run(tfvar)
    print('\ttfvar after run case: %d'%aftercase)`
```

output:
f2
f2
f1

---

isTraining: False
    tfvar before run case: 0
case result:10
    tfvar after run case: 1

---

isTraining: True
    tfvar before run case: 1
case result:-2
    tfvar after run case: 2

---

isTraining: False
    tfvar before run case: 2
case result:30
    tfvar after run case: 3

---

isTraining: True
    tfvar before run case: 3
case result:-4
    tfvar after run case: 4
"
4431,"Forward mode ad, directional derivatives","Say I have `outputs = f(inputs)` and `g` of the same shape as `inputs`. I'd like to compute the directional derivative of `outputs` with respect to `inputs` in the direction `g` - in other words, the derivative of `f(inputs + alpha * g)` with respect to `alpha` at the point `alpha=0`.

This is a straightforward application of forward-mode automatic differentiation, which should be pretty easy to implement (much easier than the already-implemented reverse-mode ad). Are there any plans to add this feature?
"
4430,"ERROR: failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command , CUDA 8.0, cudnn 5.1.5, Cuda compute 6.1","NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

https://github.com/tensorflow/tensorflow/issues/4365
### Environment info

Operating System:
Ubuntu 16.04 [inside a docker container]
https://hub.docker.com/r/nvidia/cuda/tags/
latest-ubuntu16.04

GTX 1080
CUDA 8.0
cudnn 5.1.5
Cuda compute 6.1

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
/usr/local/cuda/lib64/libcudadevrt.a    /usr/local/cuda/lib64/libcudart.so.8.0.27  /usr/local/cuda/lib64/libcudnn.so.5
/usr/local/cuda/lib64/libcudart.so      /usr/local/cuda/lib64/libcudart_static.a   /usr/local/cuda/lib64/libcudnn.so.5.1.5
/usr/local/cuda/lib64/libcudart.so.8.0  /usr/local/cuda/lib64/libcudnn.so          /usr/local/cuda/lib64/libcudnn_static.a

If installed from binary pip package, provide:
1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
   931ff427f3a55a9c6c734a4c325d6af1b53665c3
2. The output of `bazel version`

Build label: 0.3.0
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jun 10 11:38:23 2016 (1465558703)
Build timestamp: 1465558703
Build timestamp as int: 1465558703

NOTE: i've also tried with bazel 0.3.1 - same result

Build label: 0.3.1
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jul 29 09:09:52 2016 (1469783392)
Build timestamp: 1469783392
Build timestamp as int: 1469783392
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --verbose_failures
### What other attempted solutions have you tried?

i've alsot tried LATEST clone and got configuration errors:
ERROR: /home/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such target '//tensorflow/core:android_tensorflow_lib_lite_no_rtti_lite_runtime': target 'android_tensorflow_lib_lite_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /home/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.
ERROR: /home/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such target '//tensorflow/core:android_tensorflow_lib_lite_no_rtti_lite_runtime': target 'android_tensorflow_lib_lite_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /home/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.
ERROR: /home/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such target '//tensorflow/core:android_proto_lib_no_rtti_lite_runtime': target 'android_proto_lib_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /home/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.
ERROR: /home/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such package 'base': BUILD file not found on package path and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.
ERROR: Evaluation of query ""deps((//... union @bazel_tools//tools/jdk:toolchain))"" failed: errors were encountered while computing transitive closure.

such as in 
https://github.com/tensorflow/tensorflow/issues/4312
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
WARNING: /root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/protobuf/WORKSPACE:1: Workspace name in /root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/protobuf/WORKSPACE (@__main__) does not match the name given in the repository's definition (@protobuf); this will cause a build error in future versions.
WARNING: /root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/highwayhash/WORKSPACE:1: Workspace name in /root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/highwayhash/WORKSPACE (@__main__) does not match the name given in the repository's definition (@highwayhash); this will cause a build error in future versions.
WARNING: /root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/re2/WORKSPACE:1: Workspace name in /root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/re2/WORKSPACE (@__main__) does not match the name given in the repository's definition (@re2); this will cause a build error in future versions.
WARNING: /root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/boringssl_git/WORKSPACE:1: Workspace name in /root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/boringssl_git/WORKSPACE (@boringssl) does not match the name given in the repository's definition (@boringssl_git); this will cause a build error in future versions.
INFO: Found 1 target...
ERROR: /root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/protobuf/BUILD:331:1: Linking of rule '@protobuf//:protoc' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/execroot/tensorflow && \
  exec env - \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/external/protobuf/protoc bazel-out/host/bin/external/protobuf/_objs/protoc/external/protobuf/src/google/protobuf/compiler/main.o bazel-out/host/bin/external/protobuf/libprotoc_lib.a bazel-out/host/bin/external/protobuf/libprotobuf.a bazel-out/host/bin/external/protobuf/libprotobuf_lite.a -lpthread -lstdc++ -B/usr/bin/ -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,-S -Wl,--gc-sections): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127.
/usr/bin/env: 'python': No such file or directory
Target //tensorflow/cc:tutorials_example_trainer failed to build
INFO: Elapsed time: 0.950s, Critical Path: 0.21s
"
4429,A very simple script using Optimizer.compute_gradients() prodeces Nan for all gradients,"Hi all, 

```
import tensorflow as tf
import numpy as np

batch_size = 5
dim = 3
hidden_units = 8


sess = tf.Session()

with sess.as_default():
    x = tf.placeholder(dtype=tf.float32, shape=[None, dim], name=""x"")
    y = tf.placeholder(dtype=tf.int32, shape=[None], name=""y"")
    w = tf.Variable(initial_value=tf.random_normal(shape=[dim, hidden_units]), name=""w"")
    b = tf.Variable(initial_value=tf.zeros(shape=[hidden_units]), name=""b"")
    logits = tf.nn.tanh(tf.matmul(x, w) + b)

    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y,name=""xentropy"")
    # define model end


    # begin training
    optimizer = tf.train.GradientDescentOptimizer(1e-5)
    grads_and_vars = optimizer.compute_gradients(cross_entropy, tf.trainable_variables())

    # generate data
    data = np.random.randn(batch_size, dim)
    labels = np.random.randint(0, 10, size=batch_size)

    sess.run(tf.initialize_all_variables())
    gradients_and_vars = sess.run(grads_and_vars, feed_dict={x:data, y:labels})
    for g, v in gradients_and_vars:
        if g is not None:
            print ""****************this is variable*************""
            print ""variable's shape:"", v.shape
            print v
            print ""****************this is gradient*************""
            print ""gradient's shape:"", g.shape
            print g

sess.close()
```

Run it, I got the following result.

![selection_107](https://cloud.githubusercontent.com/assets/5330101/18612210/706d7be2-7d86-11e6-9796-ab98b73b74ae.png)

As you see all GRADIENTS are Nan. Is there anything wrong in my code or PC? I don't know why it cannot compute correct gradients.
"
4428,Slim Tutorial and Script broken due to Commit 8d1b23d999a563c56236a3d250e18ef825e4e66e,"Hey TF Team,

Commit 8d1b23d999a563c56236a3d250e18ef825e4e66e broke the TF Slim Tutorial and TF Slim Script:
https://github.com/tensorflow/models/blob/master/slim/scripts/finetune_inception_v3_on_flowers.sh
Reverting this file to 4d241449aeb3707143c64e1be106cc6828a0d7b0 solves the issue https://github.com/tensorflow/tensorflow/commit/4d241449aeb3707143c64e1be106cc6828a0d7b0#diff-197163b98ebda1e22c3ff820582bf0d5

In the latest TF version as well as the TF rc10 the TF Slim Image Classification is not running.
Tested on Offical Tensorflow Docker GPU Image.

Best Wishes,
Patrick
"
4427,Clarify math or paper reference behind AttentionCellWrapper,"The documentation for `AttentionCellWrapper` in contrib states that it's based on this [paper](https://arxiv.org/abs/1601.06733). However, the attention mechanism in the paper appears to be distinct from that in `AttentionCellWrapper`. For example the TF implementation involves convolutions and requires a fixed attention window, neither of which are a feature of the referenced paper. I suggest that either the correct paper is referenced, or the description is updated to reflect the actual math being implemented. The biggest issue right now is that it's unclear what the various options really mean (`attn_length`, `attn_size`, `attn_vec_size`, etc) as their descriptions are very concise and without a paper reference to ground what the documentation is referring to, it's impossible to know what the attention mechanism is doing short of going through the code line-by-line.
"
4425,"tf.nn.softmax makes ""GPU sync failed Error"" for large size inputs.","### Environment info

Operating System:
Distributor ID: Ubuntu
Description:    Ubuntu 14.04.3 LTS
Release:        14.04
### GPU info

GPU :  GTX TITAN X 
Driver :  Driver Version: 367.44 

Installed version of CUDA and cuDNN: 

CUDA 7.5
CUDNN 5.1.3

If installed from binary pip package, provide:

https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/224/artifact/pip_test/whl/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

This code makes errors.

```
import tensorflow as tf
a=tf.constant(1.0,shape=[100*30, 30000]) 
b=tf.nn.softmax(a)
sess=tf.InteractiveSession()
b.eval()
sess.close()
```

This code doesn't make errors.

```
import tensorflow as tf
a=tf.constant(1.0,shape=[20*30, 30000]) 
b=tf.nn.softmax(a)
sess=tf.InteractiveSession()
b.eval()
sess.close()
```

I think this caused by input size of the softmax function.

But, I don't understand why.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).

```
E tensorflow/stream_executor/cuda/cuda_driver.cc:1140] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available
E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS
F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:198] Unexpected Event status: 1
F tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed
[1]    57277 abort (core dumped)  CUDA_VISIBLE_DEVICES=0 ipython
```
"
4424,`tf.count_up_to` returns the old value,"Environment info: Ubuntu 16.04, IPython interactive session

Installed version of CUDA and cuDNN: 7.5.18, 5.1.3

Installed from the nightly pip package for python 3.4.

How to reproduce:

``` python
import tensorflow as tf
with tf.Session().as_default():
  var = tf.Variable(0)
  count = var.count_up_to(3)
  tf.initialize_all_variables().run()
  print(var.eval())
  print(count.eval())
  print(var.eval())
```

Expected output:

```
0
1
1
```

What I see instead:

```
0
0
1
```

For some reason, the tensor from `tf.count_up_to` evaluates to the old value instead of the updated value.
"
4423,set_shape cause gradient error in a condition case,"### Environment info

Operating System: Ubuntu 16.04

If installed from binary pip package, provide:
1. A link to the pip package you installed: [https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl](https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl)
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`. 0.10.0rc0
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

``` python
import tensorflow as tf
a = tf.ones((2, 1, 2))
a_ = tf.cond(tf.equal(tf.shape(a)[1], 1), lambda: tf.tile(a, (1, 5, 1)), lambda: a)
a_.set_shape([None, 5, None])
tf.gradients(a_, a)
# => ValueError: Shapes (2, 5, 2) and (2, 1, 2) are not compatible
```

If the set_shape line is removed, everything works fine

``` python
tf.gradients(a_, a)
# => [<tf.Tensor 'gradients_5/AddN:0' shape=(2, 1, 2) dtype=float32>]
```
"
4422,Error while install tensorflow with pip install and python 3.5.2,"I was trying to in tensorflow from pip following the standard instructions and I get this error every time:

```
$ pip install --ignore-installed --upgrade $TF_BINARY_URL
Collecting tensorflow==0.10.0 from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp35-cp35m-linux_x86_64.whl
  Using cached https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp35-cp35m-linux_x86_64.whl
Collecting six>=1.10.0 (from tensorflow==0.10.0)
  Using cached six-1.10.0-py2.py3-none-any.whl
Collecting wheel>=0.26 (from tensorflow==0.10.0)
Exception:
Traceback (most recent call last):
  File ""/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/commands/install.py"", line 310, in run
    wb.build(autobuilding=True)
  File ""/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/wheel.py"", line 750, in build
    self.requirement_set.prepare_files(self.finder)
  File ""/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/req/req_set.py"", line 370, in prepare_files
    ignore_dependencies=self.ignore_dependencies))
  File ""/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/req/req_set.py"", line 522, in _prepare_file
    finder, self.upgrade, require_hashes)
  File ""/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/req/req_install.py"", line 268, in populate_link
    self.link = finder.find_requirement(self, upgrade)
  File ""/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/index.py"", line 442, in find_requirement
    all_candidates = self.find_all_candidates(req.name)
  File ""/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/index.py"", line 400, in find_all_candidates
    for page in self._get_pages(url_locations, project_name):
  File ""/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/index.py"", line 545, in _get_pages
    page = self._get_page(location)
  File ""/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/index.py"", line 648, in _get_page
    return HTMLPage.get_page(link, session=self.session)
  File ""/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/index.py"", line 757, in get_page
    ""Cache-Control"": ""max-age=600"",
  File ""/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/_vendor/requests/sessions.py"", line 487, in get
    return self.request('GET', url, **kwargs)
  File ""/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/download.py"", line 378, in request
    return super(PipSession, self).request(method, url, *args, **kwargs)
  File ""/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/_vendor/requests/sessions.py"", line 475, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/_vendor/requests/sessions.py"", line 585, in send
    r = adapter.send(request, **kwargs)
  File ""/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/_vendor/cachecontrol/adapter.py"", line 36, in send
    cached_response = self.controller.cached_request(request)
  File ""/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/_vendor/cachecontrol/controller.py"", line 111, in cached_request
    resp = self.serializer.loads(request, cache_data)
  File ""/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/_vendor/cachecontrol/serialize.py"", line 114, in loads
    return getattr(self, ""_loads_v{0}"".format(ver))(request, data)
  File ""/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/_vendor/cachecontrol/serialize.py"", line 170, in _loads_v2
    cached = json.loads(zlib.decompress(data).decode(""utf8""))
zlib.error: Error -5 while decompressing data: incomplete or truncated stream

```

Googling didn't help me so I'm here. Some more info:

Operating System:

```
Linux openmind7 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
```

Installed version of CUDA and cuDNN: 
    cuda 7.5 
    cuDNN 5

I know its good to give reproducible examples but I'm not doing anything special. I have a virgin conda environment with 3 packes:

```
pip (8.1.2)
setuptools (27.2.0)
wheel (0.29.0)
```

Some info about my python:

```
Python 3.5.2 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:53:06)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux
```
"
4420,Out of Memory error running Inception v3 distributed on 4 machines. ,"I'm trying to run Inception v3 (https://github.com/tensorflow/models/tree/master/inception) distributed on upto 32 machines. 

I'm seeing out of memory error when I run it on **4 machines**. 

Here is the error:

```
INFO:tensorflow:Started 0 queues for processing input data.
E tensorflow/core/client/tensor_c_api.cc:485] OOM when allocating tensor with shape[2048,1001]
     [[Node: gradients/logits/logits/weights/Regularizer/L2Regularizer/L2Loss_grad/mul = Mul[T=DT_FLOAT, _device=""/job:worker/replica:0/task:0/gpu:2""](logits/logits/weights/read_S3003, gradients/logits/logits/weights/Regularizer/L2Regularizer/value_grad/tuple/control_dependency_1)]]
     [[Node: gradients/AddN_48_S3319 = _Recv[client_terminated=false, recv_device=""/job:ps/replica:0/task:3/cpu:0"", send_device=""/job:worker/replica:0/task:0/gpu:2"", send_device_incarnation=-546941133885931708, tensor_name=""edge_17701_gradients/AddN_48"", tensor_type=DT_FLOAT, _device=""/job:ps/replica:0/task:3/cpu:0""]()]]
Traceback (most recent call last):
  File ""imagenet_distributed_train.py"", line 65, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""imagenet_distributed_train.py"", line 61, in main
    inception_distributed_train.train(server.target, dataset, cluster_spec)
  File ""/home/ubuntu/indu/models/inception/inception/inception_distributed_train.py"", line 286, in train
    loss_value, step = sess.run([train_op, global_step])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 382, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 655, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 723, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 743, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shape[2048,1001]
     [[Node: gradients/logits/logits/weights/Regularizer/L2Regularizer/L2Loss_grad/mul = Mul[T=DT_FLOAT, _device=""/job:worker/replica:0/task:0/gpu:2""](logits/logits/weights/read_S3003, gradients/logits/logits/weights/Regularizer/L2Regularizer/value_grad/tuple/control_dependency_1)]]
     [[Node: gradients/AddN_48_S3319 = _Recv[client_terminated=false, recv_device=""/job:ps/replica:0/task:3/cpu:0"", send_device=""/job:worker/replica:0/task:0/gpu:2"", send_device_incarnation=-546941133885931708, tensor_name=""edge_17701_gradients/AddN_48"", tensor_type=DT_FLOAT, _device=""/job:ps/replica:0/task:3/cpu:0""]()]]
Caused by op u'gradients/logits/logits/weights/Regularizer/L2Regularizer/L2Loss_grad/mul', defined at:
  File ""imagenet_distributed_train.py"", line 65, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""imagenet_distributed_train.py"", line 61, in main
    inception_distributed_train.train(server.target, dataset, cluster_spec)
  File ""/home/ubuntu/indu/models/inception/inception/inception_distributed_train.py"", line 215, in train
    grads = opt.compute_gradients(total_loss)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/sync_replicas_optimizer.py"", line 229, in compute_gradients
    return self._opt.compute_gradients(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 253, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py"", line 478, in gradients
    in_grads = _AsList(grad_fn(op, *out_grads))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_grad.py"", line 402, in _L2LossGrad
    return op.inputs[0] * grad
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py"", line 754, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py"", line 903, in _mul_dispatch
    return gen_math_ops.mul(x, y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 1427, in mul
    result = _op_def_lib.apply_op(""Mul"", x=x, y=y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 703, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2310, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1232, in __init__
    self._traceback = _extract_stack()

...which was originally created as op u'logits/logits/weights/Regularizer/L2Regularizer/L2Loss', defined at:
  File ""imagenet_distributed_train.py"", line 65, in <module>
    tf.app.run()
[elided 1 identical lines from previous traceback]
  File ""imagenet_distributed_train.py"", line 61, in main
    inception_distributed_train.train(server.target, dataset, cluster_spec)
  File ""/home/ubuntu/indu/models/inception/inception/inception_distributed_train.py"", line 154, in train
    logits = inception.inference(images, num_classes, for_training=True)
  File ""/home/ubuntu/indu/models/inception/inception/inception_model.py"", line 87, in inference
    scope=scope)
  File ""/home/ubuntu/indu/models/inception/inception/slim/inception_model.py"", line 326, in inception_v3
    restore=restore_logits)
  File ""/home/ubuntu/indu/models/inception/inception/slim/scopes.py"", line 155, in func_with_args
    return func(*args, **current_args)
  File ""/home/ubuntu/indu/models/inception/inception/slim/ops.py"", line 300, in fc
    restore=restore)
  File ""/home/ubuntu/indu/models/inception/inception/slim/scopes.py"", line 155, in func_with_args
    return func(*args, **current_args)
  File ""/home/ubuntu/indu/models/inception/inception/slim/variables.py"", line 290, in variable
    trainable=trainable, collections=collections)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 830, in get_variable
    custom_getter=custom_getter)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 673, in get_variable
    custom_getter=custom_getter)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 217, in get_variable
    validate_shape=validate_shape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 202, in _true_getter
    caching_device=caching_device, validate_shape=validate_shape)

```

I'm using EC2 G2.8XL instances. These instances have:
1. Intel Xeon E5-2670 (Sandy Bridge) Processors
2. 60 GB memory and
3. Four GK104GL [GRID K520] GPU with 4 GB memory on each of them. 
4. 10 Gigabit NIC

I'm running Ubuntu 14.04.4 LTS on these machines.

I'm running one worker per GPU. So, in total there is **16 workers**. 
I'm running one PS per machine. So, **4 PS** in total. 

I'm using a **batch size of 8**. (4 machines run out of memory with a batch size of 8. 32 machines run out of memory even with a batch size of 2).

Installed version of CUDA and cuDNN:
ubuntu@ip-172-31-16-180:~$ ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root 322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18
-rw-r--r-- 1 root root 720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a

I installed TensorFlow from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl
ubuntu@ip-172-31-16-180:~$ python -c ""import tensorflow; print(tensorflow.**version**)""
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
0.10.0rc0

Could someone please help me figure how how to fix this and run Inception v3 in a cluster with 32 machines?

More info:
Here are the commands I'm executing on the machines in the cluster:

```
On machine1:
CUDA_VISIBLE_DEVICES='' python imagenet_distributed_train.py --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=ps --task_id=0 2>&1 &
python imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=0 > /tmp/worker0 2>&1 &
python imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=1 > /tmp/worker1 2>&1 &
python imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=2 > /tmp/worker2 2>&1 &
python imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=3 > /tmp/worker3 2>&1 &


On machine2:
CUDA_VISIBLE_DEVICES='' python imagenet_distributed_train.py --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=ps --task_id=1 2>&1 &
python imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=4 > /tmp/worker4 2>&1 &
python imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=5 > /tmp/worker5 2>&1 &
python imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=6 > /tmp/worker6 2>&1 &
python imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=7 > /tmp/worker7 2>&1 &


On machine3:
CUDA_VISIBLE_DEVICES='' python imagenet_distributed_train.py --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=ps --task_id=2 2>&1 &
python imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=8 > /tmp/worker8 2>&1 &
python imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=9 > /tmp/worker9 2>&1 &
python imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=10 > /tmp/worker10 2>&1 &
python imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=11 > /tmp/worker11 2>&1 &


On machine4:
CUDA_VISIBLE_DEVICES='' python imagenet_distributed_train.py --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=ps --task_id=3 2>&1 &
python imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=12 > /tmp/worker12 2>&1 &
python imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=13 > /tmp/worker13 2>&1 &
python imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=14 > /tmp/worker14 2>&1 &
python imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=15 > /tmp/worker15 2>&1 &
```
"
4419,tf.get_variable without an explicit initializer fails for integer types,"The following fails (shape and name are arbitrary):

```
tf.get_variable(name='foo', shape=(42,), dtype=tf.int32)
```

Exception: `TypeError: Expected int32, got -1.7320508075688772 of type 'float' instead.`

In contrast, using `tf.float32` works just fine.

The problem appears to be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variable_scope.py#L658

If an initializer is not provided (and a default one is not set), a uniform unit scaling init is used (notice that [`sqrt(3)==1.7320...`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/init_ops.py#L275)), which of course conflicts with the requested integer type.

While this can be mitigated by doing something like:

```
tf.get_variable(name='foo', dtype=tf.int32, initializer=tf.zeros_initializer(shape=(42,), dtype=tf.int32))
```

it feels like a smarter default behavior based on the variable type is warranted (or at least a less cryptic error).

Tested on the current master.
"
4416,GPU Nightly links broken,"The links to .whl files on https://github.com/tensorflow/tensorflow are broken for Linux GPU and Mac GPU

https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.10.0-cp27-none-linux_x86_64.whl

https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.10.0-py2-none-any.whl
"
4413,Unexpected behaviour for tf.InteractiveSession() in Jupyter Notebook,"I'm not convinced this is a bug, but if I run `tf.InteractiveSession()` in a cell with no assignment (e.g. `sess = tf.InteractiveSession()`), the interactive session will work only as long as the call is the last operation in the notebook cell and the output is not suppressed with `;`. This tripped me over a few times, so I'm reporting it just in case this is not the intended behaviour.

FWIW I think this is because IPython/Jupyter stores the session automatically in the cases I highlighted, making the command alike `sess = tf.InteractiveSession()` which works as expected.
### Environment info

Operating System: Linux

Installed version of CUDA and cuDNN: Using CPU version

If installed from binary pip package, provide:
1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp35-cp35m-linux_x86_64.whl
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`. 0.10.0
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

I open a Jupyter notebook, and this works:

``` python
In [1]: import tensorflow as tf
In [2]: tf.InteractiveSession()
In [3]: tf.constant(0).eval()
```

This doesn't:

``` python
In [1]: import tensorflow as tf
In [2]: tf.InteractiveSession();
In [3]: tf.constant(0).eval()
```

Neither does this one:

``` python
In [1]: import tensorflow as tf
In [2]: tf.InteractiveSession()
        print()
In [3]: tf.constant(0).eval()
```

The both fail with:

``` python
ValueError                                Traceback (most recent call last)
<ipython-input-3-b43e5df4f2f7> in <module>()
----> 1 tf.constant(0).eval()

/usr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in eval(self, feed_dict, session)
    557 
    558     """"""
--> 559     return _eval_using_default_session(self, feed_dict, self.graph, session)
    560 
    561 

/usr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in _eval_using_default_session(tensors, feed_dict, graph, session)
   3640     session = get_default_session()
   3641     if session is None:
-> 3642       raise ValueError(""Cannot evaluate tensor using `eval()`: No default ""
   3643                        ""session is registered. Use `with ""
   3644                        ""sess.as_default()` or pass an explicit session to ""

ValueError: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`
```
"
4408,Variable initialization problem,"Hi, I am getting a error ""Attempting to use uninitialized value"" on tf.initialize_all_variables when I try to incorporate the shape of a tensor via tf.shape in a loss function that depends on a dynamically shaped input tensor, and in between, on the variable that does not get initialized and on tf.scan. I am trying to get a minimal working example of the bug, but I couldn't reproduce it yet, so I thought I would ask whether this or a similar issue is already known before I continue. The only reproducible step is that the initialization works when I replace tf.to_float(tf.shape(x))[0] in my loss function with tf.constant(1.0), which makes me believe it has something to do with tf.shape or perhaps with the tf.scan.
"
4407,Error with my own model trained in android demo ,"SETTING CODE ANDROID 

  private static final int NUM_CLASSES = 26;
  private static final int INPUT_SIZE = 299;
  private static final int IMAGE_MEAN = 128;
  private static final float IMAGE_STD = 128;
  private static final String INPUT_NAME = ""Mul:0"";
  private static final String OUTPUT_NAME = ""final_result:0"";

ERROR ADB LOGCAT 

tensorflow_jni.cc:304 Error during inference: Invalid argument: No OpKernel was registered to support Op 'DecodeJpeg' with these attrs.  Registered kernels:

<no registered kernels>

 [[Node: DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, fancy_upscaling=true, ratio=1, try_recover_truncated=false](DecodeJpeg/contents)]]

09-16 01:01:02.882 3006-3029/org.tensorflow.demo A/libc: Fatal signal 6 (SIGABRT), code -6 in tid 3029 (InferenceThread)
09-16 01:01:02.989 257-257/? A/DEBUG: pid: 3006, tid: 3029, name: InferenceThread  >>> org.tensorflow.demo <<<
09-16 01:01:03.060 257-257/? A/DEBUG:     #05 pc 00610a98  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_demo.so
09-16 01:01:03.061 257-257/? A/DEBUG:     #06 pc 00610c1c  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_demo.so
09-16 01:01:03.061 257-257/? A/DEBUG:     #07 pc 00610c38  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_demo.so
09-16 01:01:03.061 257-257/? A/DEBUG:     #08 pc 00083f40  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_demo.so
09-16 01:01:03.061 257-257/? A/DEBUG:     #09 pc 0008415c  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_demo.so 
"
4403,Error while installation on virtual environment,"I'm trying to install Tensor Flow on my virtual environment with Python 2.7.6. 

When I try the command `pip install --upgrade $TF_BINARY_URL` as mentioned in the [documentation](https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#virtualenv-installation), I get the following error - 

Downloading/unpacking https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl
  Downloading tensorflow-0.10.0-cp27-none-linux_x86_64.whl (36.6MB): 8.4MB downloaded
Cleaning up...
Exception:
Traceback (most recent call last):
  File ""/home/naman/tensorflow/local/lib/python2.7/site-packages/pip/basecommand.py"", line 122, in main
    status = self.run(options, args)
  File ""/home/naman/tensorflow/local/lib/python2.7/site-packages/pip/commands/install.py"", line 278, in run
    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)
  File ""/home/naman/tensorflow/local/lib/python2.7/site-packages/pip/req.py"", line 1197, in prepare_files
    do_download,
  File ""/home/naman/tensorflow/local/lib/python2.7/site-packages/pip/req.py"", line 1375, in unpack_url
    self.session,
  File ""/home/naman/tensorflow/local/lib/python2.7/site-packages/pip/download.py"", line 572, in unpack_http_url
    download_hash = _download_url(resp, link, temp_location)
  File ""/home/naman/tensorflow/local/lib/python2.7/site-packages/pip/download.py"", line 433, in _download_url
    for chunk in resp_read(4096):
  File ""/home/naman/tensorflow/local/lib/python2.7/site-packages/pip/download.py"", line 421, in resp_read
    chunk_size, decode_content=False):
  File ""/home/naman/tensorflow/local/lib/python2.7/site-packages/pip/_vendor/requests/packages/urllib3/response.py"", line 236, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File ""/home/naman/tensorflow/local/lib/python2.7/site-packages/pip/_vendor/requests/packages/urllib3/response.py"", line 183, in read
    data = self._fp.read(amt)
  File ""/usr/lib/python2.7/httplib.py"", line 573, in read
    s = self.fp.read(amt)
  File ""/usr/lib/python2.7/socket.py"", line 380, in read
    data = self._sock.recv(left)
  File ""/usr/lib/python2.7/ssl.py"", line 341, in recv
    return self.read(buflen)
  File ""/usr/lib/python2.7/ssl.py"", line 260, in read
    return self._sslobj.read(len)
SSLError: [Errno 1] _ssl.c:1429: error:1408F119:SSL routines:SSL3_GET_RECORD:decryption failed or bad record mac

Storing debug log for failure in /tmp/tmpcvH6K0

I'm using Ubuntu 14.04 wubi on top of Windows 7. Please sort this error. 
"
4402,explicit device specification of restore operation on distributed training,"I'm trying the run the distributed training of Inception, and this is the error I'm getting, the moment workers connect to ps. 

```
tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'save/restore_slice_1268': Could not satisfy explicit device specification '/job:ps/task:0/device:CPU:0' because no devices matching that specification are registered in this process; available devices: /job:worker/replica:0/task:0/cpu:0, /job:worker/replica:0/task:0/gpu:0, /job:worker/replica:0/task:1/cpu:0, /job:worker/replica:0/task:1/gpu:0
     [[Node: save/restore_slice_1268 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=""/job:ps/task:0/device:CPU:0""](save/Const, save/restore_slice_1268/tensor_name, save/restore_slice_1268/shape_and_slice)]]

Caused by op u'save/restore_slice_1268', defined at:
  File ""/home/mbz/exp/tf_inception/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/imagenet_distributed_train.py"", line 65, in <module>
    tf.app.run()
  File ""/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""/home/mbz/exp/tf_inception/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/imagenet_distributed_train.py"", line 61, in main
    inception_distributed_train.train(server.target, dataset, cluster_spec)
  File ""/home/mbz/exp/tf_inception/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception_distributed_train.py"", line 233, in train
    saver = tf.train.Saver()
  File ""/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 861, in __init__
    restore_sequentially=restore_sequentially)
  File ""/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 519, in build
    filename_tensor, vars_to_save, restore_sequentially, reshape)
  File ""/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 272, in _AddRestoreOps
    values = self.restore_op(filename_tensor, vs, preferred_shard)
  File ""/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 187, in restore_op
    preferred_shard=preferred_shard)
  File ""/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py"", line 203, in _restore_slice
    preferred_shard, name=name)
  File ""/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 359, in _restore_slice
    preferred_shard=preferred_shard, name=name)
  File ""/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 703, in apply_op
    op_def=op_def)
  File ""/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2317, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1239, in __init__
    self._traceback = _extract_stack()
```

I'm using the latest pip TensorFlow (0.10) on Cuda 7.5 and Cudnn v5.
"
4401,Broken blog link on tensroflow.org dated 26 Aug 2016,"https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html.html

Should be 

https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html
"
4399,Android build issue: permission denied,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

None relating to permission denied issue
### Environment info

Operating System: Linux Ubuntu 64-bit 

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

android_sdk_repository(
    name = ""androidsdk"",
    api_level = 23,
    build_tools_version = ""24.0.2"",
    # Replace with path to Android SDK on your system
    path = ""/home/flavia/Documents/android-sdk-linux"",
)
# 

android_ndk_repository(
    name=""androidndk"",
    path=""/home/flavia/Documents/ndk"",
    api_level=21)
### What other attempted solutions have you tried?

Downloaded NDK version: android-ndk-r12b, unzipped it and renamed it to ndk and pointed to it in workspace
Downloade SDK: 24.4.1, run SDK manager to install build-tools, pointed to the main sdk directory
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
ERROR: /home/flavia/.cache/bazel/_bazel_flavia/1fe576225e857ae381761d2049465367/external/protobuf/BUILD:71:1: C++ compilation of rule '@protobuf//:protobuf_lite' failed: arm-linux-androideabi-gcc failed: error executing command 
  (cd /home/flavia/.cache/bazel/_bazel_flavia/1fe576225e857ae381761d2049465367/execroot/tensorflow && \
  exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \
  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables -no-canonical-prefixes -fno-canonical-system-headers '-march=armv7-a' '-mfpu=vfpv3-d16' '-mfloat-abi=softfp' -MD -MF bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/repeated_field.d '-frandom-seed=bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/repeated_field.o' -iquote external/protobuf -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/bazel_tools -isystem external/protobuf/src -isystem bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare '-Wno-error=unused-function' '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm' -isystem external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9/include -isystem external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9/include-fixed -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c external/protobuf/src/google/protobuf/repeated_field.cc -o bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/repeated_field.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
process-wrapper: execvp(""external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc"", ...): Permission denied
Target //tensorflow/examples/android:tensorflow_demo failed to build
INFO: Elapsed time: 1.330s, Critical Path: 0.43s

Basically, i've failed to decipher the permission denied. I'm no expert at android, but i'm experimenting with tensorflow.
"
4397,Docker build devel-gpu build fails with 8.0-cudnn5-devel (Cannot find cudnn.h under .../lib),"**Summary**: Why is it looking for the header file under the lib directory?
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
- https://github.com/tensorflow/tensorflow/issues/3989 (and references within)
### Environment info

Operating System: Ubuntu 16.04. Docker 1.12.1.
### If possible, provide a minimal reproducible example
1. Modify [devel-gpu](https://github.com/tensorflow/tensorflow/blob/4addf4b5806cd731949c6582a83f5824599cd1ef/tensorflow/tools/docker/Dockerfile.devel-gpu) to read ""`FROM nvidia/cuda:8.0-cudnn5-devel`"".
2. Run `docker build -f Dockerfile.devel-gpu -t tf` from the `/tensorflow/tools/docker` directory. (HEAD @ 4addf4b5806cd731949c6582a83f5824599cd1ef at time of posting)
### What other attempted solutions have you tried?

None yet, I'm not sure how to poke Bazel. I will continue poking around after filing the issue.
### Logs or other output that would be helpful

```
$ docker build -f Dockerfile.devel-gpu -t tf .

[snip]

Step 23 : RUN ./configure &&     bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --upgrade /tmp/pip/tensorflow-*.whl
 ---> Running in 1f99527c7748
No Google Cloud Platform support will be enabled for TensorFlow
INFO: Reading 'startup' options from /root/.bazelrc: --batch
Extracting Bazel installation...
____Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
INFO: Reading 'startup' options from /root/.bazelrc: --batch
ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
    File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 412
        _create_cuda_repository(repository_ctx)
    File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 344, in _create_cuda_repository
        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)
    File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 233, in _find_cudnn_header_dir
        fail(""Cannot find cudnn.h under %s"" %...)
Cannot find cudnn.h under /usr/local/cuda-8.0/targets/x86_64-linux/lib.
Configuration finished
INFO: Reading 'startup' options from /root/.bazelrc: --batch
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
    File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 412
        _create_cuda_repository(repository_ctx)
    File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 344, in _create_cuda_repository
        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)
    File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 233, in _find_cudnn_header_dir
        fail(""Cannot find cudnn.h under %s"" %...)
Cannot find cudnn.h under /usr/lib/x86_64-linux-gnu.
____Elapsed time: 0.391s
The command '/bin/sh -c ./configure &&     bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --upgrade /tmp/pip/tensorflow-*.whl' returned a non-zero code: 1
```
"
4396,Problem in using docker tensorflow installation for GPU,"Operating System: Ubuntu 16.04
I installed the gpu-docker tensorflow using these commands:

```
wget -P /tmp https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.0-rc.3/nvidia-docker_1.0.0.rc.3-1_amd64.deb
sudo dpkg -i /tmp/nvidia-docker*.deb && rm /tmp/nvidia-docker*.deb
nvidia-docker run --rm nvidia/cuda nvidia-smi
```

Now, I have cuda installed but when I try to run code with gpu support, I get this:

```
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
I tensorflow/stream_executor/cuda/cuda_dnn.cc:1562] Unable to load cuDNN DSO
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: 25e49b6d892e
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  370.28  Thu Sep  1 19:45:04 PDT 2016
GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.2) 
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 370.28
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1051] LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
```

It cant open libcuda even though I installed it. Also, how do I install cuDNN here?
output of `ldconfig -p | grep cuda`:

```
    libnvrtc.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libnvrtc.so.7.5
    libnvrtc.so (libc6,x86-64) => /usr/local/cuda/lib64/libnvrtc.so
    libnvrtc-builtins.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libnvrtc-builtins.so.7.5
    libnvrtc-builtins.so (libc6,x86-64) => /usr/local/cuda/lib64/libnvrtc-builtins.so
    libnvblas.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libnvblas.so.7.5
    libnvblas.so (libc6,x86-64) => /usr/local/cuda/lib64/libnvblas.so
    libnvToolsExt.so.1 (libc6,x86-64) => /usr/local/cuda/lib64/libnvToolsExt.so.1
    libnvToolsExt.so (libc6,x86-64) => /usr/local/cuda/lib64/libnvToolsExt.so
    libnpps.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libnpps.so.7.5
    libnpps.so (libc6,x86-64) => /usr/local/cuda/lib64/libnpps.so
    libnppi.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libnppi.so.7.5
    libnppi.so (libc6,x86-64) => /usr/local/cuda/lib64/libnppi.so
    libnppc.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libnppc.so.7.5
    libnppc.so (libc6,x86-64) => /usr/local/cuda/lib64/libnppc.so
    libcusparse.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libcusparse.so.7.5
    libcusparse.so (libc6,x86-64) => /usr/local/cuda/lib64/libcusparse.so
    libcusolver.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libcusolver.so.7.5
    libcusolver.so (libc6,x86-64) => /usr/local/cuda/lib64/libcusolver.so
    libcurand.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libcurand.so.7.5
    libcurand.so (libc6,x86-64) => /usr/local/cuda/lib64/libcurand.so
    libcufftw.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libcufftw.so.7.5
    libcufftw.so (libc6,x86-64) => /usr/local/cuda/lib64/libcufftw.so
    libcufft.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libcufft.so.7.5
    libcufft.so (libc6,x86-64) => /usr/local/cuda/lib64/libcufft.so
    libcudart.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libcudart.so.7.5
    libcudart.so (libc6,x86-64) => /usr/local/cuda/lib64/libcudart.so
    libcublas.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libcublas.so.7.5
    libcublas.so (libc6,x86-64) => /usr/local/cuda/lib64/libcublas.so
```
"
4394,Proper mesage for new contributor,"On page CONTRIBUTING.md there is a link for new contributor.
There is a need of space between how and to which i missing. But, it will look good if we change the message to getting started.
"
4393,AttributeError: 'GFile' object has no attribute 'Size',"Getting above error when running mnist_softmax.py

(tensorflow) :~/softwares/tensorflow/tensorflow/examples/tutorials/mnist$  python mnist_softmax.py 

Temporary work around would be to do following edit in the ""tensorflow/tensorflow/contrib/learn/python/learn/datasets/base.py""

Replace  Line # 160    '    size = f.Size()  '        with       '   size = f.size()  '

As 'tensorflow/tensorflow/python/lib/io/file_io.py' has the function define for **size** not **Size**.

  def size(self):
    """"""Returns the size of the file.""""""
    return stat(self.__name).length

Failure is seen on running the tensorflow for the first time or cleaning up /tmp/data
"
4392,LMDB Reading Feature,"I would like to see a native feature where LMDB files are read by tensorflow inside the graph. I know it can be done with placeholders but that's suboptimal.

In general, there are three ways of approaching this in TF.
(1) Native Implementation / Custom Data Reader
[https://www.tensorflow.org/versions/r0.10/how_tos/new_data_formats/index.html](https://www.tensorflow.org/versions/r0.10/how_tos/new_data_formats/index.html)
(2) Python Function Wrapping
[https://www.tensorflow.org/versions/r0.9/api_docs/python/script_ops.html](https://www.tensorflow.org/versions/r0.9/api_docs/python/script_ops.html)
(3) Placeholders
The functionality is neat but I require better performance..

If I were to do this myself, what are from a performance standpoint be the main differences between (1) and (2). In any case, it would be great if (1) ended up in the master branch.
"
4391,bazel build error ,"Ubuntu16.04 Cuda8.0 cudnn4 gcc-5.4 g++5.4 tensorflow0.9.0,
i experienced the issue blow when using bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package  and there also is a warning.
what should i do to correct this? i am new to tensorflow .  Need Help
WARNING: /home/keithyin/workspace/tensorflow-0.9.0/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future.

ERROR: /home/keithyin/workspace/tensorflow-0.9.0/tensorflow/stream_executor/BUILD:5:1: C++ compilation of rule '//tensorflow/stream_executor:stream_executor' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 100 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
tensorflow/stream_executor/cuda/cuda_blas.cc: In member function 'virtual bool perftools::gputools::cuda::CUDABlas::DoBlasGemm(perftools::gputools::Stream_, perftools::gputools::blas::Transpose, perftools::gputools::blas::Transpose, tensorflow::uint64, tensorflow::uint64, tensorflow::uint64, float, const perftools::gputools::DeviceMemoryEigen::half&, int, const perftools::gputools::DeviceMemoryEigen::half&, int, float, perftools::gputools::DeviceMemoryEigen::half_, int)':
tensorflow/stream_executor/cuda/cuda_blas.cc:1683:22: error: 'CUBLAS_DATA_HALF' was not declared in this scope
       CUDAMemory(a), CUBLAS_DATA_HALF, lda,
                      ^
tensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cublasOperation_t perftools::gputools::cuda::{anonymous}::CUDABlasTranspose(perftools::gputools::blas::Transpose)':
tensorflow/stream_executor/cuda/cuda_blas.cc:406:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cublasFillMode_t perftools::gputools::cuda::{anonymous}::CUDABlasUpperLower(perftools::gputools::blas::UpperLower)':
tensorflow/stream_executor/cuda/cuda_blas.cc:417:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cublasDiagType_t perftools::gputools::cuda::{anonymous}::CUDABlasDiagonal(perftools::gputools::blas::Diagonal)':
tensorflow/stream_executor/cuda/cuda_blas.cc:428:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cublasSideMode_t perftools::gputools::cuda::{anonymous}::CUDABlasSide(perftools::gputools::blas::Side)':
tensorflow/stream_executor/cuda/cuda_blas.cc:439:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 5928.835s, Critical Path: 282.48s
"
4387,Significant performance overhead with gpu kernel launches in a while_loop,"### Environment info

Operating System:
ubuntu 14.04
GeForce GTX TITAN  Driver Version: 367.44

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
-rw-r--r-- 1 root root   322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root       19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root   383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18
-rw-r--r-- 1 root root   720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 61453024 Feb 23  2016 /usr/local/cuda/lib64/libcudnn.so
-rwxr-xr-x 1 root root 61453024 Feb 23  2016 /usr/local/cuda/lib64/libcudnn.so.4
-rwxr-xr-x 1 root root 61453024 Feb 23  2016 /usr/local/cuda/lib64/libcudnn.so.4.0.7
-rw-r--r-- 1 root root 62025862 Feb 23  2016 /usr/local/cuda/lib64/libcudnn_static.a
1. A link to the pip package you installed:
   https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
   0.10.0rc0
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

import tensorflow as tf
from tensorflow.python.client import timeline

start = tf.constant(0, dtype=tf.float64)
end = tf.constant(100, dtype=tf.float64)
init = tf.ones([100], dtype=tf.float64)
b = tf.ones([100], dtype=tf.float64)

def condForWhile(i, a):
    return tf.less(i, end)

def bodyForWhile(i, a):
    i = i + 1
    a = tf.mul(a, b)
    return [i, a]

_, prod = tf.while_loop(condForWhile, bodyForWhile, [start, init])

with tf.Session() as sess:
    tf.initialize_all_variables().run()
    # run once to eliminate start-up costs
    sess.run(prod)
    # now run with tracing
    run_metadata = tf.RunMetadata()
    result = sess.run(prod,
        options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),
        run_metadata=run_metadata)

trace = timeline.Timeline(step_stats=run_metadata.step_stats)
trace_file = open('/tmp/timeline.tfwhile.json', 'w')
trace_file.write(trace.generate_chrome_trace_format(show_dataflow=False, show_memory=False))

This is a simple app that does a = a \* b 100 times in a while_loop using the tf.mul() operator. I am using the timeline feature to profile it. Inspecting the timeline shows that for each iteration, the time in the gpu stream for the multiply is about .003ms, but the time between launches is .3ms-.5ms. This seems like a lot - is that expected? 

Note that my while condition uses floats in the less function. I see the less operation happening on the gpu stream and a memcpyDtoH time for each loop iteration presumably to copy the loop counter back and forth to the gpu, which seems kind of inefficient. If instead I use  type int64 in the loop conditions (start and end), then the gap between kernels goes down to .11 - .17ms and I no longer see the less on the gpu stream or the memcpy. 

Note that in the original case where we discovered this, our gpu operator was much more complicated (and uses floats in the condition) and the time between kernel launches was still relatively constant at around .4ms. In contrast, if I write this same app in straight cuda, and launch the kernel 100 times in cuda, the gap between launches is about .007ms (about 50x improvement). I would expect some overhead in the tensorflow conditional operators, but this really seems like a lot. 
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
Attaching the timeline traces for the above code for both floats and ints in the conditional. 

[timelines.zip](https://github.com/tensorflow/tensorflow/files/473495/timelines.zip)
"
4386,bazel build should warn you if you haven't run ./configure,"The errors when not running configure are not terribly intuitive (see e.g. #4279 ). 
"
4385,MatMul operation would cost 4ms,"By stats the time costs using stats_collector_ in Executor class, I found that a matrix multiply operation of 150*150 matrix with a 150 vector would cost 4ms on CPU, And a 3 layers of DNN would cost 20ms, is that expected?
"
4380,TensorFlow master build failing: error: invalid initialization of reference ,"We are building TensorFlow master (commit id  [461caa8](https://github.com/tensorflow/tensorflow/commit/461caa86137aee3d2e40843ea308c216f10c4655) ) using bazel on 64 bit platform. 

We need a fix for 64 bit platform as mentioned in  [#1044](https://github.com/google/protobuf/pull/1044) in google/protobuf. So we tried to change the protobuf commit id used in `tensorflow/workspace.bzl` file with recent protobuf commit id as mentioned below.  

```
+++ b/tensorflow/workspace.bzl
@@ -88,7 +88,8 @@ def tf_workspace(path_prefix = """", tf_repo_name = """"):
Â Â Â native.git_repository(
Â Â Â Â Â name = ""protobuf"",
Â Â Â Â Â remote = ""https://github.com/google/protobuf"",
- commit = ""ed87c1fe2c6e1633cadb62cf54b2723b2b25c280"",
+# commit = ""ed87c1fe2c6e1633cadb62cf54b2723b2b25c280"",
+ commit = ""c59473d53eafadd126502657e5c5c33e952b67ed"",
Â Â Â )
```

But TensorFlow build is failing with below errors: 

```

ERROR: /home/test/tensorflow_proto_commit/tensorflow/tensorflow/core/BUILD:941:1: C++ compilation of rule '//tensorflow/core:framework_internal' failed: gcc failed: error executing command /opt/gccgo/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/opt/gccgo/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 108 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.

tensorflow/core/example/feature_util.cc: In function 'const typename tensorflow::internal::RepeatedFieldTrait<FeatureType>::Type& tensorflow::GetFeatureValues(const string&, const tensorflow::Example&) [with FeatureType = long long int; typename tensorflow::internal::RepeatedFieldTrait<FeatureType>::Type = google::protobuf::RepeatedField<long long int>; std::__cxx11::string = std::__cxx11::basic_string<char>]':
tensorflow/core/example/feature_util.cc:54:66: error: invalid initialization of reference of type 'const google::protobuf::RepeatedField<long long int>&' from expression of type 'const google::protobuf::RepeatedField<long int>'
   return example.features().feature().at(name).int64_list().value();
                                                                  ^
tensorflow/core/example/feature_util.cc: In function 'typename tensorflow::internal::RepeatedFieldTrait<FeatureType>::Type* tensorflow::GetFeatureValues(const string&, tensorflow::Example*) [with FeatureType = long long int; typename tensorflow::internal::RepeatedFieldTrait<FeatureType>::Type = google::protobuf::RepeatedField<long long int>; std::__cxx11::string = std::__cxx11::basic_string<char>]':
tensorflow/core/example/feature_util.cc:62:23: error: cannot convert 'google::protobuf::RepeatedField<long int>*' to 'google::protobuf::RepeatedField<long long int>*' in return
       ->mutable_value();
                       ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build

```

Is there any specific protobuf commit id to be used with TensorFlow?
We could build TensorFlow without making any change in `tensorflow/workspace.bzl` and manually adding fix for 64 bit platform in  `external/protobuf/src/google/protobuf/stubs/atomicops_internals_generic_gcc.h` .

We need to use protobuf with commit id [c59473d](https://github.com/google/protobuf/commit/c59473d53eafadd126502657e5c5c33e952b67ed) or recent , where a fix for 64 bit platform has been committed. 

**Environment info**

Operating System: **RedHat , Ubuntu**

Installed version of CUDA and cuDNN:  **Not installed** 

The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`. **master**

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)  [461caa8](https://github.com/tensorflow/tensorflow/commit/461caa86137aee3d2e40843ea308c216f10c4655)
2. The output of `bazel version` **master branch (0.3.1-2016-09-12)**

Steps to Reproduce:
bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
"
4379,having encounted an issue when i use ./configure to install tensorflow,"Hi, all.
i am new to tensorflow, and spent a lot of days to install tensorflow, but it didn't work. Need help

 sudo ./configure 
~/tensorflow ~/tensorflow
Please specify the location of python. [Default is /usr/bin/python]: 
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] n
No Google Cloud Platform support will be enabled for TensorFlow
Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]

/usr/local/lib/python2.7/dist-packages
Do you wish to build TensorFlow with GPU support? [y/N] y
GPU support will be enabled for TensorFlow
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 4
Please specify the location where cuDNN 4 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.

Found stale PID file (pid=3471). Server probably died abruptly, continuing...
..
INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
.
ERROR: /home/keithyin/tensorflow/tensorflow/contrib/session_bundle/BUILD:134:1: no such target '//tensorflow/core:android_lib_lite': target 'android_lib_lite' not declared in package 'tensorflow/core' defined by /home/keithyin/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:session_bundle_lite'.
ERROR: /home/keithyin/tensorflow/tensorflow/core/platform/default/build_config/BUILD:56:1: no such package '@jpeg_archive//': Error downloading from http://www.ijg.org/files/jpegsrc.v9a.tar.gz to /home/keithyin/.cache/bazel/_bazel_root/9192340d7b606ddb9ea35b29a97154c1/external/jpeg_archive: Error downloading http://www.ijg.org/files/jpegsrc.v9a.tar.gz to /home/keithyin/.cache/bazel/_bazel_root/9192340d7b606ddb9ea35b29a97154c1/external/jpeg_archive/jpegsrc.v9a.tar.gz: Connection timed out and referenced by '//tensorflow/core/platform/default/build_config:platformlib'.
ERROR: Evaluation of query ""deps((//... union @bazel_tools//tools/jdk:toolchain))"" failed: errors were encountered while computing transitive closure.
Configuration finished
"
4377,tensorflow.contrib.learn.TensorFlowEstimator fails under sklearn.base.clone(),"tensorflow.contrib.learn.TensorFlowEstimator fails under sklearn.base.clone()

This prevents the possibility to perform CV using sklearn.cross_validation.cross_val_Score over this estimator.
The clone fails due to the fact the TensorFlowEstimator.get_params() returns a key named ""params"" that is not part of the constructor input parameters.
This causes sklearn.base.clone to fail when trying to clone the estimator using it's constructor:
TensorFlowEstimator(**params)
"
4375,"Creating a ""Books"" section under Resources","Packt is proud to publish a new book on TensorFlow
 [Getting Started with TensorFlow](https://www.packtpub.com/big-data-and-business-intelligence/getting-started-tensorflow)

We would appreciate if you could create a ""Books"" section under Resources to feature books written on TensorFlow. 

Let me know if this can be done.

Thanks
"
4371,"""HOST_CFG"" is not defined on bazel clean","During the ./configure step of my build from source, I encounter a bazel error when trying to download 

**The main error seems to be**

```

ERROR: /home/kamal/.cache/bazel/_bazel_kamal/f9ae4eca457b390bb2ebe780caca64e0/external/protobuf/protobuf.bzl:91:19: name 'HOST_CFG' is not defined.

```

Building with CUDA 8.0, cuDNN 5.1.5, GTX 1060

Full stack:

```
~/tensorflow ~/tensorflow
Please specify the location of python. [Default is /usr/bin/python]: 
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] n
No Google Cloud Platform support will be enabled for TensorFlow
Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]

/usr/local/lib/python2.7/dist-packages
Do you wish to build TensorFlow with GPU support? [y/N] y
GPU support will be enabled for TensorFlow
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/local/cuda-8.0
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 5.1.5
Please specify the location where cuDNN 5.1.5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda-8.0]: 
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 6.1
INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
.
ERROR: /home/kamal/.cache/bazel/_bazel_kamal/f9ae4eca457b390bb2ebe780caca64e0/external/protobuf/protobuf.bzl:91:19: name 'HOST_CFG' is not defined.
ERROR: package contains errors: tensorflow/contrib/metrics.
ERROR: error loading package 'tensorflow/contrib/metrics': Extension 'protobuf.bzl' has errors.
Configuration finished
```
"
4368,Building issues,"```
bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
ERROR: The specified --crosstool_top '@local_config_cuda//crosstool:CROSSTOOL' is not a valid cc_toolchain_suite rule.
```
"
4367,Strange internal error after switching from python 3 to python 2,"I'm trying to port my code from python 3 to 2. However, I'm getting a strange error when running with python 2: `InternalError: Unsupported feed type`. This happens after trying to `sess.run` with a particular feed dict. What's odd is that I can run the following just fine:

```
for k, v in feed_dict.items():
    sess.run(k, {k:v})
```

That is, the individual arrays and tensors in the feed dict are all fine. I apologize for not being able to come up with a short example that reproduces the error, but perhaps someone might have some idea of what's going on?
"
4365,Build tensorflow from source with CUDA failed,"I am trying to build tensorflow in Ubuntu 14.04 but failed. My steps:
1. git rev-parse HEAD
   c715c3102df1556fc0ce88fc987440a3c80e5380
2. bazel version
   Warning: ignoring _JAVA_OPTIONS in environment.
   Another command is running (pid = 13101).  Waiting for it to complete...
   Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
   Build time: Thu Jan 01 00:00:00 1970 (0)
   Build timestamp: Thu Jan 01 00:00:00 1970 (0)
   Build timestamp as int: 0
3. ./configure 
   Please specify the location of python. [Default is /usr/bin/python]: 
   Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] n
   No Google Cloud Platform support will be enabled for TensorFlow
   Do you wish to build TensorFlow with GPU support? [y/N] y
   GPU support will be enabled for TensorFlow
   Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
   Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5
   Please specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
   Please specify the Cudnn version you want to use. [Leave empty to use system default]: 5
   Please specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
   Please specify a list of comma-separated Cuda compute capabilities you want to build with.
   You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
   Please note that each additional compute capability significantly increases your build time and binary size.
   [Default is: ""3.5,5.2""]: 
   Warning: ignoring _JAVA_OPTIONS in environment.
   ..
   INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
   Warning: ignoring _JAVA_OPTIONS in environment.
   .
   WARNING: /home/yu/.cache/bazel/_bazel_yu/520915e94204a8ab6d3b139e9ec5cf67/external/boringssl_git/WORKSPACE:1: Workspace name in /home/yu/.cache/bazel/_bazel_yu/520915e94204a8ab6d3b139e9ec5cf67/external/boringssl_git/WORKSPACE (@boringssl) does not match the name given in the repository's definition (@boringssl_git); this will cause a build error in future versions.
   INFO: All external dependencies fetched successfully.
   Configuration finished
4. bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer
   external/zlib_archive/zlib-1.2.8/gzread.c:591:5: warning: implicit declaration of function 'close' [-Wimplicit-function-declaration]
    ret = close(state->fd);
    ^
   ERROR: missing input file '@local_config_cuda//cuda:lib64/libcudnn.so.5':/home/yu/.cache/bazel/_bazel_yu/520915e94204a8ab6d3b139e9ec5cf67/external/local_config_cuda/cuda/lib64/libcudnn.so.5 (Permission denied).
   ERROR: /home/yu/.cache/bazel/_bazel_yu/520915e94204a8ab6d3b139e9ec5cf67/external/re2/BUILD:9:1: C++ compilation of rule '@re2//:re2' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 37 argument(s) skipped): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 15.
   ERROR: /home/yu/workspace/tensorflow/tensorflow/cc/BUILD:199:1: //tensorflow/cc:tutorials_example_trainer: missing input file '@local_config_cuda//cuda:lib64/libcudnn.so.5'.
   Target //tensorflow/cc:tutorials_example_trainer failed to build
   Use --verbose_failures to see the command lines of failed build steps.
   ERROR: /home/yu/workspace/tensorflow/tensorflow/cc/BUILD:199:1 1 input file(s) do not exist.

The libcudnn.so.5 exist and I can build and run other cudnn code. How can I fix the error?
"
4363,tensorflow:nightly-devel-gpu is broken,"Commit [""Fix nightly docker builds""](https://github.com/tensorflow/tensorflow/commit/414a5ddd47bba68661eaac08d5b6b7c92c38b808) broke the nightly builds :)

```
$ nvidia-docker run --rm -ti tensorflow/tensorflow:nightly-devel-gpu nvidia-smi
NVIDIA-SMI couldn't find libnvidia-ml.so library in your system. Please make sure that the NVIDIA Display Driver is properly installed and present in your system.
Please also try adding directory that contains libnvidia-ml.so to your system PATH.
```

You shouldn't override the existing value of `LD_LIBRARY_PATH`, we use it to point to the path where we mount the driver libraries mounted by nvidia-docker, see [here](https://github.com/NVIDIA/nvidia-docker/blob/master/ubuntu-14.04/cuda/7.5/runtime/Dockerfile#L38)

cc @3XX0 @caisq 
"
4362,CUDA 8.0 Failed to configure,"Dear everyone, I recently met this problem when I try to configure this problem, I am using CUDA8.0, Bazel 0.3 and Ubuntu 14.04.
ERROR: /home/haixi/tensorflow-master/tensorflow/tensorflow.bzl:568:26: Traceback (most recent call last):
    File ""/home/haixi/tensorflow-master/tensorflow/tensorflow.bzl"", line 562
        rule(attrs = {""srcs"": attr.label_list...""), <3 more arguments>)}, <2 more arguments>)
    File ""/home/haixi/tensorflow-master/tensorflow/tensorflow.bzl"", line 568, in rule
        attr.label_list(cfg = ""data"", allow_files = True)
expected ConfigurationTransition or NoneType for 'cfg' while calling label_list but got string instead: data.
ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension file 'tensorflow/tensorflow.bzl' has errors.

Hope that you can help me !
"
4361,Update tf.contrib.layers.batch_norm() docs,"_Tensorflow version that I use : 0.10 (pip package)_

---

I took heavy use of _[tf.contrib.layers.batch_norm()](https://github.com/tensorflow/tensorflow/blob/b826b79718e3e93148c3545e7aa3f90891744cc0/tensorflow/contrib/layers/python/layers/layers.py#L100)_ the last weeks. 

After facing some problems on how to use it correctly, I figured out that there are many devs out there who are confused as well, such as here:
- https://github.com/tensorflow/tensorflow/issues/1122
- http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow

I would suggest to do following improvements to make it more clear:

**1) Update example in doc-string:**

The example tells in case we use _update_collections_ on its defaults, we have to include this:

```
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
if update_ops:
    updates = tf.group(update_ops)
    total_loss = control_flow_ops.with_dependencies([updates], total_loss)
```

But this is actually not working or deprecated, as it throws errors. Instead, we have to do some tiny changes. I would suggest to update the docs as follows:

```
from tensorflow.python import control_flow_ops

update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
if update_ops:
    updates = tf.tuple(update_ops)
    total_loss = control_flow_ops.with_dependencies(updates, total_loss)
```

As a side question, why do we apply it to the _total_loss_, and not to the train_op directly, as described in the doc-string text. Added a dependency to _total_loss_ works, but grouping it with the _train_op_ would make the example more clear in my opinion, because we do batch-statistic updates only during training.

**2) _UPDATE_OPS_ in combination with reuse varscope:**

This is related to the question above. Let's say we have a model with which reuses an convolutional encoder (and also its batch-norm-layers) several times. Even when we reuse these layers, the update operation for the batch-statistics is added to _UPDATE_OPS_ nevertheless. Personally, I'm not sure if this is a bug, or if this is really what should be done?
Or is it required to filter the update-ops after collecting them with `update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)`, so that each one is executed just once?

To sum this up: Am I wrong that lines 213-215 should not be executed when reuse=True? So changing it to:

```
if not reuse:
    # Collect the updates to be computed later.
    ops.add_to_collections(updates_collections, update_moving_mean)
    ops.add_to_collections(updates_collections, update_moving_variance)
```

In my case, I'm using a Conv-LSTM-Conv_tp architecture, where I reuse the Conv/Conv_tp for each timestep. When I increase the number of timesteps in the LSTM, the number of update-ops increases in proportionally, while the number of model-parameters stays constant because they get reused. Currently, I'm getting 420 update-ops when calling `tf.get_collection(tf.GraphKeys.UPDATE_OPS)`. As the performance feels super slow when I use batch-norm, I guess this high number of update-ops cannot be right.

**3) Handling of _is_training_ parameter:**

I have seen a lot of examples people doing something like this in their code to handle the _is_training_ parameter:

```
def batch_norm_layer(x,train_phase,scope_bn):
    bn_train = batch_norm(x, decay=0.999, center=True, scale=True,
    updates_collections=None,
    is_training=True)
    bn_inference = batch_norm(x, decay=0.999, center=True, scale=True,
    updates_collections=None,
    is_training=False)
    bn = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)
    return bn
```

As far as I know, this was really required in the past, because is_training was just a Boolean. But since the param can be a Bool-Tensor as well, this is not required anymore. Since many devs are still ding this workaound, added a comment to the doc-string that this is not required anymore could be helpful.

**4) Usage on Multi-GPU configuration**

a) When I optimize my code for multi-GPU systems (as in the CIFAR10 example) the number of update-ops increases with the factor of num_gpus (might be related to 2) ).

b) When I use _tf.contrib.batch_norm()_ within a multi-GPU system, I get an error like this:

```
InvalidArgumentError: Cannot assign a device to node 'tower_1/inference/ConvStack/x_bn_9/moments/sufficient_statistics/SparseToDense': 
Could not satisfy explicit device specification '/device:GPU:1' because no supported kernel 
for GPU devices is available.
...
```

Hence, to we have to wrap evey _batch_norm()_ call with _tf.device(""/cpu:0"")_? I guess this might have bad impacts on performance, right?

Thanks!

_PS: Sorry in case this question would fits better to StackOverflow. As it is a combination of suggested improvements and questions. Just let me know..._
"
4359,Feature Request: plug-in support for new devices,"We need to be able to support using new devices in TensorFlow without requiring editing the TensorFlow binary source code.

This requires a few changes.  Among many others:
- The ability to dynamically register an implementation / factories for alternative devices
- New or better APIs for allowing OpKernel's to use arbitrary device resources (e.g., right now we assume either the use of StreamExecutor or EigenDevice implementations, but that's obviously not general).
"
4358,Wrong example script in the docs for preprocessing data,"I was reading the [docs for preprocessing](https://www.tensorflow.org/versions/r0.10/how_tos/reading_data/index.html#preprocessing), which is a small paragraph linking to the [CIFAR-10 network](https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/models/image/cifar10/cifar10.py) as an example. However, that script does not perform any preprocessing. Do we have a better example illustrating preprocessing steps like data normalization, distorting images, etc?
"
4357,FileIO.read signature differs from Python stdlib File.read interface,"`FileIO` does not quack quite like a File object; its `read` method should take an optional `size` argument. This discrepancy makes `GFile` not a drop-in replacement for `File`, which makes it hard to port code between Google and the outside world.

`File.read` API doc: https://docs.python.org/2/library/stdtypes.html#file.read
Current `FileIO.read`: https://github.com/tensorflow/tensorflow/blob/915b02917db21de5a0ae304a067aedb0b5dd759d/tensorflow/python/lib/io/file_io.py#L98
"
4356,Playground is not in https,"It should be as the rest of tensorflow website is in https
"
4355,Clicking on Image in Tensorboard Should Open in New Window,"Right now the image may be squashed and hard to read:
![image](https://cloud.githubusercontent.com/assets/51059/18482149/0324d058-79ad-11e6-83a4-d782b523065d.png)

The current solution is to copy the image URL and manually open in a new tab.
"
4354,Tensorboard should Update Browser Location on Tab Changes,"![image](https://cloud.githubusercontent.com/assets/51059/18482024/88fb96e0-79ac-11e6-838e-3b2a6094b16a.png)

Right now changing tabs does not change the location which means browser refresh, etc loses the current tab.
"
4353,confusion_matrix should use cast rather than convert to tensor for size,"This does not work:

```
>>> cm = tf.contrib.metrics.confusion_matrix(
...     [1,2,3], [4,5,5], num_classes=tf.constant(3, dtype=tf.int32))
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/metrics/python/ops/confusion_matrix_ops.py"", line 84, in confusion_matrix
    indices=indices, values=values, shape=shape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 981, in __init__
    shape = convert_to_tensor(shape, name=""shape"", dtype=dtypes.int64)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 628, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 571, in _TensorTensorConversionFunction
    % (dtype.name, t.dtype.name, str(t)))
ValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: 'Tensor(""confusion_matrix/pack:0"", shape=(2,), dtype=int32)'
```

where as this does:

```
tf.contrib.metrics.confusion_matrix(
    [1,2,3], [4,5,5], num_classes=tf.constant(3, dtype=tf.int64))
```

This seems related: https://github.com/tensorflow/tensorflow/pull/2187

Machine:

```
 bazel version
INFO: Reading 'startup' options from /root/.bazelrc: --batch
Extracting Bazel installation...
Build label: 0.3.0
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jun 10 11:38:23 2016 (1465558703)
Build timestamp: 1465558703
Build timestamp as int: 1465558703
```

```
 git rev-parse HEAD
58b37cf745c6c30d44878ddf5987c4283ed12c3c
```

```
ls -l /usr/lib/x86_64-linux-gnu/libcud*
lrwxrwxrwx 1 root root       29 Aug 12 05:11 /usr/lib/x86_64-linux-gnu/libcudnn.so -> /etc/alternatives/libcudnn_so
lrwxrwxrwx 1 root root       17 Jun 10 13:07 /usr/lib/x86_64-linux-gnu/libcudnn.so.5 -> libcudnn.so.5.1.3
-rw-r--r-- 1 root root 60696704 Jun 10 13:07 /usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.3
lrwxrwxrwx 1 root root       32 Aug 12 05:11 /usr/lib/x86_64-linux-gnu/libcudnn_static.a -> /etc/alternatives/libcudnn_stlib
-rw-r--r-- 1 root root 59715990 Jun 10 13:07 /usr/lib/x86_64-linux-gnu/libcudnn_static_v5.a
```

```
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally
```
"
4352, ./configure command generates  warnings,"Hi ,
When i execute ./configure in tensorflow directory  the output contains the following warnings:
( Environment is  ubuntu16.04+cuda8+cudnnV5.1+bazel-0.3.0) 

WARNING: /home/syj/.cache/bazel/_bazel_syj/47b860f752d3ee48e8d208f528a56395/external/protobuf/WORKSPACE:1: Workspace name in /home/syj/.cache/bazel/_bazel_syj/47b860f752d3ee48e8d208f528a56395/external/protobuf/WORKSPACE (@__main__) does not match the name given in the repository's definition (@protobuf); this will cause a build error in future versions.

Is this warning serious? How to avoid this ?

Thanks 
"
4350,saver problem,"```
saver.save(sess, os.path.join(model_path, 'model'), global_step=epoch)
```

  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1253, in save
    {self.saver_def.filename_tensor_name: checkpoint_file})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 717, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 915, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 965, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 985, in _do_call
    raise type(e)(node_def, op, message)
ResourceExhaustedError: ./model/model-49.tempstate5787507096018461664
         [[Node: save/save = SaveSlices[T=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, 
DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Co
nst_0, save/save/tensor_names, save/save/shapes_and_slices, Wemb, att_W/_2905, att_b/_2907, decode_lstm_W/_2909, decode_lstm_b/_2911, decode_wo
rd_W/_2913, decode_word_b/_2915, hidden_att_W/_2917, image_att_W/_2919, image_encode_W/_2921, init_hidden_W/_2923, init_hidden_b/_2925, init_me
mory_W/_2927, init_memory_b/_2929, lstm_U/_2931, lstm_W/_2933, lstm_b/_2935, pre_att_b/_2937)]]
Caused by op u'save/save', defined at:
  File ""model_tensorflow.py"", line 345, in <module>
    train(pretrained_model_path=None)
  File ""model_tensorflow.py"", line 261, in train  
    saver = tf.train.Saver(max_to_keep=50)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 966, in __init__
    self.build()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 990, in build
    restore_sequentially=self._restore_sequentially)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 614, in build
    save_tensor = self._AddSaveOps(filename_tensor, saveables)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 294, in _AddSaveOps
    save = self.save_op(filename_tensor, saveables)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 240, in save_op
    tensor_slices=tensor_slices)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/io_ops.py"", line 181, in _save
    tensors, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 438, in _save_slices
    data=data, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 710, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2334, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1253, in __init__
    self._traceback = _extract_stack()
"
4349,kernel version 367.44.0 does not match DSO version 352.99.0,"I can't seem to load GPUs.  After a first install of CUDA, I didn't have this error, but tensorflow couldn't find any GPUs (despite nvidia-smi finding one).  After a second install based on these instructions: http://tech.marksblogg.com/tensorflow-nvidia-gtx-1080.html I had the following error.

Thanks for your help!
### Environment info

Operating System: Ubuntu 14.04

Installed version of CUDA and cuDNN: 
-rw-r--r-- 1 root root 189170 Sep 12 23:50 /usr/local/cuda/lib/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Sep 12 23:50 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19 Sep 12 23:50 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 311596 Sep 12 23:50 /usr/local/cuda/lib/libcudart.so.7.5.18
-rw-r--r-- 1 root root 558020 Sep 12 23:50 /usr/local/cuda/lib/libcudart_static.a

Compiled from:
http://developer.download.nvidia.com/compute/cuda/7.5/Prod/local_installers/cuda_7.5.18_linux.run
1. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
   I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
   I tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: /usr/local/cuda/lib64:
   I tensorflow/stream_executor/cuda/cuda_dnn.cc:2259] Unable to load cuDNN DSO
   I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
   I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
   I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
   0.10.0
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

 tf.Session()
E tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:153] retrieving CUDA diagnostic information for host: XXX
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: XXX
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: 352.99.0
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:356] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.44  Wed Aug 17 22:24:07 PDT 2016
GCC version:  gcc version 4.9.4 (Ubuntu 4.9.4-2ubuntu1~14.04.1) 
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 367.44.0
E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:296] kernel version 367.44.0 does not match DSO version 352.99.0 -- cannot find working devices in this configuration
I tensorflow/core/common_runtime/gpu/gpu_init.cc:81] No GPU devices available on machine.
"
4348,bazel can not compile Tensorflow with GCC 6.2.1,"**When I launch** 
""bazel build  -c opt --jobs=10 tensorflow/tools/pip_package:build_pip_package""
**to compile tensorflow from source ,it crashed at:**
""external/com_googlesource_code_re2/BUILD:11:1: C++ compilation of rule '@com_googlesource_code_re2//:re2' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-canonical-system-headers ... (remaining 32 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
external/com_googlesource_code_re2/re2/dfa.cc: In constructor 're2::DFA::State::State()':
external/com_googlesource_code_re2/re2/dfa.cc:95:10: error: unknown array size in delete...
""
**Here are some relative infos**:
 1)bazel label: 0.3.1- (@non-git)
 2)gcc (GCC) 6.2.1 20160830
 3)tensorflow is the latest master branch
Can you tell me how to configure bazel with clang(not cross compile) ? Thanks.
"
4346,Android version of TensorFlow is missing dequantize operation,"### Environment info

Operating System: macOS Sierra, building for Android API 19
### Minimal reproducible example

Any graph that uses either of these operations. Using the quantization tool at `tensorflow/contrib/quantization/tools:quantize_graph` will add quantize and dequantize ops to a graph, which will subsequently not work on Android.
### What other attempted solutions have you tried?

Building with the `makefile` module.
"
4343,Commit that breaks Bazel 0.3.0 support,"If I try to configure the lastest master branch (db45268db0203fcdb0e48e3e558b01244424c0a6) using Bazel 0.3.0, I get:

```
ERROR: [...]/tensorflow/tensorflow/tensorflow.bzl:568:26: Traceback (most recent call last):
        File ""[...]/tensorflow/tensorflow/tensorflow.bzl"", line 562
                rule(attrs = {""srcs"": attr.label_list...""), <3 more arguments>)}, <2 more arguments>)
        File ""[...]/tensorflow/tensorflow/tensorflow.bzl"", line 568, in rule
                attr.label_list(cfg = ""data"", allow_files = True)
expected ConfigurationTransition or NoneType for 'cfg' while calling label_list but got string instead: data.
ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension file 'tensorflow/tensorflow.bzl' has errors.
```

Using 0.3.1 avoids this and reverting the offending commit 7bcdcbbf60fc08346fd8016270a0563f4b51362b also fixes it.
### Environment info

Setup: CentOS, Bazel 0.3.0, CUDA 7.5, CuDNN 5.1, Tensorflow master (db45268db0203fcdb0e48e3e558b01244424c0a6)

I run `configure` and set it up for GPU support. I see the error above if I use bazel 0.3.0.
### Fix

Upgrade to 0.3.1, obviously. However, since 0.3.0 is still officially supported, I thought this deserves to be an issue.

Another fix is to revert the commit 7bcdcbbf60fc08346fd8016270a0563f4b51362b. Note, I didn't have to revert to its parent; undoing that commit alone is enough:

```
git show -R 7bcdcbbf | git apply
```
"
4342,Invert gradient op,"This one should be pretty simple. It would be nice to have an op that acts as the identity function in feedforward, but during backpropagation it propagates the negative of the gradients. This is useful when joining two datasets as it allows for the suppression of features that distinguish between domains. Its referred to as a gradient reversal layer in [http://arxiv.org/pdf/1505.07818v4.pdf](http://arxiv.org/pdf/1505.07818v4.pdf)
"
4339,"Add ""Gamma Correction"" Image Adjustment","This appears to be different from but similar to the [existing brightness adjustment](https://www.tensorflow.org/versions/r0.10/api_docs/python/image.html#adjust_brightness).

See: http://scikit-image.org/docs/dev/api/skimage.exposure.html#skimage.exposure.adjust_gamma
"
4337,Multi-CPU kernel for sparse_tensor_dense_matmul,"Are there plans to implement a multi-cpu kernel for tf.sparse_tensor_dense_matmul? The current version seems to be single core only and is not performing well. 

I am working with a very large sparse tensor - about 8GB sparse and 200GB dense that I cannot work with as a dense tensor for memory reasons. Any suggestions for achieving fast matrix multiplication? 
"
4336,different initial states for different data,"i'm working with bidirectional rnn and it would be nice that it had an option to have two different initial states for different data to be trained.
thnx
"
4335,configure issues,"```
ERROR: /home/wenjian/pkgs/tensorflow/tensorflow/core/kernels/BUILD:2207:1: no such package 'base': BUILD file not found on package path and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.
ERROR: /home/wenjian/pkgs/tensorflow/tensorflow/core/kernels/BUILD:2207:1: no such package 'base': BUILD file not found on package path and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.
ERROR: /home/wenjian/pkgs/tensorflow/tensorflow/core/kernels/BUILD:2207:1: no such target '//tensorflow/core:android_tensorflow_lib_lite_no_rtti_lite_runtime': target 'android_tensorflow_lib_lite_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /home/wenjian/pkgs/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.
ERROR: /home/wenjian/pkgs/tensorflow/tensorflow/core/kernels/BUILD:2207:1: no such target '//tensorflow/core:android_proto_lib_no_rtti_lite_runtime': target 'android_proto_lib_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /home/wenjian/pkgs/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.
ERROR: Evaluation of query ""deps((//... union @bazel_tools//tools/jdk:toolchain))"" failed: errors were encountered while computing transitive closure.

```
"
4333,dynamic_rnn() cannot receive an input of dynamic shape during training,"Hi all, I met some problem when using dynamic_rnn() and wonder if there is any solution.

Firstly I sort all examples according to its length(variable length input). Then I use ""batch padding"" in my code. So in each step during training, data generator generates a padded minibatch, whose shape differ from the last batch. And I feed the minibatch data to the graph. Since the placeholder ""inputs"" in graph has an uncertain shape, it raised an error when tensor flows to `dynamic_rnn()` node. If I specify the placeholder's shape, everything is ok.

Now `dynamic_rnn()` can only receive inputs with a fixed shape. How can I combine it with ""batch padding""?

Thanks!
"
4332,Tensor with inconsistent dimension size? ,"I've been implementing a convolutional neural network for object detection and I met the issue below:

For object detection task, usually, one input image is associated with an undetermined number of object bounding boxes. Each bounding box can be represented by 4 coordinates. Thus, to represent bounding boxes as a tensor, the shape will be: 
    _[**batch_size, variable_num_bbox(?), 4**]_. 
Note that here, it's not just that _**variable_num_bbox**_ can't be determined before the graph construction, but also, even within one batch input, different images can have different numbers of bounding boxes. 

As an illustrative example, I would like to convert the following array into a tensor: 
    _[[[1, 2, 3, 4], [2, 3, 4, 5]], [[3, 4, 5, 6]]]_
Here, _**variable_num_bbox**_ is 2 for the first image, but it's 1 for the second image. 

I've tried and failed several ways to convert the above nested list to a tensor, which leads me to the question whether tensorflow support tensors with inconsistent dimension size? If no, is there any plan to support it to give developers such flexibility? And if it's not going to be supported, is there a way to by-pass this issue for object detection task? One solution would be to set _**batch_size=1**_ , and a bounding box can be represented as _**[variable_num_bbox(?), 4]**_, so yes, the dimension inconsistency is gone, but that will hurt the efficiencyy significantly.
"
4331,Problem with using tf.contrib.metrics.streaming_mean_iou,"I am trying to use `tf.contrib.metrics.streaming_mean_iou` in my network and I am getting following Error.

```
W tensorflow/core/framework/op_kernel.cc:940] Failed precondition: Attempting to use uninitialized value mean_iou/total_confusion_matrix
     [[Node: mean_iou/total_confusion_matrix/read = Identity[T=DT_INT64, _class=[""loc:@mean_iou/total_confusion_matrix""], _device=""/job:localhost/replica:0/task:0/cpu:0""](mean_iou/total_confusion_matrix)]]
Traceback (most recent call last):
....
....
File ""/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 710, in run
    run_metadata_ptr)
  File ""/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 908, in _run
    feed_dict_string, options, run_metadata)
  File ""/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 958, in _do_run
    target_list, options, run_metadata)
  File ""/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 978, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.FailedPreconditionError: Attempting to use uninitialized value mean_iou/total_confusion_matrix
     [[Node: mean_iou/total_confusion_matrix/read = Identity[T=DT_INT64, _class=[""loc:@mean_iou/total_confusion_matrix""], _device=""/job:localhost/replica:0/task:0/cpu:0""](mean_iou/total_confusion_matrix)]]
....
....
File ""/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/contrib/metrics/python/ops/metric_ops.py"", line 1805, in streaming_mean_iou
    dtype=dtypes.int64)
  File ""/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/contrib/metrics/python/ops/metric_ops.py"", line 79, in _create_local
    collections=collections)
  File ""/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 211, in __init__
    dtype=dtype)
  File ""/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 323, in _init_from_args
    self._snapshot = array_ops.identity(self._variable, name=""read"")
  File ""/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1106, in identity
    result = _op_def_lib.apply_op(""Identity"", input=input, name=name)
  File ""/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 703, in apply_op
    op_def=op_def)
  File ""/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2317, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1239, in __init__
    self._traceback = _extract_stack()
```

It seems to me that there might be some bug in the `tf.contrib.metrics.streaming_mean_iou`since all the variables are already initialised.

I am using it as follows:

```
accuracy_increment, iou_increment = self.session.run(
                [self.accuracy, self.mean_iou], feed_dict={
                self.X: eval_patches[
                        q * self.microbatch_size:
                        q * self.microbatch_size +
                        self.microbatch_size],
                self.Y: eval_labels[
                        q * self.microbatch_size:
                        q * self.microbatch_size +
                        self.microbatch_size],
                self.p_keep_conv: 1})
```

And the mean iou is computed in the object as:

```
self.mean_iou, _ = tf.contrib.metrics.streaming_mean_iou(
            self.predictions, tf.argmax(self.Y, dimension=3), num_classes=2)
```

I am using from source build tensorflow (branch r10.0) and it's happening to me on both CPU and GPU.

I am not 100% sure that I am using the `tf.contrib.metrics.streaming_mean_iou` correctly, but I have played with that a lot, but there is quite lack of the documentation, so I am currently stuck assuming that there is either some bug in the `tf.contrib.metrics.streaming_mean_iou` or not enough documentation for me to make it work.

I thank in advance anyone for any comments.
"
4330,overparametrized convolution error in tf.nn.separable_conv2d ,"I'm trying to build a network of layers with channel-wise separable convolution. (Just like the paper [Factorized CNN](http://128.84.21.199/abs/1608.04337))

I've found that separable_conv2d and depthwise_conv2d is the two options, and I'm trying out these two.

What I am trying to build is as below

```
depthwise_filter = tf.get_variable(""depth_conv_w"", [3,3,64,3], initialize=tf.random_normal_initializer(stddev=np.sqrt(2.0/9/32)))
pointwise_filter = tf.get_variable(""point_conv_w"", [1,1,192,64], initializer=tf.random_normal_initializer(stddev=np.sqrt(2.0/9/128)))
conv_tensor = tf.nn.depthwise_conv2d(tensor, depthwise_filter, [1,1,1,1], padding='SAME')
conv_tensor = tf.nn.conv2d(conv_tensor, pointwise_filter, [1,1,1,1], padding='VALID')
```

and it works fine.

However, if I switch the last 2 lines with

```
conv_tensor=tf.nn.separable_conv2d(tensor,depthwise_filter,pointwise_filter,[1,1,1,1],padding='SAME')
```

then tensorflow gives me 'overparamatrized convolution error' as specified in [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn.py#L705)

I think **channel_multiplier \* in_channel** is usually larger than or equal to the **out_channel**.
So I believe this is an error.

PS) I can get the same result as the **separable_conv2d** with **depthwise_conv2d and 1x1 convolution**. Is there any advantage of using **separable_conv2d**?
"
4329,typo in tutorials/mnist/beginners/index.html,"https://www.tensorflow.org/versions/r0.10/tutorials/mnist/beginners/index.html

""East entry in the tensor is a pixel intensity between 0 and 1, for a particular pixel in a particular image.""

 should probably be ""Each entry..."".

Sorry if this is an inappropriate forum to draw attention to this.  There was not corrections contact.
"
4328,"In a distribute model, if I run 10 worker jobs, how many ps jobs should run?","In a distribute model, 10 nodes each have one GPU and run a worker job, how many ps jobs should run?
"
4326,ImportError: No module named _pywrap_tensorflow,"Traceback (most recent call last):
  File ""./train.py"", line 3, in <module>
    import tensorflow as tf
  File ""/Users/thomassonderman/tensorflow_gpu/lib/python2.7/site-packages/tensorflow/**init**.py"", line 23, in <module>
    from tensorflow.python import *
  File ""/Users/thomassonderman/tensorflow_gpu/lib/python2.7/site-packages/tensorflow/python/**init**.py"", line 48, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Users/thomassonderman/tensorflow_gpu/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 21, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/Users/thomassonderman/tensorflow_gpu/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow')
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/importlib/__init__.py"", line 37, in import_module
    **import**(name)
ImportError: No module named _pywrap_tensorflow

Related Problem:
SyntaxNet ImportError: No module named _pywrap_tensorflow #97
Fix did not work

Environment:
Macbook retina Nvidia Os: El Capitan 

Installed version of CUDA and cuDNN: 
cuda 7.5

(tensorflow_gpu) Thomass-MacBook-Pro:DIS thomassonderman$ python -c ""import tensorflow; print(tensorflow.**version**)""
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.7.5.dylib locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.5.dylib locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.7.5.dylib locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.1.dylib locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.7.5.dylib locally
0.10.0rc0

If installed from source, provide 

The commit hash (`git rev-parse HEAD`)

3cb39956e622b322e43547cf2b6e337020643f21

Build label: 0.3.1-homebrew
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Aug 4 09:58:27 2016 (1470304707)
Build timestamp: 1470304707
Build timestamp as int: 1470304707

import tensorflow works, just not any examples
### What other attempted solutions have you tried?

redownloading the source and recompiling, reinstalling pip package.
setting up bazel-completion
"
4324,can't find module tensorflow after building 'quantize_graph',"Hi,

I followed this [blog post](https://petewarden.com/2016/05/03/how-to-quantize-neural-networks-with-tensorflow/) in order build the tool for quantizing graph (tensorflow/contrib/quantization/tools/quantize_graph).
I had a clean working version of tensorflow installed properly.

I used this command in order to build the quantization tool :
**bazel build tensorflow/contrib/quantization/tools:quantize_graph**

It seems that the build went ok :
**INFO: Found 1 target...
Target //tensorflow/contrib/quantization/tools:quantize_graph up-to-date:
  bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph
INFO: Elapsed time: 20.105s, Critical Path: 16.60s**

However after that finished when i open python and try to import tensorflow i get an error message :
**ImportError: No module named tensorflow**
### Environment info

Operating System: Ubuntu 16.04

Installed from source, without GPU support :
1. commit hash = 37256f4857cdadefa09e0505f4acc91ffbf626e2
2. bazel version = 
Build label: 0.3.1
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jul 29 09:09:52 2016 (1469783392)
Build timestamp: 1469783392
Build timestamp as int: 1469783392
"
4322,"Ubuntu16.04+cuda8+cudnn5.1, meets error","> INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
> .
> ERROR: /home/ei/github/test/tensorflow/tensorflow/contrib/session_bundle/BUILD:237:1: no such target '//tensorflow/core:meta_graph_portable_proto': target 'meta_graph_portable_proto' not declared in package 'tensorflow/core' defined by /home/ei/github/test/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:signature_lite'.
> ERROR: /home/ei/github/test/tensorflow/tensorflow/core/BUILD:689:1: no such package 'base': BUILD file not found on package path and referenced by '//tensorflow/core:ios_tensorflow_test_lib'.
> ERROR: Evaluation of query ""deps((//... union @bazel_tools//tools/jdk:toolchain))"" failed: errors were encountered while computing transitive closure.
> Configuration finished

I'm trying to do as http://textminingonline.com/dive-into-tensorflow-part-iii-gtx-1080-ubuntu16-04-cuda8-0-cudnn5-0-tensorflow. when it comes to './configure', terminal show errors.

Who can help me with this issue
"
4321,model.saver.save() freeze and get killed,"Below is my flow for training an encoder-decoder model.

The training runs alright. But the program froze and eventually got killed during the call to Saver.export_meta_graph(). 

However, I was able to get the saved checkpoint and continue training.

Does anyone know what may be the cause for this problem?

```
    for t in xrange(FLAGS.num_epochs):
        print(""Epoch %d"" % t)

        start_time = time.time()

        # shuffling training examples
        # random.shuffle(train_set)

        # progress bar
        for _ in tqdm(xrange(FLAGS.steps_per_checkpoint)):
            time.sleep(0.01)
            random_number_01 = np.random.random_sample()
            bucket_id = min([i for i in xrange(len(train_buckets_scale))
                             if train_buckets_scale[i] > random_number_01])
            formatted_example = model.get_batch(train_set, bucket_id)
            _, step_loss, _ = model.step(sess, formatted_example, bucket_id, 
                                         forward_only=False)
            loss += step_loss
            current_step += 1

        epoch_time = time.time() - start_time

        # Once in a while, we save checkpoint, print statistics, and run evals.
        if t % FLAGS.epochs_per_checkpoint == 0:

            # Print statistics for the previous epoch.
            loss /= FLAGS.steps_per_checkpoint
            ppx = math.exp(loss) if loss < 300 else float('inf')
            print(""learning rate %.4f epoch-time %.2f perplexity %.2f"" % (
                model.learning_rate.eval(), epoch_time, ppx))

            # Decrease learning rate if no improvement of loss was seen over last 3 times.
            if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):
                sess.run(model.learning_rate_decay_op)
            previous_losses.append(loss)

            checkpoint_path = os.path.join(FLAGS.train_dir, ""translate.ckpt"")
```
"
4319,Error when building tensorflow from source,"I am trying to build and install tensorflow from source. However when building with bazel it returns an error stating Extension file 'tensorflow/tensorflow.bzl' has errors.

Here are the commands that I ran and the logs are included.

``` bash

mkvirtualenv tensorflow_dev
brew install bazel swig
workon tensorflow_dev
pip install six numpy wheel ipython
export TF_DIR=/Users/shashank/Documents/repositories/tensorflow
cd /Users/shashank/Documents/repositories/
git clone git@github.com:tensorflow/tensorflow.git
cd $TF_DIR && ./configure

~/Documents/repositories/tensorflow ~/Documents/repositories/tensorflow
Please specify the location of python. [Default is /Users/shashank/.virtualenvs/tensorflow_dev/bin/python]:
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N]
No Google Cloud Platform support will be enabled for TensorFlow
Found possible Python library paths:
  /Users/shashank/Documents/py_config/
  /Users/shashank/.virtualenvs/tensorflow_dev/lib/python2.7/site-packages
Please input the desired Python library path to use.  Default is [/Users/shashank/Documents/py_config/]

/Users/shashank/Documents/py_config/
Do you wish to build TensorFlow with GPU support? [y/N]
No GPU support will be enabled for TensorFlow
Configuration finished

cd $TF_DIR && bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
ERROR: /Users/shashank/Documents/repositories/tensorflow/tensorflow/tensorflow.bzl:568:26: Traceback (most recent call last):
        File ""/Users/shashank/Documents/repositories/tensorflow/tensorflow/tensorflow.bzl"", line 562
            rule(attrs = {""srcs"": attr.label_list...""), <3 more arguments>)}, <2 more arguments>)
        File ""/Users/shashank/Documents/repositories/tensorflow/tensorflow/tensorflow.bzl"", line 568, in rule
            attr.label_list(cfg = ""data"", allow_files = True)
expected ConfigurationTransition or NoneType for 'cfg' while calling label_list but got string instead: data.
ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension file 'tensorflow/tensorflow.bzl' has errors.
INFO: Elapsed time: 0.063s
```

Thanks for your help
"
4317,unable to install tensorflow ,"i am using ubuntu 12.04. i was following instructions from tensorflow.org for installing the tensor flow but getting the following error after downloading the the tensor flow-
ownloading tensorflow-0.10.0-cp27-none-linux_x86_64.whl (36.6Mb): 36.6Mb downloaded
  Running setup.py egg_info for package from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl
    Traceback (most recent call last):
      File ""<string>"", line 14, in <module>
    IOError: [Errno 2] No such file or directory: '/tmp/pip-uIJFE4-build/setup.py'
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):

  File ""<string>"", line 14, in <module>

IOError: [Errno 2] No such file or directory: '/tmp/pip-uIJFE4-build/setup.py'

---

Command python setup.py egg_info failed with error code 1
Storing complete log in /home/dhanpal/.pip/pip.log
"
4314,"Add float16 support for Elu, EluGrad ops","Support for `tf.float16` dtype was recently added (#1300) to a bunch of ops, starting with matmul, conv2D and then many more. Can we add it for elu also please? 

Looking at `tensorflow/nn_ops.cc`, I see that the `Elu` and `EluGrad` ops are registered for types `{float, double}` whereas for a whole bunch of other activation functions I see the type as `T: realnumbertype` or `{half, float, double}`.

To help with triaging, here's the paper that motivates elu activation: [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](http://arxiv.org/abs/1511.07289)

And FWIW @alexatknit in #1300 commented Re: looking forward to float16 support for elu too.
"
4312,Can't configure due to undeclared packages inside if_android/if_ios,"I am having trouble configuring the latest master branch (dbe7ee0dfa9e5ab26284522379f2747510fc267b). When I run `./configure`, I get:

```
ERROR: [...]/tensorflow/tensorflow/contrib/session_bundle/BUILD:107:1: no such target '//tensorflow/core:android_lib_lite': target 'android_lib_lite' not declared in package 'tensorflow/core' defined by [...]/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:session_bundle'.
ERROR: [...]/tensorflow/tensorflow/contrib/session_bundle/BUILD:213:1: no such target '//tensorflow/core:android_lib_lite': target 'android_lib_lite' not declared in package 'tensorflow/core' defined by [...]/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:signature'.
ERROR: [...]/tensorflow/tensorflow/contrib/session_bundle/BUILD:107:1: no such target '//tensorflow/core:meta_graph_portable_proto': target 'meta_graph_portable_proto' not declared in package 'tensorflow/core' defined by [...]/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:session_bundle'.
ERROR: [...]/tensorflow/tensorflow/contrib/session_bundle/BUILD:213:1: no such target '//tensorflow/core:meta_graph_portable_proto': target 'meta_graph_portable_proto' not declared in package 'tensorflow/core' defined by [...]/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:signature'.
ERROR: [...]/tensorflow/tensorflow/contrib/session_bundle/BUILD:213:1: no such target '//tensorflow/core:meta_graph_portable_proto': target 'meta_graph_portable_proto' not declared in package 'tensorflow/core' defined by [...]/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:signature'.
ERROR: [...]/tensorflow/tensorflow/contrib/session_bundle/BUILD:107:1: no such target '//tensorflow/core:meta_graph_portable_proto': target 'meta_graph_portable_proto' not declared in package 'tensorflow/core' defined by [...]/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:session_bundle'.
ERROR: [...]/tensorflow/tensorflow/contrib/session_bundle/BUILD:107:1: no such target '//tensorflow/core:android_lib_lite': target 'android_lib_lite' not declared in package 'tensorflow/core' defined by [...]/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:session_bundle'.
ERROR: Evaluation of query ""deps((//... union @bazel_tools//tools/jdk:toolchain))"" failed: errors were encountered while computing transitive closure.
Configuration finished
```

To summarize, the dependencies that are included inside the `if_android` and `if_ios` are not found. They don't exist in the repository, so that is not that surprising. What is more surprising though is that my vanilla installation is not returning empty lists when `if_android` is called. I haven't looked into how those functions work, so not sure why that is happening.
### Environment info

Setup: CentOS, Bazel 0.3.1, CUDA 7.5, CuDNN 5.1, Tensorflow master (dbe7ee0dfa9e5ab26284522379f2747510fc267b)

I run `configure` and set it up for GPU support. Actually, I don't think this is criticial, but first I had to open up `configure` and add `--output_base=...` on the two calls to `bazel`, since my setup requires a custom cache directory.
### Fix

The `if_...` lines were added in ed87884e50e1a50f7dc7b36dc7a7ff225442bee0, so a fix that I know works is to use its parent commit 7705791619f5e851687e9a63b4315087e189f8be.
"
4311,Configure takes a lot of time and resources ,"in the last few weeks I notice that while developing operations for tensorflow I find myself doing ./configure many more times and each time takes few minutes on a good internet connection.

Can we cache some of the artifacts ? Do we have to remove all the time the complete protobuf repository ? How is this handled internally at google ? 
"
4309,"In-place operations (dropout, ReLU, etc..)","As far as I understand, tensorflow does not support in-place operation of operations such as ReLU, Dropout, etc..

If an operation outputs the same type and shape tensor and does not involve any branching in the graph then it should be possible to operate on the same chunk of memory.

As it stands, tensorflow doesn't support this which severely increases memory usage for large deep learning models.
"
4308,tf.image.rot90() returns `None` if the argument is not a Python integer,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Pointed out in this [Stack Overflow](http://stackoverflow.com/q/39418948/3574081) question.
### Environment info
1. The commit hash (`git rev-parse HEAD`): bf5b2f0185d4b61329a0bf7a56827661ef6526a1
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

``` python
image = tf.random_uniform([100, 100, 3], dtype=tf.float32)
assert tf.image.rot90(image, tf.constant(7)) is None
```
"
4306,How does conv2d_transpose exactly space out inputs in tensorflow?,"This may not be the best place to ask this question but I don't get any help from stackoverflow. Sorry if I ask this question in the wrong place.

I understand how transpose convolution works dealing with 2-D images. I just wonder how conv2d_transpose() function in tensorflow exactly adds zeros to the inputs.

Does it only pad zeros along the border, or does it insert zeros to space out inputs in a certain way?
"
4303,Cannot use TensorArray with tf.scan,"I seem to be unable to use `TensorArray`s with `tf.scan`. I am running 0.10.0rc0 on Ubuntu 14.04 with Cuda Toolkit 7.5 and cuDNN v4.

Here is a minimal example:

``` python
import tensorflow as tf

array = tf.TensorArray(tf.float32, size=10)
elements = list(range(10))

def step(state, current_input):
    return state.write(current_input, tf.zeros(shape=[100]))

out = tf.scan(step, elements, initializer=array)

with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())
    print(sess.run(out))
```

And the output is:

```
Traceback (most recent call last):
  File ""array_test.py"", line 9, in <module>
    out = tf.scan(step, elements, initializer=array)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/functional_ops.py"", line 521, in scan
    a_flat = [ops.convert_to_tensor(init) for init in initializer_flat]
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/functional_ops.py"", line 521, in <listcomp>
    a_flat = [ops.convert_to_tensor(init) for init in initializer_flat]
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py"", line 628, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/constant_op.py"", line 180, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/constant_op.py"", line 163, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/tensor_util.py"", line 422, in make_tensor_proto
    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/tensor_util.py"", line 422, in <listcomp>
    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/util/compat.py"", line 45, in as_bytes
    (bytes_or_text,))
TypeError: Expected binary or unicode string, got <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7ff0db9309e8>
```
"
4299,tensorflow-0.8.0-cp27-none-linux_86_64.whl is not a supported wheel on this platform.,"I encounted this issue when trying to install the tensorflow on mine Ubuntu system. here is the details of my system.
Linux ... 4.4.0-21-generic #37-Ubuntu SMP Mon Apr 18 18:33:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
"
4298,"it allocates on all specified devices (GPU Memory usage is high on Nvidia-smi), but does not do computations (no memory utilization from Nvidia-smi while the program is running)","Tensorflow it allocates on all specified devices (GPU Memory usage is high on Nvidia-smi), but does not do computations (no memory utilization from Nvidia-smi while the program is running),what's wrong
"
4297,'SAME' padding works incorrect,"I have 28x28 image. When I apply convolution (or pooling) with 'SAME' padding, kernel size 2x2, stride 2, it produces feature map of size 14x14, but it should be 15x15, so the last (bottom and right) image pixels would't be processed. The output size should be calculated with formula output = [(input+2*padding-kernel) / stride] + 1 (look https://arxiv.org/pdf/1603.07285.pdf). In case of 'SAME', padding size would be [kernel/2]
TF version 0.9.0
"
4296,Grid RNN Cell does not support dynamic batch sizes,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

http://stackoverflow.com/questions/38442025/tensorflow-grid-lstm-rnn-typeerror
### Environment info

Operating System: 
Ubuntu 16.04 LTS (GNU/Linux 4.4.0-31-generic x86_64)

Installed version of CUDA and cuDNN: 
-rw-r--r-- 1 root root   322936 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudadevrt.a
lrwxrwxrwx 1 root root       16 MÃ¤r 30 14:25 /usr/lib/x86_64-linux-gnu/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root       19 MÃ¤r 30 14:25 /usr/lib/x86_64-linux-gnu/libcudart.so.7.5 -> libcudart.so.7.5.18
-rw-r--r-- 1 root root   383336 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudart.so.7.5.18
-rw-r--r-- 1 root root   720192 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudart_static.a
lrwxrwxrwx 1 root root       12 Apr 14 17:53 /usr/lib/x86_64-linux-gnu/libcuda.so -> libcuda.so.1
lrwxrwxrwx 1 root root       17 Apr 14 17:53 /usr/lib/x86_64-linux-gnu/libcuda.so.1 -> libcuda.so.361.42
-rw-r--r-- 1 root root 16881416 MÃ¤r 23 01:42 /usr/lib/x86_64-linux-gnu/libcuda.so.361.42
lrwxrwxrwx 1 root root       13 Aug 31 13:32 /usr/lib/x86_64-linux-gnu/libcudnn.so -> libcudnn.so.4
lrwxrwxrwx 1 root root       17 Aug 31 13:32 /usr/lib/x86_64-linux-gnu/libcudnn.so.4 -> libcudnn.so.4.0.7
-rwxr-xr-x 1 root root 61453024 Aug 31 13:32 /usr/lib/x86_64-linux-gnu/libcudnn.so.4.0.7
-rw-r--r-- 1 root root 62025862 Aug 31 13:32 /usr/lib/x86_64-linux-gnu/libcudnn_static.a

If installed from binary pip package, provide:
1. A link to the pip package you installed:
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp35-cp35m-linux_x86_64.whl
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`: '0.10.0rc0'
### What other attempted solutions have you tried?

changed file tensorflow/tensorflow/contrib/grid_rnn/python/ops/grid_rnn_cell.py:

```
  # project input
  if inputs is not None and sum(inputs.get_shape().as_list()) > 0 and len(
      conf.inputs) > 0:
```

to:

```
  # project input
  if inputs is not None and inputs.get_shape()[1] > 0 and len(
      conf.inputs) > 0:
```

fixed the issue, but I'm not sure if it is the intended behavior
### Logs or other output that would be helpful

if not changed the following error occurs:

**\* TypeError: unsupported operand type(s) for +: 'int' and 'NoneType'

-> probably because of sum(inputs.get_shape().as_list())
"
4293,The train and test data of wide and deep example is broken,"Wide and deep example code will download train and test data from `https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data` and `https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test`. But those files are broken now and you can `wget` to read the raw data.

```
|1x3 Cross validator
25, Private, 226802, 11th, 7, Never-married, Machine-op-inspct, Own-child, Black, Male, 0, 0, 40, United-States, <=50K.
38, Private, 89814, HS-grad, 9, Married-civ-spouse, Farming-fishing, Husband, White, Male, 0, 0, 50, United-States, <=50K.
28, Local-gov, 336951, Assoc-acdm, 12, Married-civ-spouse, Protective-serv, Husband, White, Male, 0, 0, 40, United-States, >50K.
44, Private, 160323, Some-college, 10, Married-civ-spouse, Machine-op-inspct, Husband, Black, Male, 7688, 0, 40, United-States, >50K.
```

If I modify the file and run with this command, it works. Maybe we should fix the raw dataset or change to other valid files.

```
python ./wide_n_deep_tutorial.py --train_data /home/data/train_data --test_data /home/data/test_data
```
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Someone has asked the similar question in stackoverflow, http://stackoverflow.com/questions/38558976/tensorflow-wide-deep-example-not-working .
### Environment info

Operating System: Ubuntu 14.04
TensorFlow: 0.9.0
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Clone the tensorflow project and run `python wide_n_deep_tutorial.py` directly.
### Logs or other output that would be helpful

```
# python ./wide_n_deep_tutorial.py
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
Training data is downloaded to /tmp/tmpUs2und
Test data is downloaded to /tmp/tmp1aH4KY
Traceback (most recent call last):
  File ""./wide_n_deep_tutorial.py"", line 213, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""./wide_n_deep_tutorial.py"", line 209, in main
    train_and_eval()
  File ""./wide_n_deep_tutorial.py"", line 194, in train_and_eval
    df_train[""income_bracket""].apply(lambda x: "">50K"" in x)).astype(int)
  File ""/usr/lib/python2.7/dist-packages/pandas/core/series.py"", line 2023, in apply
    mapped = lib.map_infer(values, f, convert=convert_dtype)
  File ""inference.pyx"", line 920, in pandas.lib.map_infer (pandas/lib.c:44780)
  File ""./wide_n_deep_tutorial.py"", line 194, in <lambda>
    df_train[""income_bracket""].apply(lambda x: "">50K"" in x)).astype(int)
TypeError: argument of type 'float' is not iterable
```
"
4291,Tensor slice is too large to serialize,"Hi,
I met a issue said that ""**Tensor slice is too large to serialize** (conservative estimate: 2268204567 bytes)"". And i am wondered that why there is a limitation to prevents the TensorSliceWriter from attempting to serialize variables that are larger than 2GB. How can I to skip this limitation if I want to use a variable large than 2GB? 

Thanks
Jinlong
"
4290,Aspect Preserving Image Downsample That does not Require Cropping,"Right now it looks like all of the image downsize (ie resize to a smaller size) operations involve either changing aspect ratio (e.g. [resize_images](https://www.tensorflow.org/versions/r0.10/api_docs/python/image.html#resize_images)) or cropping (e.g. [resize_image_with_crop_or_pad](https://www.tensorflow.org/versions/r0.10/api_docs/python/image.html#resize_image_with_crop_or_pad)). Compare this to the [thumbnail operation in Pillow](http://pillow.readthedocs.io/en/3.1.x/reference/Image.html#PIL.Image.Image.thumbnail) that maintains the aspect ratio but does not crop.

There are some related [questions on SO](http://stackoverflow.com/questions/35208832/tensorflow-resize-image-tensor-to-dynamic-shape).
"
4289,sigmoid,"which function can replace sigmoid in LSTM or GRU?
"
4288,Error while installing tensorflow on Ubuntu 16.04,"Operating System: Ubuntu 16.04 

Installed version of CUDA and cuDNN:  CUDA 7.5, cuDNN 5
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

```
-rw-r--r-- 1 root root 189170 Sep  8 09:47 /usr/local/cuda/lib/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Sep  8 09:47 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19 Sep  8 09:47 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 311596 Sep  8 09:47 /usr/local/cuda/lib/libcudart.so.7.5.18
-rw-r--r-- 1 root root 558020 Sep  8 09:47 /usr/local/cuda/lib/libcudart_static.a
```

output of `ldconfig -p | grep libcudnn`:

```
libcudnn.so.5 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcudnn.so.5
libcudnn.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcudnn.so
```

The output of `bazel version` :

```
Build label: 0.3.1
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jul 29 09:09:52 2016 (1469783392)
Build timestamp: 1469783392
Build timestamp as int: 1469783392
```

The error I have is: 

```
ERROR: /home/cortana/Downloads/tensorflow/tensorflow/core/kernels/BUILD:1711:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:scatter_op_gpu':
this rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/scatter_op_gpu.cu.cc':
  '/usr/local/cuda-7.5/include/cuda_runtime.h'
  '/usr/local/cuda-7.5/include/host_config.h'
  '/usr/local/cuda-7.5/include/builtin_types.h'
  '/usr/local/cuda-7.5/include/device_types.h'
  '/usr/local/cuda-7.5/include/host_defines.h'
  '/usr/local/cuda-7.5/include/driver_types.h'
  '/usr/local/cuda-7.5/include/surface_types.h'
  '/usr/local/cuda-7.5/include/texture_types.h'
  '/usr/local/cuda-7.5/include/vector_types.h'
  '/usr/local/cuda-7.5/include/channel_descriptor.h'
  '/usr/local/cuda-7.5/include/cuda_runtime_api.h'
  '/usr/local/cuda-7.5/include/cuda_device_runtime_api.h'
  '/usr/local/cuda-7.5/include/driver_functions.h'
  '/usr/local/cuda-7.5/include/vector_functions.h'
  '/usr/local/cuda-7.5/include/vector_functions.hpp'
  '/usr/local/cuda-7.5/include/common_functions.h'
  '/usr/local/cuda-7.5/include/math_functions.h'
  '/usr/local/cuda-7.5/include/math_functions.hpp'
  '/usr/local/cuda-7.5/include/math_functions_dbl_ptx3.h'
  '/usr/local/cuda-7.5/include/math_functions_dbl_ptx3.hpp'
  '/usr/local/cuda-7.5/include/cuda_surface_types.h'
  '/usr/local/cuda-7.5/include/cuda_texture_types.h'
  '/usr/local/cuda-7.5/include/device_functions.h'
  '/usr/local/cuda-7.5/include/device_functions.hpp'
  '/usr/local/cuda-7.5/include/device_atomic_functions.h'
  '/usr/local/cuda-7.5/include/device_atomic_functions.hpp'
  '/usr/local/cuda-7.5/include/device_double_functions.h'
  '/usr/local/cuda-7.5/include/device_double_functions.hpp'
  '/usr/local/cuda-7.5/include/sm_20_atomic_functions.h'
  '/usr/local/cuda-7.5/include/sm_20_atomic_functions.hpp'
  '/usr/local/cuda-7.5/include/sm_32_atomic_functions.h'
  '/usr/local/cuda-7.5/include/sm_32_atomic_functions.hpp'
  '/usr/local/cuda-7.5/include/sm_35_atomic_functions.h'
  '/usr/local/cuda-7.5/include/sm_20_intrinsics.h'
  '/usr/local/cuda-7.5/include/sm_20_intrinsics.hpp'
  '/usr/local/cuda-7.5/include/sm_30_intrinsics.h'
  '/usr/local/cuda-7.5/include/sm_30_intrinsics.hpp'
  '/usr/local/cuda-7.5/include/sm_32_intrinsics.h'
  '/usr/local/cuda-7.5/include/sm_32_intrinsics.hpp'
  '/usr/local/cuda-7.5/include/sm_35_intrinsics.h'
  '/usr/local/cuda-7.5/include/surface_functions.h'
  '/usr/local/cuda-7.5/include/surface_functions.hpp'
  '/usr/local/cuda-7.5/include/texture_fetch_functions.h'
  '/usr/local/cuda-7.5/include/texture_fetch_functions.hpp'
  '/usr/local/cuda-7.5/include/texture_indirect_functions.h'
  '/usr/local/cuda-7.5/include/texture_indirect_functions.hpp'
  '/usr/local/cuda-7.5/include/surface_indirect_functions.h'
  '/usr/local/cuda-7.5/include/surface_indirect_functions.hpp'
  '/usr/local/cuda-7.5/include/device_launch_parameters.h'
  '/usr/local/cuda-7.5/include/cuda_fp16.h'
  '/usr/local/cuda-7.5/include/math_constants.h'
  '/usr/local/cuda-7.5/include/curand_kernel.h'
  '/usr/local/cuda-7.5/include/curand.h'
  '/usr/local/cuda-7.5/include/curand_discrete.h'
  '/usr/local/cuda-7.5/include/curand_precalc.h'
  '/usr/local/cuda-7.5/include/curand_mrg32k3a.h'
  '/usr/local/cuda-7.5/include/curand_mtgp32_kernel.h'
  '/usr/local/cuda-7.5/include/cuda.h'
  '/usr/local/cuda-7.5/include/curand_mtgp32.h'
  '/usr/local/cuda-7.5/include/curand_philox4x32_x.h'
  '/usr/local/cuda-7.5/include/curand_globals.h'
  '/usr/local/cuda-7.5/include/curand_uniform.h'
  '/usr/local/cuda-7.5/include/curand_normal.h'
  '/usr/local/cuda-7.5/include/curand_normal_static.h'
  '/usr/local/cuda-7.5/include/curand_lognormal.h'
  '/usr/local/cuda-7.5/include/curand_poisson.h'
  '/usr/local/cuda-7.5/include/curand_discrete2.h'.
nvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.
nvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.
/usr/include/string.h: In function 'void* __mempcpy_inline(void*, const void*, size_t)':
/usr/include/string.h:652:42: error: 'memcpy' was not declared in this scope
   return (char *) memcpy (__dest, __src, __n) + __n;
                                          ^
Target //tensorflow/cc:tutorials_example_trainer failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 46.586s, Critical Path: 44.80s
```
"
4283,Breaking API change without deprecation warning,"### Problem

The name_scope API has changed between last week and last night's latest changes in the 0.10 branch. The previous API was:

```
name_scope(*args, **kwds)
```

The current API is:

```
name_scope(name)
```

The API was changed in a breaking way, rather than the new API being added and the use of the old API triggering a warning.

I was wondering what the TensorFlow gatekeepers' policy is on breaking API changes. Is there an official policy? Is the breaking change without a deprecation warning and phase-out intentional?
"
4282,Unspecified dimension after tf.sparse_tensor_dense_matmul,"### Environment info

Operating System:

Problem encountered on linux CPU build installed from `47501a5ebc62fcb8a3d7832722d39997696897dc`.  
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```
import tensorflow as tf
indices = [[0, 0], [1, 2]]
values = [1., 1.]
shape = [4, 6]
test_sparse = tf.SparseTensor(indices, values, shape)
v = tf.ones((6, 1))

sess = tf.Session()
sess.run(tf.initialize_all_variables())

mult = tf.sparse_tensor_dense_matmul(test_sparse, v)
print(mult)
--> <tf.Tensor 'SparseTensorDenseMatMul_1/SparseTensorDenseMatMul:0' shape=(?, 1) dtype=float32>

# but of course
sess.run(mult).shape
--> (4, 1)
```

My question is: why is it that the shape of `mult` is partially unspecified before run time? It can be inferred from the shape of `test_sparse` and `v` that `mult` will have shape `(4, 1)`. 

This is very annoying because it prevents me from initializing a Variable with the result of a sparse matmul as in the following:

```
mult_val = tf.Variable(tf.sparse_tensor_dense_matmul(test_sparse, v))
--> ValueError: initial_value must have a shape specified: Tensor(""SparseTensorDenseMatMul/SparseTensorDenseMatMul:0"", shape=(?, 1), dtype=float32)
```

On the other hand, dense matmul has the desired behavior:

```
dense_mult = tf.matmul(tf.sparse_tensor_to_dense(test_sparse, v))
dense_mult.get_shape()
--> TensorShape([Dimension(4), Dimension(1)])
```

So why does sparse matmul behave differently? Is this a bug? An implementation quirk? 

In the meanwhile, how can I initialize a Variable with the result of a sparse matmul without having to densify the sparse matrix first?
"
4281,Seq2Seq example - shape error while decoding,"When I try to run the tutorial available at [https://www.tensorflow.org/versions/r0.10/tutorials/seq2seq/index.html](https://www.tensorflow.org/versions/r0.10/tutorials/seq2seq/index.html) it works great. The model is training and the proximity is decreasing. 

For training I used the default data and default command:
`python translate.py  --data_dir [your_data_directory] --train_dir [checkpoints_directory]  --en_vocab_size=40000 --fr_vocab_size=40000` (substituting directory paths of course)

However when I try using the model by running `python translate.py --decode
  --data_dir [your_data_directory] --train_dir [checkpoints_directory]` I get the following error:

```
Traceback (most recent call last):
  File ""translate.py"", line 290, in <module>
    tf.app.run()
  File ""/home/user/venv/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""translate.py"", line 285, in main
    decode()
  File ""translate.py"", line 248, in decode
    target_weights, bucket_id, True)
  File ""/www/data/user/tensorflow/tensorflow/models/rnn/translate/seq2seq_model.py"", line 244, in step
    outputs = session.run(output_feed, input_feed)
  File ""/home/user/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 382, in run
    run_metadata_ptr)
  File ""/home/user/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 640, in _run
    % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))
ValueError: Cannot feed value of shape (1,) for Tensor 'weight5:0', which has shape '(64,)'
```

It looks like the script is expecting the whole batch (which is set to 64 by default) but when I type any input, it creates only 1 sample of data.

Version of tensorflow: `tensorflow==0.10.0rc0` (git commit I use `f71cc62282bf2e066f9ebd08cf3f605fc98c6e41`)

Thank you for any ideas
"
4279,//tensorflow:libtensorflow.so target won't build,"Trying to build libtensorflow.so from source on a Mac using Bazel 3.1, I get an error. Log is attached below.
### Environment info

Operating System: MacOS X 10.11.6, Xcode 7.3.1

Installed version of CUDA and cuDNN: 
No CUDA

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)

```
2a6d7511f13a0387857081f1cf64d282d2816a62
```
1. The output of `bazel version`
   
   Build label: 0.3.1
   Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
   Build time: Fri Jul 29 09:05:18 2016 (1469783118)
   Build timestamp: 1469783118
   Build timestamp as int: 1469783118
### Build log

```
$ bazel build //tensorflow:libtensorflow.so
Extracting Bazel installation...
Sending SIGTERM to previous Bazel server (pid=8488)... done.
.
ERROR: /Users/Projects/tensorflow/tensorflow/core/BUILD:942:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /Users/Projects/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: /Users/Projects/tensorflow/tensorflow/core/BUILD:942:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /Users/Projects/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: /Users/Projects/tensorflow/tensorflow/core/BUILD:942:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /Users/Projects/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: Analysis of target '//tensorflow:libtensorflow.so' failed; build aborted.
INFO: Elapsed time: 5.724s
```
"
4278,Minor documentation / code mismatch in Optimizer,"The `Optimizer` class has the [following method](https://github.com/tensorflow/tensorflow/blob/7e7dff529fd35edea443580d58e95f5de39b5356/tensorflow/python/training/optimizer.py#L388):

``` python
  def _valid_dtypes(self):
    """"""Valid types for loss, variables and gradients.
    Defaults to `float32`. Subclasses should override to allow other types.
    Returns:
      Valid types for loss, variables and gradients.
    """"""
    return set([dtypes.float16, dtypes.float32, dtypes.float64])
```

The docstring says that `_valid_dtypes` defaults only to `float32`, but the code also allows `float16` and `float64`. The docstring should be updated to reflect that the default allowed set includes these other float types.
"
4277,Compilation of the C API in version 0.10,"In version 0.10, the C API was [moved](https://github.com/tensorflow/tensorflow/commit/788f359b7218ad46696c15459c89688ffe70955e) from `tensorflow/core` to `tensorflow/c`. The master branch contains a [commit](https://github.com/tensorflow/tensorflow/commit/d03f2545ecc3012e6c941a3a1e957f7d7f8d5040) adding a new target to `tensorflow/BUILD` for building the C API in the new location. However, this commit and the corresponding target are not present on branch `r0.10`. Iâ€™m wondering how one is supposed to build the C API in version 0.10. Thank you.

Regards,
Ivan
"
4276,"Tensor name ""hiddenlayer_2/biases"" not found in checkpoint files model","I have created my model on one machine and then sent the zipped folder 'model' by email to myself so I would be able to use this model on my second PC. 

Here is the code how I'm recreating a model:

```
feature_columns = learn.infer_real_valued_columns_from_input(x_train)
classifier = learn.DNNClassifier(feature_columns=feature_columns, hidden_units=[40, 80, 40], n_classes=3, model_dir='model')
```

As I understood, to reuse a model I need to make 'fit' 1-3 steps. (This fact I found here in TF issues)

`classifier.fit(x_train, y_train, steps=3)`

Then when I call `predicted = classifier.predict(x_test)` , it throws an exception:  
tensorflow.python.framework.errors.NotFoundError: Tensor name ""hiddenlayer_2/biases"" not found in checkpoint files model/model.ckpt-30015-?????-of-00001

This model is working well on the machine where it has been created. 
How can I make it work on any PC?

Thanks!
"
4273,Crashing when parsing names with special character [] in saver.restore(): bug or expected behavior ?,"Disclaimer: I have tensorflow 0.7.0 (a bit outdated I suppose). I have not tested it on more recent versions.
When I try:

``` python
import tensorflow as tf
x=tf.placeholder(tf.float32,[])
w=tf.Variable(2.)
y=tf.mul(x,w)
saver=tf.train.Saver()
init=tf.initialize_all_variables()
sess1=tf.Session()
sess1.run(init)
name_model=""./""+""model""
saver.save(sess,name_model)
sess1.close()
sess2=tf.Session()
path_meta=name_model+"".meta""
path_model=name_model
new_saver=tf.train.import_meta_graph(path_meta)
new_saver.restore(sess2,path_model)
print sess2.run(y, feed_dict={x: 1.})==2
```

It runs perfectly fine but if I change name_model to ""./model[1]"" for example it crashes with message: `Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./model[1].`
I was wondering if this is expected or if it is a bug. If this is buggy what would be aworkaround ?
"
4272,Training time for CIFAR-10 is higher on multi GPU cards compared to single,"When running CIFAR-10 example on two GPU cards in a single machine, the training time is higher compared to a single GPU card.

It is utilizing both cards but overall execution time is higher compared to single GPU.

![multi-gpu-cards](https://cloud.githubusercontent.com/assets/7776101/18342617/48dfeb66-75cd-11e6-83cb-473be9746f3a.png)

One more thing I noticed is that the queue is filled twice. I got following messages in log.
`Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.`
`Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.`

Ideally, It should be filled once and distribute across two GPUs but that is not happening. 
Could anyone please help me out to resolve this?
"
4269,bazel build failing - not generating bazel-bin directory when  build from source with CUDA 8.0 error ,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

http://stackoverflow.com/questions/38773402/tensorflow-bazel-build-failing-not-generating-bazel-bin-directory

https://github.com/tensorflow/tensorflow/issues/1498
### Environment info

Operating System:

ozzie@debian:~$ uname -a
Linux debian 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt25-2 (2016-04-08) x86_64 GNU/Linux

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

ozzie@debian:~$ ls /usr/local/cuda-8.0/lib64/libcud*
/usr/local/cuda-8.0/lib64/libcudadevrt.a
/usr/local/cuda-8.0/lib64/libcudart.so
/usr/local/cuda-8.0/lib64/libcudart.so.7.5
/usr/local/cuda-8.0/lib64/libcudart.so.8.0
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.27
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudnn.so
/usr/local/cuda-8.0/lib64/libcudnn.so.4
/usr/local/cuda-8.0/lib64/libcudnn.so.4.0.7
/usr/local/cuda-8.0/lib64/libcudnn.so.5
/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5
/usr/local/cuda-8.0/lib64/libcudnn_static.a

(note: libcudart.so.7.5 is a link of libcudart.so.8.0)

Using Nvidia Quadro K4000 when I run 

If installed from binary pip package, provide:
1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
   ozzie@debian:~/working/work/ML/tensorflow$ git rev-parse HEAD
   2a6d7511f13a0387857081f1cf64d282d2816a62
2. The output of `bazel version`

ozzie@debian:~/working/work/ML/tensorflow$ bazel version
Build label: 0.3.1-jdk7
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jul 29 09:07:03 2016 (1469783223)
Build timestamp: 1469783223
Build timestamp as int: 1469783223
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

ozzie@debian:~/working/work/ML/tensorflow$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer
### What other attempted solutions have you tried?

 try to modify GPU architecture to compute_30 (Nvidia Quadro K4000 code),  but can not sure which file contain this GPU architecture info?
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).

'/usr/local/cuda-8.0/include/curand_discrete2.h'.
nvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.
nvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.
Target //tensorflow/cc:tutorials_example_trainer failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 22.435s, Critical Path: 21.92s
"
4268,resize_image_with_crop_or_pad loses channel information,"``` python
png = tf.image.resize_image_with_crop_or_pad(png_raw, 100, 100)
```

before:

```
TensorShape([Dimension(None), Dimension(None), Dimension(3)])
```

after:

```
TensorShape([Dimension(100), Dimension(100), Dimension(None)]
```

`tf.image.pad_to_bounding_box` has the same problem.
The combination of these two lines seems questionable:

```
height, width, depth = _ImageDimensions(image, static_only=False)
padded_shape = [None if is_tensor(i) else i
                  for i in [target_height, target_width, depth]]
```

why set `static_only=False`? The docs for `_ImageDimensions` say:

>    list of integers `[batch, height, width, channels]`, when static shape is
>     fully defined or `static_only` is `True`.
>     list of integer scalar tensors `[batch, height, width, channels]`, when
>     static shape is not fully defined.

so unless all dimensions of the image are defined you will get tensors which will then be changed to `None`.
"
4267,libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program,"i am trying to install tensorflow on my laptop LenovoY700, so that Ubuntu16.04 is more suitable with laptop.
I installed cuda7.5+cudnn4 with GTX960M and nvidia-version is 367.

When I was running 'convolutional.py', something goes wrong.

I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: /usr/local/cuda-7.5/lib64:
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: Y700
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:356] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.44  Wed Aug 17 22:24:07 PDT 2016
GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.2) 
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 367.44.0
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1077] LD_LIBRARY_PATH: /usr/local/cuda-7.5/lib64:
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1078] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally

Could somebody give me a hand?
Thanks for your help.
Simon
"
4266,"How to change the model and label file in ""assets"" of Android Demo?","When I change these files in ""assets"" folder , the Demo App will crash. How can I change the .txt and .pb file to change the verify model?
"
4265,ERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.,"Operating System: 
Ubuntu 16.04

Installed version of CUDA and cuDNN: 
server@server:~$ ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root 560184 9æœˆ   7 09:56 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root     16 9æœˆ   7 09:56 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root     19 9æœˆ   7 09:56 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 root root 394472 9æœˆ   7 09:56 /usr/local/cuda/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 root root 737516 9æœˆ   7 09:56 /usr/local/cuda/lib64/libcudart_static.a

The commit hash (`git rev-parse HEAD`)
server@server:~/tensorflow$ git rev-parse HEAD
2ab7e6326296987ea0ce975afb3434a16d1aa21a

The output of `bazel version`
server@server:~$ bazel version
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Jan 01 00:00:00 1970 (0)
Build timestamp: Thu Jan 01 00:00:00 1970 (0)
Build timestamp as int: 0

Here is my bazel build result:
INFO: From Compiling tensorflow/core/kernels/tile_ops_gpu.cu.cc:
nvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.
nvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.
INFO: From Compiling tensorflow/core/kernels/tile_ops_gpu.cu.cc:
nvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.
nvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.
Target //tensorflow/cc:tutorials_example_trainer up-to-date:
bazel-bin/tensorflow/cc/tutorials_example_trainer
INFO: Elapsed time: 985.075s, Critical Path: 952.03s

Here is my bazel-bin result:
000008/000006 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]
000007/000009 lambda = 39.000000 x = [3.605551 -0.000000] y = [10.816654 -3.605551]
000006/000006 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]
000001/000002 lambda =      nan x = [     nan      nan] y = [     nan      nan]
000002/000006 lambda =      nan x = [     nan      nan] y = [     nan      nan]
000006/000008 lambda =      nan x = [     nan      nan] y = [     nan      nan]
000004/000004 lambda = 1.200000 x = [-0.894427 0.447214] y = [-0.894427 0.894427]
000006/000007 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]
000005/000005 lambda = 249281724416.000000 x = [0.894427 -0.447214] y = [278705438720.000000 -130.000000]
000008/000003 lambda =      nan x = [     nan      nan] y = [     nan      nan]
000007/000009 lambda = 2.400000 x = [0.948683 -0.316228] y = [2.213594 -0.948683]
000008/000004 lambda =      nan x = [     nan      nan] y = [     nan      nan]
000003/000009 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]
000000/000004 lambda = 2.255962 x = [0.930408 -0.366524] y = [2.058176 -0.930408]
000001/000002 lambda =      nan x = [     nan      nan] y = [     nan      nan]
000004/000004 lambda =      nan x = [     nan 0.707107] y = [     nan      nan]
000008/000000 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]
000003/000008 lambda =      nan x = [     nan      nan] y = [     nan      nan]
000006/000007 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]
000007/000009 lambda = 2.172414 x = [0.919145 -0.393919] y = [1.969597 -0.919145]
000008/000006 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]
000008/000003 lambda =      nan x = [     nan      nan] y = [     nan      nan]
000002/000006 lambda =      nan x = [     nan      nan] y = [     nan      nan]
000000/000004 lambda = 2.115613 x = [0.911220 -0.411921] y = [1.909816 -0.911220]
000003/000008 lambda =      nan x = [     nan      nan] y = [     nan      nan]
000008/000004 lambda =      nan x = [     nan      nan] y = [     nan      nan]
000003/000009 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]
000006/000006 lambda = 1786627917493567488.000000 x = [715827904.000000 348406848.000000] y = [2844297216.000000 -715827904.000000]
000008/000000 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]
000007/000009 lambda = 2.080292 x = [0.906183 -0.422885] y = [1.872779 -0.906183]
000008/000003 lambda =      nan x = [     nan      nan] y = [     nan      nan]
000003/000008 lambda =      nan x = [     nan      nan] y = [     nan      nan]
000008/000004 lambda =      nan x = [     nan      nan] y = [     nan      nan]
000006/000007 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]
000008/000006 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]
000004/000004 lambda =      nan x = [     nan      nan] y = [     nan      nan]
000005/000005 lambda = 3.000000 x = [1.000000 -0.000000] y = [3.000000 -1.000000]
000003/000009 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]
000008/000000 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]
000008/000006 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]
000006/000006 lambda = 2.584623 x = [0.969760 -0.244061] y = [2.421158 -0.969760]
000004/000004 lambda =      nan x = [     nan      nan] y = [     nan      nan]
000007/000009 lambda = 2.038786 x = [0.900159 -0.435561] y = [1.829356 -0.900159]

I've searched the nan problem, according to https://github.com/tensorflow/tensorflow/issues/2037, i've tried ""--num_concurrent_sessions=1"" and ""--num_concurrent_steps=1"", and it seems to be fine
Here is the output:
server@server:~/tensorflow$ bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu --num_concurrent_sessions=1 --num_concurrent_steps=1
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:105] Couldn't open CUDA library libcudnn.so.5. LD_LIBRARY_PATH: /usr/local/cuda-8.0/lib64:
I tensorflow/stream_executor/cuda/cuda_dnn.cc:3304] Unable to load cuDNN DSO
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:02:00.0
Total memory: 11.92GiB
Free memory: 11.40GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)
000000/000000 lambda = 1.841570 x = [0.669396 0.742906] y = [3.493999 -0.669396]
000000/000000 lambda = 2.708984 x = [0.982138 -0.188162] y = [2.570089 -0.982138]
000000/000000 lambda = 2.284280 x = [0.934118 -0.356965] y = [2.088423 -0.934118]
000000/000000 lambda = 2.127152 x = [0.912847 -0.408302] y = [1.921937 -0.912847]
000000/000000 lambda = 2.060266 x = [0.903291 -0.429029] y = [1.851815 -0.903291]
000000/000000 lambda = 2.029356 x = [0.898775 -0.438410] y = [1.819504 -0.898775]
000000/000000 lambda = 2.014491 x = [0.896580 -0.442881] y = [1.803979 -0.896580]
000000/000000 lambda = 2.007199 x = [0.895499 -0.445064] y = [1.796367 -0.895499]
000000/000000 lambda = 2.003588 x = [0.894962 -0.446143] y = [1.792599 -0.894962]
000000/000000 lambda = 2.001791 x = [0.894694 -0.446679] y = [1.790724 -0.894694]
000000/000000 lambda = 2.000895 x = [0.894561 -0.446947] y = [1.789788 -0.894561]
000000/000000 lambda = 2.000447 x = [0.894494 -0.447080] y = [1.789321 -0.894494]
000000/000000 lambda = 2.000224 x = [0.894460 -0.447147] y = [1.789088 -0.894460]
000000/000000 lambda = 2.000112 x = [0.894444 -0.447180] y = [1.788971 -0.894444]
000000/000000 lambda = 2.000056 x = [0.894436 -0.447197] y = [1.788913 -0.894436]
000000/000000 lambda = 2.000028 x = [0.894431 -0.447205] y = [1.788883 -0.894431]
000000/000000 lambda = 2.000014 x = [0.894429 -0.447209] y = [1.788869 -0.894429]

When I build the pip-packages. i got another error.
Here is the output:
server@server:~/tensorflow$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
ERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.
ERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.
INFO: Elapsed time: 0.037s

Where am I wrong?
"
4261,PIP package contains CUDA libraries,"As discussed offline: the PIP package size has greatly increased since the `cuda_configure` change because the CUDA libraries under `external/local_config_cuda` are now being included by `build_pip_package.sh`.

The simplest fix for this is to special-case `local_config_cuda` to exclude it from being copied in `build_pip_package.sh` since there does not seem to be a way to specifically exclude all files under `@local_config_cuda//` in the `sh_binary` rule itself.

The reason why we did not experience this previously is because the CUDA libraries were symlinked under `third_party`, and `build_pip_package.sh` rsyncs `third_party/eigen3` specifically.
"
4259,Command line processor fails silently on mistyped args,"### Problem:

The command line processor doesn't raise exceptions when the existing flags that are in common use throughout TensorFlow (at the very least by convention) are mistyped. For example, let's take

```
--num_gpus
```

which is frequently encountered, e.g. in Inception v3, CIFAR-10, etc. If I type a single dash instead of two, the CLI arg processor just picks up whatever the default value is defined in my app and silently ignores the misformed CLI argument. I realize that one can't restrict the args that are supported because they are pulled from sys.argv and there may be other args passed on to the user's app code, so it's not the same level of strictness as say in processing CLI switches for Unix apps which have a fixed set of options. However, perhaps the solution is to look for small typos such as Damerau-Levenshtein distance of 1 (one deletion, insertion, substitution or transposition, with 1 transposition still treated as edit distance of 1) from the arg specified as recognizable in FLAGS that should raise an exception unless the users forces literal interpretation of the args?
### Affected versions:

latest 0.10, probably master top of tree and others as well
"
4257,Move all model implementations to tensorflow/models ?,"If I am correct, tensorflow.models is being deprecated.
But some model implementations are still kept inside this directory because of tensorflow tutorials. 
I think it would be more straight forward and relevant to keep all the model implementations in tensorflow/models repository.

(This is a same issue discussed in [tensorflow/models issue 361](https://github.com/tensorflow/models/issues/361).)

I made a [pull request](https://github.com/tensorflow/models/pull/393) that moves all tensorflow/tensorflow model implementations to tensorflow/models without losing commit histories.

I can make another pull request to tensorflow/tensorflow to fully deprecate the model implementations and update tutorials.
"
4256,Tensorflow in Azure ML,"I've so far seen people using tensorflow in Azure using in this [link](http://www.mikelanzetta.com/tensorflow-on-azure-using-docker.html).
Also using the advantage of ubuntu in windows tensorflow can be run on
windows pc as well.Here is the [link](http://www.hanselman.com/blog/PlayingWithTensorFlowOnWindows.aspx).
However during a conversation with Windows Azure engineer Hai Ning it came out
that ""Azure ML PaaS VMs use Windows OS; TensorFlow is not supported on Windows as of now.""
Hence there is no direct way of running tensorflow in Azure ML.
Is there any work around anyone figured out that allows running tensorflow in Azure ML.

NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
### Environment info

Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:
1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
### What other attempted solutions have you tried?
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
"
4255,iOS memory warnings,"I tried posting this to SO first and it's been there a week with no activity: https://stackoverflow.com/questions/39255211/tensorflow-ios-memory-warnings

We are building an iOS app to perform image classification using the TensorFlow library.

Using our machine learning model (91MB, 400 classes) and the TensorFlow 'simple' example, we get memory warnings on any iOS device with 1GB of RAM. 2GB models do not experience any warnings, while < 1GB models completely run out of memory and crash the app.

We are using the latest TensorFlow code from the master branch that includes [this iOS memory performance commit](https://github.com/tensorflow/tensorflow/commit/459c2fed498530b794c4871892fd68d1e6834ac6), which we thought might help but didn't.

We have also tried setting various GPU options on our TF session object, including `set_allow_growth(true)` and `set_per_process_gpu_memory_fraction()`.

Our only changes to the TF 'simple' example code is a `wanted_width` and `wanted_height` of 299, and an `input_mean` and `input_std` of 128.

Any thoughts? Is our model simply too big?
"
4253,How can I use dropout on input layer?,"Hi everyone:
I try to combine softmax classifier and dropout algorithm, because there are only input layer and classfier. According tutorial, I want to use dropout on input layer like following:
![image](https://cloud.githubusercontent.com/assets/12611573/18319468/6cd13298-7558-11e6-997d-0f87349b0015.png)
But I got the following error:
Traceback (most recent call last):
  File ""softmax.py"", line 255, in <module>
    Start_building_and_training(num_sample, x_data_merged_dnaid[:], y_label[:], num_batch)
  File ""softmax.py"", line 213, in Start_building_and_training
    prob = sess.run(y, feed_dict={x: batch_xs, y_: batch_ys})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 717, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 915, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 965, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 985, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder_1' with dtype float
     [[Node: Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[], _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
     [[Node: Softmax/_5 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_24_Softmax"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
Caused by op u'Placeholder_1', defined at:
  File ""softmax.py"", line 255, in <module>
    Start_building_and_training(num_sample, x_data_merged_dnaid[:], y_label[:], num_batch)
  File ""softmax.py"", line 194, in Start_building_and_training
    keep_x = tf.placeholder(tf.float32)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 1270, in placeholder
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 1530, in _placeholder
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 703, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2334, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1253, in __init__
    self._traceback = _extract_stack()

Can anyone help me ? Thanks a lot!
"
4252,make -f tensorflow/contrib/makefile/Makefile for iOS is failing,"Probably related to [issue 2896](https://github.com/tensorflow/tensorflow/issues/2896)

I am trying to build tensorflow on a mac for iOS. The first two commands work well:
`tensorflow/contrib/makefile/download_dependencies.sh`
and
`compile_ios_protobuf.sh`

However, when I call
`make -f tensorflow/contrib/makefile/Makefile \
 TARGET=IOS \
 IOS_ARCH=ARM64`

I receive a linker error:

> Undefined symbols for architecture x86_64:
>   ""tensorflow::io::InputStreamInterface::SkipNBytes(long long)"", referenced from:
>       vtable for tensorflow::io::ZlibInputStream in zlib_inputstream.o
>   ""tensorflow::io::RandomAccessInputStream::RandomAccessInputStream(tensorflow::RandomAccessFile_)"", referenced from:
>       tensorflow::io::RecordReader::RecordReader(tensorflow::RandomAccessFile_, tensorflow::io::RecordReaderOptions const&) in record_reader.o
>   ""typeinfo for tensorflow::io::InputStreamInterface"", referenced from:
>       typeinfo for tensorflow::io::ZlibInputStream in zlib_inputstream.o
>   ""vtable for tensorflow::io::InputStreamInterface"", referenced from:
>       tensorflow::io::InputStreamInterface::InputStreamInterface() in zlib_inputstream.o
>   NOTE: a missing vtable usually means the first non-inline virtual member function has no definition.
> ld: symbol(s) not found for architecture x86_64
> clang: error: linker command failed with exit code 1 (use -v to see invocation)
> make: **\* [/Users/senad/repos/tensorflow2/tensorflow/contrib/makefile/gen/host_bin/proto_text] Error 1
> - '[' 2 -ne 0 ']'
> - echo 'armv7 compilation failed.'
>   armv7 compilation failed.
> - exit 1

I tried it with the current master commit `f71cc62f71cc62`. After reading the discussion of issue 2896, I also tried it with the commit `582e6c8582e6c8`. This is the one that supposedly fixed the issue in the other ticket.
"
4251,"pip installation 0.10.0 requires cuDNN v5, not cuDNN v4","[Download and Setup, Pip installation instructions for 0.10.0](https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#pip-installation) say that `Ubuntu/Linux 64-bit, GPU enabled, Python 2.7` version `Requires CUDA toolkit 7.5 and CuDNN v4.`,

That setup results in:

`E tensorflow/stream_executor/cuda/cuda_dnn.cc:347] Loaded runtime CuDNN library: 4007 (compatibility version 4000) but source was compiled with 5103 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.`

Edit Sep 9, 2016: as @vinaynath pointed out, `cudnn-7.5-linux-x64-v5.1.tgz` should be installed for CUDA 7.5
**Installation of cuDNN v5 (`cudnn-7.5-linux-x64-v5.1.tgz`~~`cudnn-8.0-linux-x64-v5.1`~~) resolves the problem.**
I think the instructions need an update to `Requires CUDA toolkit 7.5 and CuDNN v5.`.
### Environment info

Operating System: Ubuntu 14.04.4 LTS (GNU/Linux 3.19.0-68-generic x86_64)

Installed version of CUDA and cuDNN: CUDA 7.5, cuDNN v4 (`cudnn-7.0-linux-x64-v4.0`)
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

```
$ ls -l /usr/local/cuda/lib/libcud*
-rw-r--r-- 1 root root 189170 Jul 25 11:00 /usr/local/cuda/lib/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Jul 25 11:00 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19 Jul 25 11:00 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 311596 Jul 25 11:00 /usr/local/cuda/lib/libcudart.so.7.5.18
-rw-r--r-- 1 root root 558020 Jul 25 11:00 /usr/local/cuda/lib/libcudart_static.a
```
1. A link to the pip package you installed: `https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl`
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

```
$ python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
0.10.0
```
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```
$ python -m tensorflow.models.image.mnist.convolutional
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:
name: GRID K2
major: 3 minor: 0 memoryClockRate (GHz) 0.745
pciBusID 0000:00:04.0
Total memory: 3.44GiB
Free memory: 3.41GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K2, pci bus id: 0000:00:04.0)
Initialized!
E tensorflow/stream_executor/cuda/cuda_dnn.cc:347] Loaded runtime CuDNN library: 4007 (compatibility version 4000) but source was compiled with 5103 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.
F tensorflow/core/kernels/conv_ops.cc:457] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)
Aborted (core dumped)
```
"
4250,Warning produced by Logging and Monitoring Basics with tf.contrib.learn example,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Similar issue here:
1. http://stackoverflow.com/questions/39249704/tensorflow-valueerror-duplicate-feature-column-key-found-for-column
2. have tried example at 
https://www.tensorflow.org/versions/master/tutorials/monitors/index.html
still exhibits same issue
3. I will look at github deltas in 0.10.0rc0 between 1 above head and see if same fix applied to tf.contrib.learn 
### Environment info

Operating System:
OS-X 10.11.6 Python 3.5 vi Homebrew 
Installed version of CUDA and cuDNN: 
N/A
## installed from binary pip package:
1. link to the pip package you installed:
   https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.10.0rc0-py3-none-any.whl
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
   0.10.0rc0
### minimal reproducible example (

``` python

import tensorflow as tf
import numpy as np

# Data sets
IRIS_TRAINING = ""iris_training.csv""
IRIS_TEST = ""iris_test.csv""

# Load datasets.
training_set = tf.contrib.learn.datasets.base.load_csv(filename=IRIS_TRAINING,
                                                       target_dtype=np.int)
test_set = tf.contrib.learn.datasets.base.load_csv(filename=IRIS_TEST,
                                                   target_dtype=np.int)

# Specify that all features have real-value data
feature_columns = [tf.contrib.layers.real_valued_column("""", dimension=4)]

# Build 3 layer DNN with 10, 20, 10 units respectively.
classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
                                            hidden_units=[10, 20, 10],
                                            n_classes=3,
                                            model_dir=""/tmp/iris_model"")

# Fit model.
classifier.fit(x=training_set.data,
               y=training_set.target,
               steps=2000)

# Evaluate accuracy.
accuracy_score = classifier.evaluate(x=test_set.data,
                                     y=test_set.target)[""accuracy""]
print('Accuracy: {0:f}'.format(accuracy_score))

# Classify two new flower samples.
new_samples = np.array(
    [[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]], dtype=float)
y = classifier.predict(new_samples)
print('Predictions: {}'.format(str(y)))
```
### What other attempted solutions have you tried?

Referenced head example 
### Logs or other output that would be helpful

WARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(4)]), is_sparse=False)
WARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False)
WARNING:tensorflow:Given features: Tensor(""input:0"", shape=(?, 4), dtype=float32), required signatures: TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(4)]), is_sparse=False).
WARNING:tensorflow:Given targets: Tensor(""output:0"", shape=(?,), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False).
"
4249,Inception-v3 checkpoint - Tensor name not found,"Trying to run the Inceptionv3 Tensorflow model with the architecture and the checkpoint provided by Google [here](https://research.googleblog.com/2016/08/improving-inception-and-image.html).

The issue is that tensorflow crashes on `saver.restore(sess, ""./inception_v3.ckpt"")` with the following error:

`tensorflow.python.framework.errors.NotFoundError: Tensor name ""InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/biases"" not found in checkpoint files ./inception_v3.ckpt`

I get the same errors with the inception resnet v2 checkpoint/model combination.
Missing tensor in the checkpoint or issue with the model file?

Thank you
"
4248,Dose CTC need SOFTMAX?,"It seems CTC dose not need SOFTMAX in many example,so how it works?,and if ctc_loss itself has a SOFTMAX,dose it share SOFTMAX with ctc_greedy_decoder,I mean if there is no SOFTMAX how can I use it to predict ?
Thanks for any help
"
4247,"Which TensorFlow-Slim is the ""real"" one?","I have found 3 TensorFlow-Slim in repo `tensorflow/tensorflow` and `tensorflow/model`
- (tensorflow/tensorflow) [`tensorflow.contrib.slim`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim)
- (tensorflow/model) [`model.slim`](https://github.com/tensorflow/models/blob/master/slim)
- (tensorflow/model) [`model.inception.inception.slim`](https://github.com/tensorflow/models/blob/master/inception/inception/slim)

Among these places, there are some duplicated codes.
e.g.
- (tensorflow/tensorflow) [`tensorflow/tensorflow/contrib/slim/python/slim/nets/overfeat.py`](https://github.com/tensorflow/tensorflow/blob/3d1ee95e612b1987e664ca46a7c584872d36dde9/tensorflow/contrib/slim/python/slim/nets/overfeat.py)
- (tensorflow/model) [`slim/nets/overfeat.py`](https://github.com/tensorflow/models/blob/master/slim/nets/overfeat.py)
### Question:
1. What are these 3 `slim` packages used for? What's the differences?
2. In the future, which one may be deprecated? Or which one will be the future.
"
4246,tf.train.Coordinator not closing threads,"I am using a number of threads to feed training examples to a `tf.RandomShuffleQueue`, however gracefully closing the threads seems not to be working nicely. This is related to https://github.com/tensorflow/tensorflow/issues/2130 and [this question on StackOverflow](http://stackoverflow.com/questions/36210162/tensorflow-stopping-threads-via-coordinator-seems-not-to-work) however I am wondering whether there is going to be a better way of closing the threads.

The problem is that there are pending enqueue operations when you request the threads to stop. You can force these to be cancelled using `queue.close(cancel_pending_enqueues=True)`, but then the threads raise an `tf.errors.CancelledError`. As a work around you need to surround the enqueue operation with a try/except block, however the whole process is not very elegant. I am wondering whether there are any plans to improve the process of closing the threads using the Coordinator. 

The code below illustrates a working example, however as you can see it is not a very nice solution.

``` python

q = tf.RandomShuffleQueue( ... )
enqueue_op = q.enqueue_many([self.queue_inputs, self.queue_targets])

def load_and_enqueue( ... ):

    while not coord.should_stop():
        # ...
        feed_dict = {
            queue_inputs:  inputs,
            queue_targets: targets
         }

         # Catch the exception that arises when you ask to close pending enqueue operations
         try:
             sess.run(enqueue_op, feed_dict=feed_dict)
         except tf.errors.CancelledError:
             return

# Coordinator for threads
coord = tf.train.Coordinator()

# Start a thread to enqueue data asynchronously, and hide I/O latency.
threads = [threading.Thread(target=load_and_enqueue, args=(...,)) for i in range(4)]

for t in threads: t.start()

# ... training loop

# Ask the threads to stop and wait until they do it
sess.run(q.close(cancel_pending_enqueues=True))
coord.request_stop()
coord.join(threads, stop_grace_period_secs=5)

sess.close()
```
"
4245,Docker build parameterized_docker_build.sh breaks on cURL ssl issue,"I checked out 2ab7e6326296987ea0ce975afb3434a16d1aa21a, set some options,

```
export TF_DOCKER_BUILD_TYPE=GPU TF_DOCKER_BUILD_IS_DEVEL=NO TF_DOCKER_BUILD_CENTRAL_PIP=1
```

and then executed the Docker build script:

```
./tensorflow/tools/docker/parameterized_docker_build.sh
```

I from there got an error:

```
Step 4 : RUN curl -O https://bootstrap.pypa.io/get-pip.py &&     python get-pip.py &&     rm get-pip.py
 ---> Running in f267d0b1207b
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
curl: (60) SSL certificate problem: unable to get local issuer certificate
More details here: http://curl.haxx.se/docs/sslcerts.html

curl performs SSL certificate verification by default, using a ""bundle""
 of Certificate Authority (CA) public keys (CA certs). If the default
 bundle file isn't adequate, you can specify an alternate file
 using the --cacert option.
If this HTTPS server uses a certificate signed by a CA represented in
 the bundle, the certificate verification probably failed due to a
 problem with the certificate (it might be expired, or the name might
 not match the domain name in the URL).
If you'd like to turn off curl's verification of the certificate, use
 the -k (or --insecure) option.
The command '/bin/sh -c curl -O https://bootstrap.pypa.io/get-pip.py &&     python get-pip.py &&     rm get-pip.py' returned a non-zero code: 60
FAIL: docker build of hholst/tensorflow:latest-gpu with Dockerfile /tmp/tmp.Ppxl6vutKu/Dockerfile.gpu failed
```
"
4244,Feature request: Docker image build (parameterized_docker_build.sh) lacks option for AVX2 optimization,"As far as I can understand there is no way to tell the script `parameterized_docker_build.sh`  to build an AVX2 optimized docker image.

I see a note about Python 2 and 3 but nothing about AVX2:

```
# TODO(cais): Add support for TF_DOCKER_BUILD_PYTHON_VERSION (PYTHON2/PYTHON3)
```
"
4243,"CTC ERROR: Saw a non-null label (index >= num_classes - 1) following a null label, batch: 0 num_classes: 12 labels: 2,10,5,1","I use TF rc 0.10 to run a project (https://github.com/synckey/tensorflow_lstm_ctc_ocr), but the program raise a error:
Invalid argument: Saw a non-null label (index >= num_classes - 1) following a null label, batch: 0 num_classes: 12 labels: 2,10,5,1

what does it mean?
"
4242,Why is --whole-archive neccesary when building with the session api?,"### Environment info

Operating System:
Linux version 4.8.0-rc1-ga0cba21 
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Building a project using the C++ session API fails without the --whole-archive option but this slows down the linking process immensely and the output binaries are huge (by embedded systems standards). Why can't the linker link this archive normally?
"
4235,How to implement dynamic network with TensorFlow ?,"Hi, I am trying to implement a dynamic network, which is able to change the network structure according to the input data. Here is an example  https://arxiv.org/abs/1511.02799

I wonder it that possible to use TensorFlow to implement dynamic network?
I think we may need to use placeholder to control the network?

Thank you very much.
"
4234,Error installing from source.,"I'm installing tensorflow from source (git hash 891f8f73cc1967a5c2da89057884bc3dc1f9091a, bazel version 0.3.1).

```
./configure
bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
```

The first two steps work, but the last one gives an error:

```
error: can't copy 'tensorflow/models/embedding/gen_word2vec.py': doesn't exist or not a regular file
```

That file indeed does not exist.
"
4232,Create tf.substr Op,"Would be useful for a number of purposes, such as extracting data, substring matches (#4009), etc. Proposed API (matches [C++ style substrings](http://www.cplusplus.com/reference/string/string/substr/)):

```
Args:
  inputs: a `Tensor` of type `string`. The strings that will have substrings extracted
  pos: `int32` or `int64` `Tensor`. Position of the first character to be copied as a substring
  len: `int32` or `int64` `Tensor`. Number of characters to include in the substring

Returns:
  A `Tensor` of type `string` containing substrings of `inputs`
```
"
4231,variable_scope raise exception if reuse=False and no name_or_scope,"Right now `variable_scope` [raises an exception](https://github.com/tensorflow/tensorflow/blob/5c07198777fd641ffb5f55fa6604050ccf55e8c8/tensorflow/python/ops/variable_scope.py#L1373) if `reuse=True` and there is no `name_or_scope`, but there is no similar error when `reuse=False` (explicitly set and not using default of `None`). 

Obviously no error should be raised when `reuse=None` but I think as a safety check if I have explicitly set `reuse=False`, then I do want to raise an error if I have forgotten to set the scope for a given layer / node.

I am using this construct in tf-slim to deal with creating a different graph for training vs validation. Right now if I forget to assign a `scope` to a given layer, than the call to create the graph will not fail even though I want it to.
"
4229,Tensors have values set to 0 if code is run on GPU.,"Hello team, 
I have a strange issue, which very well might be my own fault, but appreciate if somebody could comment on it.
TF 0.10 with CUDA 8
Running code like this on GPU (GeForce GTX Titan X) :

`import tensorflow as tf
import numpy as np
with tf.device(""/gpu:0""):
    a = tf.placeholder(tf.float32, shape=[2,2])
    b = tf.placeholder(tf.float32, shape=[2,2])
    r = tf.squared_difference(a, b)

t1 = np.asarray([[1,1],[1,-5]])
t2 = np.asarray([[2,3],[4,5]])

feed_dict = {a:t1, b:t2}

with tf.Session() as sess:
     t1, t2, sd = sess.run([a,b,r], feed_dict=feed_dict)
    print(""a = {}\n b = {}\n sqared diff = {}"".format(t1, t2, sd))`

Prints result like this 

`a = [[ 1.  1.]
 [ 1. -5.]]
 b = [[ 2.  3.]
 [ 4.  5.]]
 sqared diff = [[ 0.  0.]
 [ 0.  0.]]`

While if I run the same code on CPU , the result is correct 
`a = [[ 1.  1.]
 [ 1. -5.]]
 b = [[ 2.  3.]
 [ 4.  5.]]
 sqared diff = [[   1.    4.]
 [   9.  100.]]`

I'd expect squared_difference to be a perfect operation for GPU and as I tensors ""a"" and ""b"" are populated properly, it doesn't look like TF has issues getting those values into GPU

Appreciate any comments on this.

Operating System: 4.4.0-34-generic #53-Ubuntu

`python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.8.0 locally
0.10.0rc0`.
"
4228,AttributeError: No svd operator in 0.10.0rc0,"Issue #2207 and [documentation](https://www.tensorflow.org/versions/r0.10/api_docs/python/math_ops.html#svd) both indicate that tensorflow has a svd operator.
However, I can't find it in fresh install of tensorflow0.10 with pip for python3.5 (both cpu and gpu )
### Environment info

Operating System: Linux (arch, kernel 4.7.2)
Pip installed (in two different virtualenv):
- https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp35-cp35m-linux_x86_64.whl  
- https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp35-cp35m-linux_x86_64.whl
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

``` python3
import tensorflow as tf
tf.svd

AttributeError: module 'tensorflow' has no attribute 'svd'
```
"
4227,SparseTensor doesn't have get_shape() function,"I've been trying to add SparseTensor support to Keras (https://github.com/fchollet/keras/pull/3695) and one of the issues I've run into is that because of the lack of get_shape() I need to add SparseTensor-specific code.

Luckily so far I've only needed the rank of SparseTensors, so sparse.shape.get_shape()[0] has worked for me, but if I pass in a shape in the sparse_placeholder constructor (https://github.com/tensorflow/tensorflow/issues/4226) I would like to be able to retrieve it with sparse.get_shape()
"
4226,sparse_placeholder doesn't accept shapes with None parts,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

None
1. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
   0.10.0rc0
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```
tf.sparse_placeholder(tf.int32, shape=(None, 2, 3))
```
### What other attempted solutions have you tried?

For the moment I'm passing in a value of the wrong shape but correct rank, which seems to work fine combined with passing in a correct shape when feeding it, but this is inconsistent with the normal placeholder.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).

```
TypeErrorTraceback (most recent call last)
<ipython-input-8-c6a6ad1fab2e> in <module>()
----> 1 tf.sparse_placeholder(tf.int32, shape=(None, 2, 3))

/home/ubuntu/virtualenv/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc in sparse_placeholder(dtype, shape, name)
   1318   else:
   1319     shape = ops.convert_to_tensor(
-> 1320         shape, name=(name + ""/shape"") if name is not None else None)
   1321   return ops.SparseTensor(
   1322       values=placeholder(

/home/ubuntu/virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in convert_to_tensor(value, dtype, name, as_ref)
    627     for base_type, conversion_func in funcs_at_priority:
    628       if isinstance(value, base_type):
--> 629         ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    630         if ret is NotImplemented:
    631           continue

/home/ubuntu/virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    178                                          as_ref=False):
    179   _ = as_ref
--> 180   return constant(v, dtype=dtype, name=name)
    181 
    182 

/home/ubuntu/virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc in constant(value, dtype, shape, name)
    161   tensor_value = attr_value_pb2.AttrValue()
    162   tensor_value.tensor.CopyFrom(
--> 163       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))
    164   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)
    165   const_tensor = g.create_op(

/home/ubuntu/virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.pyc in make_tensor_proto(values, dtype, shape)
    420   if numpy_dtype == dtypes.string and not isinstance(values, np.ndarray):
    421     proto_values = _FlattenToStrings(values)
--> 422     tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])
    423     return tensor_proto
    424 

/home/ubuntu/virtualenv/local/lib/python2.7/site-packages/tensorflow/python/util/compat.pyc in as_bytes(bytes_or_text)
     43   else:
     44     raise TypeError('Expected binary or unicode string, got %r' %
---> 45                     (bytes_or_text,))
     46 
     47 

TypeError: Expected binary or unicode string, got (None, 2, 3)
```
"
4224,Print command not being called through gradient operation.,"I have the following code.  When I call the image_graident function I expect it to print the image_reg variable.  It does not print anything when imageGraident() is called.

```
image_reg = tf.reduce_mean(tf.abs(input_image))
dream_obj = tf.nn.l2_loss(net.fc8) - image_reg
dream_obj = tf.Print(dream_obj, [image_reg])
delta_image = tf.gradients(dream_obj, input_image)[0]

delta_image = tf.Print(delta_image, [image_reg])

def imageGraident(img):
    orig_shape = img.shape
    img = np.reshape(img, (-1, 224, 224, 3))
    dimg = sess.run(delta_image, feed_dict = {input_image: img})
    return np.reshape(dimg, orig_shape)
```

However the following does work fine and it prints the value.

```
image_reg = tf.reduce_mean(tf.abs(input_image))
dream_obj = tf.nn.l2_loss(net.fc8) - image_reg

delta_image = tf.gradients(dream_obj, input_image)[0]
delta_image = tf.Print(delta_image, [image_reg])

def imageGraident(img):
    orig_shape = img.shape
    img = np.reshape(img, (-1, 224, 224, 3))
    dimg = sess.run(delta_image, feed_dict = {input_image: img})
    return np.reshape(dimg, orig_shape)
```

This seems like a bug to me but maybe I am missing something.
"
4219,upgrade tensorflow from cpu to gpu,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
### Environment info

Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:
1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
### What other attempted solutions have you tried?
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
"
4218,seq2seq: Cannot parse tensor from proto: dtype: DT_FLOAT,"My platform is ubuntu 14.04 with tensorflow version of 0.10, CPU version.

I am using functions in `seq2seq.py` to build a model. The code works fine when I use the `embedding_rnn_seq2seq` function, but when I use `embedding_attention_seq2seq`, it has error like this:

`AttributeError: 'LSTMStateTuple' object has no attribute 'get_shape'`

To fix this, I turned the `state_is_tuple` to `false`, but now it has another problem:

```
W tensorflow/core/framework/op_kernel.cc:926] Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT
tensor_shape {
  dim {
    size: 52772
  }
  dim {
    size: 52767
  }
}
float_val: 0

E tensorflow/core/common_runtime/executor.cc:334] Executor failed to create kernel. Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT
tensor_shape {
  dim {
    size: 52772
  }
  dim {
    size: 52767
  }
}
float_val: 0

         [[Node: zeros_29 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 52772 } dim { size: 52767 } } float_val: 0>, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
W tensorflow/core/framework/op_kernel.cc:926] Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT
tensor_shape {
  dim {
    size: 52772
  }
  dim {
    size: 52767
  }
}
float_val: 0

E tensorflow/core/common_runtime/executor.cc:334] Executor failed to create kernel. Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT
tensor_shape {
  dim {
    size: 52772
  }
  dim {
    size: 52767
  }
}
float_val: 0

         [[Node: zeros_28 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 52772 } dim { size: 52767 } } float_val: 0>, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
W tensorflow/core/framework/op_kernel.cc:926] Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT
tensor_shape {
  dim {
    size: 52772
  }
  dim {
    size: 52767
  }
}
float_val: 0

E tensorflow/core/common_runtime/executor.cc:334] Executor failed to create kernel. Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT
tensor_shape {
  dim {
    size: 52772
  }
  dim {
    size: 52767
  }
}
float_val: 0

         [[Node: zeros_28 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 52772 } dim { size: 52767 } } float_val: 0>, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
E tensorflow/core/client/tensor_c_api.cc:485] Cannot parse tensor from proto: dtype: DT_FLOAT
tensor_shape {
  dim {
    size: 52772
  }
  dim {
    size: 52767
  }
}
float_val: 0

         [[Node: zeros_28 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 52772 } dim { size: 52767 } } float_val: 0>, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
Traceback (most recent call last):
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py"", line 730, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py"", line 712, in _run_fn
    status, run_metadata)
  File ""/usr/lib/python3.4/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors.py"", line 450, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors.InvalidArgumentError: Cannot parse tensor from proto: dtype: DT_FLOAT
tensor_shape {
  dim {
    size: 52772
  }
  dim {
    size: 52767
  }
}
float_val: 0

         [[Node: zeros_28 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 52772 } dim { size: 52767 } } float_val: 0>, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""main.py"", line 29, in <module>
    chatbot.main()
  File ""/mnt/d/DeepQA/chatbot/chatbot.py"", line 208, in main
    self.sess.run(tf.initialize_all_variables())
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py"", line 382, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py"", line 655, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py"", line 723, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py"", line 743, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.InvalidArgumentError: Cannot parse tensor from proto: dtype: DT_FLOAT
tensor_shape {
  dim {
    size: 52772
  }
  dim {
    size: 52767
  }
}
float_val: 0

         [[Node: zeros_28 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 52772 } dim { size: 52767 } } float_val: 0>, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
Caused by op 'zeros_28', defined at:
  File ""main.py"", line 29, in <module>
    chatbot.main()
  File ""/mnt/d/DeepQA/chatbot/chatbot.py"", line 169, in main
    self.model = Model(self.args, self.textData)
  File ""/mnt/d/DeepQA/chatbot/model.py"", line 58, in __init__
    self.buildNetwork()
  File ""/mnt/d/DeepQA/chatbot/model.py"", line 118, in buildNetwork
    self.optOp = opt.minimize(self.lossFct)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py"", line 198, in minimize
    name=name)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py"", line 300, in apply_gradients
    self._create_slots(var_list)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/adam.py"", line 118, in _create_slots
    self._zeros_slot(v, ""m"", self._name)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py"", line 494, in _zeros_slot
    named_slots[var] = slot_creator.create_zeros_slot(var, op_name)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/slot_creator.py"", line 106, in create_zeros_slot
    val = array_ops.zeros(primary.get_shape().as_list(), dtype=dtype)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/array_ops.py"", line 1131, in zeros
    output = constant(0, shape=shape, dtype=dtype, name=name)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/constant_op.py"", line 167, in constant
    attrs={""value"": tensor_value, ""dtype"": dtype_value}, name=name).outputs[0]
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py"", line 2310, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py"", line 1232, in __init__
    self._traceback = _extract_stack()
```

Code is as follows:

```
# Creation of the rnn cell
encoDecoCell = tf.nn.rnn_cell.BasicLSTMCell(self.args.hiddenSize, state_is_tuple=False)  # Or GRUCell, LSTMCell(args.hiddenSize)
#encoDecoCell = tf.nn.rnn_cell.DropoutWrapper(encoDecoCell, input_keep_prob=1.0, output_keep_prob=1.0)  # TODO: Custom values (WARNING: No dropout when testing !!!)
encoDecoCell = tf.nn.rnn_cell.MultiRNNCell([encoDecoCell] * self.args.numLayers, state_is_tuple=False)

# Network input (placeholders)

self.encoderInputs  = [tf.placeholder(tf.int32,   [None, ]) for _ in range(self.args.maxLengthEnco)]  # Batch size * sequence length * input dim

self.decoderInputs  = [tf.placeholder(tf.int32,   [None, ], name='inputs') for _ in range(self.args.maxLengthDeco)]  # Same sentence length for input and output (Right ?)
self.decoderTargets = [tf.placeholder(tf.int32,   [None, ], name='targets') for _ in range(self.args.maxLengthDeco)]
self.decoderWeights = [tf.placeholder(tf.float32, [None, ], name='weights') for _ in range(self.args.maxLengthDeco)]

# Define the network
# Here we use an embedding model, it takes integer as input and convert them into word vector for
# better word representation
decoderOutputs, states = tf.nn.seq2seq.embedding_attention_seq2seq(
    self.encoderInputs,  # List<[batch=?, inputDim=1]>, list of size args.maxLength
    self.decoderInputs,  # For training, we force the correct output (feed_previous=False)
    encoDecoCell,
    self.textData.getVocabularySize(),
    self.textData.getVocabularySize(),  # Both encoder and decoder have the same number of class
    embedding_size=self.args.embeddingSize,  # Dimension of each word
    output_projection=None,  # Eventually
    feed_previous=bool(self.args.test)  # When we test (self.args.test), we use previous output as next input (feed_previous)
)
```
"
4217,the tensorflow binary package in tensorflow r0.10 is build with cudnn5.1 not cudnn4.0,"I use pip to install tf r0.10.
# Ubuntu/Linux 64-bit, GPU enabled, Python 2.7
# Requires CUDA toolkit 7.5 and CuDNN v4. For other versions, see ""Install from sources"" below.

$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl
# Python 2

$ sudo pip install --upgrade $TF_BINARY_URL

When I run the program, the program raise error for the cudnn version. 
Actually, the binary package is builded with cudnn5.1 and cuda7.5.
"
4216,Can not load workspace.bzl,"I have a project that I want to build with Bazel, which includes Python code and C++ files. I am trying to use the same approach as in [Syntaxnet](https://github.com/tensorflow/models/tree/master/syntaxnet) from the model zoo. I add `tensorflow` as a submodule to my repository and put the following snippet to my `WORKSPACE` file:

```
local_repository(
  name = ""org_tensorflow"",
  path = __workspace_dir__ + ""/tensorflow"",
)

load('//tensorflow/tensorflow:workspace.bzl', 'tf_workspace')
tf_workspace(""tensorflow/"", ""@org_tensorflow"")
```

which results in an error

```
ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension file not found. Unable to load package for '//third_party/gpus:cuda_configure.bzl': BUILD file not found on package path.
```

The source of the error is clear to me: [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L3) contains a `load` call with a hard-coded path. SyntaxNet uses an older version of Tensorflow that does not contain this line. 

My question is if this is a bug or not. Is this a recommended way of organizing the project that should be compiled with Tensorflow, and if not, should it be used in Syntaxnet model? Or should I just copy the `WORKSPACE` from Tensorflow's root directory?
"
4214,C++ compilation of rule '//tensorflow/core/kernels:gather_nd_op' failed,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

none
### Environment info

Operating System:

Ubuntu 16.04 64bit

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

```
# ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root   560184 Sep  5 22:07 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Sep  5 22:07 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Sep  5 22:07 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 root root   394472 Sep  5 22:07 /usr/local/cuda/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 root root   737516 Sep  5 22:07 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 Sep  5 22:08 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       17 Sep  5 22:08 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxr-xr-x 1 root root 79337624 Sep  5 22:08 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Sep  5 22:08 /usr/local/cuda/lib64/libcudnn_static.a
```

I've downloaded these from Nvidia and installed them per ""official"" instructions:
- cuda_8.0.27_linux.run
- cuda_8.0.27.1_linux.run
- cudnn-8.0-linux-x64-v5.1.tgz

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)

```
# git rev-parse HEAD
2ab7e6326296987ea0ce975afb3434a16d1aa21a
```
1. The output of `bazel version`

```
# bazel version
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Jan 01 00:00:00 1970 (0)
Build timestamp: Thu Jan 01 00:00:00 1970 (0)
Build timestamp as int: 0
```

```
# dpkg -l | grep bazel | awk '{print $3}'
0.3.1
```
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```
./configure

# Hit ENTER on every question except:
# Do you wish to build TensorFlow with GPU support? (answer: y)
# Please specify a list of comma-separated Cuda compute capabilities you want to build with. (answer: 6.1)
# Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: (answer 8.0)
```
### What other attempted solutions have you tried?

Tried to google around, hoping it's a known issue. Could not find anything related.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).

Gist with last messages during compilation:

https://gist.github.com/FlorinAndrei/23ada4fb714776e68c2693502c615305
"
4213,FCNs cannot be used for segmentation of variable size images after training.,"Yann LeCun pointed,  ConvNets don't need to have a fixed-size input.
One can train Fully-convolutional network using small images, and than use the model (without scaling or cropping) and feed it with large images for segmentation.
It works right away with Keras+Theano.
If one trains the same model with TensorFlow as Keras'es backend, during application of pretrained model on image larger than the images used for training, he will get A ValueError due to tensor shape mismatch (same for manual model definition directly in TF).

The shape check needs to be disabled and the convolution layers need to work properly with variable size during model using/testing phase if the framework is to be competitive.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

https://www.facebook.com/yann.lecun/posts/10152820758292143
http://stackoverflow.com/questions/39050557/fully-convolutional-network-training-image-size
https://www.quora.com/How-does-the-conversion-of-last-layers-of-CNN-from-fully-connected-to-fully-convolutional-allow-it-to-process-images-of-different-size 
http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-pixels.pdf
### Environment info

Operating System:  Ubuntu 14.04.4 LTS 
Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
[cuda_config.txt](https://github.com/tensorflow/tensorflow/files/455666/cuda_config.txt)
### Error log:

ValueError: Cannot feed value of shape (1, 3, 350, 600) for Tensor u'convolution2d_input_2:0', which has shape '(?, 3, 15, 15)'
"
4212,How to implement Multiplexer,"How to takes a single input and select one of many output lines, which is connected to the input, by using placeholder?

For example if we have a TF variable as follow, how could we forward it to the 1st output ?

```
intputs = tf.constant(0.2, shape=[2, 2])
sel = tf.placeholder(tf.int32)
n_out = 3
out = xxxx(sel, inputs, n_out)  # here

sess.run(out, feed_dict = {sel: 0}) 
# [[[0.2, 0.2],[0.2, 0.2]], 
   [[0, 0],[0, 0]], 
    [[0, 0],[0, 0]]]

```
"
4210,deepdream tutorial rename_nodes error,"Hi -
    The tensorflow/examples/deepdream README.md says that Python 2.7 is required. However, loading the deepdream.ipynb notebook fires up the 3.5 kernel (due to info I believe is stored in the notebook itself. 

But the error I am encountering on the line:
tmp_def = rename_nodes(graph_def, lambda s:""/"".join(s.split('_',1)))

may or may not be due to a python version issue. The line generates the error:
TypeError: '<stripped 37632 bytes>' has type <class 'str'>, but expected one of: (<class 'bytes'>,)

I'm running Mac OS X 10.10.5, python 2.7 for the Mac, and 3.5 for everything else. 
Best,
            - lonce
"
4209,"tf.concat returns a list not Tensor, is it a bug?","My code is below

```
from __future__ import print_function
import tensorflow as tf
import numpy as np
import shutil
import os

class Model(object):
    def __init__(self, input_length, input_dim=10, filter_nums=[3,4,5], window_sizes=[2,3,4]):
        self.filter_nums = filter_nums
        self.window_sizes = window_sizes

        # placeholder
        self._inputs = tf.placeholder(tf.float32, shape=[None, input_dim,
        input_length], name=""inputs"")

    def _conv2d(self, inputs, filter_num, window_size, init_scale=0.1, data_format=""NHWC"", name=""conv2d""):
        # Implement a conv2d for nlp task, which in_heigth = filter_height.
        # assume inputs dim is 4.
        with tf.variable_scope(name):
            if data_format == ""NHWC"":
                filter_height = int(inputs.get_shape()[1])
            else:
                filter_height = int(inputs.get_shape()[2])
            in_channels = int(inputs.get_shape()[-1])
            W = tf.get_variable(""W"", shape=[filter_height, window_size,
            in_channels, filter_num],
            initializer=tf.random_uniform_initializer(-init_scale, init_scale))
            conv = tf.nn.conv2d(inputs, W, [1,1,1,1], ""VALID"")
            return conv

    def _max_pool(self, value, name=""max_pool""):
        with tf.variable_scope(name):
            pool_width = int(value.get_shape()[-2])
            maxpool = tf.nn.max_pool(value, [1,1,pool_width,1], [1,1,1,1], ""VALID"")
            return maxpool

    def multi_conv_pool(self):
        pooled = []
        expanded_inputs = tf.expand_dims(self._inputs, -1)
        for filter_num, window_size in zip(self.filter_nums,
        self.window_sizes):
            conv = self._conv2d(expanded_inputs, filter_num, window_size,
            name=""conv2d_width_%d"" % window_size)
            maxpool = self._max_pool(conv)
            pooled.append(maxpool)
        concat_pooled = tf.concat(3, pooled)
        return concat_pooled

    @property
    def inputs(self):
        return self._inputs

def main():
    logdir = 'logdir'
    if os.path.exists(logdir):
        shutil.rmtree(logdir)

    model = Model(input_length=9, input_dim=4)
    concat_pooled = model.multi_conv_pool()

    data = np.random.randn(3, 4, 9)

    sess = tf.Session()

    summary_writer = tf.train.SummaryWriter(logdir, graph=sess.graph)

    sess.run(tf.initialize_all_variables())
    outputs = sess.run([concat_pooled], feed_dict={model.inputs: data})
    print(""data(shape = (3, 4, 9)):"")
    print(data)
    print(""after multi conv and pool, its output should be (3, 1, 1, 12):"")
    print(outputs)
    print(""outputs' type:"", type(outputs))

if __name__ == '__main__':
    main()

```

Run it and gets a list. `outputs` here is the return value of tf.concat(). It should be an array but now it's a list. Could anyone tell me why? Thanks!
"
4208,Batch Normalization slow?,"I am using tf.contrib.layers.batch_norm
This is very slow. Any better implementation?
"
4207,Feature request: Ogg (or equivalent) compression for AudioSummary with TensorBoard,"When logging an AudioSummary every couple of epochs, the logs grow quickly (as in several gigabytes), and checking TensorBoard remotely (as in not on the same LAN) also becomes very slow. Would it be possible to have builtin support for Ogg compression or an equivalent lossy format? There are multiple use cases where this would be beneficial.
"
4206,transient error:  Enqueue operation was cancelled,"[Related issue](http://stackoverflow.com/questions/38678371/tensorflow-enqueue-operation-was-cancelled) seems to still hapen in 0.10rc0
Operating System: Ubuntu 15.04
Installed version of CUDA and cuDNN: CUDA 7.5, cuDNN 4
output of `ls -l /path/to/cuda/lib/libcud*`:

```
-rw-r--r-- 1 root root   322936 iul  5 16:00 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudadevrt.a
lrwxrwxrwx 1 root root       16 iul  5 16:00 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root       19 iul  5 16:00 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root   383336 iul  5 16:00 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudart.so.7.5.18
-rw-r--r-- 1 root root   720192 iul  5 16:00 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudart_static.a
-rwxr-xr-x 1 root root 61453024 iul  5 16:09 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudnn.so
-rwxr-xr-x 1 root root 61453024 iul  5 16:09 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudnn.so.4
-rwxr-xr-x 1 root root 61453024 iul  5 16:09 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudnn.so.4.0.7
-rwxr-xr-x 1 root root 59909104 iul  5 16:01 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudnn.so.5
-rwxr-xr-x 1 root root 59909104 iul  5 16:01 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudnn.so.5.0.5
-rw-r--r-- 1 root root 62025862 iul  5 16:09 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudnn_static.a
```

Installed tensorflow 0.10 with GPU, for python 2.7 in virtualenv with pip
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl
0.10.0rc0

[Here's a repo with a self contained reproducible example](https://github.com/titusnicolae/tf-issue/commit/7c13aa79550b6c89701fcc5e3ebf1c46effd0c6f)
tfwriter.py creates the range.tfrecord file with numbers from 0 to 9 as TFRecords
When running tfreader.py I get one of the following outputs:

```
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX 980 Ti
major: 5 minor: 2 memoryClockRate (GHz) 1.2405
pciBusID 0000:01:00.0
Total memory: 6.00GiB
Free memory: 5.46GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)
(0, ['range.tfrecord:0', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x00'])
(1, ['range.tfrecord:32', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x01'])
(2, ['range.tfrecord:64', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x02'])
(3, ['range.tfrecord:96', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x03'])
(4, ['range.tfrecord:128', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x04'])
(5, ['range.tfrecord:160', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x05'])
(6, ['range.tfrecord:192', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x06'])
(7, ['range.tfrecord:224', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x07'])
(8, ['range.tfrecord:256', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x08'])
(9, ['range.tfrecord:288', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\t'])
(10, ['range.tfrecord:0', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x00'])
(11, ['range.tfrecord:32', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x01'])
(12, ['range.tfrecord:64', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x02'])
(13, ['range.tfrecord:96', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x03'])
(14, ['range.tfrecord:128', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x04'])
(15, ['range.tfrecord:160', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x05'])
(16, ['range.tfrecord:192', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x06'])
(17, ['range.tfrecord:224', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x07'])
(18, ['range.tfrecord:256', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x08'])
(19, ['range.tfrecord:288', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\t'])
```

or

```
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX 980 Ti
major: 5 minor: 2 memoryClockRate (GHz) 1.2405
pciBusID 0000:01:00.0
Total memory: 6.00GiB
Free memory: 5.46GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)
(0, ['range.tfrecord:0', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x00'])
(1, ['range.tfrecord:32', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x01'])
(2, ['range.tfrecord:64', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x02'])
(3, ['range.tfrecord:96', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x03'])
(4, ['range.tfrecord:128', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x04'])
(5, ['range.tfrecord:160', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x05'])
(6, ['range.tfrecord:192', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x06'])
(7, ['range.tfrecord:224', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x07'])
(8, ['range.tfrecord:256', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x08'])
(9, ['range.tfrecord:288', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\t'])
(10, ['range.tfrecord:0', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x00'])
(11, ['range.tfrecord:32', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x01'])
(12, ['range.tfrecord:64', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x02'])
(13, ['range.tfrecord:96', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x03'])
(14, ['range.tfrecord:128', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x04'])
(15, ['range.tfrecord:160', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x05'])
(16, ['range.tfrecord:192', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x06'])
(17, ['range.tfrecord:224', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x07'])
(18, ['range.tfrecord:256', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\x08'])
(19, ['range.tfrecord:288', '\n\x0e\n\x0c\n\x03int\x12\x05\x1a\x03\n\x01\t'])
E tensorflow/core/client/tensor_c_api.cc:485] FIFOQueue '_0_input_producer' is closed.
     [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[""loc:@input_producer""], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](input_producer, input_producer/RandomShuffle)]]
```
"
4205,Misleading argument name on seq2seq module,"Not really major issue, but can make lose some time.

For the functions [sequence_loss_by_example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L980) and [sequence_loss](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L1026), the name of the parameter `softmax_loss_function` seems to imply that only softmax based loss are compatibles which is not true (for instance `tf.nn.sigmoid_cross_entropy_with_logits` works too). Maybe the name was that just to indicate the default behavior but I find it misleading.

It shouldn't be really difficult to change (as there are very few references in the repository). I can send a pull request if needed.
"
4204,[SOLVED] Using opencv with tensorflow causes segfault.,"Using opencv with tensorflow causes segfault.

```
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
[1]    12012 segmentation fault (core dumped)  ./test.py
```
### Related GitHub issues or StackOverflow:

http://stackoverflow.com/questions/37535765/segmentation-fault-core-dumped-error-for-cifar10-example-tensorflow
https://github.com/tensorflow/tensorflow/issues/1924
### Environment info

1 Operating System:
Arch linux

```
$ uname -a
Linux justes_notebook 4.7.2-1-ARCH #1 SMP PREEMPT Sat Aug 20 23:02:56 CEST 2016 x86_64 GNU/Linux
```

2 CUDA
cuda 7.5.18
cudnn 5.1.3

```
$ ls -l /opt/cuda/lib/libcud*
-rw-r--r-- 1 root root 189170 jul 12 02:19 /opt/cuda/lib/libcudadevrt.a
lrwxrwxrwx 1 root root     16 jul 12 02:19 /opt/cuda/lib/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19 jul 12 02:19 /opt/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 311596 jul 12 02:19 /opt/cuda/lib/libcudart.so.7.5.18
-rw-r--r-- 1 root root 558020 jul 12 02:19 /opt/cuda/lib/libcudart_static.a
```

3 The commit hash 
2ab7e6326296987ea0ce975afb3434a16d1aa21a

4 bazel

```
$ bazel version
Build label: 0.3.1- (@non-git)
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Ð¡Ñ€ Ð°Ð²Ð³ 24 04:26:11 2016 (1472012771)
Build timestamp: 1472012771
Build timestamp as int: 1472012771
```

5 python 

```
$ python --version 
Python 3.5.2
```

6 opencv
opencv 3.1.0
### Example

```
#!/usr/bin/python

import cv2
import tensorflow as tf

img = cv2.imread('im.jpg')
img = cv2.resize(img, (100,100))
sess = tf.Session()
```
"
4203,BUG in graph_actions.py,"Hello I am using `tf.contrib.learn.Estimator` to train my model, and size of validation sample is small,(validation sample size=32, eval_batch_size=16)

I have created an EvaluationMonitor to evaluate the validation sample

```
class EvaluationMonitor(tf.contrib.learn.monitors.EveryN):
        def every_n_step_end(self, step, outputs):
            eval_results = self._estimator.evaluate(
                input_fn=input_fn_eval,
                metrics=eval_metrics,
                steps=None)
            print ""Evaluation Result: %s"" % eval_results
```

and the `eval_metrics` is is a list of `tf.contrib.metrics.streaming_sparse_recall_at_k`, `input_fn_eval` is a function to read TFRecord from file

but in the evaluation step, I always get the exception:

> File ""...../tensorflow/contrib/learn/python/learn/graph_actions.py"", line 610, in _eval_results_to_str
>     return ', '.join('%s = %s' % (k, v) for k, v in eval_results.items())

When i looks inside the evaluate method(Line 632- Line 779) in the source of [`graph_actions.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/graph_actions.py#L632)

```
def evaluate(graph,
             output_dir,
             checkpoint_path,
             eval_dict,
             update_op=None,
             global_step_tensor=None,
             supervisor_master='',
             log_every_steps=10,
             feed_fn=None,
             max_steps=None):
......
try:
      try:
        while (max_steps is None) or (step < max_steps):
          step += 1
          start_time = time.time()
          feed_dict = feed_fn() if feed_fn is not None else None
          if update_op is not None:
            session.run(update_op, feed_dict=feed_dict)
          else:
            eval_results = session.run(eval_dict, feed_dict=feed_dict)
            eval_step = step

          if step % log_every_steps == 0:
            if eval_step is None or step != eval_step:
              eval_results = session.run(eval_dict, feed_dict=feed_dict)
              eval_step = step
            duration = time.time() - start_time
            logging.info('Results after %d steps (%.3f sec/batch): %s.',
                         step, float(duration),
                         _eval_results_to_str(eval_results))
      finally:
        if eval_results is None or step != eval_step:
          eval_results = session.run(eval_dict, feed_dict=feed_dict)
          eval_step = step

......

 # Save summaries for this evaluation.
  _write_summary_results(output_dir, eval_results, current_global_step)

```

when my code call the function,   the `max step=2`(validation sample size=32, eval_batch_size=16) will be less than the default parameter( `log_every_steps=10`), therefore the code only can run the evaluation in the `finally` block, 

```
 ....
finally:
        if eval_results is None or step != eval_step:
          eval_results = session.run(eval_dict, feed_dict=feed_dict)
          eval_step = step
...
```

but for the metrics of t`tf.contrib.metrics.streaming_sparse_recall_at_k,` the `update_op` is not Not, which means the line

```
if update_op is not None
    session.run(update_op, feed_dict=feed_dict)
```

will alway be invoked, and the batched_data in the queue will be _empty_ when the code jump into `finally` block, therefore an exception of `OutOfRangeError` will be raised, and `eval_results` will be None when the code execute at line

```
_write_summary_results(output_dir, eval_results, current_global_step)
```

I think it is caused by the default value of `log_every_steps`, when the `_evaluate_model` method in `tf.contrib.learn.Estimator`  invoke the method in line [679](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L673),

```
eval_results, current_global_step = graph_actions.evaluate(
          graph=g,
          output_dir=eval_dir,
          checkpoint_path=checkpoint_path,
          eval_dict=eval_dict,
          update_op=update_op,
          global_step_tensor=global_step,
          supervisor_master=self._config.master,
          feed_fn=feed_fn,
          max_steps=steps)

```

it did not set the value of `log_every_steps` and  the default value of   `log_every_steps` will be 10, so the exception will be raised when the validation sample size is small (or total evaluation steps is less than 10)

When I set the hyper parameter `eval_batch_size` to a small value (eg. 2, which ensure total evaluation 32/2 = 16 > 10), my code is ok
"
4202,undefined reference to `tensorflow::CanUseDeepConv2D(...)' building with Makefile method,"Building tensorflow with makefile method gets stuck at 

```
gcc --std=c++11 -DIS_SLIM_BUILD -O0 -I. -I/home/tensorflow/tensorflow/contrib/makefile/downloads/ -I/home/tensorflow/tensorflow/contrib/makefile/downloads/eigen-eigen-9e1b48c333aa -I/home/tensorflow/tensorflow/contrib/makefile/gen/proto/ -I/home/tensorflow/tensorflow/contrib/makefile/gen/proto_text/ -I/usr/local/include -I/usr/local/include/c++/4.9.4/ -I/usr/local/include/c++/4.9.4/arm-linux-gnueabihf/ \
-o /home/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark /home/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/core/util/reporter.o /home/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/tools/benchmark/benchmark_model.o /home/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/tools/benchmark/benchmark_model_main.o \
 -Wl,--allow-multiple-definition -Wl,--whole-archive /home/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a -L/usr/local/lib -lstdc++ -lprotobuf -lz -lm -ldl -lpthread
/home/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(conv_ops.o): In function `tensorflow::LaunchDeepConvOp<Eigen::ThreadPoolDevice, float>::Run(tensorflow::OpKernelContext*, tensorflow::Tensor const&, tensorflow::Tensor const&, int, int, int, int, int, int, int, int, int, int, int, int, int, tensorflow::Tensor*, tensorflow::TensorFormat)':
conv_ops.cc:(.text._ZN10tensorflow16LaunchDeepConvOpIN5Eigen16ThreadPoolDeviceEfE3RunEPNS_15OpKernelContextERKNS_6TensorES8_iiiiiiiiiiiiiPS6_NS_12TensorFormatE[_ZN10tensorflow16LaunchDeepConvOpIN5Eigen16ThreadPoolDeviceEfE3RunEPNS_15OpKernelContextERKNS_6TensorES8_iiiiiiiiiiiiiPS6_NS_12TensorFormatE]+0x58): undefined reference to `tensorflow::CanUseDeepConv2D(int, int, int, int, int, int, int, int)'
conv_ops.cc:(.text._ZN10tensorflow16LaunchDeepConvOpIN5Eigen16ThreadPoolDeviceEfE3RunEPNS_15OpKernelContextERKNS_6TensorES8_iiiiiiiiiiiiiPS6_NS_12TensorFormatE[_ZN10tensorflow16LaunchDeepConvOpIN5Eigen16ThreadPoolDeviceEfE3RunEPNS_15OpKernelContextERKNS_6TensorES8_iiiiiiiiiiiiiPS6_NS_12TensorFormatE]+0x174): undefined reference to `tensorflow::functor::DeepConv2D<Eigen::ThreadPoolDevice, float>::operator()(tensorflow::OpKernelContext*, tensorflow::Conv2DArgs const&, float const*, float const*, float*)'
collect2: error: ld returned 1 exit status
tensorflow/contrib/makefile/Makefile:485: recipe for target '/home/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark' failed
make: *** [/home/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark] Error 1
```
"
4201,"After I used SyncReplicasOptimizer, tf hang at sess.run ","After I used SyncReplicasOptimizer and chief queue runner in distribute tensorflow, it hangs at sess.run.
But after I added below:
        queue_runners = tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS)
        sv.start_queue_runners(sess, queue_runners)  
this queue runs OK.
Anyone can tell me why?
I added this code before:
      if is_chief:
          sv.start_queue_runners(sess, chief_queue_runner)
          sess.run(init_token_op)
"
4200,TypeError: __init__() got an unexpected keyword argument 'dtype',"The module is `models/rnn/translate`
I'm running

```
$ python -c 'import tensorflow as tf; print tf.__version__'
0.10.0rc0
```

I did the training:

```
python translate.py --data_dir /mnt/ft1/translate/ --train_dir /mnt/ft1/translate/ --size=256 --num_layers=2 --steps_per_checkpoint=50
```

and then run

```
python translate.py --decode --data_dir /mnt/ft1/translate/ --train_dir /mnt/ft1/translate/
```

```
Traceback (most recent call last):
  File ""translate.py"", line 290, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""translate.py"", line 285, in main
    decode()
  File ""translate.py"", line 222, in decode
    model = create_model(sess, True)
  File ""translate.py"", line 131, in create_model
    dtype=dtype)
TypeError: __init__() got an unexpected keyword argument 'dtype'
```

My data dir looks like

```
-rw-rw-r-- 1 ubuntu ubuntu   21393583 Aug 31 15:21 dev-v2.tgz
-rw-rw-r-- 1 ubuntu ubuntu 3789873031 Aug 31 15:21 giga-fren.release2.en
lrwxrwxrwx 1 ubuntu ubuntu         30 Aug 31 15:17 giga-fren.release2.en.gz -> giga-fren.release2.fixed.en.gz
-rw-rw-r-- 1 ubuntu ubuntu 1214224978 Aug 30 19:55 giga-fren.release2.fixed.en.gz
-rw-rw-r-- 1 ubuntu ubuntu 1380871453 Aug 29 21:43 giga-fren.release2.fixed.fr.gz
-rw-rw-r-- 1 ubuntu ubuntu 4565271815 Aug 31 15:19 giga-fren.release2.fr
lrwxrwxrwx 1 ubuntu ubuntu         30 Aug 31 15:13 giga-fren.release2.fr.gz -> giga-fren.release2.fixed.fr.gz
-rw-rw-r-- 1 ubuntu ubuntu 2380084216 Sep  2 11:29 giga-fren.release2.ids40000.en
-rw-rw-r-- 1 ubuntu ubuntu 1222033408 Aug 31 17:17 giga-fren.release2.ids40000.fr
-rw-r--r-- 1 ubuntu ubuntu     332974 Dec 13  2013 newstest2013.en
-rw-r--r-- 1 ubuntu ubuntu     393465 Dec 13  2013 newstest2013.fr
-rw-rw-r-- 1 ubuntu ubuntu     231941 Sep  2 11:29 newstest2013.ids40000.en
-rw-rw-r-- 1 ubuntu ubuntu     268151 Sep  2 11:29 newstest2013.ids40000.fr
-rw-rw-r-- 1 ubuntu ubuntu 2595102720 Aug 31 10:53 training-giga-fren.tar
-rw-rw-r-- 1 ubuntu ubuntu     343512 Aug 31 16:54 vocab40000.en
-rw-rw-r-- 1 ubuntu ubuntu     380736 Aug 31 16:11 vocab40000.fr
```

The `symlink` in the `data_dir` are due to https://github.com/tensorflow/tensorflow/issues/4122
"
4199,Unable to create a pip whl. ,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

https://github.com/tensorflow/tensorflow/issues/2040
### Environment info

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

-rw-r--r-- 1 root root  189170 Dec 23  2015 /usr/local/cuda/lib/libcudadevrt.a
lrwxrwxrwx 1 root root      16 Dec 23  2015 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root      19 Dec 23  2015 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root  311596 Dec 23  2015 /usr/local/cuda/lib/libcudart.so.7.5.18
-rw-r--r-- 1 root root  558020 Dec 23  2015 /usr/local/cuda/lib/libcudart_static.a
lrwxrwxrwx 1 root root      17 Dec 23  2015 /usr/local/cuda/lib/libcuinj32.so -> libcuinj32.so.7.5
lrwxrwxrwx 1 root root      20 Dec 23  2015 /usr/local/cuda/lib/libcuinj32.so.7.5 -> libcuinj32.so.7.5.18
-rwxr-xr-x 1 root root 5396088 Dec 23  2015 /usr/local/cuda/lib/libcuinj32.so.7.5.18

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)

9b69ec3960cf9225df557fdc1ab673bd36bde4fb
1. The output of `bazel version`

Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Jan 01 00:00:00 1970 (0)
Build timestamp: Thu Jan 01 00:00:00 1970 (0)
Build timestamp as int: 0
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Quite simply, I am going through the instructions here(https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#create-the-pip-package-and-install), and get the following error:

tc:/deeplearning/tensorflow$ bazel-bin/tensorflow/tools/pip_package/build_pip_package.sh /tmp/tensorflow_pkg
bash: bazel-bin/tensorflow/tools/pip_package/build_pip_package.sh: No such file or directory
### What other attempted solutions have you tried?

The fix from here (https://github.com/tensorflow/tensorflow/issues/2040), which was to do: cp -r bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/**main**/\* bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/

seemed to run properly, but this did not work in the end. 
"
4197,Incorrect documentation for dynamic_rnn,"The help for `tf.nn.dynamic_rnn` states that the optional `initial_state` kwarg, if `cell.state_size` is an integer, must be a `Tensor` of appropriate type and shape `[batch_size x cell.state_size]`. This is incorrect; the shape must be actually be `[batch_size, cell.state_size]`.
"
4196,Error when running two simultaneous sessions,"Can I run two simultaneous sessions?

I start with: ""with tf.Graph().as_default(), tf.Session() as sess:""

When I run my script alone, the result is OK, but when I run 2 instances the result is completely wrong or throw some errors like:

Error max() arg is an empty sequence
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:924] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: Quadro K2000M
major: 3 minor: 0 memoryClockRate (GHz) 0.745
pciBusID 0000:01:00.0
Total memory: 1.95GiB
Free memory: 1.47GiB

(and also)
float division by zero

Can I do something in my code do avoid these errors?

Maybe check if TF is already running and hold it?

Than you very much.

OS: Arch Linux

$ ls -l /opt/cuda/lib/libcud*
/opt/cuda/lib/libcudadevrt.a
/opt/cuda/lib/libcudart.so.7.5.18
/opt/cuda/lib/libcudart_static.a
/opt/cuda/lib/libcudnn.so -> /opt/cuda/lib64/libcudnn.so.4.0.7

PIP: 0.9.0
"
4194,Syntax Errors in Python 3.5 : Installed using pip3,"I followed the instructions on [installing TensorFlow using pip](https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#optional-install-cuda-gpus-on-linux) and installed both the `Ubuntu/Linux 64-bit, GPU enabled, Python 3.5` and `Ubuntu/Linux 64-bit, GPU enabled, Python 2.7` versions using pip3 and pip2 respectively. The package imports correctly in Python 2.7 but the following SyntaxErrors occur for Python 3.5.

``` terminal
python3
Python 3.5.2 (default, Jul  5 2016, 12:43:10) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
```

``` python
>>> import tensorflow as tf
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
...
  File ""/home/gszep/.local/lib/python3.5/site-packages/dateutil/parser.py"", line 158
    l.append(""%s=%s"" % (attr, `value`))
                              ^
SyntaxError: invalid syntax
```
"
4193,CTC loss is numerically unstable for long sequence lengths,"Hi All, great work TF team, thanks for implementing CTC loss ops recently!

I've noticed that CTC loss becomes quite numerically unstable for long sequences, in the attached code that reproduces the plot from the CTC paper, sequences longer than 10,000 or so degrade the quality of the gradients quite severely.

I'm not sure what the best fix is, trying to improve the numerical stability of CTC loss calculation could help, but I have no idea how difficult this would be.

In the interim, I suppose it might be worth issuing a warning when the op is used with long sequences, or placing internal consistency checks that warn the user when CTC loss may be producing bogus outputs. Or even just improving the documentation for CTC loss so that users know that this is a pitfall.

This is also sort of a public service announcement so nobody gets burned on long sequences.

Again, great work, thanks for all your effort.

:)
### Environment info

Pip Package Version: 0.10.0rc0
### Minimal Example

``` python
import numpy as np
import tensorflow as tf

%matplotlib inline
import matplotlib.pyplot as plt

# n = 100
n = 20000
k = 4

with tf.Session() as sess:
    inputs = tf.zeros((n, 1, k+1), dtype=tf.float32)
    labels_indices = tf.constant([[0, 0], [0, 1], [0, 2], [0, 3], [0, 4]], dtype=tf.int64)
    labels_values  = tf.constant([0, 1, 2, 3, 4], dtype=tf.int32)
    labels = tf.SparseTensor(indices=labels_indices, values=labels_values, shape=(1, k+1))
    sequence_length = np.array([n])

    loss = tf.nn.ctc_loss(inputs, labels, sequence_length)
    g, = tf.gradients(loss, inputs)

    g_v = sess.run(g)

plt.plot(-g_v[:,0,:])
```
"
4190,ERROR: undeclared inclusion(s) in rule '//tensorflow/core/kernels:multinomial_op_gpu',"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

none
### Environment info

Operating System:

Ubuntu 16.04 64bit with all updates applied

Installed version of CUDA and cuDNN: 

I've downloaded these from Nvidia and installed them per ""official"" instructions:
- cuda_8.0.27_linux.run
- cuda_8.0.27.1_linux.run
- cudnn-8.0-linux-x64-v5.1.tgz

(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

```
# ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root   560184 Sep  3 21:37 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Sep  3 21:37 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Sep  3 21:37 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 root root   394472 Sep  3 21:37 /usr/local/cuda/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 root root   737516 Sep  3 21:37 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 Sep  3 21:37 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       17 Sep  3 21:37 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxr-xr-x 1 root root 79337624 Sep  3 21:37 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Sep  3 21:37 /usr/local/cuda/lib64/libcudnn_static.a
```

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)

```
# git rev-parse HEAD
3c177a54cf48efdab49d716ee27936ead31c3388
```
1. The output of `bazel version`

```
# bazel version     
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Jan 01 00:00:00 1970 (0)
Build timestamp: Thu Jan 01 00:00:00 1970 (0)
Build timestamp as int: 0
```

It's bazel 0.3.1 actually. No idea why the output from version is so useless. I've installed it from the official deb repo.
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
./configure

# Hit ENTER on every question except:
# Do you wish to build TensorFlow with GPU support? (answer: y)
# Please specify a list of comma-separated Cuda compute capabilities you want to build with. (answer: 6.1)

bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```

It's actually a bit more complicated, as the first attempt fails to even begin to compile. So I delete the TF repo, clone again, and configure/build again. It begins to compile, keeps chugging along for a few minutes, and then I get the error shown in the title.

More details here (you'll find there a fully automated procedure to completely reproduce the environment and build process):

https://github.com/tensorflow/tensorflow/issues/4105#issuecomment-244573108
### What other attempted solutions have you tried?

Tried to google around, hoping it's a known issue. Could not find anything related.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).

This is the end of the build messages:

https://gist.github.com/FlorinAndrei/b378e45709185ef40808e7537a3df4e8
"
4187,Library not loaded: @rpath/libcudart.7.5.dylib,"+@trevorwelch

From the discussion in #4105 and #4145, this is the tracking bug for the following error:

```
ERROR: /Users/production204/Github/tensorflow/tensorflow/cc/BUILD:179:1: Executing genrule //tensorflow/cc:training_ops_genrule failed: bash failed: error executing command 
  (cd /private/var/tmp/_bazel_production204/ed2bbf43bcd665c40f1e3ebaa04f68f6/execroot/tensorflow && \
  exec env - \
    PATH=/usr/local/cuda/bin:/Library/Frameworks/Python.framework/Versions/2.7/bin:/usr/local/bin:usr/local/sbin:/usr/local/mysql/bin:/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin \
    TMPDIR=/var/folders/h3/pn9k79xn6qd9jgksqbkpn3l80000gn/T/ \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/cc/ops/training_ops_gen_cc bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/training_ops.h bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/training_ops.cc 0'): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 5.
dyld: Library not loaded: @rpath/libcudart.7.5.dylib
  Referenced from: /private/var/tmp/_bazel_production204/ed2bbf43bcd665c40f1e3ebaa04f68f6/execroot/tensorflow/bazel-out/host/bin/tensorflow/cc/ops/training_ops_gen_cc
  Reason: image not found
/bin/bash: line 1: 74845 Trace/BPT trap: 5       bazel-out/host/bin/tensorflow/cc/ops/training_ops_gen_cc bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/training_ops.h bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/training_ops.cc 0
Target //tensorflow/cc:tutorials_example_trainer failed to build
INFO: Elapsed time: 3111.405s, Critical Path: 3097.65s

production204@Trevors-MacBook-Pro tensorflow $ 
```
"
4181,Document events file format,"Hello,
In the documentation for Tensorboard, I can't find a description for the format of the events file beyond that it contains Event protobufs. Is it recordio? Are there any tools for writing event files for 3rd-party Tensorflow clients that aren't using the Python API? 
"
4178,What do readers do after their input is exhausted?,"I would like to train for multiple epochs on a dataset, using TensorFlow readers and queues. How do reader signal the end of the dataset? I couldn't find any [documentation](https://www.tensorflow.org/versions/r0.10/api_docs/python/io_ops.html#readers) on this.
"
4176,Why reuse default to True for loop_function() in seq2seq.rnn_decoder ?,"My question concerns the rnn_decoder() in seq2seq.

I am batchnormalizing activation maps in the loop_function(). To this end, I call the tf.contrib.layers.batch_norm() function. Like many other optionalities, it calls variables using tf.get_variable().

Ideally, you'd set reuse=None during the first call of loop_function(). That way, tf.get_variable() functions can construct variables. During second and subsequent calls of loop_function() you set reuse=True.

However, in rnn_decoder() reuse is set to True also during the first call. This doesn't allow the tf.get_variable() functions to construct variables. 

My question is: why is this so? Am I doing something wrong? Or what is the logic here?

More specifically, my loop_function is:

``` python
    def loop_function(output, i):
      ##some code##
      some_variable = tf.contrib.layers.batch_norm(some_variable,is_training= self.is_training)
      ##some code##
      return some_variable
```

I made a work-around, by defining my own rnn_decoder. I copy-paste the Tensorflow implementation and set reuse=None during the first call

``` python
def rnn_decoder_rob(decoder_inputs, initial_state, cell, loop_function=None,
                scope=None):
  REUSE=None
  with variable_scope.variable_scope(scope or ""rnn_decoder""):
    state = initial_state
    outputs = []
    prev = None
    for i, inp in enumerate(decoder_inputs):
      if loop_function is not None and prev is not None:
        with variable_scope.variable_scope(""loop_function"", reuse=REUSE):
          inp = loop_function(prev, i)
      if i > 0:
        variable_scope.get_variable_scope().reuse_variables()
        REUSE = True
      output, state = cell(inp, state)
      outputs.append(output)
      if loop_function is not None:
        prev = output
  return outputs, state
```
"
4175,Contrib metric `metric_ops.streaming_auc` type error within function,"### Environment info

Version 0.10.0rc0
Ubuntu

I'm using a `RandomForest` Estimator and am feeding it `tf.contrib.metrics.streaming_auc` and it fails.
### What other attempted solutions have you tried?

Modifying source to convert both inputs to float using `tf.to_float`. It worked, but still an issue.
### Logs or other output that would be helpful

```
Traceback (most recent call last):
  File ""main.py"", line 284, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""main.py"", line 219, in main
    eval_steps=1
  File ""/home/user/Documents/tensorflow/cross_validation/evaluate.py"", line 45, in evaluate
    metrics=metrics
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 356, in evaluate
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 630, in _evaluate_model
    eval_dict = self._get_eval_ops(features, targets, metrics)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/random_forest.py"", line 205, in _get_eval_ops
    result[name] = metric(predictions, labels)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/metrics/python/ops/metric_ops.py"", line 795, in streaming_auc
    fp_update_op) = _tp_fn_tn_fp(predictions, labels, thresholds, ignore_mask)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/metrics/python/ops/metric_ops.py"", line 667, in _tp_fn_tn_fp
    (thresh_tiled)),
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 845, in greater
    result = _op_def_lib.apply_op(""Greater"", x=x, y=y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 468, in apply_op
    inferred_from[input_arg.type_attr]))
TypeError: Input 'y' of 'Greater' Op has type float32 that does not match type int64 of argument 'x'.
```
"
4174,tf.nn.bias_add does not support multiple derivatives (using with contrib.learn),"Apparently, `tf.nn.bias_add` does not support differentiating twice: I created a neural network with `tf.contrib.layers`, and attempting to compute a Hessian matrix (actually Hessian-vector product, based on https://en.wikipedia.org/wiki/Hessian_automatic_differentiation) yields a `LookupError` saying that `BiasAddGrad` has no gradient.

From what I understand, `bias_add` is basically `add` with support for different types.

Is there any way to allow `bias_add` to be differentiated multiple times? Perhaps at the very least we could make the use of `bias_add` optional in `layers`.
"
4173,error shows when doing the command 'python -m tensorflow.models.image.mnist.convolutional' #3674,"After I install tensorflow using pip, I do the simplest command to train minst. But it shows error:

I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally Extracting data/train-images-idx3-ubyte.gz Traceback (most recent call last): File ""/usr/lib/python2.7/runpy.py"", line 162, in _run_module_as_main ""__main__"", fname, loader, pkg_name) File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals File ""/usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist/convolutional.py"", line 326, in <module> tf.app.run() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 30, in run sys.exit(main(sys.argv)) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist/convolutional.py"", line 138, in main train_data = extract_data(train_data_filename, 60000) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist/convolutional.py"", line 85, in extract_data buf = bytestream.read(IMAGE_SIZE \* IMAGE_SIZE \* num_images \* NUM_CHANNELS) File ""/usr/lib/python2.7/gzip.py"", line 261, in read self._read(readsize) File ""/usr/lib/python2.7/gzip.py"", line 308, in _read self._read_eof() File ""/usr/lib/python2.7/gzip.py"", line 347, in _read_eof hex(self.crc))) IOError: CRC check failed 0x56d16c09 != 0x39bbe345L 
"
4172,Missing files and flag for building linkable archives with `make` on Linux (Ubuntu),"Trying to build under Linux, I have faced two issues, one for PIC missing, and one for a missing file in the source build list.

I guess the origin of the problem is the `Makefile` depends on Bazel's build, and changes may be missing.

How does this looks for you? Here is a temporary [patch](https://github.com/tensorflow/tensorflow/compare/master...ic:master).
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
### Environment info

Operating System: Linux 16_04

Installed version of CUDA and cuDNN: None

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`): 461caa86137aee3d2e40843ea308c216f10c4655
2. The output of `bazel version`: It does not apply here (using `make` from the `tensor flow/contrib/makefile` project.
### If possible, provide a minimal reproducible example:

```
cd TF_HOME
./tensorflow/contrib/makefile/download_dependencies.sh
cd tensorflow/contrib/makefile/downloads/protobuf
make && make check && sudo make install && sudo ldconfig
cd -
make -f tensorflow/contrib/makefile/Makefile

# Long build
# Leading to error when building `benchmark`
```
### What other attempted solutions have you tried?

I have tried to build on Ubuntu 14_04 and Mac OS X (10.10). It worked only on Mac OS X. Then I realized that the build chokes on my Linux boxes for PIC, and then for a missing file. Adding them (see a [solution here](https://github.com/tensorflow/tensorflow/compare/master...ic:master)) fixes all.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).

The build fails with (added breaks for readability):

```
gcc --std=c++11 -DIS_SLIM_BUILD -O0 -fPIC -I. 
-I./tensorflow/contrib/makefile/downloads/ 
-I./tensorflow/contrib/makefile/downloads/eigen-eigen-9e1b48c333aa 
-I./tensorflow/contrib/makefile/gen/proto/ 
-I./tensorflow/contrib/makefile/gen/proto_text/ -I/usr/local/include 
-o ./tensorflow/contrib/makefile/gen/bin/benchmark
./tensorflow/contrib/makefile/gen/obj/tensorflow/core/util/reporter.o
./tensorflow/contrib/makefile/gen/obj/tensorflow/tools/benchmark/benchmark_model.o
./tensorflow/contrib/makefile/gen/obj/tensorflow/tools/benchmark/benchmark_model_main.o -Wl,--allow-multiple-definition -Wl,--whole-archive ./tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a -L/usr/local/lib -lstdc++ -lprotobuf -lz -lm -ldl -lpthread
./tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(conv_ops.o): In function `tensorflow::LaunchDeepConvOp<Eigen::ThreadPoolDevice, float>::Run(tensorflow::OpKernelContext*, tensorflow::Tensor const&, tensorflow::Tensor const&, int, int, int, int, int, int, int, int, int, int, int, int, int, tensorflow::Tensor*, tensorflow::TensorFormat)':
conv_ops.cc:
(.text._ZN10tensorflow16LaunchDeepConvOpIN5Eigen16ThreadPoolDeviceEfE3RunEPNS_15OpKernelContextERKNS_6TensorES8_iiiiiiiiiiiiiPS6_NS_12TensorFormatE[_ZN10tensorflow16LaunchDeepConvOpIN5Eigen16ThreadPoolDeviceEfE3RunEPNS_15OpKernelContextERKNS_6TensorES8_iiiiiiiiiiiiiPS6_NS_12TensorFormatE]+0x70): 
undefined reference to `tensorflow::CanUseDeepConv2D(int, int, int, int, int, int, int, int)'
conv_ops.cc:
(.text._ZN10tensorflow16LaunchDeepConvOpIN5Eigen16ThreadPoolDeviceEfE3RunEPNS_15OpKernelContextERKNS_6TensorES8_iiiiiiiiiiiiiPS6_NS_12TensorFormatE[_ZN10tensorflow16LaunchDeepConvOpIN5Eigen16ThreadPoolDeviceEfE3RunEPNS_15OpKernelContextERKNS_6TensorES8_iiiiiiiiiiiiiPS6_NS_12TensorFormatE]+0x184): 
undefined reference to `tensorflow::functor::DeepConv2D<Eigen::ThreadPoolDevice, float>::operator()(tensorflow::OpKernelContext*, tensorflow::Conv2DArgs const&, float const*, float const*, float*)'
collect2: error: ld returned 1 exit status
```
"
4171,distribute tensorflow read data problem,"Hi,
when I want to build a distribute model with cifar-10  (https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/image/cifar10/),
it hanged at sess.run , I think it is the input queue problem. It can not read image and label from file.
But when run with no distribute, everything is OK.
Any suggestions?

in train():
images, labels = model.distorted_inputs()

in model.py:
def distorted_inputs():
  batch_size=FLAGS.batch_size)
  if not FLAGS.data_dir:
    raise ValueError('Please supply a data_dir')
  data_dir = os.path.join(FLAGS.data_dir, '')
  return model_input.distorted_inputs(data_dir=data_dir,
                                        batch_size=FLAGS.batch_size)

in model_input.py:
def distorted_inputs(data_dir, batch_size):
  filenames = [os.path.join(data_dir, '%d.bin' % i)
               for i in xrange(1, 5)]
  for f in filenames:
    if not gfile.Exists(f):
      raise ValueError('Failed to find file: ' + f)
  filename_queue = tf.train.string_input_producer(filenames)
  read_input = read_cifar10(filename_queue)
  reshaped_image = tf.cast(read_input.uint8image, tf.float32)
  reshaped_image = tf.Print(reshaped_image, [reshaped_image], 'this is float image')
  min_fraction_of_examples_in_queue = 0.4
  min_queue_examples = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN *
                           min_fraction_of_examples_in_queue)
  print ('Filling queue with %d CIFAR images before starting to train. '
         'This will take a few minutes.' % min_queue_examples)
  return _generate_image_and_label_batch(reshaped_image, read_input.label,
                                         min_queue_examples, batch_size,test)

def _generate_image_and_label_batch(image, label, min_queue_examples,
                                    batch_size,test):
  num_preprocess_threads = 16
  images, label_batch = tf.train.shuffle_batch(
      [image, label],
      batch_size=batch_size,
      num_threads=num_preprocess_threads,
      capacity=min_queue_examples + 3 \* batch_size,
      min_after_dequeue=min_queue_examples)
  return images, tf.reshape(label_batch, [batch_size])
"
4169,"My project worked well before r0.8, but got NAN after updating to r0.10","I can not figure out what has been changed in r0.10.

After updating to r0.10, I modify some of the import to fit r0.10. But when after run ""sess.run([train_op, loss])"" for one iteration, the loss is NAN. However, all these code work well before r0.8.

And the trained model also produced different result (random precision).

Any suggestions?

The code for building all ops is:

```
    print(""Preparing GPU %d ......""%(FLAGS.gpu))
    with tf.device('/gpu:%d' % FLAGS.gpu):
        with tf.name_scope('%s' % (FLAGS.TOWER_NAME)):
            # Calculate the loss for one tower of the model. This function
            # constructs the entire model but shares the variables across
            # all towers.
            temp_istates = []
            for i in xrange(FLAGS.num_lstm):
                initial_state = tf.placeholder(tf.float32, shape=(batch_size_per_gpu/FLAGS.n_steps, 1*num_lstm_units[i]),
                                                name='train_lstm_%d_istate'%i)
                train_initial_states[initial_state] = []
                temp_istates.append(initial_state)
            train_net = model.net_class({'data':train_data}, trainable=True, TOWER_NAME=FLAGS.TOWER_NAME,
                    NUM_CLASSES=FLAGS.NUM_CLASSES, n_steps=FLAGS.n_steps,
                    initial_state=temp_istates, lstm_num_units=num_lstm_units)
            logits = train_net.get_output()
            train_net.slim_loss(logits, train_label, FLAGS.NUM_CLASSES)

            # Reuse variables for the next tower.
            tf.get_variable_scope().reuse_variables()
            losses = tf.get_collection(slim.losses.LOSSES_COLLECTION)

            # Calculate the total loss for the current tower.
            regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
            total_loss = tf.add_n(losses + regularization_losses, name='total_loss')

            # Compute the moving average of all individual losses and the total loss.
            loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')
            loss_averages_op = loss_averages.apply(losses + [total_loss])

            # Attach a scalar summary to all individual losses and the total loss; do the
            # same for the averaged version of the losses.
            for l in losses + [total_loss]:
                # Name each loss as '(raw)' and name the moving average version of the loss
                # as the original loss name.
                tf.scalar_summary(l.op.name +' (raw)', l)
                tf.scalar_summary(l.op.name, loss_averages.average(l))

            with tf.control_dependencies([loss_averages_op]):
                loss = tf.identity(total_loss)

            # Retain the Batch Normalization updates operations only from the
            # final tower. Ideally, we should grab the updates from all towers
            # but these stats accumulate extremely fast so we can ignore the
            # other stats from the other towers without significant detriment.
            batchnorm_updates = tf.get_collection(slim.ops.UPDATE_OPS_COLLECTION)

            with tf.control_dependencies([loss_averages_op]):
                # Create an optimizer that performs gradient descent.
                if FLAGS.optimizer == 'SGD':
                    opt = tf.train.GradientDescentOptimizer(lr)
                elif FLAGS.optimizer == 'Momentum':
                    opt = tf.train.MomentumOptimizer(lr, momentum=FLAGS.momentum)
                elif FLAGS.optimizer == 'Adagrad':
                    opt = tf.train.AdagradOptimizer(lr)
                elif FLAGS.optimizer == 'Adam':
                    opt = tf.train.AdamOptimizer(lr)
                elif FLAGS.optimizer == 'Ftrl':
                    opt = tf.train.FtrlOptimizer(lr)
                elif FLAGS.optimizer == 'RMSProp':
                    opt = tf.train.RMSPropOptimizer(lr, FLAGS.RMSPROP_DECAY,
                                                    momentum=FLAGS.RMSPROP_MOMENTUM,
                                                    epsilon=FLAGS.RMSPROP_EPSILON)
                else:
                    raise NotImplementedError('Optimizer %s not implemented.'%FLAGS.optimizer)
                # We must calculate the mean of each gradient. Note that this is the
                # synchronization point across all towers.
                grads = opt.compute_gradients(loss, colocate_gradients_with_ops=True)

            grads = decay_grads(grads)
            # Apply the gradients to adjust the shared variables.
            apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)

            # Add histograms for gradients.
            for grad, var in grads:
                if grad is not None:
                    tf.histogram_summary(var.op.name + '/gradients', grad)

            # Add histograms for trainable variables.
            for var in tf.trainable_variables():
                tf.histogram_summary(var.op.name, var)

            # Track the moving averages of all trainable variables.
            variable_averages = tf.train.ExponentialMovingAverage(
                FLAGS.MOVING_AVERAGE_DECAY, global_step)

            # Another possiblility is to use tf.slim.get_variables().
            variables_to_average = (tf.trainable_variables() +
                                    tf.moving_average_variables())
            variables_averages_op = variable_averages.apply(variables_to_average)

            # Group all updates to into a single train op.
            batchnorm_updates_op = tf.group(*batchnorm_updates)
            train_op = tf.group(apply_gradient_op, variables_averages_op,
                                batchnorm_updates_op)

            # Build the summary operation from the last tower summaries.
            summary_op = tf.merge_all_summaries()
```
"
4168,"Cannot find cudnn, but cudnn (5005) works with my Theano install","I just installed tensorflow from source with no errors but upon running the example from the ""get started"" page...

`$ cd tensorflow/models/image/mnist`
`$ python convolutional.py`

I receive the error that cudnn cannot be found. Here is the full output:

`I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: 
I tensorflow/stream_executor/cuda/cuda_dnn.cc:3304] Unable to load cuDNN DSO
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.
Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.
Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.
Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.7465
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 7.40GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
Initialized!
F tensorflow/stream_executor/cuda/cuda_dnn.cc:208] could not find cudnnCreate in cudnn DSO; dlerror: /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: cudnnCreate
Aborted (core dumped)`

I am not sure what to try because cudnn works with Theano on this machine, it also states that it finds the GPU.
"
4167,"how to batch learning when using input builder function, in skflow","I have got some problem in modifying wide_n_deep_tutorial.py
The problem is the same as 
http://nticformation.com/solutions_problems.php?tuto=145994&subCategory=python+tensorflow+skflow&Category=python

Sorry for my poor english. And many thanks in advance.
"
4166,"tensorboard error, segement fault core dump","Hi, i got an error when using tensorboard.

envirnoment is:
- python: 2.7
- os: docker on centos 7, 64bit
- tensorflow : 0.9.8

i ran the tensorboard on mnist example,and get log file as `/tmp/mnist_logs/` .When i ran tensorboard directily as below ,error as `segement fault, core dump`

```
tensoboard --logdir=/tmp/mnist_logs/

```

if the command changed to 

```
python /usr/lib/python2.7/site-packages/tensorflow/tensoboard/tensorboard.py --logdir=/tmp/mnist_logs/
```

error same. Can anybody help me?
"
4165,WARNING: /home/dl/.cache/bazel/_bazel_dl/5c17eaa27118632cc679170e55a5e91b/external/boringssl_git/WORKSPACE:1,"WARNING: /home/dl/.cache/bazel/_bazel_dl/5c17eaa27118632cc679170e55a5e91b/external/boringssl_git/WORKSPACE:1: Workspace name in /home/dl/.cache/bazel/_bazel_dl/5c17eaa27118632cc679170e55a5e91b/external/boringssl_git/WORKSPACE (@boringssl) does not match the name given in the repository's definition (@boringssl_git); this will cause a build error in future versions.
### Environment info

Operating System: Ubuntu 14.04

Installed version of CUDA and cuDNN: cuda7.5 cuDNN5
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
 ls -l /usr/local/cuda/lib/libcud*
-rw-r--r-- 1 root root 189170  6æœˆ 15 13:58 /usr/local/cuda/lib/libcudadevrt.a
lrwxrwxrwx 1 root root     16  6æœˆ 15 13:58 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19  6æœˆ 15 13:58 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 311596  6æœˆ 15 13:58 /usr/local/cuda/lib/libcudart.so.7.5.18
-rw-r--r-- 1 root root 558020  6æœˆ 15 13:58 /usr/local/cuda/lib/libcudart_static.a

I install tensorflow from the latest source
1. The commit hash (`git rev-parse HEAD`)
1f681d207f56fe3f2ef684873484011e8189ec05
1. The output of `bazel version`
   0.3.1
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

this warning produced when I run ./configure. I worry that this will cause some trouble if I upgrade my tensorflow someday
"
4164,Performance issue for Distributed TF,"Hi All,

We just enable distributed TF to train our model,  but we get very slow performance comparing to train on single machine. We just setup two machines one ps, one worker, and each machine is Xeon CPU with nvidia K40 GPU:
GPU 0: Quadro K420 (UUID: GPU-954fc4b7-6198-f5f9-1efb-69e73d001d5c)
GPU 1: Tesla K40c (UUID: GPU-ac243789-45bd-448f-87d9-db21c1c88c87)
GPU 2: Tesla K40c (UUID: GPU-d7b7783e-9041-e4e2-91f0-dc4188fd84e9)

If we train on one machine we can meet 200+examples/sec, but on two machines, it is just 20+ examples/sec. 10 times slower....

Our two machines are connected in 1GBits network, and we find the network bandwidth is just 40%.

Some topic discussed this issue, but I haven't got any solutions... 

One machine:
2016-09-02 09:04:35.330931: step 240, lr = 0.0001, loss = 1.59115 (107.4 examples/sec; 0.14900 sec/batch)
2016-09-02 09:04:36.968806: step 250, lr = 0.0001, loss = 1.60836 (225.6 examples/sec; 0.07091 sec/batch)
2016-09-02 09:04:39.037962: step 260, lr = 0.0001, loss = 1.56989 (212.2 examples/sec; 0.07541 sec/batch)
2016-09-02 09:04:40.491946: step 270, lr = 0.0001, loss = 1.68502 (226.8 examples/sec; 0.07056 sec/batch)
2016-09-02 09:04:42.754090: step 280, lr = 0.0001, loss = 1.65222 (214.8 examples/sec; 0.07450 sec/batch)
2016-09-02 09:04:44.510914: step 290, lr = 0.0001, loss = 1.64218 (241.8 examples/sec; 0.06617 sec/batch)
2016-09-02 09:04:47.301761: step 300, lr = 0.0001, loss = 1.64549 (219.8 examples/sec; 0.07278 sec/batch)
2016-09-02 09:04:50.331641: step 310, lr = 0.0001, loss = 1.56824 (208.1 examples/sec; 0.07689 sec/batch)
2016-09-02 09:04:52.485210: step 320, lr = 0.0001, loss = 1.49187 (226.4 examples/sec; 0.07067 sec/batch)
2016-09-02 09:04:53.878589: step 330, lr = 0.0001, loss = 1.57825 (216.5 examples/sec; 0.07389 sec/batch)
2016-09-02 09:04:55.613968: step 340, lr = 0.0001, loss = 1.57355 (227.0 examples/sec; 0.07047 sec/batch)
2016-09-02 09:04:57.887825: step 350, lr = 0.0001, loss = 1.58644 (231.2 examples/sec; 0.06920 sec/batch)
2016-09-02 09:05:00.791740: step 360, lr = 0.0001, loss = 1.57324 (224.1 examples/sec; 0.07139 sec/batch)
2016-09-02 09:05:03.385293: step 370, lr = 0.0001, loss = 1.56881 (214.0 examples/sec; 0.07476 sec/batch)
2016-09-02 09:05:05.684695: step 380, lr = 0.0001, loss = 1.57652 (223.3 examples/sec; 0.07166 sec/batch)
2016-09-02 09:05:07.671139: step 390, lr = 0.0001, loss = 1.59098 (219.5 examples/sec; 0.07288 sec/batch)
2016-09-02 09:05:09.384184: step 400, lr = 0.0001, loss = 1.57292 (222.9 examples/sec; 0.07177 sec/batch)
2016-09-02 09:05:12.028866: step 410, lr = 0.0001, loss = 1.59291 (224.0 examples/sec; 0.07144 sec/batch)
2016-09-02 09:05:13.912951: step 420, lr = 0.0001, loss = 1.56065 (206.7 examples/sec; 0.07741 sec/batch)
2016-09-02 09:05:15.567746: step 430, lr = 0.0001, loss = 1.48168 (220.1 examples/sec; 0.07270 sec/batch)
2016-09-02 09:05:17.913313: step 440, lr = 0.0001, loss = 1.52358 (220.5 examples/sec; 0.07256 sec/batch)
2016-09-02 09:05:20.215508: step 450, lr = 0.0001, loss = 1.37813 (163.3 examples/sec; 0.09800 sec/batch)
2016-09-02 09:05:21.685596: step 460, lr = 0.0001, loss = 1.66846 (237.3 examples/sec; 0.06741 sec/batch)
2016-09-02 09:05:24.225214: step 470, lr = 0.0001, loss = 1.70520 (219.6 examples/sec; 0.07287 sec/batch)
2016-09-02 09:05:26.203767: step 480, lr = 0.0001, loss = 1.58498 (234.3 examples/sec; 0.06827 sec/batch)
2016-09-02 09:05:28.258299: step 490, lr = 0.0001, loss = 1.50158 (226.4 examples/sec; 0.07067 sec/batch)

two machines:
NFO:tensorflow:Finished running Summary operation.
INFO:tensorflow:Worker 0: 2016-09-01 17:16:20.459047: step 12450, loss = 0.66(21.7 examples/sec; 0.737  sec/batch)
INFO:tensorflow:Worker 0: 2016-09-01 17:16:43.294597: step 12480, loss = 0.50(20.7 examples/sec; 0.773  sec/batch)
INFO:tensorflow:Worker 0: 2016-09-01 17:17:06.094232: step 12510, loss = 0.54(21.8 examples/sec; 0.733  sec/batch)
INFO:tensorflow:Worker 0: 2016-09-01 17:17:29.405047: step 12540, loss = 0.51(20.5 examples/sec; 0.781  sec/batch)
INFO:tensorflow:Worker 0: 2016-09-01 17:17:52.529421: step 12570, loss = 0.37(20.8 examples/sec; 0.768  sec/batch)
INFO:tensorflow:Worker 0: 2016-09-01 17:18:15.177700: step 12600, loss = 0.49(21.3 examples/sec; 0.750  sec/batch)
INFO:tensorflow:Worker 0: 2016-09-01 17:18:38.046958: step 12630, loss = 0.43(21.9 examples/sec; 0.731  sec/batch)
INFO:tensorflow:Worker 0: 2016-09-01 17:19:00.978593: step 12660, loss = 0.48(20.7 examples/sec; 0.773  sec/batch)
INFO:tensorflow:Running Summary operation on the chief.
INFO:tensorflow:Finished running Summary operation.
INFO:tensorflow:Worker 0: 2016-09-01 17:19:24.747284: step 12690, loss = 0.47(20.7 examples/sec; 0.772  sec/batch)
INFO:tensorflow:Worker 0: 2016-09-01 17:20:10.874324: step 12750, loss = 0.58(20.3 examples/sec; 0.788  sec/batch)
INFO:tensorflow:Worker 0: 2016-09-01 17:20:33.881611: step 12780, loss = 0.49(21.0 examples/sec; 0.762  sec/batch)
INFO:tensorflow:Worker 0: 2016-09-01 17:20:56.813524: step 12810, loss = 0.63(21.2 examples/sec; 0.755  sec/batch)
INFO:tensorflow:Worker 0: 2016-09-01 17:21:19.631475: step 12840, loss = 0.36(21.0 examples/sec; 0.763  sec/batch)
INFO:tensorflow:Worker 0: 2016-09-01 17:21:42.239483: step 12870, loss = 0.30(20.7 examples/sec; 0.773  sec/batch)
INFO:tensorflow:Worker 0: 2016-09-01 17:22:04.912129: step 12900, loss = 0.75(21.3 examples/sec; 0.750  sec/batch)
"
4162,contrib.layers: TypeError: optimize_loss() got an unexpected keyword argument 'summaries',"Trying to provide a custom list of summaries to `tf.contrib.layers.optimize_loss` fails 

```
>>> import tensorflow as tf
>>> tf.contrib.layers.optimize_loss(tf.constant(1), tf.constant(1), 0.01, tf.train.AdamOptimizer, summaries=['loss', 'gradients'])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: optimize_loss() got an unexpected keyword argument 'summaries'
>>> 
```

Tested on a fresh 0.10.0rc0 install, as well as in the `tensorflow/tensorflow:nightly` docker image. 

Little mystified by how this is occuring since the it's correct in source. I suspect it has something to do with everything being brought into the layers namespace. 
"
4158,Add support for NVIDIA nccl library,"The nccl library supports fast GPU-GPU communications and tensorflow could leverage its use.

https://github.com/NVIDIA/nccl

I think I read somewhere that Google is implementing this internally, but would be good to know what is the plan.
"
4155,Tensorboard: Feature and/or documentation request: Viz hyper parameter set ,"(I am not sure if this is a documentation issue or a feature request.)

Running multiple studies using different hyper parameters can be difficult when more than a handful of hyper parameters are investigated over a shorter timespan. The way we done it is to show the active hyper parameter set  in an ascii string (the log directory name). 

We already have 10 or so hyper parameters and showing them all in the directory name of the plot and comparing them is simply not a sustainable workflow. Naming the log directories the same as the input file (spec. the hyperparameters) is not so good either because the researcher have to open up the file and see / remind himself what the specific hyperparamteres where set to.

I wonder what people suggests using in this workflow? Is there a efficient workflow for this that I just have not discovered yet, or is some new feature required?
"
4154,wrapper for NVIDIA's gpu inference engine?,"does this help improving the throughput of the infence for tensorflow?
can we make wrapper for it?

https://developer.nvidia.com/gpu-inference-engine
"
4153,Why I get the slow result after Quantization tools on CPU?,"here is my benchmark info (The model is inception-2015-12-05):
I tensorflow/core/util/stat_summarizer.cc:262] 50 runs, avg 2880 ms, 1960 nodes defined 1664 nodes observed
============ Top by duration =================
  [start]  [first]    [avg]      [%]      [cdf%]          [Op]  [Name]
  348.471  199.520  162.116   5.629%      5.629%    QuantizedConv2D conv_4/Conv2D_eightbit_quantized_conv
  153.190  115.819  109.304   3.796%      9.425%    QuantizedConv2D conv_2/Conv2D_eightbit_quantized_conv
  824.866   80.364   76.637   2.661%     12.086%    QuantizedConv2D mixed_3/conv/Conv2D_eightbit_quantized_conv
   54.955   74.988   65.929   2.289%     14.375%    QuantizedConv2D conv_1/Conv2D_eightbit_quantized_conv
 1594.582   51.488   52.699   1.830%     16.205%    QuantizedConv2D mixed_10/tower_1/conv_1/Conv2D_eightbit_quantized_conv
 1464.195   48.764   48.403   1.681%     17.886%    QuantizedConv2D mixed_9/tower_1/conv_1/Conv2D_eightbit_quantized_conv
 1553.345   40.343   42.581   1.479%     19.365%    QuantizedConv2D mixed_10/tower_1/conv/Conv2D_eightbit_quantized_conv
  588.883   36.744   39.660   1.377%     20.742%    QuantizedConv2D mixed/tower/conv_1/Conv2D_eightbit_quantized_conv
 1553.350   45.938   38.929   1.352%     22.094%    QuantizedConv2D mixed_10/tower/conv/Conv2D_eightbit_quantized_conv
  672.714   33.140   35.053   1.217%     23.311%    QuantizedConv2D mixed_1/tower/conv_1/Conv2D_eightbit_quantized_conv
 1210.702   28.586   34.865   1.211%     24.521%    QuantizedConv2D mixed_7/conv/Conv2D_eightbit_quantized_conv
  757.103   37.392   33.253   1.155%     25.676%    QuantizedConv2D mixed_2/tower/conv_1/Conv2D_eightbit_quantized_conv
  986.100   28.534   33.197   1.153%     26.829%    QuantizedConv2D mixed_5/conv/Conv2D_eightbit_quantized_conv
 1210.706   33.822   32.936   1.144%     27.973%    QuantizedConv2D mixed_7/tower/conv/Conv2D_eightbit_quantized_conv
 1553.349   44.802   32.891   1.142%     29.115%    QuantizedConv2D mixed_10/conv/Conv2D_eightbit_quantized_conv
 1210.701   35.172   32.204   1.118%     30.233%    QuantizedConv2D mixed_7/tower_1/conv/Conv2D_eightbit_quantized_conv
 1090.376   37.017   31.968   1.110%     31.343%    QuantizedConv2D mixed_6/conv/Conv2D_eightbit_quantized_conv
 1213.365   32.057   31.738   1.102%     32.445%    QuantizedConv2D mixed_7/tower_2/conv/Conv2D_eightbit_quantized_conv
  849.401   44.313   31.509   1.094%     33.539%    QuantizedConv2D mixed_3/tower/conv_1/Conv2D_eightbit_quantized_conv
  589.508   34.897   30.761   1.068%     34.607%    QuantizedConv2D mixed/tower_1/conv_1/Conv2D_eightbit_quantized_conv
  908.076   26.733   30.611   1.063%     35.670%    QuantizedConv2D mixed_4/conv/Conv2D_eightbit_quantized_conv
 1093.255   34.497   29.926   1.039%     36.709%    QuantizedConv2D mixed_6/tower_2/conv/Conv2D_eightbit_quantized_conv
  988.679   27.107   29.897   1.038%     37.748%    QuantizedConv2D mixed_5/tower_2/conv/Conv2D_eightbit_quantized_conv
 1090.377   34.881   29.526   1.025%     38.773%    QuantizedConv2D mixed_6/tower/conv/Conv2D_eightbit_quantized_conv
  986.103   29.144   29.099   1.010%     39.783%    QuantizedConv2D mixed_5/tower/conv/Conv2D_eightbit_quantized_conv
  911.030   32.084   28.822   1.001%     40.784%    QuantizedConv2D mixed_4/tower_2/conv/Conv2D_eightbit_quantized_conv
  628.323   22.784   28.299   0.983%     41.767%    QuantizedConv2D mixed/tower_1/conv_2/Conv2D_eightbit_quantized_conv
  682.759   24.791   28.184   0.979%     42.745%    QuantizedConv2D mixed_1/tower_1/conv_1/Conv2D_eightbit_quantized_conv
 1090.373   37.104   27.889   0.968%     43.714%    QuantizedConv2D mixed_6/tower_1/conv/Conv2D_eightbit_quantized_conv
  986.098   28.129   27.769   0.964%     44.678%    QuantizedConv2D mixed_5/tower_1/conv/Conv2D_eightbit_quantized_conv
 1554.331   35.148   26.506   0.920%     45.599%    QuantizedConv2D mixed_10/tower_2/conv/Conv2D_eightbit_quantized_conv
 1246.348   28.499   26.234   0.911%     46.510%    QuantizedConv2D mixed_7/tower/conv_1/Conv2D_eightbit_quantized_conv
  908.072   32.080   26.203   0.910%     47.419%    QuantizedConv2D mixed_4/tower/conv/Conv2D_eightbit_quantized_conv
  274.045   35.524   25.959   0.901%     48.321%    QuantizedBatchNormWithGlobalNormalization   conv_2/batchnorm_eightbit_quantized_batch_norm
  757.224   38.495   25.621   0.890%     49.210%    QuantizedConv2D mixed_2/tower_1/conv_1/Conv2D_eightbit_quantized_conv
 1670.984   18.898   25.575   0.888%     50.099%    QuantizedMatMul softmax/logits/MatMul_eightbit_quantized_bias_add
  908.072   19.688   25.503   0.886%     50.984%    QuantizedConv2D mixed_4/tower_1/conv/Conv2D_eightbit_quantized_conv
 1431.550   31.564   25.326   0.879%     51.864%    QuantizedConv2D mixed_9/tower_1/conv/Conv2D_eightbit_quantized_conv
  798.592   22.487   25.139   0.873%     52.736%    QuantizedConv2D mixed_2/tower_1/conv_2/Conv2D_eightbit_quantized_conv
  710.682   28.952   25.041   0.870%     53.606%    QuantizedConv2D mixed_1/tower_1/conv_2/Conv2D_eightbit_quantized_conv
 1249.958   26.722   24.774   0.860%     54.466%    QuantizedConv2D mixed_7/tower_1/conv_1/Conv2D_eightbit_quantized_conv
 1276.259   24.995   24.427   0.848%     55.315%    QuantizedConv2D mixed_7/tower/conv_2/Conv2D_eightbit_quantized_conv
 1289.982   17.597   23.551   0.818%     56.132%    QuantizedConv2D mixed_7/tower_1/conv_2/Conv2D_eightbit_quantized_conv
 1431.555   18.537   22.818   0.792%     56.925%    QuantizedConv2D mixed_9/tower/conv/Conv2D_eightbit_quantized_conv
  824.860   21.110   22.180   0.770%     57.695%    QuantizedConv2D mixed_3/tower/conv/Conv2D_eightbit_quantized_conv
 1646.740   22.990   21.676   0.753%     58.448%    QuantizedConv2D mixed_10/tower_1/mixed/conv_1/Conv2D_eightbit_quantized_conv
 1646.744   22.860   21.567   0.749%     59.196%    QuantizedConv2D mixed_10/tower_1/mixed/conv/Conv2D_eightbit_quantized_conv
 1371.468   20.181   21.385   0.743%     59.939%    QuantizedConv2D mixed_8/tower/conv_1/Conv2D_eightbit_quantized_conv
 1372.601   23.081   20.983   0.729%     60.668%    QuantizedConv2D mixed_8/tower_1/conv_1/Conv2D_eightbit_quantized_conv
 1043.432   26.873   20.580   0.715%     61.382%    QuantizedConv2D mixed_5/tower/conv_2/Conv2D_eightbit_quantized_conv
 1149.722   16.327   20.291   0.705%     62.087%    QuantizedConv2D mixed_6/tower/conv_2/Conv2D_eightbit_quantized_conv
 1600.192   25.630   19.435   0.675%     62.762%    QuantizedConv2D mixed_10/tower/mixed/conv/Conv2D_eightbit_quantized_conv
 1017.888   21.327   19.281   0.670%     63.431%    QuantizedConv2D mixed_5/tower/conv_1/Conv2D_eightbit_quantized_conv
 1308.978   18.376   19.117   0.664%     64.095%    QuantizedConv2D mixed_7/tower_1/conv_3/Conv2D_eightbit_quantized_conv
 1128.559   19.422   18.884   0.656%     64.751%    QuantizedConv2D mixed_6/tower/conv_1/Conv2D_eightbit_quantized_conv
 1513.946   38.340   18.755   0.651%     65.402%    QuantizedConv2D mixed_9/tower_1/mixed/conv/Conv2D_eightbit_quantized_conv
 1600.189   29.234   18.674   0.648%     66.050%    QuantizedConv2D mixed_10/tower/mixed/conv_1/Conv2D_eightbit_quantized_conv
 1397.119   19.557   18.436   0.640%     66.691%    QuantizedConv2D mixed_8/tower_1/conv_2/Conv2D_eightbit_quantized_conv
 1015.906   20.308   18.426   0.640%     67.330%    QuantizedConv2D mixed_5/tower_1/conv_1/Conv2D_eightbit_quantized_conv
 1135.983   14.258   18.416   0.639%     67.970%    QuantizedConv2D mixed_6/tower_1/conv_1/Conv2D_eightbit_quantized_conv
 1450.805   29.566   18.226   0.633%     68.603%    QuantizedConv2D mixed_9/tower/mixed/conv_1/Conv2D_eightbit_quantized_conv
 1328.743   22.393   18.113   0.629%     69.232%    QuantizedConv2D mixed_7/tower_1/conv_4/Conv2D_eightbit_quantized_conv
 1450.809   28.226   17.926   0.622%     69.854%    QuantizedConv2D mixed_9/tower/mixed/conv/Conv2D_eightbit_quantized_conv
  743.108   12.778   17.708   0.615%     70.469%    QuantizedConv2D mixed_2/conv/Conv2D_eightbit_quantized_conv
  551.000   18.514   17.690   0.614%     71.083%    QuantizedBatchNormWithGlobalNormalization   conv_4/batchnorm_eightbit_quantized_batch_norm
  743.106   11.945   17.686   0.614%     71.698%    QuantizedConv2D mixed_2/tower_1/conv/Conv2D_eightbit_quantized_conv
 1158.867   17.448   17.553   0.610%     72.307%    QuantizedConv2D mixed_6/tower_1/conv_2/Conv2D_eightbit_quantized_conv
 1431.553   15.412   17.497   0.608%     72.915%    QuantizedConv2D mixed_9/conv/Conv2D_eightbit_quantized_conv
  654.795   18.570   17.457   0.606%     73.521%    QuantizedConv2D mixed_1/tower_1/conv/Conv2D_eightbit_quantized_conv
 1513.932   16.375   17.418   0.605%     74.126%    QuantizedConv2D mixed_9/tower_1/mixed/conv_1/Conv2D_eightbit_quantized_conv
  654.796   16.350   16.927   0.588%     74.713%    QuantizedConv2D mixed_1/conv/Conv2D_eightbit_quantized_conv
 1037.406   21.229   16.914   0.587%     75.301%    QuantizedConv2D mixed_5/tower_1/conv_2/Conv2D_eightbit_quantized_conv
  743.109   12.459   16.895   0.587%     75.887%    QuantizedConv2D mixed_2/tower/conv/Conv2D_eightbit_quantized_conv
 1352.871   17.220   16.892   0.587%     76.474%    QuantizedConv2D mixed_8/tower/conv/Conv2D_eightbit_quantized_conv
 1352.867   18.301   16.870   0.586%     77.060%    QuantizedConv2D mixed_8/tower_1/conv/Conv2D_eightbit_quantized_conv
 1432.532   22.308   16.561   0.575%     77.635%    QuantizedConv2D mixed_9/tower_2/conv/Conv2D_eightbit_quantized_conv
  952.475   12.333   16.346   0.568%     78.202%    QuantizedConv2D mixed_4/tower/conv_2/Conv2D_eightbit_quantized_conv
  654.800   16.098   16.232   0.564%     78.766%    QuantizedConv2D mixed_1/tower/conv/Conv2D_eightbit_quantized_conv
 1191.637   17.268   15.125   0.525%     79.291%    QuantizedConv2D mixed_6/tower_1/conv_4/Conv2D_eightbit_quantized_conv
 1073.325   15.302   14.567   0.506%     79.797%    QuantizedConv2D mixed_5/tower_1/conv_4/Conv2D_eightbit_quantized_conv
   27.923   23.420   14.489   0.503%     80.300%    QuantizedBatchNormWithGlobalNormalization   conv/batchnorm_eightbit_quantized_batch_norm
  658.731   11.937   13.903   0.483%     80.783%    QuantizedConv2D mixed_1/tower_2/conv/Conv2D_eightbit_quantized_conv
  757.306   25.541   13.874   0.482%     81.265%    QuantizedConv2D mixed_2/tower_2/conv/Conv2D_eightbit_quantized_conv
 1177.505   12.944   13.679   0.475%     81.740%    QuantizedConv2D mixed_6/tower_1/conv_3/Conv2D_eightbit_quantized_conv
  574.255   13.735   13.379   0.465%     82.204%    QuantizedConv2D mixed/conv/Conv2D_eightbit_quantized_conv
  574.230   11.661   13.310   0.462%     82.667%    QuantizedConv2D mixed/tower_1/conv/Conv2D_eightbit_quantized_conv
 1060.333   11.805   13.287   0.461%     83.128%    QuantizedConv2D mixed_5/tower_1/conv_3/Conv2D_eightbit_quantized_conv
  132.755   17.705   13.125   0.456%     83.584%    QuantizedBatchNormWithGlobalNormalization   conv_1/batchnorm_eightbit_quantized_batch_norm
  971.865   12.488   12.591   0.437%     84.021%    QuantizedConv2D mixed_4/tower_1/conv_4/Conv2D_eightbit_quantized_conv
  574.298   12.449   12.557   0.436%     84.457%    QuantizedConv2D mixed/tower/conv/Conv2D_eightbit_quantized_conv
  946.591   15.766   12.203   0.424%     84.881%    QuantizedConv2D mixed_4/tower_1/conv_2/Conv2D_eightbit_quantized_conv
  941.124   10.019   12.132   0.421%     85.302%    QuantizedConv2D mixed_4/tower/conv_1/Conv2D_eightbit_quantized_conv
  930.408   14.499   12.010   0.417%     85.719%    QuantizedConv2D mixed_4/tower_1/conv_1/Conv2D_eightbit_quantized_conv
 1418.545   12.330   10.408   0.361%     86.080%    QuantizedConv2D mixed_8/tower_1/conv_3/Conv2D_eightbit_quantized_conv
  319.410   10.155    9.309   0.323%     86.404%    QuantizedConv2D conv_3/Conv2D_eightbit_quantized_conv
  963.651    7.257    8.809   0.306%     86.709%    QuantizedConv2D mixed_4/tower_1/conv_3/Conv2D_eightbit_quantized_conv
  331.914   14.339    8.533   0.296%     87.006%    QuantizedBatchNormWithGlobalNormalization   conv_3/batchnorm_eightbit_quantized_batch_norm
    7.234   16.748    8.518   0.296%     87.302%    QuantizedConv2D conv/Conv2D_eightbit_quantized_conv
  578.657    7.823    8.201   0.285%     87.586%    QuantizedConv2D mixed/tower_2/conv/Conv2D_eightbit_quantized_conv
  590.466    8.945    7.803   0.271%     87.857%    QuantizeDownAndShrinkRange  mixed/conv/batchnorm_eightbit_quantize_down
  758.138   11.852    7.449   0.259%     88.116%    QuantizeDownAndShrinkRange  mixed_2/conv/batchnorm_eightbit_quantize_down
  896.657    7.011    7.431   0.258%     88.374%    QuantizedConv2D mixed_3/tower/conv_2/Conv2D_eightbit_quantized_conv
  654.795    3.927    5.455   0.189%     88.563%    QuantizedAvgPool    mixed_1/tower_2/pool_eightbit_quantized
  743.107   14.192    5.226   0.181%     88.745%    QuantizedAvgPool    mixed_2/tower_2/pool_eightbit_quantized
  784.526    0.270    5.046   0.175%     88.920%    QuantizeDownAndShrinkRange  mixed_2/tower_2/conv/batchnorm_eightbit_quantize_down
  673.545    0.263    4.586   0.159%     89.079%    QuantizeDownAndShrinkRange  mixed_1/conv/batchnorm_eightbit_quantize_down
  845.982    0.646    4.385   0.152%     89.232%    QuantizeDownAndShrinkRange  mixed_3/tower/conv/Conv2D_eightbit_quantize_down
  673.023    0.279    4.126   0.143%     89.375%    QuantizeDownAndShrinkRange  mixed_1/tower_2/conv/batchnorm_eightbit_quantize_down
  269.022    5.018    4.039   0.140%     89.515%    QuantizeDownAndShrinkRange  conv_2/Conv2D_eightbit_quantize_down
  574.249    4.398    3.825   0.133%     89.648%    QuantizedAvgPool    mixed/tower_2/pool_eightbit_quantized
 1127.402    0.210    3.734   0.130%     89.778%    QuantizeDownAndShrinkRange  mixed_6/conv/Conv2D_eightbit_quantize_down
  309.582    4.882    3.708   0.129%     89.906%    QuantizeDownAndShrinkRange  conv_2/batchnorm_eightbit_quantize_down
 1246.651    0.211    3.177   0.110%     90.017%    QuantizeDownAndShrinkRange  mixed_7/tower_2/conv/batchnorm_eightbit_quantize_down
 1239.296    2.590    3.160   0.110%     90.126%    QuantizeDownAndShrinkRange  mixed_7/conv/Conv2D_eightbit_quantize_down
 1015.791    0.184    3.154   0.110%     90.236%    QuantizeDownAndShrinkRange  mixed_5/tower_2/conv/Conv2D_eightbit_quantize_down
  589.067    0.368    3.109   0.108%     90.344%    QuantizeDownAndShrinkRange  mixed/tower_1/conv/batchnorm_eightbit_quantize_down
 1245.428    0.211    3.066   0.106%     90.450%    QuantizeDownAndShrinkRange  mixed_7/tower_2/conv/Conv2D_eightbit_quantize_down
 1243.366    0.220    2.937   0.102%     90.552%    QuantizeDownAndShrinkRange  mixed_7/conv/batchnorm_eightbit_quantize_down
  314.719    4.671    2.925   0.102%     90.654%    QuantizedMaxPool    pool_eightbit_quantized
  548.004    2.990    2.676   0.093%     90.747%    QuantizeDownAndShrinkRange  conv_4/Conv2D_eightbit_quantize_down
  986.100    2.571    2.643   0.092%     90.839%    QuantizedAvgPool    mixed_5/tower_2/pool_eightbit_quantized
  624.922    2.845    2.627   0.091%     90.930%    QuantizedBatchNormWithGlobalNormalization   mixed/tower_1/conv_1/batchnorm_eightbit_quantized_batch_norm
 1210.698    2.659    2.564   0.089%     91.019%    QuantizedAvgPool    mixed_7/tower_2/pool_eightbit_quantized
  569.522    2.728    2.518   0.087%     91.106%    QuantizeDownAndShrinkRange  conv_4/batchnorm_eightbit_quantize_down
 1014.642    0.200    2.483   0.086%     91.192%    QuantizeDownAndShrinkRange  mixed_5/conv/Conv2D_eightbit_quantize_down
  908.075    2.947    2.448   0.085%     91.277%    QuantizedAvgPool    mixed_4/tower_2/pool_eightbit_quantized
   24.010    3.897    2.405   0.084%     91.361%    QuantizeDownAndShrinkRange  conv/Conv2D_eightbit_quantize_down
 1090.373    2.874    2.401   0.083%     91.444%    QuantizedAvgPool    mixed_6/tower_2/pool_eightbit_quantized
  794.763    1.403    2.383   0.083%     91.527%    QuantizedBatchNormWithGlobalNormalization   mixed_2/tower/conv_1/batchnorm_eightbit_quantized_batch_norm
 1371.251    0.178    2.374   0.082%     91.610%    QuantizeDownAndShrinkRange  mixed_8/tower/conv/batchnorm_eightbit_quantize_down
  651.526    2.474    2.333   0.081%     91.691%    QuantizedBatchNormWithGlobalNormalization   mixed/tower_1/conv_2/batchnorm_eightbit_quantized_batch_norm
  707.929    2.311    2.318   0.081%     91.771%    QuantizedBatchNormWithGlobalNormalization   mixed_1/tower_1/conv_1/batchnorm_eightbit_quantized_batch_norm
 1129.115    5.158    2.287   0.079%     91.850%    QuantizeDownAndShrinkRange  mixed_6/conv/batchnorm_eightbit_quantize_down
  939.397    0.211    2.285   0.079%     91.930%    QuantizeDownAndShrinkRange  mixed_4/conv/batchnorm_eightbit_quantize_down
   51.354    3.404    2.267   0.079%     92.009%    QuantizeDownAndShrinkRange  conv/batchnorm_eightbit_quantize_down
  676.107    6.540    2.243   0.078%     92.086%    QuantizeDownAndShrinkRange  mixed_1/tower_1/conv/batchnorm_eightbit_quantize_down
  894.118    2.012    2.242   0.078%     92.164%    QuantizedBatchNormWithGlobalNormalization   mixed_3/tower/conv_1/batchnorm_eightbit_quantized_batch_norm
  947.078    3.976    2.233   0.078%     92.242%    QuantizeDownAndShrinkRange  mixed_4/tower_2/conv/batchnorm_eightbit_quantize_down
  796.073    2.036    2.229   0.077%     92.319%    QuantizedBatchNormWithGlobalNormalization   mixed_2/tower_1/conv_1/batchnorm_eightbit_quantized_batch_norm
  755.892    0.239    2.216   0.077%     92.396%    QuantizeDownAndShrinkRange  mixed_2/conv/Conv2D_eightbit_quantize_down
 1016.312    0.250    2.215   0.077%     92.473%    QuantizeDownAndShrinkRange  mixed_5/conv/batchnorm_eightbit_quantize_down
  707.511    0.252    2.187   0.076%     92.549%    QuantizeDownAndShrinkRange  mixed_1/tower/conv_1/batchnorm_eightbit_quantize_down
 1129.453    4.033    2.144   0.074%     92.624%    QuantizeDownAndShrinkRange  mixed_6/tower_2/conv/batchnorm_eightbit_quantize_down
  670.963    2.054    2.113   0.073%     92.697%    QuantizedBatchNormWithGlobalNormalization   mixed_1/tower_2/conv/batchnorm_eightbit_quantized_batch_norm
  588.324    2.136    2.103   0.073%     92.770%    QuantizedBatchNormWithGlobalNormalization   mixed/conv/batchnorm_eightbit_quantized_batch_norm
  756.135    1.997    2.080   0.072%     92.842%    QuantizedBatchNormWithGlobalNormalization   mixed_2/conv/batchnorm_eightbit_quantized_batch_norm
  756.893    0.281    2.072   0.072%     92.914%    QuantizeDownAndShrinkRange  mixed_2/tower_1/conv/batchnorm_eightbit_quantize_down
  740.059    2.084    2.069   0.072%     92.986%    QuantizedBatchNormWithGlobalNormalization   mixed_1/tower_1/conv_2/batchnorm_eightbit_quantized_batch_norm
  821.437    2.547    2.067   0.072%     93.058%    QuantizedBatchNormWithGlobalNormalization   mixed_2/tower_1/conv_2/batchnorm_eightbit_quantized_batch_norm
 1017.169    0.408    2.062   0.072%     93.129%    QuantizeDownAndShrinkRange  mixed_5/tower_2/conv/batchnorm_eightbit_quantize_down
  625.635    0.293    2.044   0.071%     93.200%    QuantizeDownAndShrinkRange  mixed/tower/conv_1/Conv2D_eightbit_quantize_down
  129.959    2.789    2.007   0.070%     93.270%    QuantizeDownAndShrinkRange  conv_1/Conv2D_eightbit_quantize_down
  905.567    1.910    1.991   0.069%     93.339%    QuantizedBatchNormWithGlobalNormalization   mixed_3/conv/batchnorm_eightbit_quantized_batch_norm
 1301.262    0.225    1.967   0.068%     93.407%    QuantizeDownAndShrinkRange  mixed_7/tower/conv_2/Conv2D_eightbit_quantize_down
  846.632    2.042    1.963   0.068%     93.476%    QuantizedBatchNormWithGlobalNormalization   mixed_3/tower/conv/batchnorm_eightbit_quantized_batch_norm
  625.932    1.353    1.955   0.068%     93.543%    QuantizedBatchNormWithGlobalNormalization   mixed/tower/conv_1/batchnorm_eightbit_quantized_batch_norm
  150.469    2.571    1.954   0.068%     93.611%    QuantizeDownAndShrinkRange  conv_1/batchnorm_eightbit_quantize_down
  755.340    1.549    1.951   0.068%     93.679%    QuantizedBatchNormWithGlobalNormalization   mixed_2/tower_1/conv/batchnorm_eightbit_quantized_batch_norm
  848.680    0.633    1.913   0.066%     93.745%    QuantizeDownAndShrinkRange  mixed_3/tower/conv/batchnorm_eightbit_quantize_down
  673.656    2.444    1.904   0.066%     93.812%    QuantizedBatchNormWithGlobalNormalization   mixed_1/tower_1/conv/batchnorm_eightbit_quantized_batch_norm
 1276.689    9.140    1.887   0.066%     93.877%    QuantizeDownAndShrinkRange  mixed_7/tower_1/conv_1/Conv2D_eightbit_quantize_down
  586.394    2.666    1.884   0.065%     93.943%    QuantizedBatchNormWithGlobalNormalization   mixed/tower_1/conv/batchnorm_eightbit_quantized_batch_norm
  572.418    1.798    1.769   0.061%     94.004%    QuantizedMaxPool    pool_1_eightbit_quantized
  783.151    1.370    1.756   0.061%     94.065%    QuantizedBatchNormWithGlobalNormalization   mixed_2/tower_2/conv/batchnorm_eightbit_quantized_batch_norm
  671.424    2.115    1.747   0.061%     94.126%    QuantizedBatchNormWithGlobalNormalization   mixed_1/conv/batchnorm_eightbit_quantized_batch_norm
 1127.757    0.201    1.734   0.060%     94.186%    QuantizeDownAndShrinkRange  mixed_6/tower_2/conv/Conv2D_eightbit_quantize_down
  706.134    1.372    1.714   0.060%     94.245%    QuantizedBatchNormWithGlobalNormalization   mixed_1/tower/conv_1/batchnorm_eightbit_quantized_batch_norm
  934.819    3.298    1.690   0.059%     94.304%    QuantizeDownAndShrinkRange  mixed_4/conv/Conv2D_eightbit_quantize_down
  945.612    1.458    1.690   0.059%     94.363%    QuantizedBatchNormWithGlobalNormalization   mixed_4/tower_2/conv/batchnorm_eightbit_quantized_batch_norm
  586.995    1.576    1.690   0.059%     94.421%    QuantizedBatchNormWithGlobalNormalization   mixed/tower/conv/batchnorm_eightbit_quantized_batch_norm
 1241.891    1.468    1.645   0.057%     94.479%    QuantizedBatchNormWithGlobalNormalization   mixed_7/conv/batchnorm_eightbit_quantized_batch_norm
 1244.535    0.188    1.629   0.057%     94.535%    QuantizeDownAndShrinkRange  mixed_7/tower/conv/Conv2D_eightbit_quantize_down
  794.502    0.257    1.624   0.056%     94.592%    QuantizeDownAndShrinkRange  mixed_2/tower/conv_1/Conv2D_eightbit_quantize_down
 1247.636    2.254    1.475   0.051%     94.643%    QuantizeDownAndShrinkRange  mixed_7/tower_1/conv/batchnorm_eightbit_quantize_down
  782.853    0.294    1.471   0.051%     94.694%    QuantizeDownAndShrinkRange  mixed_2/tower_2/conv/Conv2D_eightbit_quantize_down
  796.173    0.242    1.458   0.051%     94.744%    QuantizeDownAndShrinkRange  mixed_2/tower/conv_1/batchnorm_eightbit_quantize_down
 1014.845    1.460    1.450   0.050%     94.795%    QuantizedBatchNormWithGlobalNormalization   mixed_5/conv/batchnorm_eightbit_quantized_batch_norm
 1245.882    0.292    1.438   0.050%     94.845%    QuantizeDownAndShrinkRange  mixed_7/tower_1/conv/Conv2D_eightbit_quantize_down
 1245.641    1.005    1.438   0.050%     94.895%    QuantizedBatchNormWithGlobalNormalization   mixed_7/tower_2/conv/batchnorm_eightbit_quantized_batch_norm
  329.577    2.330    1.415   0.049%     94.944%    QuantizeDownAndShrinkRange  conv_3/Conv2D_eightbit_quantize_down
  755.816    1.034    1.409   0.049%     94.993%    QuantizedBatchNormWithGlobalNormalization   mixed_2/tower/conv/batchnorm_eightbit_quantized_batch_norm
 1285.835    1.461    1.408   0.049%     95.042%    QuantizedBatchNormWithGlobalNormalization   mixed_7/tower_1/conv_1/batchnorm_eightbit_quantized_batch_norm
  943.121    2.486    1.391   0.048%     95.090%    QuantizeDownAndShrinkRange  mixed_4/tower_2/conv/Conv2D_eightbit_quantize_down
  671.166    1.264    1.389   0.048%     95.138%    QuantizedBatchNormWithGlobalNormalization   mixed_1/tower/conv/batchnorm_eightbit_quantized_batch_norm
 1015.256    0.178    1.369   0.048%     95.186%    QuantizeDownAndShrinkRange  mixed_5/tower/conv/Conv2D_eightbit_quantize_down
  938.122    1.264    1.356   0.047%     95.233%    QuantizedBatchNormWithGlobalNormalization   mixed_4/conv/batchnorm_eightbit_quantized_batch_norm
 1015.978    1.184    1.350   0.047%     95.280%    QuantizedBatchNormWithGlobalNormalization   mixed_5/tower_2/conv/batchnorm_eightbit_quantized_batch_norm
  627.290    0.280    1.347   0.047%     95.326%    QuantizeDownAndShrinkRange  mixed/tower/conv_1/batchnorm_eightbit_quantize_down
  705.870    0.261    1.347   0.047%     95.373%    QuantizeDownAndShrinkRange  mixed_1/tower/conv_1/Conv2D_eightbit_quantize_down
 1015.665    0.182    1.340   0.047%     95.420%    QuantizeDownAndShrinkRange  mixed_5/tower_1/conv/batchnorm_eightbit_quantize_down
 1036.229    0.172    1.338   0.046%     95.466%    QuantizeDownAndShrinkRange  mixed_5/tower_1/conv_1/Conv2D_eightbit_quantize_down
 1127.962    1.485    1.338   0.046%     95.513%    QuantizedBatchNormWithGlobalNormalization   mixed_6/tower_2/conv/batchnorm_eightbit_quantized_batch_norm
 1127.616    1.493    1.329   0.046%     95.559%    QuantizedBatchNormWithGlobalNormalization   mixed_6/conv/batchnorm_eightbit_quantized_batch_norm
 1301.492    1.461    1.324   0.046%     95.605%    QuantizedBatchNormWithGlobalNormalization   mixed_7/tower/conv_2/batchnorm_eightbit_quantized_batch_norm
  346.260    2.064    1.319   0.046%     95.651%    QuantizeDownAndShrinkRange  conv_3/batchnorm_eightbit_quantize_down
 1275.068    0.957    1.314   0.046%     95.696%    QuantizedBatchNormWithGlobalNormalization   mixed_7/tower/conv_1/batchnorm_eightbit_quantized_batch_norm
  755.057    0.280    1.281   0.044%     95.741%    QuantizeDownAndShrinkRange  mixed_2/tower_1/conv/Conv2D_eightbit_quantize_down
  673.372    0.281    1.278   0.044%     95.785%    QuantizeDownAndShrinkRange  mixed_1/tower_1/conv/Conv2D_eightbit_quantize_down
  967.976    1.451    1.244   0.043%     95.828%    QuantizedBatchNormWithGlobalNormalization   mixed_4/tower/conv_2/batchnorm_eightbit_quantized_batch_norm
 1244.726    1.385    1.237   0.043%     95.871%    QuantizedBatchNormWithGlobalNormalization   mixed_7/tower/conv/batchnorm_eightbit_quantized_batch_norm
 1246.178    1.451    1.234   0.043%     95.914%    QuantizedBatchNormWithGlobalNormalization   mixed_7/tower_1/conv/batchnorm_eightbit_quantized_batch_norm
 1176.319    0.165    1.228   0.043%     95.957%    QuantizeDownAndShrinkRange  mixed_6/tower_1/conv_2/Conv2D_eightbit_quantize_down
  671.153    0.267    1.220   0.042%     95.999%    QuantizeDownAndShrinkRange  mixed_1/conv/Conv2D_eightbit_quantize_down
 1166.058    0.416    1.216   0.042%     96.041%    QuantizeDownAndShrinkRange  mixed_6/tower/conv_2/Conv2D_eightbit_quantize_down
 1166.479    1.325    1.195   0.041%     96.083%    QuantizedBatchNormWithGlobalNormalization   mixed_6/tower/conv_2/batchnorm_eightbit_quantized_batch_norm
  927.772    1.473    1.193   0.041%     96.124%    QuantizeDownAndShrinkRange  mixed_4/tower_1/conv/Conv2D_eightbit_quantize_down
 1070.499    0.956    1.190   0.041%     96.166%    QuantizedBatchNormWithGlobalNormalization   mixed_5/tower/conv_2/batchnorm_eightbit_quantized_batch_norm
  586.682    1.051    1.169   0.041%     96.206%    QuantizedBatchNormWithGlobalNormalization   mixed/tower_2/conv/batchnorm_eightbit_quantized_batch_norm
 1127.483    0.175    1.164   0.040%     96.247%    QuantizeDownAndShrinkRange  mixed_6/tower_1/conv/Conv2D_eightbit_quantize_down
 1039.225    0.279    1.148   0.040%     96.286%    QuantizeDownAndShrinkRange  mixed_5/tower/conv_1/Conv2D_eightbit_quantize_down
 1370.289    0.956    1.129   0.039%     96.326%    QuantizedBatchNormWithGlobalNormalization   mixed_8/tower/conv/batchnorm_eightbit_quantized_batch_norm
  824.866    1.453    1.122   0.039%     96.365%    QuantizedMaxPool    mixed_3/pool_eightbit_quantized
 1128.346    0.157    1.109   0.039%     96.403%    QuantizeDownAndShrinkRange  mixed_6/tower/conv/batchnorm_eightbit_quantize_down
 1125.273    2.229    1.099   0.038%     96.441%    QuantizeDownAndShrinkRange  mixed_6/tower/conv/Conv2D_eightbit_quantize_down
  588.578    0.230    1.098   0.038%     96.479%    QuantizeDownAndShrinkRange  mixed/tower/conv/batchnorm_eightbit_quantize_down
  587.738    0.192    1.096   0.038%     96.517%    QuantizeDownAndShrinkRange  mixed/tower_2/conv/batchnorm_eightbit_quantize_down
 1036.406    0.799    1.094   0.038%     96.555%    QuantizedBatchNormWithGlobalNormalization   mixed_5/tower_1/conv_1/batchnorm_eightbit_quantized_batch_norm
 1148.374    1.069    1.091   0.038%     96.593%    QuantizedBatchNormWithGlobalNormalization   mixed_6/tower/conv_1/batchnorm_eightbit_quantized_batch_norm
 1371.398    0.965    1.091   0.038%     96.631%    QuantizedBatchNormWithGlobalNormalization   mixed_8/tower_1/conv/batchnorm_eightbit_quantized_batch_norm
 1287.305    2.608    1.086   0.038%     96.669%    QuantizeDownAndShrinkRange  mixed_7/tower_1/conv_1/batchnorm_eightbit_quantize_down
  672.436    0.234    1.086   0.038%     96.707%    QuantizeDownAndShrinkRange  mixed_1/tower/conv/batchnorm_eightbit_quantize_down
 1014.450    1.208    1.083   0.038%     96.744%    QuantizedBatchNormWithGlobalNormalization   mixed_5/tower_1/conv/batchnorm_eightbit_quantized_batch_norm
 1395.893    0.969    1.073   0.037%     96.782%    QuantizedBatchNormWithGlobalNormalization   mixed_8/tower_1/conv_1/batchnorm_eightbit_quantized_batch_norm
 1307.784    0.954    1.062   0.037%     96.818%    QuantizedBatchNormWithGlobalNormalization   mixed_7/tower_1/conv_2/batchnorm_eightbit_quantized_batch_norm
 1015.437    1.117    1.052   0.037%     96.855%    QuantizedBatchNormWithGlobalNormalization   mixed_5/tower/conv/batchnorm_eightbit_quantized_batch_norm
 1127.663    1.496    1.031   0.036%     96.891%    QuantizedBatchNormWithGlobalNormalization   mixed_6/tower_1/conv/batchnorm_eightbit_quantized_batch_norm
  944.914    0.606    1.026   0.036%     96.926%    QuantizeDownAndShrinkRange  mixed_4/tower_1/conv_1/Conv2D_eightbit_quantize_down
 1416.925    1.304    1.026   0.036%     96.962%    QuantizedBatchNormWithGlobalNormalization   mixed_8/tower_1/conv_2/batchnorm_eightbit_quantized_batch_norm
 1351.363    0.955    1.024   0.036%     96.997%    QuantizedBatchNormWithGlobalNormalization   mixed_7/tower_1/conv_4/batchnorm_eightbit_quantized_batch_norm
 1070.312    0.183    1.023   0.036%     97.033%    QuantizeDownAndShrinkRange  mixed_5/tower/conv_2/Conv2D_eightbit_quantize_down
 1327.551    0.954    1.015   0.035%     97.068%    QuantizedBatchNormWithGlobalNormalization   mixed_7/tower_1/conv_3/batchnorm_eightbit_quantized_batch_norm
 1039.509    1.049    1.012   0.035%     97.103%    QuantizedBatchNormWithGlobalNormalization   mixed_5/tower/conv_1/batchnorm_eightbit_quantized_batch_norm
  670.675    0.284    1.010   0.035%     97.138%    QuantizeDownAndShrinkRange  mixed_1/tower_2/conv/Conv2D_eightbit_quantize_down
 1127.506    0.836    1.006   0.035%     97.173%    QuantizedBatchNormWithGlobalNormalization   mixed_6/tower/conv/batchnorm_eightbit_quantized_batch_norm
  964.815    3.157    1.006   0.035%     97.208%    QuantizeDownAndShrinkRange  mixed_4/tower/conv_2/Conv2D_eightbit_quantize_down
  587.998    0.322    0.999   0.035%     97.243%    QuantizeDownAndShrinkRange  mixed/conv/Conv2D_eightbit_quantize_down
 1209.109    0.959    0.997   0.035%     97.278%    QuantizedBatchNormWithGlobalNormalization   mixed_6/tower_1/conv_4/batchnorm_eightbit_quantized_batch_norm
  929.251    0.883    0.995   0.035%     97.312%    QuantizedBatchNormWithGlobalNormalization   mixed_4/tower_1/conv/batchnorm_eightbit_quantized_batch_norm
 1088.848    0.952    0.993   0.034%     97.347%    QuantizedBatchNormWithGlobalNormalization   mixed_5/tower_1/conv_4/batchnorm_eightbit_quantized_batch_norm
 1391.652    0.090    0.992   0.034%     97.381%    QuantizeDownAndShrinkRange  mixed_8/tower/conv_1/Conv2D_eightbit_quantize_down
 1246.118    0.192    0.975   0.034%     97.415%    QuantizeDownAndShrinkRange  mixed_7/tower/conv/batchnorm_eightbit_quantize_down
  984.569    0.956    0.959   0.033%     97.448%    QuantizedBatchNormWithGlobalNormalization   mixed_4/tower_1/conv_4/batchnorm_eightbit_quantized_batch_norm
 1157.845    0.806    0.957   0.033%     97.482%    QuantizedBatchNormWithGlobalNormalization   mixed_6/tower_1/conv_1/batchnorm_eightbit_quantized_batch_norm
 1150.248    7.594    0.951   0.033%     97.515%    QuantizeDownAndShrinkRange  mixed_6/tower_1/conv_1/Conv2D_eightbit_quantize_down
 1058.866    1.213    0.949   0.033%     97.547%    QuantizedBatchNormWithGlobalNormalization   mixed_5/tower_1/conv_2/batchnorm_eightbit_quantized_batch_norm
 1274.860    0.205    0.943   0.033%     97.580%    QuantizeDownAndShrinkRange  mixed_7/tower/conv_1/Conv2D_eightbit_quantize_down
 1040.567    2.812    0.930   0.032%     97.613%    QuantizeDownAndShrinkRange  mixed_5/tower/conv_1/batchnorm_eightbit_quantize_down
 1148.006    0.364    0.925   0.032%     97.645%    QuantizeDownAndShrinkRange  mixed_6/tower/conv_1/Conv2D_eightbit_quantize_down
 1129.167    6.752    0.919   0.032%     97.677%    QuantizeDownAndShrinkRange  mixed_6/tower_1/conv/batchnorm_eightbit_quantize_down
 1014.241    0.205    0.912   0.032%     97.708%    QuantizeDownAndShrinkRange  mixed_5/tower_1/conv/Conv2D_eightbit_quantize_down
 1176.487    0.796    0.902   0.031%     97.740%    QuantizedBatchNormWithGlobalNormalization   mixed_6/tower_1/conv_2/batchnorm_eightbit_quantized_batch_norm
  969.433    0.211    0.894   0.031%     97.771%    QuantizeDownAndShrinkRange  mixed_4/tower/conv_2/batchnorm_eightbit_quantize_down
 1071.460    0.184    0.863   0.030%     97.801%    QuantizeDownAndShrinkRange  mixed_5/tower/conv_2/batchnorm_eightbit_quantize_down
  940.305    0.641    0.854   0.030%     97.830%    QuantizedBatchNormWithGlobalNormalization   mixed_4/tower/conv/batchnorm_eightbit_quantized_batch_norm
 1190.636    0.795    0.853   0.030%     97.860%    QuantizedBatchNormWithGlobalNormalization   mixed_6/tower_1/conv_3/batchnorm_eightbit_quantized_batch_norm
 1058.651    0.211    0.833   0.029%     97.889%    QuantizeDownAndShrinkRange  mixed_5/tower_1/conv_2/Conv2D_eightbit_quantize_down
 1149.454    0.211    0.827   0.029%     97.917%    QuantizeDownAndShrinkRange  mixed_6/tower/conv_1/batchnorm_eightbit_quantize_down
  951.307    0.975    0.824   0.029%     97.946%    QuantizedBatchNormWithGlobalNormalization   mixed_4/tower/conv_1/batchnorm_eightbit_quantized_batch_norm
 1072.316    0.795    0.823   0.029%     97.975%    QuantizedBatchNormWithGlobalNormalization   mixed_5/tower_1/conv_3/batchnorm_eightbit_quantized_batch_norm
  756.856    0.200    0.821   0.029%     98.003%    QuantizeDownAndShrinkRange  mixed_2/tower/conv/batchnorm_eightbit_quantize_down
 1167.811    0.208    0.821   0.029%     98.032%    QuantizeDownAndShrinkRange  mixed_6/tower/conv_2/batchnorm_eightbit_quantize_down
 1370.098    0.187    0.819   0.028%     98.060%    QuantizeDownAndShrinkRange  mixed_8/tower/conv/Conv2D_eightbit_quantize_down
 1553.349    0.971    0.810   0.028%     98.088%    QuantizedMaxPool    mixed_10/tower_2/pool_eightbit_quantized
 1016.560    1.268    0.810   0.028%     98.116%    QuantizeDownAndShrinkRange  mixed_5/tower/conv/batchnorm_eightbit_quantize_down
  795.726    0.341    0.803   0.028%     98.144%    QuantizeDownAndShrinkRange  mixed_2/tower_1/conv_1/Conv2D_eightbit_quantize_down
  951.149    0.154    0.789   0.027%     98.172%    QuantizeDownAndShrinkRange  mixed_4/tower/conv_1/Conv2D_eightbit_quantize_down
  945.523    0.884    0.789   0.027%     98.199%    QuantizedBatchNormWithGlobalNormalization   mixed_4/tower_1/conv_1/batchnorm_eightbit_quantized_batch_norm
  940.952    0.143    0.747   0.026%     98.225%    QuantizeDownAndShrinkRange  mixed_4/tower/conv/batchnorm_eightbit_quantize_down
  962.594    0.873    0.730   0.025%     98.250%    QuantizedBatchNormWithGlobalNormalization   mixed_4/tower_1/conv_2/batchnorm_eightbit_quantized_batch_norm
  952.288    0.155    0.728   0.025%     98.276%    QuantizeDownAndShrinkRange  mixed_4/tower/conv_1/batchnorm_eightbit_quantize_down
 1276.031    0.188    0.705   0.024%     98.300%    QuantizeDownAndShrinkRange  mixed_7/tower/conv_1/batchnorm_eightbit_quantize_down
  962.364    0.227    0.690   0.024%     98.324%    QuantizeDownAndShrinkRange  mixed_4/tower_1/conv_2/Conv2D_eightbit_quantize_down
  586.757    0.233    0.679   0.024%     98.348%    QuantizeDownAndShrinkRange  mixed/tower/conv/Conv2D_eightbit_quantize_down
  971.048    0.638    0.660   0.023%     98.370%    QuantizedBatchNormWithGlobalNormalization   mixed_4/tower_1/conv_3/batchnorm_eightbit_quantized_batch_norm
 1431.550    0.976    0.658   0.023%     98.393%    QuantizedAvgPool    mixed_9/tower_2/pool_eightbit_quantized
 1463.275    0.709    0.650   0.023%     98.416%    QuantizedBatchNormWithGlobalNormalization   mixed_9/tower_1/conv/batchnorm_eightbit_quantized_batch_norm
  940.158    0.142    0.634   0.022%     98.438%    QuantizeDownAndShrinkRange  mixed_4/tower/conv/Conv2D_eightbit_quantize_down
 1593.936    0.467    0.619   0.022%     98.459%    QuantizedBatchNormWithGlobalNormalization   mixed_10/tower_1/conv/batchnorm_eightbit_quantized_batch_norm
  585.913    0.475    0.611   0.021%     98.481%    QuantizeDownAndShrinkRange  mixed/tower_1/conv/Conv2D_eightbit_quantize_down
 1302.961    0.304    0.578   0.020%     98.501%    QuantizeDownAndShrinkRange  mixed_7/tower/conv_2/batchnorm_eightbit_quantize_down
 1392.113    0.097    0.569   0.020%     98.520%    QuantizeDownAndShrinkRange  mixed_8/tower/conv_1/batchnorm_eightbit_quantize_down
  903.803    0.748    0.541   0.019%     98.539%    QuantizedBatchNormWithGlobalNormalization   mixed_3/tower/conv_2/batchnorm_eightbit_quantized_batch_norm
 1669.749    0.458    0.540   0.019%     98.558%    QuantizedBatchNormWithGlobalNormalization   mixed_10/tower_1/mixed/conv/batchnorm_eightbit_quantized_batch_norm
  707.555    0.370    0.540   0.019%     98.577%    QuantizeDownAndShrinkRange  mixed_1/tower_1/conv_1/Conv2D_eightbit_quantize_down
  907.481    0.346    0.529   0.018%     98.595%    QuantizeDownAndShrinkRange  mixed_3/conv/batchnorm_eightbit_quantize_down
  930.142    0.220    0.529   0.018%     98.614%    QuantizeDownAndShrinkRange  mixed_4/tower_1/conv/batchnorm_eightbit_quantize_down
 1629.581    0.498    0.518   0.018%     98.632%    QuantizedBatchNormWithGlobalNormalization   mixed_10/tower/mixed/conv_1/batchnorm_eightbit_quantized_batch_norm
  624.420    0.495    0.516   0.018%     98.649%    QuantizeDownAndShrinkRange  mixed/tower_1/conv_1/Conv2D_eightbit_quantize_down
  654.004    0.422    0.513   0.018%     98.667%    QuantizeDownAndShrinkRange  mixed/tower_1/conv_2/batchnorm_eightbit_quantize_down
 1450.225    0.441    0.508   0.018%     98.685%    QuantizedBatchNormWithGlobalNormalization   mixed_9/tower/conv/batchnorm_eightbit_quantized_batch_norm
 1625.984    0.441    0.504   0.018%     98.702%    QuantizedBatchNormWithGlobalNormalization   mixed_10/tower/mixed/conv/batchnorm_eightbit_quantized_batch_norm
 1479.160    0.426    0.501   0.017%     98.720%    QuantizedBatchNormWithGlobalNormalization   mixed_9/tower/mixed/conv/batchnorm_eightbit_quantized_batch_norm
 1552.490    0.405    0.499   0.017%     98.737%    QuantizedBatchNormWithGlobalNormalization   mixed_9/tower_1/mixed/conv/batchnorm_eightbit_quantized_batch_norm
 1599.427    0.593    0.497   0.017%     98.754%    QuantizedBatchNormWithGlobalNormalization   mixed_10/tower/conv/batchnorm_eightbit_quantized_batch_norm
 1158.655    0.177    0.496   0.017%     98.772%    QuantizeDownAndShrinkRange  mixed_6/tower_1/conv_1/batchnorm_eightbit_quantize_down
 1480.531    0.649    0.495   0.017%     98.789%    QuantizedBatchNormWithGlobalNormalization   mixed_9/tower/mixed/conv_1/batchnorm_eightbit_quantized_batch_norm
 1530.484    0.612    0.484   0.017%     98.806%    QuantizedBatchNormWithGlobalNormalization   mixed_9/tower_1/mixed/conv_1/batchnorm_eightbit_quantized_batch_norm
 1447.088    0.558    0.479   0.017%     98.822%    QuantizedBatchNormWithGlobalNormalization   mixed_9/conv/batchnorm_eightbit_quantized_batch_norm
 1598.267    0.370    0.475   0.016%     98.839%    QuantizedBatchNormWithGlobalNormalization   mixed_10/conv/batchnorm_eightbit_quantized_batch_norm
  798.113    0.408    0.474   0.016%     98.855%    QuantizeDownAndShrinkRange  mixed_2/tower_1/conv_1/batchnorm_eightbit_quantize_down
 1513.106    0.619    0.473   0.016%     98.872%    QuantizedBatchNormWithGlobalNormalization   mixed_9/tower_1/conv_1/batchnorm_eightbit_quantized_batch_norm
 1669.868    0.474    0.468   0.016%     98.888%    QuantizedBatchNormWithGlobalNormalization   mixed_10/tower_1/mixed/conv_1/batchnorm_eightbit_quantized_batch_norm
  893.727    0.384    0.468   0.016%     98.904%    QuantizeDownAndShrinkRange  mixed_3/tower/conv_1/Conv2D_eightbit_quantize_down
  742.641    0.452    0.464   0.016%     98.920%    QuantizedConcat mixed_1/join_eightbit_quantized_concat
 1037.213    0.158    0.458   0.016%     98.936%    QuantizeDownAndShrinkRange  mixed_5/tower_1/conv_1/batchnorm_eightbit_quantize_down
 1391.744    0.366    0.455   0.016%     98.952%    QuantizedBatchNormWithGlobalNormalization   mixed_8/tower/conv_1/batchnorm_eightbit_quantized_batch_norm
  824.421    0.428    0.454   0.016%     98.968%    QuantizedConcat mixed_2/join_eightbit_quantized_concat
 1060.085    0.190    0.454   0.016%     98.983%    QuantizeDownAndShrinkRange  mixed_5/tower_1/conv_2/batchnorm_eightbit_quantize_down
 1646.188    0.401    0.449   0.016%     98.999%    QuantizedBatchNormWithGlobalNormalization   mixed_10/tower_1/conv_1/batchnorm_eightbit_quantized_batch_norm
  627.772    0.458    0.443   0.015%     99.014%    QuantizeDownAndShrinkRange  mixed/tower_1/conv_1/batchnorm_eightbit_quantize_down
    6.239    0.976    0.429   0.015%     99.029%    QuantizeV2  conv/Conv2D_eightbit_quantize_Mul
  651.111    0.411    0.415   0.014%     99.044%    QuantizeDownAndShrinkRange  mixed/tower_1/conv_2/Conv2D_eightbit_quantize_down
  586.490    0.187    0.410   0.014%     99.058%    QuantizeDownAndShrinkRange  mixed/tower_2/conv/Conv2D_eightbit_quantize_down
  946.413    0.133    0.395   0.014%     99.072%    QuantizeDownAndShrinkRange  mixed_4/tower_1/conv_1/batchnorm_eightbit_quantize_down
    0.000    0.165    0.388   0.013%     99.085%                _SOURCE
 1598.155    0.108    0.379   0.013%     99.098%    QuantizeDownAndShrinkRange  mixed_10/conv/Conv2D_eightbit_quantize_down
  905.237    0.326    0.375   0.013%     99.111%    QuantizeDownAndShrinkRange  mixed_3/conv/Conv2D_eightbit_quantize_down
  896.134    0.375    0.373   0.013%     99.124%    QuantizeDownAndShrinkRange  mixed_3/tower/conv_1/batchnorm_eightbit_quantize_down
  710.245    0.360    0.371   0.013%     99.137%    QuantizeDownAndShrinkRange  mixed_1/tower_1/conv_1/batchnorm_eightbit_quantize_down
  742.147    0.420    0.369   0.013%     99.150%    QuantizeDownAndShrinkRange  mixed_1/tower_1/conv_2/batchnorm_eightbit_quantize_down
  739.647    0.407    0.369   0.013%     99.163%    QuantizeDownAndShrinkRange  mixed_1/tower_1/conv_2/Conv2D_eightbit_quantize_down
 1307.586    0.195    0.366   0.013%     99.175%    QuantizeDownAndShrinkRange  mixed_7/tower_1/conv_2/Conv2D_eightbit_quantize_down
 1670.654    0.283    0.364   0.013%     99.188%    QuantizedAvgPool    pool_3_eightbit_quantized
  821.088    0.345    0.364   0.013%     99.201%    QuantizeDownAndShrinkRange  mixed_2/tower_1/conv_2/Conv2D_eightbit_quantize_down
 1352.869    0.305    0.361   0.013%     99.213%    QuantizedMaxPool    mixed_8/pool_eightbit_quantized
  823.993    0.354    0.356   0.012%     99.226%    QuantizeDownAndShrinkRange  mixed_2/tower_1/conv_2/batchnorm_eightbit_quantize_down
 1352.588    0.267    0.338   0.012%     99.237%    QuantizedConcat mixed_7/join_eightbit_quantized_concat
 1090.046    0.315    0.338   0.012%     99.249%    QuantizedConcat mixed_5/join_eightbit_quantized_concat
 1190.454    0.179    0.337   0.012%     99.261%    QuantizeDownAndShrinkRange  mixed_6/tower_1/conv_3/Conv2D_eightbit_quantize_down
 1210.351    0.332    0.331   0.012%     99.272%    QuantizedConcat mixed_6/join_eightbit_quantized_concat
 1308.742    0.192    0.322   0.011%     99.283%    QuantizeDownAndShrinkRange  mixed_7/tower_1/conv_2/batchnorm_eightbit_quantize_down
 1463.990    0.166    0.321   0.011%     99.295%    QuantizeDownAndShrinkRange  mixed_9/tower_1/conv/batchnorm_eightbit_quantize_down
  673.305    0.085    0.317   0.011%     99.306%    QuantizedRelu   mixed_1/tower_2/conv_eightbit_quantized
  985.777    0.308    0.316   0.011%     99.317%    QuantizedConcat mixed_4/join_eightbit_quantized_concat
  755.577    0.234    0.304   0.011%     99.327%    QuantizeDownAndShrinkRange  mixed_2/tower/conv/Conv2D_eightbit_quantize_down
 1589.839    0.089    0.297   0.010%     99.337%    QuantizeDownAndShrinkRange  mixed_10/tower_2/conv/batchnorm_eightbit_quantize_down
  670.908    0.254    0.293   0.010%     99.348%    QuantizeDownAndShrinkRange  mixed_1/tower/conv/Conv2D_eightbit_quantize_down
  654.512    0.270    0.293   0.010%     99.358%    QuantizedConcat mixed/join_eightbit_quantized_concat
 1372.369    0.194    0.292   0.010%     99.368%    QuantizeDownAndShrinkRange  mixed_8/tower_1/conv/batchnorm_eightbit_quantize_down
 1454.917    0.343    0.284   0.010%     99.378%    QuantizedBatchNormWithGlobalNormalization   mixed_9/tower_2/conv/batchnorm_eightbit_quantized_batch_norm
 1589.616    0.220    0.280   0.010%     99.388%    QuantizedBatchNormWithGlobalNormalization   mixed_10/tower_2/conv/batchnorm_eightbit_quantized_batch_norm
 1073.115    0.178    0.277   0.010%     99.397%    QuantizeDownAndShrinkRange  mixed_5/tower_1/conv_3/batchnorm_eightbit_quantize_down
 1593.697    0.236    0.266   0.009%     99.406%    QuantizeDownAndShrinkRange  mixed_10/tower_1/conv/Conv2D_eightbit_quantize_down
 1454.845    0.069    0.261   0.009%     99.416%    QuantizeDownAndShrinkRange  mixed_9/tower_2/conv/Conv2D_eightbit_quantize_down
 1430.985    0.315    0.244   0.008%     99.424%    QuantizedBatchNormWithGlobalNormalization   mixed_8/tower_1/conv_3/batchnorm_eightbit_quantized_batch_norm
 1553.082    0.245    0.224   0.008%     99.432%    QuantizedConcat mixed_9/join_eightbit_quantized_concat
 1395.688    0.202    0.224   0.008%     99.440%    QuantizeDownAndShrinkRange  mixed_8/tower_1/conv_1/Conv2D_eightbit_quantize_down
 1531.102    0.764    0.221   0.008%     99.447%    QuantizeDownAndShrinkRange  mixed_9/tower_1/mixed/conv_1/batchnorm_eightbit_quantize_down
 1447.650    0.192    0.218   0.008%     99.455%    QuantizeDownAndShrinkRange  mixed_9/conv/batchnorm_eightbit_quantize_down
 1455.264    2.639    0.217   0.008%     99.462%    QuantizeDownAndShrinkRange  mixed_9/tower_2/conv/batchnorm_eightbit_quantize_down
 1327.362    0.186    0.209   0.007%     99.470%    QuantizeDownAndShrinkRange  mixed_7/tower_1/conv_3/Conv2D_eightbit_quantize_down
 1396.867    0.213    0.208   0.007%     99.477%    QuantizeDownAndShrinkRange  mixed_8/tower_1/conv_1/batchnorm_eightbit_quantize_down
 1210.073    0.213    0.208   0.007%     99.484%    QuantizeDownAndShrinkRange  mixed_6/tower_1/conv_4/batchnorm_eightbit_quantize_down
 1352.322    0.222    0.207   0.007%     99.491%    QuantizeDownAndShrinkRange  mixed_7/tower_1/conv_4/batchnorm_eightbit_quantize_down
 1670.479    0.166    0.204   0.007%     99.498%    QuantizedConcat mixed_10/join_eightbit_quantized_concat
 1351.148    0.212    0.203   0.007%     99.505%    QuantizeDownAndShrinkRange  mixed_7/tower_1/conv_4/Conv2D_eightbit_quantize_down
 1418.237    0.240    0.202   0.007%     99.512%    QuantizeDownAndShrinkRange  mixed_8/tower_1/conv_2/batchnorm_eightbit_quantize_down
 1416.688    0.233    0.201   0.007%     99.519%    QuantizeDownAndShrinkRange  mixed_8/tower_1/conv_2/Conv2D_eightbit_quantize_down
 1328.509    0.196    0.200   0.007%     99.526%    QuantizeDownAndShrinkRange  mixed_7/tower_1/conv_3/batchnorm_eightbit_quantize_down
  985.531    0.208    0.199   0.007%     99.533%    QuantizeDownAndShrinkRange  mixed_4/tower_1/conv_4/batchnorm_eightbit_quantize_down
 1089.804    0.203    0.198   0.007%     99.540%    QuantizeDownAndShrinkRange  mixed_5/tower_1/conv_4/batchnorm_eightbit_quantize_down
 1371.173    0.220    0.198   0.007%     99.547%    QuantizeDownAndShrinkRange  mixed_8/tower_1/conv/Conv2D_eightbit_quantize_down
 1208.909    0.198    0.197   0.007%     99.554%    QuantizeDownAndShrinkRange  mixed_6/tower_1/conv_4/Conv2D_eightbit_quantize_down
 1088.632    0.213    0.197   0.007%     99.561%    QuantizeDownAndShrinkRange  mixed_5/tower_1/conv_4/Conv2D_eightbit_quantize_down
  984.358    0.206    0.197   0.007%     99.567%    QuantizeDownAndShrinkRange  mixed_4/tower_1/conv_4/Conv2D_eightbit_quantize_down
 1479.039    0.118    0.196   0.007%     99.574%    QuantizeDownAndShrinkRange  mixed_9/tower/mixed/conv/Conv2D_eightbit_quantize_down
 1191.435    0.170    0.193   0.007%     99.581%    QuantizeDownAndShrinkRange  mixed_6/tower_1/conv_3/batchnorm_eightbit_quantize_down
  314.482    0.231    0.191   0.007%     99.588%    QuantizedRelu   conv_2_eightbit_quantized
 1446.973    0.111    0.185   0.006%     99.594%    QuantizeDownAndShrinkRange  mixed_9/conv/Conv2D_eightbit_quantize_down
 1177.287    0.185    0.183   0.006%     99.600%    QuantizeDownAndShrinkRange  mixed_6/tower_1/conv_2/batchnorm_eightbit_quantize_down
  971.691    0.145    0.183   0.006%     99.607%    QuantizeDownAndShrinkRange  mixed_4/tower_1/conv_3/batchnorm_eightbit_quantize_down
  907.896    0.164    0.182   0.006%     99.613%    QuantizedConcat mixed_3/join_eightbit_quantized_concat
  963.472    0.148    0.173   0.006%     99.619%    QuantizeDownAndShrinkRange  mixed_4/tower_1/conv_2/batchnorm_eightbit_quantize_down
 1072.144    0.170    0.171   0.006%     99.625%    QuantizeDownAndShrinkRange  mixed_5/tower_1/conv_3/Conv2D_eightbit_quantize_down
 1463.126    0.144    0.163   0.006%     99.631%    QuantizeDownAndShrinkRange  mixed_9/tower_1/conv/Conv2D_eightbit_quantize_down
 1594.407    0.135    0.159   0.006%     99.636%    QuantizeDownAndShrinkRange  mixed_10/tower_1/conv/batchnorm_eightbit_quantize_down
 1589.488    0.123    0.157   0.005%     99.642%    QuantizeDownAndShrinkRange  mixed_10/tower_2/conv/Conv2D_eightbit_quantize_down
 1598.640    0.151    0.154   0.005%     99.647%    QuantizeDownAndShrinkRange  mixed_10/conv/batchnorm_eightbit_quantize_down
 1599.293    0.129    0.154   0.005%     99.652%    QuantizeDownAndShrinkRange  mixed_10/tower/conv/Conv2D_eightbit_quantize_down
  970.913    0.132    0.149   0.005%     99.658%    QuantizeDownAndShrinkRange  mixed_4/tower_1/conv_3/Conv2D_eightbit_quantize_down
   54.775    0.173    0.149   0.005%     99.663%    QuantizedRelu   conv_eightbit_quantized
 1512.964    0.138    0.143   0.005%     99.668%    QuantizeDownAndShrinkRange  mixed_9/tower_1/conv_1/Conv2D_eightbit_quantize_down
 1629.432    0.145    0.140   0.005%     99.673%    QuantizeDownAndShrinkRange  mixed_10/tower/mixed/conv_1/Conv2D_eightbit_quantize_down
  572.257    0.153    0.139   0.005%     99.677%    QuantizedRelu   conv_4_eightbit_quantized
 1480.376    0.151    0.138   0.005%     99.682%    QuantizeDownAndShrinkRange  mixed_9/tower/mixed/conv_1/Conv2D_eightbit_quantize_down
 1669.608    0.138    0.136   0.005%     99.687%    QuantizeDownAndShrinkRange  mixed_10/tower_1/mixed/conv/Conv2D_eightbit_quantize_down
 1670.210    0.172    0.135   0.005%     99.692%    QuantizeDownAndShrinkRange  mixed_10/tower_1/mixed/conv/batchnorm_eightbit_quantize_down
 1646.075    0.110    0.135   0.005%     99.696%    QuantizeDownAndShrinkRange  mixed_10/tower_1/conv_1/Conv2D_eightbit_quantize_down
 1625.831    0.150    0.133   0.005%     99.701%    QuantizeDownAndShrinkRange  mixed_10/tower/mixed/conv/Conv2D_eightbit_quantize_down
 1481.185    0.129    0.133   0.005%     99.705%    QuantizeDownAndShrinkRange  mixed_9/tower/mixed/conv_1/batchnorm_eightbit_quantize_down
 1552.297    0.190    0.132   0.005%     99.710%    QuantizeDownAndShrinkRange  mixed_9/tower_1/mixed/conv/Conv2D_eightbit_quantize_down
 1530.315    0.165    0.130   0.005%     99.715%    QuantizeDownAndShrinkRange  mixed_9/tower_1/mixed/conv_1/Conv2D_eightbit_quantize_down
 1670.345    0.109    0.128   0.004%     99.719%    QuantizeDownAndShrinkRange  mixed_10/tower_1/mixed/conv_1/batchnorm_eightbit_quantize_down
  153.046    0.140    0.128   0.004%     99.723%    QuantizedRelu   conv_1_eightbit_quantized
 1626.428    0.112    0.128   0.004%     99.728%    QuantizeDownAndShrinkRange  mixed_10/tower/mixed/conv/batchnorm_eightbit_quantize_down
 1450.098    0.124    0.128   0.004%     99.732%    QuantizeDownAndShrinkRange  mixed_9/tower/conv/Conv2D_eightbit_quantize_down
 1630.082    0.111    0.127   0.004%     99.737%    QuantizeDownAndShrinkRange  mixed_10/tower/mixed/conv_1/batchnorm_eightbit_quantize_down
 1669.735    0.129    0.127   0.004%     99.741%    QuantizeDownAndShrinkRange  mixed_10/tower_1/mixed/conv_1/Conv2D_eightbit_quantize_down
  904.558    0.141    0.126   0.004%     99.746%    QuantizeDownAndShrinkRange  mixed_3/tower/conv_2/batchnorm_eightbit_quantize_down
 1600.024    0.116    0.126   0.004%     99.750%    QuantizeDownAndShrinkRange  mixed_10/tower/conv/batchnorm_eightbit_quantize_down
 1646.592    0.124    0.126   0.004%     99.754%    QuantizeDownAndShrinkRange  mixed_10/tower_1/conv_1/batchnorm_eightbit_quantize_down
  903.675    0.123    0.125   0.004%     99.759%    QuantizeDownAndShrinkRange  mixed_3/tower/conv_2/Conv2D_eightbit_quantize_down
 1513.729    0.164    0.124   0.004%     99.763%    QuantizeDownAndShrinkRange  mixed_9/tower_1/conv_1/batchnorm_eightbit_quantize_down
 1479.588    0.145    0.124   0.004%     99.767%    QuantizeDownAndShrinkRange  mixed_9/tower/mixed/conv/batchnorm_eightbit_quantize_down
 1552.898    0.161    0.123   0.004%     99.771%    QuantizeDownAndShrinkRange  mixed_9/tower_1/mixed/conv/batchnorm_eightbit_quantize_down
 1450.669    0.106    0.122   0.004%     99.776%    QuantizeDownAndShrinkRange  mixed_9/tower/conv/batchnorm_eightbit_quantize_down
  348.332    0.135    0.118   0.004%     99.780%    QuantizedRelu   conv_3_eightbit_quantized
  599.414    0.090    0.091   0.003%     99.783%    QuantizedRelu   mixed/conv_eightbit_quantized
  628.234    0.086    0.089   0.003%     99.786%    QuantizedRelu   mixed/tower_1/conv_1_eightbit_quantized
    5.808    0.225    0.084   0.003%     99.789%           Max  conv/Conv2D_eightbit_max_Mul
  896.518    0.134    0.083   0.003%     99.792%    QuantizedRelu   mixed_3/tower/conv_1_eightbit_quantized
 1430.886    0.095    0.081   0.003%     99.795%    QuantizeDownAndShrinkRange  mixed_8/tower_1/conv_3/Conv2D_eightbit_quantize_down
  654.430    0.079    0.080   0.003%     99.797%    QuantizedRelu   mixed/tower_1/conv_2_eightbit_quantized
  798.525    0.064    0.080   0.003%     99.800%    QuantizedRelu   mixed_2/tower_1/conv_1_eightbit_quantized
    5.848    0.383    0.079   0.003%     99.803%           Min  conv/Conv2D_eightbit_min_Mul
  849.317    0.081    0.079   0.003%     99.806%    QuantizedRelu   mixed_3/tower/conv_eightbit_quantized
  589.438    0.067    0.079   0.003%     99.808%    QuantizedRelu   mixed/tower_1/conv_eightbit_quantized
 1431.410    0.125    0.078   0.003%     99.811%    QuantizedConcat mixed_8/join_eightbit_quantized_concat
 1431.305    0.078    0.078   0.003%     99.814%    QuantizeDownAndShrinkRange  mixed_8/tower_1/conv_3/batchnorm_eightbit_quantize_down
  710.609    0.070    0.078   0.003%     99.817%    QuantizedRelu   mixed_1/tower_1/conv_1_eightbit_quantized
  627.572    0.046    0.078   0.003%     99.819%    QuantizedRelu   mixed/tower/conv_1_eightbit_quantized
  769.994    0.082    0.076   0.003%     99.822%    QuantizedRelu   mixed_2/conv_eightbit_quantized
  796.418    0.045    0.074   0.003%     99.825%    QuantizedRelu   mixed_2/tower/conv_1_eightbit_quantized
  682.652    0.104    0.074   0.003%     99.827%    QuantizedRelu   mixed_1/tower_1/conv_eightbit_quantized
  784.800    0.080    0.073   0.003%     99.830%    QuantizedRelu   mixed_2/tower_2/conv_eightbit_quantized
  707.766    0.046    0.073   0.003%     99.832%    QuantizedRelu   mixed_1/tower/conv_1_eightbit_quantized
  757.177    0.045    0.072   0.003%     99.835%    QuantizedRelu   mixed_2/tower_1/conv_eightbit_quantized
  824.352    0.066    0.072   0.002%     99.837%    QuantizedRelu   mixed_2/tower_1/conv_2_eightbit_quantized
  742.570    0.068    0.070   0.002%     99.840%    QuantizedRelu   mixed_1/tower_1/conv_2_eightbit_quantized
  907.830    0.063    0.069   0.002%     99.842%    QuantizedRelu   mixed_3/conv_eightbit_quantized
  673.812    0.079    0.069   0.002%     99.844%    QuantizedRelu   mixed_1/conv_eightbit_quantized
  588.811    0.069    0.062   0.002%     99.847%    QuantizedRelu   mixed/tower/conv_eightbit_quantized
  939.612    0.064    0.061   0.002%     99.849%    QuantizedRelu   mixed_4/conv_eightbit_quantized
 1134.276    0.065    0.058   0.002%     99.851%    QuantizedRelu   mixed_6/conv_eightbit_quantized
 1017.581    0.065    0.058   0.002%     99.853%    QuantizedRelu   mixed_5/tower_2/conv_eightbit_quantized
 1016.566    0.061    0.058   0.002%     99.855%    QuantizedRelu   mixed_5/conv_eightbit_quantized
  672.675    0.036    0.057   0.002%     99.857%    QuantizedRelu   mixed_1/tower/conv_eightbit_quantized
  951.059    0.068    0.057   0.002%     99.859%    QuantizedRelu   mixed_4/tower_2/conv_eightbit_quantized
 1243.589    0.067    0.057   0.002%     99.861%    QuantizedRelu   mixed_7/conv_eightbit_quantized
 1246.866    0.066    0.056   0.002%     99.863%    QuantizedRelu   mixed_7/tower_2/conv_eightbit_quantized
 1133.490    0.059    0.054   0.002%     99.864%    QuantizedRelu   mixed_6/tower_2/conv_eightbit_quantized
 1168.023    0.034    0.053   0.002%     99.866%    QuantizedRelu   mixed_6/tower/conv_2_eightbit_quantized
 1303.269    0.060    0.052   0.002%     99.868%    QuantizedRelu   mixed_7/tower/conv_2_eightbit_quantized
 1249.895    0.061    0.051   0.002%     99.870%    QuantizedRelu   mixed_7/tower_1/conv_eightbit_quantized
  969.648    0.065    0.051   0.002%     99.872%    QuantizedRelu   mixed_4/tower/conv_2_eightbit_quantized
 1289.918    0.061    0.050   0.002%     99.873%    QuantizedRelu   mixed_7/tower_1/conv_1_eightbit_quantized
  757.059    0.041    0.050   0.002%     99.875%    QuantizedRelu   mixed_2/tower/conv_eightbit_quantized
 1246.313    0.033    0.049   0.002%     99.877%    QuantizedRelu   mixed_7/tower/conv_eightbit_quantized
 1071.647    0.034    0.049   0.002%     99.878%    QuantizedRelu   mixed_5/tower/conv_2_eightbit_quantized
 1371.432    0.033    0.049   0.002%     99.880%    QuantizedRelu   mixed_8/tower/conv_eightbit_quantized
  587.936    0.042    0.048   0.002%     99.882%    QuantizedRelu   mixed/tower_2/conv_eightbit_quantized
 1015.851    0.053    0.047   0.002%     99.883%    QuantizedRelu   mixed_5/tower_1/conv_eightbit_quantized
 1017.831    0.054    0.047   0.002%     99.885%    QuantizedRelu   mixed_5/tower/conv_eightbit_quantized
 1043.385    0.044    0.045   0.002%     99.887%    QuantizedRelu   mixed_5/tower/conv_1_eightbit_quantized
 1276.223    0.034    0.044   0.002%     99.888%    QuantizedRelu   mixed_7/tower/conv_1_eightbit_quantized
 1149.669    0.050    0.044   0.002%     99.890%    QuantizedRelu   mixed_6/tower/conv_1_eightbit_quantized
 1372.566    0.033    0.043   0.001%     99.891%    QuantizedRelu   mixed_8/tower_1/conv_eightbit_quantized
 1135.923    0.057    0.043   0.001%     99.893%    QuantizedRelu   mixed_6/tower_1/conv_eightbit_quantized
 1397.084    0.033    0.043   0.001%     99.894%    QuantizedRelu   mixed_8/tower_1/conv_1_eightbit_quantized
 1308.938    0.038    0.043   0.001%     99.896%    QuantizedRelu   mixed_7/tower_1/conv_2_eightbit_quantized
 1037.374    0.029    0.041   0.001%     99.897%    QuantizedRelu   mixed_5/tower_1/conv_1_eightbit_quantized
 1128.507    0.049    0.041   0.001%     99.899%    QuantizedRelu   mixed_6/tower/conv_eightbit_quantized
 1158.836    0.029    0.041   0.001%     99.900%    QuantizedRelu   mixed_6/tower_1/conv_1_eightbit_quantized
 1060.278    0.052    0.040   0.001%     99.901%    QuantizedRelu   mixed_5/tower_1/conv_2_eightbit_quantized
 1352.549    0.036    0.040   0.001%     99.903%    QuantizedRelu   mixed_7/tower_1/conv_4_eightbit_quantized
 1090.010    0.034    0.040   0.001%     99.904%    QuantizedRelu   mixed_5/tower_1/conv_4_eightbit_quantized
 1328.708    0.034    0.039   0.001%     99.906%    QuantizedRelu   mixed_7/tower_1/conv_3_eightbit_quantized
 1418.481    0.061    0.039   0.001%     99.907%    QuantizedRelu   mixed_8/tower_1/conv_2_eightbit_quantized
 1210.292    0.056    0.038   0.001%     99.908%    QuantizedRelu   mixed_6/tower_1/conv_4_eightbit_quantized
  930.366    0.039    0.037   0.001%     99.910%    QuantizedRelu   mixed_4/tower_1/conv_eightbit_quantized
  952.447    0.026    0.037   0.001%     99.911%    QuantizedRelu   mixed_4/tower/conv_1_eightbit_quantized
 1177.475    0.028    0.037   0.001%     99.912%    QuantizedRelu   mixed_6/tower_1/conv_2_eightbit_quantized
  941.097    0.025    0.036   0.001%     99.913%    QuantizedRelu   mixed_4/tower/conv_eightbit_quantized
  985.741    0.035    0.035   0.001%     99.915%    QuantizedRelu   mixed_4/tower_1/conv_4_eightbit_quantized
 1191.607    0.029    0.034   0.001%     99.916%    QuantizedRelu   mixed_6/tower_1/conv_3_eightbit_quantized
  946.550    0.038    0.034   0.001%     99.917%    QuantizedRelu   mixed_4/tower_1/conv_1_eightbit_quantized
 1073.295    0.028    0.033   0.001%     99.918%    QuantizedRelu   mixed_5/tower_1/conv_3_eightbit_quantized
 1689.900    0.028    0.033   0.001%     99.919%    QuantizedBiasAdd    softmax/logits_eightbit_quantized_bias_add
 1689.945    0.028    0.033   0.001%     99.920%       Softmax  softmax
  963.624    0.025    0.032   0.001%     99.921%    QuantizedRelu   mixed_4/tower_1/conv_2_eightbit_quantized
 1630.197    0.038    0.030   0.001%     99.922%    QuantizedRelu   mixed_10/tower/mixed/conv_1_eightbit_quantized
 1464.160    0.032    0.029   0.001%     99.924%    QuantizedRelu   mixed_9/tower_1/conv_eightbit_quantized
 1594.545    0.035    0.028   0.001%     99.924%    QuantizedRelu   mixed_10/tower_1/conv_eightbit_quantized
  971.838    0.025    0.027   0.001%     99.925%    QuantizedRelu   mixed_4/tower_1/conv_3_eightbit_quantized
 1689.985    0.026    0.026   0.001%     99.926%                _SINK
 1670.385    0.018    0.026   0.001%     99.927%    QuantizedRelu   mixed_10/tower_1/mixed/conv_eightbit_quantized
  904.702    0.037    0.025   0.001%     99.928%    QuantizedRelu   mixed_3/tower/conv_2_eightbit_quantized
 1598.794    0.017    0.025   0.001%     99.929%    QuantizedRelu   mixed_10/conv_eightbit_quantized
 1626.543    0.019    0.025   0.001%     99.930%    QuantizedRelu   mixed_10/tower/mixed/conv_eightbit_quantized
 1553.062    0.017    0.024   0.001%     99.931%    QuantizedRelu   mixed_9/tower_1/mixed/conv_eightbit_quantized
 1481.317    0.030    0.024   0.001%     99.932%    QuantizedRelu   mixed_9/tower/mixed/conv_1_eightbit_quantized
 1479.735    0.018    0.024   0.001%     99.932%    QuantizedRelu   mixed_9/tower/mixed/conv_eightbit_quantized
 1600.144    0.037    0.024   0.001%     99.933%    QuantizedRelu   mixed_10/tower/conv_eightbit_quantized
 1447.845    0.031    0.024   0.001%     99.934%    QuantizedRelu   mixed_9/conv_eightbit_quantized
 1392.212    0.016    0.024   0.001%     99.935%    QuantizedRelu   mixed_8/tower/conv_1_eightbit_quantized
 1450.780    0.020    0.024   0.001%     99.936%    QuantizedRelu   mixed_9/tower/conv_eightbit_quantized
 1670.458    0.018    0.024   0.001%     99.937%    QuantizedRelu   mixed_10/tower_1/mixed/conv_1_eightbit_quantized
 1531.870    0.028    0.023   0.001%     99.937%    QuantizedRelu   mixed_9/tower_1/mixed/conv_1_eightbit_quantized
 1513.897    0.029    0.022   0.001%     99.938%    QuantizedRelu   mixed_9/tower_1/conv_1_eightbit_quantized
 1646.719    0.017    0.022   0.001%     99.939%    QuantizedRelu   mixed_10/tower_1/conv_1_eightbit_quantized
    4.674    0.033    0.022   0.001%     99.940%         Const  mixed_9/tower_2/conv/batchnorm/gamma_min
    4.709    0.024    0.018   0.001%     99.940%         Const  mixed_9/tower_2/conv/batchnorm/gamma_max
 1589.931    0.012    0.018   0.001%     99.941%    QuantizedRelu   mixed_10/tower_2/conv_eightbit_quantized
 1457.907    0.014    0.017   0.001%     99.941%    QuantizedRelu   mixed_9/tower_2/conv_eightbit_quantized
 1670.970    0.012    0.017   0.001%     99.942%    QuantizeV2  softmax/logits/MatMul_eightbit_quantize_pool_3/_reshape
 1431.387    0.020    0.015   0.001%     99.943%    QuantizedRelu   mixed_8/tower_1/conv_3_eightbit_quantized
 1689.888    0.010    0.013   0.000%     99.943%    QuantizeDownAndShrinkRange  softmax/logits/MatMul_eightbit_quantize_down
 1670.939    0.009    0.011   0.000%     99.943%    Dequantize  pool_3
    4.656    0.016    0.010   0.000%     99.944%         Const  mixed_9/tower_2/conv/batchnorm/gamma_quint8_const
 1689.930    0.007    0.009   0.000%     99.944%    QuantizeDownAndShrinkRange  softmax/logits_eightbit_quantize_down
    0.247    0.013    0.009   0.000%     99.944%         Const  mixed_10/join/concat_dim
 1689.938    0.006    0.007   0.000%     99.945%    Dequantize  softmax/logits
    0.441    0.008    0.007   0.000%     99.945%         Const  conv_2/batchnorm/gamma_quint8_const
 1670.959    0.005    0.007   0.000%     99.945%           Max  softmax/logits/MatMul_eightbit_max_pool_3/_reshape
 1670.966    0.003    0.005   0.000%     99.945%           Min  softmax/logits/MatMul_eightbit_min_pool_3/_reshape
    2.727    0.014    0.005   0.000%     99.945%         Const  mixed_5/tower/conv/batchnorm/gamma_quint8_const
    0.895    0.005    0.004   0.000%     99.945%         Const  mixed/tower_1/conv_1/batchnorm/gamma_quint8_const
    4.852    0.010    0.004   0.000%     99.946%         Const  mixed_10/tower/conv/batchnorm/gamma_quint8_const
    5.789    0.006    0.003   0.000%     99.946%       Reshape  conv/Conv2D_eightbit_reshape_Mul
 1670.949    0.003    0.003   0.000%     99.946%       Reshape  pool_3/_reshape
    2.164    0.007    0.003   0.000%     99.946%         Const  mixed_4/tower/conv/batchnorm/gamma_quint8_const
 1670.954    0.002    0.003   0.000%     99.946%       Reshape  softmax/logits/MatMul_eightbit_reshape_pool_3/_reshape
    4.225    0.003    0.002   0.000%     99.946%         Const  mixed_7/tower_2/conv/conv2d_params_quint8_const
    0.265    0.004    0.002   0.000%     99.946%         Const  conv/conv2d_params_quint8_const
    1.023    0.007    0.002   0.000%     99.946%         Const  mixed_1/conv/conv2d_params_quint8_const
    0.324    0.003    0.002   0.000%     99.946%         Const  conv/batchnorm/gamma_quint8_const
    0.664    0.005    0.002   0.000%     99.946%         Const  mixed/tower/conv/batchnorm/gamma_quint8_const
    2.054    0.003    0.002   0.000%     99.946%         Const  mixed_4/conv/conv2d_params_quint8_const
    1.136    0.005    0.002   0.000%     99.947%         Const  mixed_1/tower/conv/batchnorm/beta_quint8_const
    4.734    0.004    0.002   0.000%     99.947%         Const  mixed_10/conv/conv2d_params_quint8_const
    0.282    0.003    0.002   0.000%     99.947%         Const  conv/batchnorm/moving_mean_quint8_const
    4.442    0.003    0.002   0.000%     99.947%         Const  mixed_8/tower_1/conv_1/conv2d_params_quint8_const
    2.971    0.004    0.002   0.000%     99.947%         Const  mixed_5/tower_1/conv_2/conv2d_params_quint8_const
    5.310    0.004    0.002   0.000%     99.947%         Const  softmax/logits/MatMul_eightbit_reduction_dims
    5.227    0.003    0.002   0.000%     99.947%         Const  mixed_10/tower_2/conv/conv2d_params_quint8_const
    0.836    0.006    0.002   0.000%     99.947%         Const  mixed/tower_1/conv_1/conv2d_params_quint8_const
    0.592    0.003    0.002   0.000%     99.947%         Const  mixed/conv/batchnorm/moving_variance_min
    4.791    0.003    0.002   0.000%     99.947%         Const  mixed_10/conv/batchnorm/gamma_quint8_const
    1.536    0.003    0.002   0.000%     99.947%         Const  mixed_2/tower/conv_1/conv2d_params_quint8_const
    1.853    0.028    0.002   0.000%     99.947%         Const  mixed_3/conv/batchnorm/beta_max
    3.844    0.005    0.002   0.000%     99.947%         Const  mixed_7/tower/conv_1/conv2d_params_quint8_const
    1.720    0.002    0.002   0.000%     99.947%         Const  mixed_2/tower_1/conv_2/batchnorm/moving_mean_max
    0.559    0.003    0.002   0.000%     99.947%         Const  mixed/conv/conv2d_params_quint8_const
    3.300    0.003    0.002   0.000%     99.948%         Const  mixed_6/tower/conv_1/conv2d_params_quint8_const
    3.193    0.002    0.002   0.000%     99.948%         Const  mixed_6/conv/conv2d_params_quint8_const
    1.205    0.003    0.002   0.000%     99.948%         Const  mixed_1/tower_1/conv/conv2d_params_quint8_const
    0.329    0.004    0.002   0.000%     99.948%         Const  conv_1/conv2d_params_quint8_const
    4.277    0.006    0.002   0.000%     99.948%         Const  mixed_8/tower/conv/conv2d_params_quint8_const
    3.683    0.003    0.002   0.000%     99.948%         Const  mixed_6/tower_2/conv/conv2d_params_quint8_const
    3.352    0.007    0.002   0.000%     99.948%         Const  mixed_6/tower/conv_2/conv2d_params_quint8_const
    2.336    0.005    0.002   0.000%     99.948%         Const  mixed_4/tower_1/conv_1/conv2d_params_quint8_const
    1.643    0.003    0.002   0.000%     99.948%         Const  mixed_2/tower_1/conv_1/conv2d_params_quint8_const
    3.789    0.005    0.002   0.000%     99.948%         Const  mixed_7/tower/conv/conv2d_params_quint8_const
    1.994    0.004    0.002   0.000%     99.948%         Const  mixed_3/tower/conv_2/conv2d_params_quint8_const
    1.424    0.003    0.002   0.000%     99.948%         Const  mixed_2/conv/conv2d_params_quint8_const
    0.451    0.003    0.002   0.000%     99.948%         Const  conv_3/conv2d_params_quint8_const
    0.310    0.003    0.002   0.000%     99.948%         Const  conv/batchnorm/beta_quint8_const
    5.335    0.006    0.002   0.000%     99.948%         Const  mixed_9/tower/conv/conv2d_params_quint8_const
    5.102    0.007    0.002   0.000%     99.948%         Const  mixed_10/tower_1/mixed/conv/conv2d_params_quint8_const
    4.796    0.006    0.002   0.000%     99.949%         Const  mixed_10/tower/conv/conv2d_params_quint8_const
    4.133    0.003    0.002   0.000%     99.949%         Const  mixed_7/tower_1/conv_3/batchnorm/moving_mean_min
    2.395    0.003    0.002   0.000%     99.949%         Const  mixed_4/tower_1/conv_2/conv2d_params_quint8_const
    1.190    0.006    0.002   0.000%     99.949%         Const  mixed_1/tower/conv_1/batchnorm/beta_quint8_const
    0.506    0.004    0.002   0.000%     99.949%         Const  conv_4/conv2d_params_quint8_const
    0.388    0.003    0.002   0.000%     99.949%         Const  conv_2/conv2d_params_quint8_const
    3.737    0.002    0.002   0.000%     99.949%         Const  mixed_7/conv/conv2d_params_quint8_const
    3.463    0.002    0.002   0.000%     99.949%         Const  mixed_6/tower_1/conv_1/conv2d_params_quint8_const
    1.883    0.003    0.002   0.000%     99.949%         Const  mixed_3/tower/conv/conv2d_params_quint8_const
    0.920    0.004    0.002   0.000%     99.949%         Const  mixed/tower_1/conv_2/batchnorm/moving_mean_quint8_const
    5.392    0.006    0.002   0.000%     99.949%         Const  mixed_9/tower/mixed/conv/conv2d_params_quint8_const
    5.165    0.005    0.002   0.000%     99.949%         Const  mixed_10/tower_1/mixed/conv_1/conv2d_params_quint8_const
    4.986    0.005    0.002   0.000%     99.949%         Const  mixed_10/tower_1/conv/conv2d_params_quint8_const
    4.603    0.003    0.002   0.000%     99.949%         Const  mixed_9/conv/conv2d_params_quint8_const
    3.629    0.003    0.002   0.000%     99.949%         Const  mixed_6/tower_1/conv_4/conv2d_params_quint8_const
    3.517    0.004    0.002   0.000%     99.949%         Const  mixed_6/tower_1/conv_2/conv2d_params_quint8_const
    2.805    0.003    0.002   0.000%     99.950%         Const  mixed_5/tower/conv_2/conv2d_params_quint8_const
    2.621    0.003    0.002   0.000%     99.950%         Const  mixed_5/conv/conv2d_params_quint8_const
    1.261    0.002    0.002   0.000%     99.950%         Const  mixed_1/tower_1/conv_1/conv2d_params_quint8_const
    1.220    0.002    0.002   0.000%     99.950%         Const  mixed_1/tower_1/conv/batchnorm/moving_mean_quint8_const
    0.691    0.005    0.002   0.000%     99.950%         Const  mixed/tower/conv_1/batchnorm/moving_mean_quint8_const
    4.930    0.005    0.002   0.000%     99.950%         Const  mixed_10/tower/mixed/conv_1/conv2d_params_quint8_const
    4.870    0.004    0.002   0.000%     99.950%         Const  mixed_10/tower/mixed/conv/conv2d_params_quint8_const
    2.857    0.010    0.002   0.000%     99.950%         Const  mixed_5/tower_1/conv/conv2d_params_quint8_const
    0.968    0.003    0.002   0.000%     99.950%         Const  mixed/tower_2/conv/conv2d_params_quint8_const
    0.902    0.006    0.002   0.000%     99.950%         Const  mixed/tower_1/conv_2/conv2d_params_quint8_const
    0.489    0.003    0.002   0.000%     99.950%         Const  conv_3/batchnorm/beta_quint8_const
    0.425    0.003    0.002   0.000%     99.950%         Const  conv_2/batchnorm/moving_variance_max
    0.402    0.003    0.002   0.000%     99.950%         Const  conv_2/batchnorm/moving_mean_quint8_const
    4.549    0.003    0.002   0.000%     99.950%         Const  mixed_8/tower_1/conv_3/conv2d_params_quint8_const
    4.115    0.003    0.002   0.000%     99.950%         Const  mixed_7/tower_1/conv_3/conv2d_params_quint8_const
    4.062    0.003    0.002   0.000%     99.950%         Const  mixed_7/tower_1/conv_2/conv2d_params_quint8_const
    3.084    0.003    0.002   0.000%     99.950%         Const  mixed_5/tower_1/conv_4/conv2d_params_quint8_const
    2.676    0.003    0.002   0.000%     99.951%         Const  mixed_5/tower/conv/conv2d_params_quint8_const
    2.282    0.007    0.002   0.000%     99.951%         Const  mixed_4/tower_1/conv/conv2d_params_quint8_const
    1.817    0.003    0.002   0.000%     99.951%         Const  mixed_3/conv/batchnorm/moving_mean_quint8_const
    1.749    0.005    0.002   0.000%     99.951%         Const  mixed_2/tower_2/conv/conv2d_params_quint8_const
    1.694    0.005    0.002   0.000%     99.951%         Const  mixed_2/tower_1/conv_2/conv2d_params_quint8_const
    1.082    0.006    0.002   0.000%     99.951%         Const  mixed_1/tower/conv/conv2d_params_quint8_const
    0.851    0.003    0.002   0.000%     99.951%         Const  mixed/tower_1/conv_1/batchnorm/moving_mean_quint8_const
    0.752    0.005    0.002   0.000%     99.951%         Const  mixed/tower_1/conv/conv2d_params_quint8_const
    0.613    0.003    0.002   0.000%     99.951%         Const  mixed/tower/conv/conv2d_params_quint8_const
    5.045    0.006    0.002   0.000%     99.951%         Const  mixed_10/tower_1/conv_1/conv2d_params_quint8_const
    4.331    0.006    0.002   0.000%     99.951%         Const  mixed_8/tower/conv_1/conv2d_params_quint8_const
    3.027    0.003    0.002   0.000%     99.951%         Const  mixed_5/tower_1/conv_3/conv2d_params_quint8_const
    2.918    0.005    0.002   0.000%     99.951%         Const  mixed_5/tower_1/conv_1/conv2d_params_quint8_const
    2.226    0.003    0.002   0.000%     99.951%         Const  mixed_4/tower/conv_2/conv2d_params_quint8_const
    2.172    0.003    0.002   0.000%     99.951%         Const  mixed_4/tower/conv_1/conv2d_params_quint8_const
    1.937    0.003    0.002   0.000%     99.951%         Const  mixed_3/tower/conv_1/conv2d_params_quint8_const
    0.464    0.003    0.002   0.000%     99.951%         Const  conv_3/batchnorm/moving_mean_quint8_const
    0.272    0.003    0.002   0.000%     99.952%         Const  conv/conv2d_params_min
    5.525    0.003    0.002   0.000%     99.952%         Const  mixed_9/tower_1/conv/batchnorm/moving_mean_quint8_const
    4.170    0.002    0.002   0.000%     99.952%         Const  mixed_7/tower_1/conv_4/conv2d_params_quint8_const
    3.598    0.005    0.002   0.000%     99.952%         Const  mixed_6/tower_1/conv_3/batchnorm/moving_variance_quint8_const
    3.571    0.003    0.002   0.000%     99.952%         Const  mixed_6/tower_1/conv_3/conv2d_params_quint8_const
    3.408    0.005    0.002   0.000%     99.952%         Const  mixed_6/tower_1/conv/conv2d_params_quint8_const
    3.246    0.003    0.002   0.000%     99.952%         Const  mixed_6/tower/conv/conv2d_params_quint8_const
    2.816    0.007    0.002   0.000%     99.952%         Const  mixed_5/tower/conv_2/batchnorm/moving_mean_quint8_const
    2.109    0.004    0.002   0.000%     99.952%         Const  mixed_4/tower/conv/conv2d_params_quint8_const
    1.802    0.003    0.002   0.000%     99.952%         Const  mixed_3/conv/conv2d_params_quint8_const
    1.370    0.003    0.002   0.000%     99.952%         Const  mixed_1/tower_2/conv/conv2d_params_quint8_const
    1.051    0.003    0.002   0.000%     99.952%         Const  mixed_1/conv/batchnorm/moving_mean_max
    5.680    0.003    0.002   0.000%     99.952%         Const  mixed_9/tower_1/mixed/conv_1/conv2d_params_quint8_const
    5.623    0.004    0.002   0.000%     99.952%         Const  mixed_9/tower_1/mixed/conv/conv2d_params_quint8_const
    5.509    0.004    0.002   0.000%     99.952%         Const  mixed_9/tower_1/conv/conv2d_params_quint8_const
    5.449    0.003    0.002   0.000%     99.952%         Const  mixed_9/tower/mixed/conv_1/conv2d_params_quint8_const
    3.954    0.003    0.002   0.000%     99.952%         Const  mixed_7/tower_1/conv/conv2d_params_quint8_const
    2.508    0.003    0.002   0.000%     99.952%         Const  mixed_4/tower_1/conv_4/conv2d_params_quint8_const
    2.449    0.003    0.002   0.000%     99.953%         Const  mixed_4/tower_1/conv_3/conv2d_params_quint8_const
    1.287    0.005    0.002   0.000%     99.953%         Const  mixed_1/tower_1/conv_1/batchnorm/moving_variance_quint8_const
    0.374    0.002    0.002   0.000%     99.953%         Const  conv_1/batchnorm/beta_quint8_const
    5.566    0.003    0.002   0.000%     99.953%         Const  mixed_9/tower_1/conv_1/conv2d_params_quint8_const
    5.123    0.003    0.002   0.000%     99.953%         Const  mixed_10/tower_1/mixed/conv/batchnorm/moving_mean_quint8_const
    4.495    0.003    0.002   0.000%     99.953%         Const  mixed_8/tower_1/conv_2/conv2d_params_quint8_const
    4.008    0.003    0.002   0.000%     99.953%         Const  mixed_7/tower_1/conv_1/conv2d_params_quint8_const
    2.688    0.005    0.002   0.000%     99.953%         Const  mixed_5/tower/conv/batchnorm/moving_mean_quint8_const
    2.562    0.003    0.002   0.000%     99.953%         Const  mixed_4/tower_2/conv/conv2d_params_quint8_const
    2.346    0.003    0.002   0.000%     99.953%         Const  mixed_4/tower_1/conv_1/conv2d_params_max
    2.066    0.005    0.002   0.000%     99.953%         Const  mixed_4/conv/batchnorm/moving_mean_quint8_const
    0.711    0.004    0.002   0.000%     99.953%         Const  mixed/tower/conv_1/batchnorm/moving_variance_quint8_const
    0.626    0.002    0.002   0.000%     99.953%         Const  mixed/tower/conv/batchnorm/moving_mean_quint8_const
    0.319    0.003    0.002   0.000%     99.953%         Const  conv/batchnorm/beta_max
    5.737    0.003    0.002   0.000%     99.953%         Const  mixed_9/tower_2/conv/conv2d_params_quint8_const
    4.097    0.003    0.002   0.000%     99.953%         Const  mixed_7/tower_1/conv_2/batchnorm/moving_variance_max
    3.748    0.006    0.002   0.000%     99.953%         Const  mixed_7/conv/batchnorm/moving_mean_quint8_const
    3.113    0.003    0.002   0.000%     99.953%         Const  mixed_5/tower_1/conv_4/batchnorm/moving_variance_quint8_const
    2.751    0.003    0.002   0.000%     99.954%         Const  mixed_5/tower/conv_1/conv2d_params_quint8_const
    1.654    0.006    0.002   0.000%     99.954%         Const  mixed_2/tower_1/conv_1/batchnorm/moving_mean_quint8_const
    1.315    0.003    0.002   0.000%     99.954%         Const  mixed_1/tower_1/conv_2/conv2d_params_quint8_const
    1.164    0.003    0.002   0.000%     99.954%         Const  mixed_1/tower/conv_1/batchnorm/moving_mean_quint8_const
    0.813    0.007    0.002   0.000%     99.954%         Const  mixed/tower_1/conv/batchnorm/moving_variance_max
    0.588    0.003    0.002   0.000%     99.954%         Const  mixed/conv/batchnorm/moving_variance_quint8_const
    0.532    0.003    0.002   0.000%     99.954%         Const  conv_4/batchnorm/moving_variance_quint8_const
    0.477    0.003    0.002   0.000%     99.954%         Const  conv_3/batchnorm/moving_variance_quint8_const
    5.350    0.003    0.002   0.000%     99.954%         Const  mixed_9/tower/conv/batchnorm/moving_mean_quint8_const
    4.386    0.003    0.002   0.000%     99.954%         Const  mixed_8/tower_1/conv/conv2d_params_quint8_const
    3.899    0.003    0.002   0.000%     99.954%         Const  mixed_7/tower/conv_2/conv2d_params_quint8_const
    3.368    0.003    0.002   0.000%     99.954%         Const  mixed_6/tower/conv_2/batchnorm/moving_mean_quint8_const
    2.143    0.002    0.002   0.000%     99.954%         Const  mixed_4/tower/conv/batchnorm/moving_variance_min
    1.590    0.003    0.002   0.000%     99.954%         Const  mixed_2/tower_1/conv/conv2d_params_quint8_const
    1.491    0.002    0.002   0.000%     99.954%         Const  mixed_2/tower/conv/batchnorm/moving_mean_quint8_const
    1.436    0.005    0.002   0.000%     99.954%         Const  mixed_2/conv/batchnorm/moving_mean_quint8_const
    0.980    0.006    0.002   0.000%     99.954%         Const  mixed/tower_2/conv/batchnorm/moving_mean_quint8_const
    0.866    0.003    0.002   0.000%     99.954%         Const  mixed/tower_1/conv_1/batchnorm/moving_variance_quint8_const
    0.416    0.003    0.002   0.000%     99.955%         Const  conv_2/batchnorm/moving_variance_quint8_const
    0.305    0.003    0.002   0.000%     99.955%         Const  conv/batchnorm/moving_variance_max
    5.289    0.007    0.002   0.000%     99.955%         Const  softmax/weights_quint8_const
    4.945    0.003    0.002   0.000%     99.955%         Const  mixed_10/tower/mixed/conv_1/batchnorm/moving_mean_quint8_const
    3.695    0.005    0.002   0.000%     99.955%         Const  mixed_6/tower_2/conv/batchnorm/moving_mean_quint8_const
    3.258    0.005    0.002   0.000%     99.955%         Const  mixed_6/tower/conv/batchnorm/moving_mean_quint8_const
    3.045    0.002    0.002   0.000%     99.955%         Const  mixed_5/tower_1/conv_3/batchnorm/moving_mean_quint8_const
    2.194    0.003    0.002   0.000%     99.955%         Const  mixed_4/tower/conv_1/batchnorm/moving_mean_max
    1.988    0.003    0.002   0.000%     99.955%         Const  mixed_3/tower/conv_1/batchnorm/beta_max
    1.476    0.006    0.002   0.000%     99.955%         Const  mixed_2/tower/conv/conv2d_params_quint8_const
    0.952    0.003    0.002   0.000%     99.955%         Const  mixed/tower_1/conv_2/batchnorm/beta_quint8_const
    0.832    0.003    0.002   0.000%     99.955%         Const  mixed/tower_1/conv/batchnorm/beta_max
    0.780    0.009    0.002   0.000%     99.955%         Const  mixed/tower_1/conv/batchnorm/moving_mean_min
    0.772    0.005    0.002   0.000%     99.955%         Const  mixed/tower_1/conv/batchnorm/moving_mean_quint8_const
    0.469    0.002    0.002   0.000%     99.955%         Const  conv_3/batchnorm/moving_mean_min
    0.360    0.003    0.002   0.000%     99.955%         Const  conv_1/batchnorm/moving_variance_quint8_const
    0.335    0.003    0.002   0.000%     99.955%         Const  conv_1/conv2d_params_min
    5.486    0.002    0.002   0.000%     99.955%         Const  mixed_9/tower/mixed/conv_1/batchnorm/moving_variance_min
    4.034    0.005    0.002   0.000%     99.955%         Const  mixed_7/tower_1/conv_1/batchnorm/moving_variance_quint8_const
    3.803    0.003    0.002   0.000%     99.956%         Const  mixed_7/tower/conv/batchnorm/moving_mean_quint8_const
    3.477    0.003    0.002   0.000%     99.956%         Const  mixed_6/tower_1/conv_1/batchnorm/moving_mean_quint8_const
    3.139    0.003    0.002   0.000%     99.956%         Const  mixed_5/tower_2/conv/conv2d_params_quint8_const
    2.836    0.002    0.002   0.000%     99.956%         Const  mixed_5/tower/conv_2/batchnorm/moving_variance_min
    2.492    0.004    0.002   0.000%     99.956%         Const  mixed_4/tower_1/conv_3/batchnorm/beta_quint8_const
    1.763    0.003    0.002   0.000%     99.956%         Const  mixed_2/tower_2/conv/batchnorm/moving_mean_quint8_const
    1.341    0.003    0.002   0.000%     99.956%         Const  mixed_1/tower_1/conv_2/batchnorm/moving_variance_quint8_const
    1.150    0.002    0.002   0.000%     99.956%         Const  mixed_1/tower/conv_1/conv2d_params_quint8_const
    1.120    0.003    0.002   0.000%     99.956%         Const  mixed_1/tower/conv/batchnorm/moving_mean_max
    0.823    0.003    0.002   0.000%     99.956%         Const  mixed/tower_1/conv/batchnorm/beta_quint8_const
    0.651    0.003    0.002   0.000%     99.956%         Const  mixed/tower/conv/batchnorm/beta_quint8_const
    0.519    0.003    0.002   0.000%     99.956%         Const  conv_4/batchnorm/moving_mean_quint8_const
    0.502    0.003    0.002   0.000%     99.956%         Const  conv_3/batchnorm/gamma_quint8_const
    0.456    0.003    0.002   0.000%     99.956%         Const  conv_3/conv2d_params_min
    0.365    0.002    0.002   0.000%     99.956%         Const  conv_1/batchnorm/moving_variance_min
    5.684    0.003    0.002   0.000%     99.956%         Const  mixed_9/tower_1/mixed/conv_1/conv2d_params_min
    4.292    0.002    0.002   0.000%     99.956%         Const  mixed_8/tower/conv/batchnorm/moving_mean_quint8_const
    4.238    0.002    0.002   0.000%     99.956%         Const  mixed_7/tower_2/conv/batchnorm/moving_mean_quint8_const
    3.968    0.003    0.002   0.000%     99.956%         Const  mixed_7/tower_1/conv/batchnorm/moving_mean_quint8_const
    3.832    0.003    0.002   0.000%     99.957%         Const  mixed_7/tower/conv/batchnorm/beta_quint8_const
    3.586    0.003    0.002   0.000%     99.957%         Const  mixed_6/tower_1/conv_3/batchnorm/moving_mean_quint8_const
    3.532    0.003    0.002   0.000%     99.957%         Const  mixed_6/tower_1/conv_2/batchnorm/moving_mean_quint8_const
    3.423    0.002    0.002   0.000%     99.957%         Const  mixed_6/tower_1/conv/batchnorm/moving_mean_quint8_const
    3.207    0.003    0.002   0.000%     99.957%         Const  mixed_6/conv/batchnorm/moving_mean_quint8_const
    2.354    0.003    0.002   0.000%     99.957%         Const  mixed_4/tower_1/conv_1/batchnorm/moving_mean_min
    2.160    0.002    0.002   0.000%     99.957%         Const  mixed_4/tower/conv/batchnorm/beta_max
    1.668    0.003    0.002   0.000%     99.957%         Const  mixed_2/tower_1/conv_1/batchnorm/moving_variance_quint8_const
    1.382    0.006    0.002   0.000%     99.957%         Const  mixed_1/tower_2/conv/batchnorm/moving_mean_quint8_const
    1.209    0.005    0.002   0.000%     99.957%         Const  mixed_1/tower_1/conv/conv2d_params_min
    1.179    0.002    0.002   0.000%     99.957%         Const  mixed_1/tower/conv_1/batchnorm/moving_variance_quint8_const
    1.070    0.003    0.002   0.000%     99.957%         Const  mixed_1/conv/batchnorm/beta_quint8_const
    0.991    0.003    0.002   0.000%     99.957%         Const  mixed/tower_2/conv/batchnorm/moving_mean_max
    0.930    0.003    0.002   0.000%     99.957%         Const  mixed/tower_1/conv_2/batchnorm/moving_mean_max
    0.799    0.004    0.002   0.000%     99.957%         Const  mixed/tower_1/conv/batchnorm/moving_variance_quint8_const
    0.759    0.004    0.002   0.000%     99.957%         Const  mixed/tower_1/conv/conv2d_params_min
    0.731    0.004    0.002   0.000%     99.957%         Const  mixed/tower/conv_1/batchnorm/beta_quint8_const
    0.671    0.005    0.002   0.000%     99.957%         Const  mixed/tower/conv_1/conv2d_params_quint8_const
    0.541    0.002    0.002   0.000%     99.957%         Const  conv_4/batchnorm/moving_variance_max
    0.494    0.002    0.002   0.000%     99.958%         Const  conv_3/batchnorm/beta_min
    0.346    0.003    0.002   0.000%     99.958%         Const  conv_1/batchnorm/moving_mean_quint8_const
    5.306    0.003    0.001   0.000%     99.958%         Const  softmax/logits/MatMul_eightbit_reshape_dims
    4.811    0.003    0.001   0.000%     99.958%         Const  mixed_10/tower/conv/batchnorm/moving_mean_quint8_const
    3.616    0.008    0.001   0.000%     99.958%         Const  mixed_6/tower_1/conv_3/batchnorm/beta_min
    3.612    0.002    0.001   0.000%     99.958%         Const  mixed_6/tower_1/conv_3/batchnorm/beta_quint8_const
    3.503    0.005    0.001   0.000%     99.958%         Const  mixed_6/tower_1/conv_1/batchnorm/beta_quint8_const
    2.059    0.002    0.001   0.000%     99.958%         Const  mixed_4/conv/conv2d_params_min
    1.962    0.006    0.001   0.000%     99.958%         Const  mixed_3/tower/conv_1/batchnorm/moving_variance_quint8_const
    1.330    0.003    0.001   0.000%     99.958%         Const  mixed_1/tower_1/conv_2/batchnorm/moving_mean_quint8_const
    1.039    0.003    0.001   0.000%     99.958%         Const  mixed_1/conv/batchnorm/moving_mean_quint8_const
    0.973    0.002    0.001   0.000%     99.958%         Const  mixed/tower_2/conv/conv2d_params_min
    0.909    0.003    0.001   0.000%     99.958%         Const  mixed/tower_1/conv_2/conv2d_params_min
    0.577    0.005    0.001   0.000%     99.958%         Const  mixed/conv/batchnorm/moving_mean_min
    5.342    0.003    0.001   0.000%     99.958%         Const  mixed_9/tower/conv/conv2d_params_min
    5.285    0.003    0.001   0.000%     99.958%         Const  pool_3/_reshape/shape
    4.456    0.003    0.001   0.000%     99.958%         Const  mixed_8/tower_1/conv_1/batchnorm/moving_mean_quint8_const
    3.914    0.003    0.001   0.000%     99.958%         Const  mixed_7/tower/conv_2/batchnorm/moving_mean_quint8_const
    3.655    0.003    0.001   0.000%     99.958%         Const  mixed_6/tower_1/conv_4/batchnorm/moving_variance_quint8_const
    3.644    0.002    0.001   0.000%     99.958%         Const  mixed_6/tower_1/conv_4/batchnorm/moving_mean_quint8_const
    3.557    0.003    0.001   0.000%     99.959%         Const  mixed_6/tower_1/conv_2/batchnorm/beta_quint8_const
    2.791    0.002    0.001   0.000%     99.959%         Const  mixed_5/tower/conv_1/batchnorm/beta_quint8_const
    2.716    0.003    0.001   0.000%     99.959%         Const  mixed_5/tower/conv/batchnorm/beta_quint8_const
    2.428    0.002    0.001   0.000%     99.959%         Const  mixed_4/tower_1/conv_2/batchnorm/moving_variance_min
    2.271    0.003    0.001   0.000%     99.959%         Const  mixed_4/tower/conv_2/batchnorm/beta_quint8_const
    2.241    0.004    0.001   0.000%     99.959%         Const  mixed_4/tower/conv_2/batchnorm/moving_mean_quint8_const
    2.184    0.005    0.001   0.000%     99.959%         Const  mixed_4/tower/conv_1/batchnorm/moving_mean_quint8_const
    2.153    0.003    0.001   0.000%     99.959%         Const  mixed_4/tower/conv/batchnorm/beta_quint8_const
    1.825    0.005    0.001   0.000%     99.959%         Const  mixed_3/conv/batchnorm/moving_mean_max
    1.602    0.005    0.001   0.000%     99.959%         Const  mixed_2/tower_1/conv/batchnorm/moving_mean_quint8_const
    1.429    0.002    0.001   0.000%     99.959%         Const  mixed_2/conv/conv2d_params_min
    1.378    0.003    0.001   0.000%     99.959%         Const  mixed_1/tower_2/conv/conv2d_params_max
    1.275    0.003    0.001   0.000%     99.959%         Const  mixed_1/tower_1/conv_1/batchnorm/moving_mean_quint8_const
    1.234    0.003    0.001   0.000%     99.959%         Const  mixed_1/tower_1/conv/batchnorm/moving_variance_quint8_const
    1.097    0.003    0.001   0.000%     99.959%         Const  mixed_1/tower/conv/batchnorm/moving_mean_quint8_const
    0.638    0.003    0.001   0.000%     99.959%         Const  mixed/tower/conv/batchnorm/moving_variance_quint8_const
    0.596    0.003    0.001   0.000%     99.959%         Const  mixed/conv/batchnorm/moving_variance_max
    0.545    0.002    0.001   0.000%     99.959%         Const  conv_4/batchnorm/beta_quint8_const
    0.383    0.003    0.001   0.000%     99.959%         Const  conv_1/batchnorm/beta_max
    5.365    0.003    0.001   0.000%     99.959%         Const  mixed_9/tower/conv/batchnorm/moving_variance_quint8_const
    5.153    0.003    0.001   0.000%     99.960%         Const  mixed_10/tower_1/mixed/conv/batchnorm/beta_quint8_const
    4.765    0.003    0.001   0.000%     99.960%         Const  mixed_10/conv/batchnorm/moving_variance_quint8_const
    4.650    0.002    0.001   0.000%     99.960%         Const  mixed_9/tower_2/conv/batchnorm/beta_max
    4.615    0.005    0.001   0.000%     99.960%         Const  mixed_9/conv/batchnorm/moving_mean_quint8_const
    4.346    0.002    0.001   0.000%     99.960%         Const  mixed_8/tower/conv_1/batchnorm/moving_mean_quint8_const
    4.266    0.003    0.001   0.000%     99.960%         Const  mixed_7/tower_2/conv/batchnorm/beta_quint8_const
    4.129    0.003    0.001   0.000%     99.960%         Const  mixed_7/tower_1/conv_3/batchnorm/moving_mean_quint8_const
    3.777    0.003    0.001   0.000%     99.960%         Const  mixed_7/conv/batchnorm/beta_quint8_const
    3.448    0.006    0.001   0.000%     99.960%         Const  mixed_6/tower_1/conv/batchnorm/beta_quint8_const
    3.341    0.002    0.001   0.000%     99.960%         Const  mixed_6/tower/conv_1/batchnorm/beta_quint8_const
    2.987    0.003    0.001   0.000%     99.960%         Const  mixed_5/tower_1/conv_2/batchnorm/moving_mean_quint8_const
    2.932    0.003    0.001   0.000%     99.960%         Const  mixed_5/tower_1/conv_1/batchnorm/moving_mean_quint8_const
    2.764    0.003    0.001   0.000%     99.960%         Const  mixed_5/tower/conv_1/batchnorm/moving_mean_quint8_const
    2.463    0.003    0.001   0.000%     99.960%         Const  mixed_4/tower_1/conv_3/batchnorm/moving_mean_quint8_const
    2.424    0.003    0.001   0.000%     99.960%         Const  mixed_4/tower_1/conv_2/batchnorm/moving_variance_quint8_const
    2.076    0.002    0.001   0.000%     99.960%         Const  mixed_4/conv/batchnorm/moving_mean_max
    2.014    0.003    0.001   0.000%     99.960%         Const  mixed_3/tower/conv_2/batchnorm/moving_mean_quint8_const
    1.976    0.002    0.001   0.000%     99.960%         Const  mixed_3/tower/conv_1/batchnorm/beta_quint8_const
    1.842    0.006    0.001   0.000%     99.960%         Const  mixed_3/conv/batchnorm/beta_quint8_const
    1.832    0.002    0.001   0.000%     99.960%         Const  mixed_3/conv/batchnorm/moving_variance_quint8_const
    1.708    0.003    0.001   0.000%     99.961%         Const  mixed_2/tower_1/conv_2/batchnorm/moving_mean_quint8_const
    1.450    0.002    0.001   0.000%     99.961%         Const  mixed_2/conv/batchnorm/moving_variance_quint8_const
    1.363    0.006    0.001   0.000%     99.961%         Const  mixed_1/tower_1/conv_2/batchnorm/beta_max
    1.324    0.005    0.001   0.000%     99.961%         Const  mixed_1/tower_1/conv_2/conv2d_params_max
    1.251    0.004    0.001   0.000%     99.961%         Const  mixed_1/tower_1/conv/batchnorm/beta_min
    1.154    0.005    0.001   0.000%     99.961%         Const  mixed_1/tower/conv_1/conv2d_params_min
    1.011    0.003    0.001   0.000%     99.961%         Const  mixed/tower_2/conv/batchnorm/beta_quint8_const
    0.939    0.008    0.001   0.000%     99.961%         Const  mixed/tower_1/conv_2/batchnorm/moving_variance_min
    0.605    0.002    0.001   0.000%     99.961%         Const  mixed/conv/batchnorm/beta_min
    0.473    0.002    0.001   0.000%     99.961%         Const  conv_3/batchnorm/moving_mean_max
    0.407    0.002    0.001   0.000%     99.961%         Const  conv_2/batchnorm/moving_mean_min
    0.296    0.003    0.001   0.000%     99.961%         Const  conv/batchnorm/moving_variance_quint8_const
    5.583    0.002    0.001   0.000%     99.961%         Const  mixed_9/tower_1/conv_1/batchnorm/moving_mean_quint8_const
    4.885    0.003    0.001   0.000%     99.961%         Const  mixed_10/tower/mixed/conv/batchnorm/moving_mean_quint8_const
    4.825    0.003    0.001   0.000%     99.961%         Const  mixed_10/tower/conv/batchnorm/moving_variance_quint8_const
    4.759    0.005    0.001   0.000%     99.961%         Const  mixed_10/conv/batchnorm/moving_mean_max
    4.743    0.003    0.001   0.000%     99.961%         Const  mixed_10/conv/conv2d_params_min
    4.589    0.002    0.001   0.000%     99.961%         Const  mixed_8/tower_1/conv_3/batchnorm/beta_quint8_const
    4.563    0.003    0.001   0.000%     99.961%         Const  mixed_8/tower_1/conv_3/batchnorm/moving_mean_quint8_const
    4.510    0.003    0.001   0.000%     99.961%         Const  mixed_8/tower_1/conv_2/batchnorm/moving_mean_quint8_const
    4.417    0.003    0.001   0.000%     99.962%         Const  mixed_8/tower_1/conv/batchnorm/moving_variance_quint8_const
    4.320    0.002    0.001   0.000%     99.962%         Const  mixed_8/tower/conv/batchnorm/beta_quint8_const
    4.229    0.003    0.001   0.000%     99.962%         Const  mixed_7/tower_2/conv/conv2d_params_min
    4.195    0.003    0.001   0.000%     99.962%         Const  mixed_7/tower_1/conv_4/batchnorm/moving_variance_quint8_const
    4.076    0.003    0.001   0.000%     99.962%         Const  mixed_7/tower_1/conv_2/batchnorm/moving_mean_quint8_const
    4.022    0.003    0.001   0.000%     99.962%         Const  mixed_7/tower_1/conv_1/batchnorm/moving_mean_quint8_const
    3.940    0.005    0.001   0.000%     99.962%         Const  mixed_7/tower/conv_2/batchnorm/beta_quint8_const
    3.872    0.003    0.001   0.000%     99.962%         Const  mixed_7/tower/conv_1/batchnorm/moving_variance_quint8_const
    3.763    0.002    0.001   0.000%     99.962%         Const  mixed_7/conv/batchnorm/moving_variance_quint8_const
    3.441    0.002    0.001   0.000%     99.962%         Const  mixed_6/tower_1/conv/batchnorm/moving_variance_min
    3.360    0.003    0.001   0.000%     99.962%         Const  mixed_6/tower/conv_2/conv2d_params_min
    3.165    0.005    0.001   0.000%     99.962%         Const  mixed_5/tower_2/conv/batchnorm/moving_variance_quint8_const
    3.070    0.005    0.001   0.000%     99.962%         Const  mixed_5/tower_1/conv_3/batchnorm/beta_quint8_const
    3.013    0.005    0.001   0.000%     99.962%         Const  mixed_5/tower_1/conv_2/batchnorm/beta_quint8_const
    2.662    0.002    0.001   0.000%     99.962%         Const  mixed_5/conv/batchnorm/beta_quint8_const
    2.567    0.002    0.001   0.000%     99.962%         Const  mixed_4/tower_2/conv/conv2d_params_min
    2.523    0.002    0.001   0.000%     99.962%         Const  mixed_4/tower_1/conv_4/batchnorm/moving_mean_quint8_const
    2.435    0.005    0.001   0.000%     99.962%         Const  mixed_4/tower_1/conv_2/batchnorm/beta_quint8_const
    2.139    0.003    0.001   0.000%     99.962%         Const  mixed_4/tower/conv/batchnorm/moving_variance_quint8_const
    1.923    0.002    0.001   0.000%     99.962%         Const  mixed_3/tower/conv/batchnorm/beta_quint8_const
    1.912    0.002    0.001   0.000%     99.963%         Const  mixed_3/tower/conv/batchnorm/moving_variance_quint8_const
    1.777    0.002    0.001   0.000%     99.963%         Const  mixed_2/tower_2/conv/batchnorm/moving_variance_quint8_const
    1.576    0.003    0.001   0.000%     99.963%         Const  mixed_2/tower/conv_1/batchnorm/beta_quint8_const
    1.305    0.005    0.001   0.000%     99.963%         Const  mixed_1/tower_1/conv_1/batchnorm/beta_min
    1.297    0.003    0.001   0.000%     99.963%         Const  mixed_1/tower_1/conv_1/batchnorm/moving_variance_max
    0.960    0.006    0.001   0.000%     99.963%         Const  mixed/tower_1/conv_2/batchnorm/beta_max
    0.630    0.003    0.001   0.000%     99.963%         Const  mixed/tower/conv/batchnorm/moving_mean_min
    0.429    0.003    0.001   0.000%     99.963%         Const  conv_2/batchnorm/beta_quint8_const
    0.398    0.002    0.001   0.000%     99.963%         Const  conv_2/conv2d_params_max
    0.340    0.003    0.001   0.000%     99.963%         Const  conv_1/conv2d_params_max
    0.301    0.002    0.001   0.000%     99.963%         Const  conv/batchnorm/moving_variance_min
    5.766    0.003    0.001   0.000%     99.963%         Const  mixed_9/tower_2/conv/batchnorm/moving_variance_quint8_const
    5.750    0.006    0.001   0.000%     99.963%         Const  mixed_9/tower_2/conv/batchnorm/moving_mean_quint8_const
    5.695    0.003    0.001   0.000%     99.963%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/moving_mean_quint8_const
    5.594    0.006    0.001   0.000%     99.963%         Const  mixed_9/tower_1/conv_1/batchnorm/moving_variance_quint8_const
    5.552    0.005    0.001   0.000%     99.963%         Const  mixed_9/tower_1/conv/batchnorm/beta_quint8_const
    5.494    0.006    0.001   0.000%     99.963%         Const  mixed_9/tower/mixed/conv_1/batchnorm/beta_quint8_const
    5.473    0.007    0.001   0.000%     99.963%         Const  mixed_9/tower/mixed/conv_1/batchnorm/moving_mean_max
    5.408    0.002    0.001   0.000%     99.963%         Const  mixed_9/tower/mixed/conv/batchnorm/moving_mean_quint8_const
    5.380    0.003    0.001   0.000%     99.963%         Const  mixed_9/tower/conv/batchnorm/beta_quint8_const
    5.243    0.003    0.001   0.000%     99.964%         Const  mixed_10/tower_2/conv/batchnorm/moving_mean_quint8_const
    5.034    0.002    0.001   0.000%     99.964%         Const  mixed_10/tower_1/conv/batchnorm/beta_min
    4.902    0.002    0.001   0.000%     99.964%         Const  mixed_10/tower/mixed/conv/batchnorm/moving_variance_quint8_const
    4.777    0.005    0.001   0.000%     99.964%         Const  mixed_10/conv/batchnorm/beta_quint8_const
    4.751    0.002    0.001   0.000%     99.964%         Const  mixed_10/conv/batchnorm/moving_mean_quint8_const
    4.428    0.005    0.001   0.000%     99.964%         Const  mixed_8/tower_1/conv/batchnorm/beta_quint8_const
    4.401    0.003    0.001   0.000%     99.964%         Const  mixed_8/tower_1/conv/batchnorm/moving_mean_quint8_const
    4.360    0.003    0.001   0.000%     99.964%         Const  mixed_8/tower/conv_1/batchnorm/moving_variance_quint8_const
    3.983    0.002    0.001   0.000%     99.964%         Const  mixed_7/tower_1/conv/batchnorm/moving_variance_quint8_const
    3.922    0.006    0.001   0.000%     99.964%         Const  mixed_7/tower/conv_2/batchnorm/moving_mean_max
    3.907    0.002    0.001   0.000%     99.964%         Const  mixed_7/tower/conv_2/conv2d_params_min
    3.741    0.002    0.001   0.000%     99.964%         Const  mixed_7/conv/conv2d_params_min
    3.709    0.002    0.001   0.000%     99.964%         Const  mixed_6/tower_2/conv/batchnorm/moving_variance_quint8_const
    3.633    0.003    0.001   0.000%     99.964%         Const  mixed_6/tower_1/conv_4/conv2d_params_min
    3.444    0.003    0.001   0.000%     99.964%         Const  mixed_6/tower_1/conv/batchnorm/moving_variance_max
    3.437    0.003    0.001   0.000%     99.964%         Const  mixed_6/tower_1/conv/batchnorm/moving_variance_quint8_const
    3.311    0.003    0.001   0.000%     99.964%         Const  mixed_6/tower/conv_1/batchnorm/moving_mean_quint8_const
    3.225    0.002    0.001   0.000%     99.964%         Const  mixed_6/conv/batchnorm/moving_variance_min
    3.178    0.003    0.001   0.000%     99.964%         Const  mixed_5/tower_2/conv/batchnorm/beta_quint8_const
    2.946    0.002    0.001   0.000%     99.964%         Const  mixed_5/tower_1/conv_1/batchnorm/moving_variance_quint8_const
    2.925    0.002    0.001   0.000%     99.964%         Const  mixed_5/tower_1/conv_1/conv2d_params_min
    2.892    0.003    0.001   0.000%     99.965%         Const  mixed_5/tower_1/conv/batchnorm/moving_variance_quint8_const
    2.878    0.002    0.001   0.000%     99.965%         Const  mixed_5/tower_1/conv/batchnorm/moving_mean_quint8_const
    2.853    0.003    0.001   0.000%     99.965%         Const  mixed_5/tower/conv_2/batchnorm/beta_max
    2.846    0.003    0.001   0.000%     99.965%         Const  mixed_5/tower/conv_2/batchnorm/beta_quint8_const
    2.706    0.002    0.001   0.000%     99.965%         Const  mixed_5/tower/conv/batchnorm/moving_variance_min
    2.633    0.006    0.001   0.000%     99.965%         Const  mixed_5/conv/batchnorm/moving_mean_quint8_const
    2.588    0.005    0.001   0.000%     99.965%         Const  mixed_4/tower_2/conv/batchnorm/moving_variance_quint8_const
    2.453    0.005    0.001   0.000%     99.965%         Const  mixed_4/tower_1/conv_3/conv2d_params_min
    2.147    0.005    0.001   0.000%     99.965%         Const  mixed_4/tower/conv/batchnorm/moving_variance_max
    2.080    0.002    0.001   0.000%     99.965%         Const  mixed_4/conv/batchnorm/moving_variance_quint8_const
    2.025    0.003    0.001   0.000%     99.965%         Const  mixed_3/tower/conv_2/batchnorm/moving_variance_quint8_const
    1.767    0.005    0.001   0.000%     99.965%         Const  mixed_2/tower_2/conv/batchnorm/moving_mean_min
    1.626    0.002    0.001   0.000%     99.965%         Const  mixed_2/tower_1/conv/batchnorm/moving_variance_max
    1.551    0.002    0.001   0.000%     99.965%         Const  mixed_2/tower/conv_1/batchnorm/moving_mean_quint8_const
    1.410    0.003    0.001   0.000%     99.965%         Const  mixed_1/tower_2/conv/batchnorm/beta_quint8_const
    1.397    0.002    0.001   0.000%     99.965%         Const  mixed_1/tower_2/conv/batchnorm/moving_variance_quint8_const
    1.338    0.002    0.001   0.000%     99.965%         Const  mixed_1/tower_1/conv_2/batchnorm/moving_mean_max
    1.257    0.002    0.001   0.000%     99.965%         Const  mixed_1/tower_1/conv/batchnorm/beta_max
    1.227    0.003    0.001   0.000%     99.965%         Const  mixed_1/tower_1/conv/batchnorm/moving_mean_max
    1.160    0.003    0.001   0.000%     99.965%         Const  mixed_1/tower/conv_1/conv2d_params_max
    1.031    0.003    0.001   0.000%     99.965%         Const  mixed_1/conv/conv2d_params_min
    0.891    0.002    0.001   0.000%     99.966%         Const  mixed/tower_1/conv_1/batchnorm/beta_max
    0.887    0.002    0.001   0.000%     99.966%         Const  mixed/tower_1/conv_1/batchnorm/beta_min
    0.874    0.003    0.001   0.000%     99.966%         Const  mixed/tower_1/conv_1/batchnorm/moving_variance_max
    0.862    0.003    0.001   0.000%     99.966%         Const  mixed/tower_1/conv_1/batchnorm/moving_mean_max
    0.647    0.002    0.001   0.000%     99.966%         Const  mixed/tower/conv/batchnorm/moving_variance_max
    0.643    0.002    0.001   0.000%     99.966%         Const  mixed/tower/conv/batchnorm/moving_variance_min
    0.634    0.003    0.001   0.000%     99.966%         Const  mixed/tower/conv/batchnorm/moving_mean_max
    0.601    0.002    0.001   0.000%     99.966%         Const  mixed/conv/batchnorm/beta_quint8_const
    0.485    0.003    0.001   0.000%     99.966%         Const  conv_3/batchnorm/moving_variance_max
    5.664    0.003    0.001   0.000%     99.966%         Const  mixed_9/tower_1/mixed/conv/batchnorm/beta_quint8_const
    5.426    0.003    0.001   0.000%     99.966%         Const  mixed_9/tower/mixed/conv/batchnorm/moving_variance_min
    5.270    0.005    0.001   0.000%     99.966%         Const  mixed_10/tower_2/conv/batchnorm/beta_quint8_const
    5.235    0.003    0.001   0.000%     99.966%         Const  mixed_10/tower_2/conv/conv2d_params_min
    5.041    0.003    0.001   0.000%     99.966%         Const  mixed_10/tower_1/conv/batchnorm/gamma_quint8_const
    5.030    0.002    0.001   0.000%     99.966%         Const  mixed_10/tower_1/conv/batchnorm/beta_quint8_const
    4.960    0.003    0.001   0.000%     99.966%         Const  mixed_10/tower/mixed/conv_1/batchnorm/moving_variance_quint8_const
    4.937    0.003    0.001   0.000%     99.966%         Const  mixed_10/tower/mixed/conv_1/conv2d_params_min
    4.829    0.002    0.001   0.000%     99.966%         Const  mixed_10/tower/conv/batchnorm/moving_variance_min
    4.642    0.003    0.001   0.000%     99.966%         Const  mixed_9/conv/batchnorm/beta_quint8_const
    4.567    0.003    0.001   0.000%     99.966%         Const  mixed_8/tower_1/conv_3/batchnorm/moving_mean_min
    4.364    0.002    0.001   0.000%     99.967%         Const  mixed_8/tower/conv_1/batchnorm/moving_variance_min
    4.156    0.002    0.001   0.000%     99.967%         Const  mixed_7/tower_1/conv_3/batchnorm/beta_quint8_const
    4.141    0.002    0.001   0.000%     99.967%         Const  mixed_7/tower_1/conv_3/batchnorm/moving_variance_quint8_const
    4.066    0.003    0.001   0.000%     99.967%         Const  mixed_7/tower_1/conv_2/conv2d_params_min
    3.858    0.003    0.001   0.000%     99.967%         Const  mixed_7/tower/conv_1/batchnorm/moving_mean_quint8_const
    3.759    0.002    0.001   0.000%     99.967%         Const  mixed_7/conv/batchnorm/moving_mean_max
    3.672    0.003    0.001   0.000%     99.967%         Const  mixed_6/tower_1/conv_4/batchnorm/beta_min
    3.522    0.005    0.001   0.000%     99.967%         Const  mixed_6/tower_1/conv_2/conv2d_params_min
    3.513    0.003    0.001   0.000%     99.967%         Const  mixed_6/tower_1/conv_1/batchnorm/beta_max
    3.326    0.003    0.001   0.000%     99.967%         Const  mixed_6/tower/conv_1/batchnorm/moving_variance_quint8_const
    3.272    0.002    0.001   0.000%     99.967%         Const  mixed_6/tower/conv/batchnorm/moving_variance_quint8_const
    3.219    0.002    0.001   0.000%     99.967%         Const  mixed_6/conv/batchnorm/moving_variance_quint8_const
    3.032    0.008    0.001   0.000%     99.967%         Const  mixed_5/tower_1/conv_3/conv2d_params_min
    2.906    0.003    0.001   0.000%     99.967%         Const  mixed_5/tower_1/conv/batchnorm/beta_quint8_const
    2.787    0.003    0.001   0.000%     99.967%         Const  mixed_5/tower/conv_1/batchnorm/moving_variance_max
    2.776    0.003    0.001   0.000%     99.967%         Const  mixed_5/tower/conv_1/batchnorm/moving_variance_quint8_const
    2.769    0.002    0.001   0.000%     99.967%         Const  mixed_5/tower/conv_1/batchnorm/moving_mean_min
    2.577    0.002    0.001   0.000%     99.967%         Const  mixed_4/tower_2/conv/batchnorm/moving_mean_quint8_const
    2.381    0.006    0.001   0.000%     99.967%         Const  mixed_4/tower_1/conv_1/batchnorm/beta_quint8_const
    2.350    0.003    0.001   0.000%     99.967%         Const  mixed_4/tower_1/conv_1/batchnorm/moving_mean_quint8_const
    2.343    0.002    0.001   0.000%     99.967%         Const  mixed_4/tower_1/conv_1/conv2d_params_min
    2.254    0.002    0.001   0.000%     99.968%         Const  mixed_4/tower/conv_2/batchnorm/moving_mean_max
    2.230    0.003    0.001   0.000%     99.968%         Const  mixed_4/tower/conv_2/conv2d_params_min
    2.047    0.006    0.001   0.000%     99.968%         Const  mixed_3/tower/conv_2/batchnorm/beta_max
    2.022    0.002    0.001   0.000%     99.968%         Const  mixed_3/tower/conv_2/batchnorm/moving_mean_max
    1.951    0.003    0.001   0.000%     99.968%         Const  mixed_3/tower/conv_1/batchnorm/moving_mean_quint8_const
    1.933    0.003    0.001   0.000%     99.968%         Const  mixed_3/tower/conv/batchnorm/beta_max
    1.629    0.003    0.001   0.000%     99.968%         Const  mixed_2/tower_1/conv/batchnorm/beta_quint8_const
    1.519    0.003    0.001   0.000%     99.968%         Const  mixed_2/tower/conv/batchnorm/beta_quint8_const
    1.301    0.002    0.001   0.000%     99.968%         Const  mixed_1/tower_1/conv_1/batchnorm/beta_quint8_const
    1.074    0.003    0.001   0.000%     99.968%         Const  mixed_1/conv/batchnorm/beta_min
    1.063    0.006    0.001   0.000%     99.968%         Const  mixed_1/conv/batchnorm/moving_variance_max
    1.055    0.003    0.001   0.000%     99.968%         Const  mixed_1/conv/batchnorm/moving_variance_quint8_const
    0.738    0.004    0.001   0.000%     99.968%         Const  mixed/tower/conv_1/batchnorm/beta_min
    0.724    0.004    0.001   0.000%     99.968%         Const  mixed/tower/conv_1/batchnorm/moving_variance_max
    0.524    0.002    0.001   0.000%     99.968%         Const  conv_4/batchnorm/moving_mean_min
    0.511    0.003    0.001   0.000%     99.968%         Const  conv_4/conv2d_params_min
    0.498    0.002    0.001   0.000%     99.968%         Const  conv_3/batchnorm/beta_max
    0.460    0.003    0.001   0.000%     99.968%         Const  conv_3/conv2d_params_max
    0.433    0.003    0.001   0.000%     99.968%         Const  conv_2/batchnorm/beta_min
    0.421    0.002    0.001   0.000%     99.968%         Const  conv_2/batchnorm/moving_variance_min
    0.393    0.003    0.001   0.000%     99.968%         Const  conv_2/conv2d_params_min
    0.287    0.003    0.001   0.000%     99.969%         Const  conv/batchnorm/moving_mean_min
    5.540    0.003    0.001   0.000%     99.969%         Const  mixed_9/tower_1/conv/batchnorm/moving_variance_quint8_const
    5.138    0.003    0.001   0.000%     99.969%         Const  mixed_10/tower_1/mixed/conv/batchnorm/moving_variance_quint8_const
    4.876    0.003    0.001   0.000%     99.969%         Const  mixed_10/tower/mixed/conv/conv2d_params_min
    4.821    0.003    0.001   0.000%     99.969%         Const  mixed_10/tower/conv/batchnorm/moving_mean_max
    4.787    0.003    0.001   0.000%     99.969%         Const  mixed_10/conv/batchnorm/beta_max
    4.638    0.003    0.001   0.000%     99.969%         Const  mixed_9/conv/batchnorm/moving_variance_max
    4.521    0.005    0.001   0.000%     99.969%         Const  mixed_8/tower_1/conv_2/batchnorm/moving_variance_quint8_const
    4.390    0.006    0.001   0.000%     99.969%         Const  mixed_8/tower_1/conv/conv2d_params_min
    4.048    0.003    0.001   0.000%     99.969%         Const  mixed_7/tower_1/conv_1/batchnorm/beta_quint8_const
    4.004    0.003    0.001   0.000%     99.969%         Const  mixed_7/tower_1/conv/batchnorm/beta_max
    3.851    0.002    0.001   0.000%     99.969%         Const  mixed_7/tower/conv_1/conv2d_params_min
    3.539    0.003    0.001   0.000%     99.969%         Const  mixed_6/tower_1/conv_2/batchnorm/moving_mean_max
    3.492    0.002    0.001   0.000%     99.969%         Const  mixed_6/tower_1/conv_1/batchnorm/moving_variance_quint8_const
    3.322    0.003    0.001   0.000%     99.969%         Const  mixed_6/tower/conv_1/batchnorm/moving_mean_max
    3.304    0.003    0.001   0.000%     99.969%         Const  mixed_6/tower/conv_1/conv2d_params_min
    3.286    0.002    0.001   0.000%     99.969%         Const  mixed_6/tower/conv/batchnorm/beta_quint8_const
    3.088    0.003    0.001   0.000%     99.969%         Const  mixed_5/tower_1/conv_4/conv2d_params_min
    3.023    0.003    0.001   0.000%     99.969%         Const  mixed_5/tower_1/conv_2/batchnorm/beta_max
    3.002    0.002    0.001   0.000%     99.969%         Const  mixed_5/tower_1/conv_2/batchnorm/moving_variance_quint8_const
    2.942    0.002    0.001   0.000%     99.969%         Const  mixed_5/tower_1/conv_1/batchnorm/moving_mean_max
    2.882    0.006    0.001   0.000%     99.970%         Const  mixed_5/tower_1/conv/batchnorm/moving_mean_min
    2.798    0.005    0.001   0.000%     99.970%         Const  mixed_5/tower/conv_1/batchnorm/beta_max
    2.773    0.002    0.001   0.000%     99.970%         Const  mixed_5/tower/conv_1/batchnorm/moving_mean_max
    2.702    0.002    0.001   0.000%     99.970%         Const  mixed_5/tower/conv/batchnorm/moving_variance_quint8_const
    2.698    0.003    0.001   0.000%     99.970%         Const  mixed_5/tower/conv/batchnorm/moving_mean_max
    2.648    0.002    0.001   0.000%     99.970%         Const  mixed_5/conv/batchnorm/moving_variance_quint8_const
    2.534    0.005    0.001   0.000%     99.970%         Const  mixed_4/tower_1/conv_4/batchnorm/moving_variance_quint8_const
    2.513    0.002    0.001   0.000%     99.970%         Const  mixed_4/tower_1/conv_4/conv2d_params_min
    2.410    0.003    0.001   0.000%     99.970%         Const  mixed_4/tower_1/conv_2/batchnorm/moving_mean_quint8_const
    2.298    0.002    0.001   0.000%     99.970%         Const  mixed_4/tower_1/conv/batchnorm/moving_mean_quint8_const
    2.265    0.005    0.001   0.000%     99.970%         Const  mixed_4/tower/conv_2/batchnorm/moving_variance_max
    2.198    0.002    0.001   0.000%     99.970%         Const  mixed_4/tower/conv_1/batchnorm/moving_variance_quint8_const
    2.090    0.002    0.001   0.000%     99.970%         Const  mixed_4/conv/batchnorm/moving_variance_max
    2.000    0.004    0.001   0.000%     99.970%         Const  mixed_3/tower/conv_2/conv2d_params_min
    1.897    0.003    0.001   0.000%     99.970%         Const  mixed_3/tower/conv/batchnorm/moving_mean_quint8_const
    1.887    0.005    0.001   0.000%     99.970%         Const  mixed_3/tower/conv/conv2d_params_min
    1.810    0.002    0.001   0.000%     99.970%         Const  mixed_3/conv/conv2d_params_min
    1.798    0.002    0.001   0.000%     99.970%         Const  mixed_2/tower_2/conv/batchnorm/beta_max
    1.712    0.007    0.001   0.000%     99.970%         Const  mixed_2/tower_1/conv_2/batchnorm/moving_mean_min
    1.690    0.002    0.001   0.000%     99.970%         Const  mixed_2/tower_1/conv_1/batchnorm/beta_max
    1.615    0.003    0.001   0.000%     99.970%         Const  mixed_2/tower_1/conv/batchnorm/moving_variance_quint8_const
    1.513    0.005    0.001   0.000%     99.971%         Const  mixed_2/tower/conv/batchnorm/moving_variance_max
    1.268    0.006    0.001   0.000%     99.971%         Const  mixed_1/tower_1/conv_1/conv2d_params_max
    1.265    0.002    0.001   0.000%     99.971%         Const  mixed_1/tower_1/conv_1/conv2d_params_min
    1.246    0.003    0.001   0.000%     99.971%         Const  mixed_1/tower_1/conv/batchnorm/beta_quint8_const
    1.059    0.003    0.001   0.000%     99.971%         Const  mixed_1/conv/batchnorm/moving_variance_min
    0.704    0.004    0.001   0.000%     99.971%         Const  mixed/tower/conv_1/batchnorm/moving_mean_max
    0.583    0.003    0.001   0.000%     99.971%         Const  mixed/conv/batchnorm/moving_mean_max
    0.528    0.002    0.001   0.000%     99.971%         Const  conv_4/batchnorm/moving_mean_max
    0.351    0.003    0.001   0.000%     99.971%         Const  conv_1/batchnorm/moving_mean_min
    0.315    0.002    0.001   0.000%     99.971%         Const  conv/batchnorm/beta_min
    5.609    0.002    0.001   0.000%     99.971%         Const  mixed_9/tower_1/conv_1/batchnorm/beta_quint8_const
    5.482    0.002    0.001   0.000%     99.971%         Const  mixed_9/tower/mixed/conv_1/batchnorm/moving_variance_quint8_const
    5.388    0.003    0.001   0.000%     99.971%         Const  mixed_9/tower/conv/batchnorm/beta_max
    5.373    0.005    0.001   0.000%     99.971%         Const  mixed_9/tower/conv/batchnorm/moving_variance_max
    5.190    0.003    0.001   0.000%     99.971%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/moving_mean_quint8_const
    5.011    0.003    0.001   0.000%     99.971%         Const  mixed_10/tower_1/conv/batchnorm/moving_mean_max
    4.803    0.003    0.001   0.000%     99.971%         Const  mixed_10/tower/conv/conv2d_params_min
    4.632    0.005    0.001   0.000%     99.971%         Const  mixed_9/conv/batchnorm/moving_variance_min
    4.553    0.003    0.001   0.000%     99.971%         Const  mixed_8/tower_1/conv_3/conv2d_params_min
    4.409    0.006    0.001   0.000%     99.971%         Const  mixed_8/tower_1/conv/batchnorm/moving_mean_max
    4.182    0.005    0.001   0.000%     99.971%         Const  mixed_7/tower_1/conv_4/batchnorm/moving_mean_quint8_const
    4.070    0.005    0.001   0.000%     99.971%         Const  mixed_7/tower_1/conv_2/conv2d_params_max
    3.888    0.002    0.001   0.000%     99.972%         Const  mixed_7/tower/conv_1/batchnorm/beta_quint8_const
    3.826    0.005    0.001   0.000%     99.972%         Const  mixed_7/tower/conv/batchnorm/moving_variance_max
    3.807    0.006    0.001   0.000%     99.972%         Const  mixed_7/tower/conv/batchnorm/moving_mean_min
    3.800    0.002    0.001   0.000%     99.972%         Const  mixed_7/tower/conv/conv2d_params_max
    3.773    0.003    0.001   0.000%     99.972%         Const  mixed_7/conv/batchnorm/moving_variance_max
    3.722    0.003    0.001   0.000%     99.972%         Const  mixed_6/tower_2/conv/batchnorm/beta_quint8_const
    3.546    0.003    0.001   0.000%     99.972%         Const  mixed_6/tower_1/conv_2/batchnorm/moving_variance_quint8_const
    3.485    0.005    0.001   0.000%     99.972%         Const  mixed_6/tower_1/conv_1/batchnorm/moving_mean_max
    3.397    0.002    0.001   0.000%     99.972%         Const  mixed_6/tower/conv_2/batchnorm/beta_quint8_const
    3.372    0.005    0.001   0.000%     99.972%         Const  mixed_6/tower/conv_2/batchnorm/moving_mean_min
    3.293    0.002    0.001   0.000%     99.972%         Const  mixed_6/tower/conv/batchnorm/beta_max
    3.251    0.002    0.001   0.000%     99.972%         Const  mixed_6/tower/conv/conv2d_params_min
    3.157    0.003    0.001   0.000%     99.972%         Const  mixed_5/tower_2/conv/batchnorm/moving_mean_min
    3.052    0.005    0.001   0.000%     99.972%         Const  mixed_5/tower_1/conv_3/batchnorm/moving_mean_max
    2.900    0.005    0.001   0.000%     99.972%         Const  mixed_5/tower_1/conv/batchnorm/moving_variance_max
    2.828    0.002    0.001   0.000%     99.972%         Const  mixed_5/tower/conv_2/batchnorm/moving_mean_max
    2.780    0.006    0.001   0.000%     99.972%         Const  mixed_5/tower/conv_1/batchnorm/moving_variance_min
    2.756    0.002    0.001   0.000%     99.972%         Const  mixed_5/tower/conv_1/conv2d_params_min
    2.651    0.006    0.001   0.000%     99.972%         Const  mixed_5/conv/batchnorm/moving_variance_min
    2.551    0.007    0.001   0.000%     99.972%         Const  mixed_4/tower_1/conv_4/batchnorm/beta_min
    2.477    0.002    0.001   0.000%     99.972%         Const  mixed_4/tower_1/conv_3/batchnorm/moving_variance_quint8_const
    2.157    0.002    0.001   0.000%     99.973%         Const  mixed_4/tower/conv/batchnorm/beta_min
    2.098    0.002    0.001   0.000%     99.973%         Const  mixed_4/conv/batchnorm/beta_min
    2.018    0.002    0.001   0.000%     99.973%         Const  mixed_3/tower/conv_2/batchnorm/moving_mean_min
    1.742    0.002    0.001   0.000%     99.973%         Const  mixed_2/tower_1/conv_2/batchnorm/beta_min
    1.683    0.002    0.001   0.000%     99.973%         Const  mixed_2/tower_1/conv_1/batchnorm/beta_quint8_const
    1.527    0.003    0.001   0.000%     99.973%         Const  mixed_2/tower/conv/batchnorm/beta_min
    1.495    0.005    0.001   0.000%     99.973%         Const  mixed_2/tower/conv/batchnorm/moving_mean_min
    1.375    0.002    0.001   0.000%     99.973%         Const  mixed_1/tower_2/conv/conv2d_params_min
    1.312    0.002    0.001   0.000%     99.973%         Const  mixed_1/tower_1/conv_1/batchnorm/beta_max
    1.242    0.003    0.001   0.000%     99.973%         Const  mixed_1/tower_1/conv/batchnorm/moving_variance_max
    1.128    0.002    0.001   0.000%     99.973%         Const  mixed_1/tower/conv/batchnorm/moving_variance_min
    1.089    0.003    0.001   0.000%     99.973%         Const  mixed_1/tower/conv/conv2d_params_min
    0.948    0.003    0.001   0.000%     99.973%         Const  mixed/tower_1/conv_2/batchnorm/moving_variance_max
    0.882    0.003    0.001   0.000%     99.973%         Const  mixed/tower_1/conv_1/batchnorm/beta_quint8_const
    0.792    0.004    0.001   0.000%     99.973%         Const  mixed/tower_1/conv/batchnorm/moving_mean_max
    0.745    0.004    0.001   0.000%     99.973%         Const  mixed/tower/conv_1/batchnorm/beta_max
    0.572    0.003    0.001   0.000%     99.973%         Const  mixed/conv/batchnorm/moving_mean_quint8_const
    0.568    0.003    0.001   0.000%     99.973%         Const  mixed/conv/conv2d_params_max
    0.549    0.002    0.001   0.000%     99.973%         Const  conv_4/batchnorm/beta_min
    0.369    0.003    0.001   0.000%     99.973%         Const  conv_1/batchnorm/moving_variance_max
    5.781    0.002    0.001   0.000%     99.973%         Const  mixed_9/tower_2/conv/batchnorm/beta_quint8_const
    5.726    0.002    0.001   0.000%     99.973%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/beta_min
    5.722    0.002    0.001   0.000%     99.974%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/beta_quint8_const
    5.441    0.003    0.001   0.000%     99.974%         Const  mixed_9/tower/mixed/conv/batchnorm/beta_min
    5.369    0.003    0.001   0.000%     99.974%         Const  mixed_9/tower/conv/batchnorm/moving_variance_min
    5.142    0.003    0.001   0.000%     99.974%         Const  mixed_10/tower_1/mixed/conv/batchnorm/moving_variance_min
    5.077    0.002    0.001   0.000%     99.974%         Const  mixed_10/tower_1/conv_1/batchnorm/moving_variance_quint8_const
    5.001    0.002    0.001   0.000%     99.974%         Const  mixed_10/tower_1/conv/batchnorm/moving_mean_quint8_const
    4.993    0.003    0.001   0.000%     99.974%         Const  mixed_10/tower_1/conv/conv2d_params_min
    4.956    0.003    0.001   0.000%     99.974%         Const  mixed_10/tower/mixed/conv_1/batchnorm/moving_mean_max
    4.918    0.003    0.001   0.000%     99.974%         Const  mixed_10/tower/mixed/conv/batchnorm/beta_quint8_const
    4.898    0.002    0.001   0.000%     99.974%         Const  mixed_10/tower/mixed/conv/batchnorm/moving_mean_max
    4.629    0.002    0.001   0.000%     99.974%         Const  mixed_9/conv/batchnorm/moving_variance_quint8_const
    4.539    0.005    0.001   0.000%     99.974%         Const  mixed_8/tower_1/conv_2/batchnorm/beta_min
    4.371    0.006    0.001   0.000%     99.974%         Const  mixed_8/tower/conv_1/batchnorm/beta_quint8_const
    4.310    0.002    0.001   0.000%     99.974%         Const  mixed_8/tower/conv/batchnorm/moving_variance_min
    4.274    0.002    0.001   0.000%     99.974%         Const  mixed_7/tower_2/conv/batchnorm/beta_max
    4.211    0.002    0.001   0.000%     99.974%         Const  mixed_7/tower_1/conv_4/batchnorm/beta_quint8_const
    4.101    0.002    0.001   0.000%     99.974%         Const  mixed_7/tower_1/conv_2/batchnorm/beta_quint8_const
    4.087    0.005    0.001   0.000%     99.974%         Const  mixed_7/tower_1/conv_2/batchnorm/moving_variance_quint8_const
    4.058    0.003    0.001   0.000%     99.974%         Const  mixed_7/tower_1/conv_1/batchnorm/beta_max
    4.030    0.003    0.001   0.000%     99.974%         Const  mixed_7/tower_1/conv_1/batchnorm/moving_mean_max
    3.994    0.002    0.001   0.000%     99.974%         Const  mixed_7/tower_1/conv/batchnorm/beta_quint8_const
    3.990    0.003    0.001   0.000%     99.974%         Const  mixed_7/tower_1/conv/batchnorm/moving_variance_max
    3.958    0.005    0.001   0.000%     99.975%         Const  mixed_7/tower_1/conv/conv2d_params_min
    3.929    0.003    0.001   0.000%     99.975%         Const  mixed_7/tower/conv_2/batchnorm/moving_variance_quint8_const
    3.880    0.003    0.001   0.000%     99.975%         Const  mixed_7/tower/conv_1/batchnorm/moving_variance_max
    3.840    0.002    0.001   0.000%     99.975%         Const  mixed_7/tower/conv/batchnorm/beta_max
    3.836    0.003    0.001   0.000%     99.975%         Const  mixed_7/tower/conv/batchnorm/beta_min
    3.796    0.002    0.001   0.000%     99.975%         Const  mixed_7/tower/conv/conv2d_params_min
    3.605    0.002    0.001   0.000%     99.975%         Const  mixed_6/tower_1/conv_3/batchnorm/moving_variance_min
    3.579    0.006    0.001   0.000%     99.975%         Const  mixed_6/tower_1/conv_3/conv2d_params_max
    3.568    0.002    0.001   0.000%     99.975%         Const  mixed_6/tower_1/conv_2/batchnorm/beta_max
    3.383    0.002    0.001   0.000%     99.975%         Const  mixed_6/tower/conv_2/batchnorm/moving_variance_quint8_const
    3.319    0.002    0.001   0.000%     99.975%         Const  mixed_6/tower/conv_1/batchnorm/moving_mean_min
    3.276    0.005    0.001   0.000%     99.975%         Const  mixed_6/tower/conv/batchnorm/moving_variance_min
    3.268    0.002    0.001   0.000%     99.975%         Const  mixed_6/tower/conv/batchnorm/moving_mean_max
    3.175    0.002    0.001   0.000%     99.975%         Const  mixed_5/tower_2/conv/batchnorm/moving_variance_max
    3.154    0.002    0.001   0.000%     99.975%         Const  mixed_5/tower_2/conv/batchnorm/moving_mean_quint8_const
    3.143    0.002    0.001   0.000%     99.975%         Const  mixed_5/tower_2/conv/conv2d_params_min
    3.099    0.003    0.001   0.000%     99.975%         Const  mixed_5/tower_1/conv_4/batchnorm/moving_mean_quint8_const
    3.062    0.003    0.001   0.000%     99.975%         Const  mixed_5/tower_1/conv_3/batchnorm/moving_variance_min
    3.006    0.002    0.001   0.000%     99.975%         Const  mixed_5/tower_1/conv_2/batchnorm/moving_variance_min
    2.936    0.005    0.001   0.000%     99.975%         Const  mixed_5/tower_1/conv_1/batchnorm/moving_mean_min
    2.832    0.002    0.001   0.000%     99.975%         Const  mixed_5/tower/conv_2/batchnorm/moving_variance_quint8_const
    2.813    0.002    0.001   0.000%     99.976%         Const  mixed_5/tower/conv_2/conv2d_params_max
    2.695    0.002    0.001   0.000%     99.976%         Const  mixed_5/tower/conv/batchnorm/moving_mean_min
    2.680    0.003    0.001   0.000%     99.976%         Const  mixed_5/tower/conv/conv2d_params_min
    2.625    0.003    0.001   0.000%     99.976%         Const  mixed_5/conv/conv2d_params_min
    2.400    0.005    0.001   0.000%     99.976%         Const  mixed_4/tower_1/conv_2/conv2d_params_min
    2.369    0.003    0.001   0.000%     99.976%         Const  mixed_4/tower_1/conv_1/batchnorm/moving_variance_quint8_const
    2.325    0.002    0.001   0.000%     99.976%         Const  mixed_4/tower_1/conv/batchnorm/beta_quint8_const
    2.311    0.003    0.001   0.000%     99.976%         Const  mixed_4/tower_1/conv/batchnorm/moving_variance_quint8_const
    2.301    0.005    0.001   0.000%     99.976%         Const  mixed_4/tower_1/conv/batchnorm/moving_mean_min
    2.290    0.003    0.001   0.000%     99.976%         Const  mixed_4/tower_1/conv/conv2d_params_min
    2.258    0.002    0.001   0.000%     99.976%         Const  mixed_4/tower/conv_2/batchnorm/moving_variance_quint8_const
    2.212    0.002    0.001   0.000%     99.976%         Const  mixed_4/tower/conv_1/batchnorm/beta_quint8_const
    2.125    0.002    0.001   0.000%     99.976%         Const  mixed_4/tower/conv/batchnorm/moving_mean_quint8_const
    2.094    0.002    0.001   0.000%     99.976%         Const  mixed_4/conv/batchnorm/beta_quint8_const
    2.073    0.002    0.001   0.000%     99.976%         Const  mixed_4/conv/batchnorm/moving_mean_min
    2.040    0.002    0.001   0.000%     99.976%         Const  mixed_3/tower/conv_2/batchnorm/beta_quint8_const
    1.969    0.002    0.001   0.000%     99.976%         Const  mixed_3/tower/conv_1/batchnorm/moving_variance_min
    1.959    0.002    0.001   0.000%     99.976%         Const  mixed_3/tower/conv_1/batchnorm/moving_mean_max
    1.835    0.003    0.001   0.000%     99.976%         Const  mixed_3/conv/batchnorm/moving_variance_min
    1.724    0.002    0.001   0.000%     99.976%         Const  mixed_2/tower_1/conv_2/batchnorm/moving_variance_quint8_const
    1.701    0.002    0.001   0.000%     99.976%         Const  mixed_2/tower_1/conv_2/conv2d_params_min
    1.562    0.002    0.001   0.000%     99.976%         Const  mixed_2/tower/conv_1/batchnorm/moving_variance_quint8_const
    1.407    0.002    0.001   0.000%     99.977%         Const  mixed_1/tower_2/conv/batchnorm/moving_variance_max
    1.400    0.005    0.001   0.000%     99.977%         Const  mixed_1/tower_2/conv/batchnorm/moving_variance_min
    1.320    0.002    0.001   0.000%     99.977%         Const  mixed_1/tower_1/conv_2/conv2d_params_min
    1.283    0.003    0.001   0.000%     99.977%         Const  mixed_1/tower_1/conv_1/batchnorm/moving_mean_max
    1.239    0.002    0.001   0.000%     99.977%         Const  mixed_1/tower_1/conv/batchnorm/moving_variance_min
    1.201    0.002    0.001   0.000%     99.977%         Const  mixed_1/tower/conv_1/batchnorm/beta_max
    1.186    0.002    0.001   0.000%     99.977%         Const  mixed_1/tower/conv_1/batchnorm/moving_variance_max
    1.172    0.005    0.001   0.000%     99.977%         Const  mixed_1/tower/conv_1/batchnorm/moving_mean_max
    1.142    0.003    0.001   0.000%     99.977%         Const  mixed_1/tower/conv/batchnorm/beta_min
    1.132    0.002    0.001   0.000%     99.977%         Const  mixed_1/tower/conv/batchnorm/moving_variance_max
    1.124    0.003    0.001   0.000%     99.977%         Const  mixed_1/tower/conv/batchnorm/moving_variance_quint8_const
    1.007    0.003    0.001   0.000%     99.977%         Const  mixed/tower_2/conv/batchnorm/moving_variance_max
    0.999    0.003    0.001   0.000%     99.977%         Const  mixed/tower_2/conv/batchnorm/moving_variance_min
    0.871    0.002    0.001   0.000%     99.977%         Const  mixed/tower_1/conv_1/batchnorm/moving_variance_min
    0.765    0.004    0.001   0.000%     99.977%         Const  mixed/tower_1/conv/conv2d_params_max
    0.655    0.003    0.001   0.000%     99.977%         Const  mixed/tower/conv/batchnorm/beta_min
    0.553    0.002    0.001   0.000%     99.977%         Const  conv_4/batchnorm/beta_max
    0.536    0.003    0.001   0.000%     99.977%         Const  conv_4/batchnorm/moving_variance_min
    0.378    0.003    0.001   0.000%     99.977%         Const  conv_1/batchnorm/beta_min
    0.356    0.002    0.001   0.000%     99.977%         Const  conv_1/batchnorm/moving_mean_max
    5.770    0.005    0.001   0.000%     99.977%         Const  mixed_9/tower_2/conv/batchnorm/moving_variance_min
    5.758    0.003    0.001   0.000%     99.977%         Const  mixed_9/tower_2/conv/batchnorm/moving_mean_min
    5.742    0.003    0.001   0.000%     99.978%         Const  mixed_9/tower_2/conv/conv2d_params_min
    5.718    0.002    0.001   0.000%     99.978%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/moving_variance_max
    5.661    0.002    0.001   0.000%     99.978%         Const  mixed_9/tower_1/mixed/conv/batchnorm/moving_variance_max
    5.638    0.003    0.001   0.000%     99.978%         Const  mixed_9/tower_1/mixed/conv/batchnorm/moving_mean_quint8_const
    5.562    0.003    0.001   0.000%     99.978%         Const  mixed_9/tower_1/conv/batchnorm/beta_max
    5.400    0.002    0.001   0.000%     99.978%         Const  mixed_9/tower/mixed/conv/conv2d_params_min
    5.354    0.006    0.001   0.000%     99.978%         Const  mixed_9/tower/conv/batchnorm/moving_mean_min
    5.215    0.002    0.001   0.000%     99.978%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/beta_quint8_const
    5.127    0.006    0.001   0.000%     99.978%         Const  mixed_10/tower_1/mixed/conv/batchnorm/moving_mean_min
    5.084    0.006    0.001   0.000%     99.978%         Const  mixed_10/tower_1/conv_1/batchnorm/moving_variance_max
    5.060    0.004    0.001   0.000%     99.978%         Const  mixed_10/tower_1/conv_1/batchnorm/moving_mean_quint8_const
    4.974    0.003    0.001   0.000%     99.978%         Const  mixed_10/tower/mixed/conv_1/batchnorm/beta_quint8_const
    4.881    0.003    0.001   0.000%     99.978%         Const  mixed_10/tower/mixed/conv/conv2d_params_max
    4.754    0.003    0.001   0.000%     99.978%         Const  mixed_10/conv/batchnorm/moving_mean_min
    4.608    0.002    0.001   0.000%     99.978%         Const  mixed_9/conv/conv2d_params_min
    4.517    0.003    0.001   0.000%     99.978%         Const  mixed_8/tower_1/conv_2/batchnorm/moving_mean_max
    4.252    0.003    0.001   0.000%     99.978%         Const  mixed_7/tower_2/conv/batchnorm/moving_variance_quint8_const
    4.199    0.007    0.001   0.000%     99.978%         Const  mixed_7/tower_1/conv_4/batchnorm/moving_variance_min
    4.188    0.003    0.001   0.000%     99.978%         Const  mixed_7/tower_1/conv_4/batchnorm/moving_mean_min
    4.174    0.002    0.001   0.000%     99.978%         Const  mixed_7/tower_1/conv_4/conv2d_params_min
    4.145    0.006    0.001   0.000%     99.978%         Const  mixed_7/tower_1/conv_3/batchnorm/moving_variance_min
    4.105    0.002    0.001   0.000%     99.978%         Const  mixed_7/tower_1/conv_2/batchnorm/beta_min
    4.080    0.002    0.001   0.000%     99.979%         Const  mixed_7/tower_1/conv_2/batchnorm/moving_mean_min
    4.052    0.005    0.001   0.000%     99.979%         Const  mixed_7/tower_1/conv_1/batchnorm/beta_min
    3.876    0.003    0.001   0.000%     99.979%         Const  mixed_7/tower/conv_1/batchnorm/moving_variance_min
    3.854    0.003    0.001   0.000%     99.979%         Const  mixed_7/tower/conv_1/conv2d_params_max
    3.766    0.003    0.001   0.000%     99.979%         Const  mixed_7/conv/batchnorm/moving_variance_min
    3.726    0.003    0.001   0.000%     99.979%         Const  mixed_6/tower_2/conv/batchnorm/beta_min
    3.712    0.006    0.001   0.000%     99.979%         Const  mixed_6/tower_2/conv/batchnorm/moving_variance_min
    3.705    0.002    0.001   0.000%     99.979%         Const  mixed_6/tower_2/conv/batchnorm/moving_mean_max
    3.668    0.003    0.001   0.000%     99.979%         Const  mixed_6/tower_1/conv_4/batchnorm/beta_quint8_const
    3.648    0.002    0.001   0.000%     99.979%         Const  mixed_6/tower_1/conv_4/batchnorm/moving_mean_min
    3.590    0.003    0.001   0.000%     99.979%         Const  mixed_6/tower_1/conv_3/batchnorm/moving_mean_min
    3.576    0.002    0.001   0.000%     99.979%         Const  mixed_6/tower_1/conv_3/conv2d_params_min
    3.482    0.002    0.001   0.000%     99.979%         Const  mixed_6/tower_1/conv_1/batchnorm/moving_mean_min
    3.415    0.002    0.001   0.000%     99.979%         Const  mixed_6/tower_1/conv/conv2d_params_min
    3.289    0.003    0.001   0.000%     99.979%         Const  mixed_6/tower/conv/batchnorm/beta_min
    3.240    0.005    0.001   0.000%     99.979%         Const  mixed_6/conv/batchnorm/beta_max
    3.215    0.002    0.001   0.000%     99.979%         Const  mixed_6/conv/batchnorm/moving_mean_max
    3.197    0.002    0.001   0.000%     99.979%         Const  mixed_6/conv/conv2d_params_min
    3.041    0.002    0.001   0.000%     99.979%         Const  mixed_5/tower_1/conv_3/conv2d_params_max
    2.979    0.003    0.001   0.000%     99.979%         Const  mixed_5/tower_1/conv_2/conv2d_params_min
    2.896    0.003    0.001   0.000%     99.979%         Const  mixed_5/tower_1/conv/batchnorm/moving_variance_min
    2.723    0.003    0.001   0.000%     99.979%         Const  mixed_5/tower/conv/batchnorm/beta_max
    2.713    0.002    0.001   0.000%     99.980%         Const  mixed_5/tower/conv/batchnorm/moving_variance_max
    2.669    0.005    0.001   0.000%     99.980%         Const  mixed_5/conv/batchnorm/beta_max
    2.602    0.002    0.001   0.000%     99.980%         Const  mixed_4/tower_2/conv/batchnorm/beta_quint8_const
    2.559    0.002    0.001   0.000%     99.980%         Const  mixed_4/tower_1/conv_4/batchnorm/beta_max
    2.445    0.003    0.001   0.000%     99.980%         Const  mixed_4/tower_1/conv_2/batchnorm/beta_max
    2.406    0.003    0.001   0.000%     99.980%         Const  mixed_4/tower_1/conv_2/conv2d_params_max
    2.328    0.003    0.001   0.000%     99.980%         Const  mixed_4/tower_1/conv/batchnorm/beta_min
    2.318    0.005    0.001   0.000%     99.980%         Const  mixed_4/tower_1/conv/batchnorm/moving_variance_max
    2.202    0.005    0.001   0.000%     99.980%         Const  mixed_4/tower/conv_1/batchnorm/moving_variance_min
    2.180    0.003    0.001   0.000%     99.980%         Const  mixed_4/tower/conv_1/conv2d_params_max
    2.116    0.004    0.001   0.000%     99.980%         Const  mixed_4/tower/conv/conv2d_params_min
    1.945    0.005    0.001   0.000%     99.980%         Const  mixed_3/tower/conv_1/conv2d_params_max
    1.941    0.002    0.001   0.000%     99.980%         Const  mixed_3/tower/conv_1/conv2d_params_min
    1.814    0.002    0.001   0.000%     99.980%         Const  mixed_3/conv/conv2d_params_max
    1.791    0.002    0.001   0.000%     99.980%         Const  mixed_2/tower_2/conv/batchnorm/beta_quint8_const
    1.784    0.002    0.001   0.000%     99.980%         Const  mixed_2/tower_2/conv/batchnorm/moving_variance_max
    1.738    0.002    0.001   0.000%     99.980%         Const  mixed_2/tower_1/conv_2/batchnorm/beta_quint8_const
    1.636    0.003    0.001   0.000%     99.980%         Const  mixed_2/tower_1/conv/batchnorm/beta_max
    1.619    0.005    0.001   0.000%     99.980%         Const  mixed_2/tower_1/conv/batchnorm/moving_variance_min
    1.558    0.003    0.001   0.000%     99.980%         Const  mixed_2/tower/conv_1/batchnorm/moving_mean_max
    1.414    0.003    0.001   0.000%     99.980%         Const  mixed_1/tower_2/conv/batchnorm/beta_min
    1.356    0.002    0.001   0.000%     99.980%         Const  mixed_1/tower_1/conv_2/batchnorm/beta_quint8_const
    1.279    0.003    0.001   0.000%     99.980%         Const  mixed_1/tower_1/conv_1/batchnorm/moving_mean_min
    1.182    0.003    0.001   0.000%     99.981%         Const  mixed_1/tower/conv_1/batchnorm/moving_variance_min
    1.146    0.002    0.001   0.000%     99.981%         Const  mixed_1/tower/conv/batchnorm/beta_max
    1.101    0.003    0.001   0.000%     99.981%         Const  mixed_1/tower/conv/batchnorm/moving_mean_min
    1.044    0.006    0.001   0.000%     99.981%         Const  mixed_1/conv/batchnorm/moving_mean_min
    1.035    0.003    0.001   0.000%     99.981%         Const  mixed_1/conv/conv2d_params_max
    0.995    0.003    0.001   0.000%     99.981%         Const  mixed/tower_2/conv/batchnorm/moving_variance_quint8_const
    0.935    0.002    0.001   0.000%     99.981%         Const  mixed/tower_1/conv_2/batchnorm/moving_variance_quint8_const
    0.916    0.002    0.001   0.000%     99.981%         Const  mixed/tower_1/conv_2/conv2d_params_max
    0.847    0.003    0.001   0.000%     99.981%         Const  mixed/tower_1/conv_1/conv2d_params_max
    0.806    0.004    0.001   0.000%     99.981%         Const  mixed/tower_1/conv/batchnorm/moving_variance_min
    0.618    0.002    0.001   0.000%     99.981%         Const  mixed/tower/conv/conv2d_params_min
    0.515    0.003    0.001   0.000%     99.981%         Const  conv_4/conv2d_params_max
    0.291    0.003    0.001   0.000%     99.981%         Const  conv/batchnorm/moving_mean_max
    5.707    0.003    0.001   0.000%     99.981%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/moving_variance_quint8_const
    5.650    0.005    0.001   0.000%     99.981%         Const  mixed_9/tower_1/mixed/conv/batchnorm/moving_variance_quint8_const
    5.465    0.002    0.001   0.000%     99.981%         Const  mixed_9/tower/mixed/conv_1/batchnorm/moving_mean_quint8_const
    5.437    0.003    0.001   0.000%     99.981%         Const  mixed_9/tower/mixed/conv/batchnorm/beta_quint8_const
    5.422    0.002    0.001   0.000%     99.981%         Const  mixed_9/tower/mixed/conv/batchnorm/moving_variance_quint8_const
    5.384    0.003    0.001   0.000%     99.981%         Const  mixed_9/tower/conv/batchnorm/beta_min
    5.331    0.003    0.001   0.000%     99.981%         Const  mixed_9/conv/batchnorm/beta_max
    5.297    0.003    0.001   0.000%     99.981%         Const  softmax/weights_min
    5.280    0.003    0.001   0.000%     99.981%         Const  mixed_10/tower_2/conv/batchnorm/beta_max
    5.202    0.003    0.001   0.000%     99.982%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/moving_variance_quint8_const
    5.172    0.002    0.001   0.000%     99.982%         Const  mixed_10/tower_1/mixed/conv_1/conv2d_params_min
    5.161    0.002    0.001   0.000%     99.982%         Const  mixed_10/tower_1/mixed/conv/batchnorm/beta_max
    5.053    0.002    0.001   0.000%     99.982%         Const  mixed_10/tower_1/conv_1/conv2d_params_min
    5.015    0.002    0.001   0.000%     99.982%         Const  mixed_10/tower_1/conv/batchnorm/moving_variance_quint8_const
    4.964    0.002    0.001   0.000%     99.982%         Const  mixed_10/tower/mixed/conv_1/batchnorm/moving_variance_min
    4.912    0.004    0.001   0.000%     99.982%         Const  mixed_10/tower/mixed/conv/batchnorm/moving_variance_max
    4.840    0.003    0.001   0.000%     99.982%         Const  mixed_10/tower/conv/batchnorm/beta_quint8_const
    4.773    0.002    0.001   0.000%     99.982%         Const  mixed_10/conv/batchnorm/moving_variance_max
    4.769    0.003    0.001   0.000%     99.982%         Const  mixed_10/conv/batchnorm/moving_variance_min
    4.596    0.006    0.001   0.000%     99.982%         Const  mixed_8/tower_1/conv_3/batchnorm/beta_max
    4.500    0.002    0.001   0.000%     99.982%         Const  mixed_8/tower_1/conv_2/conv2d_params_min
    4.485    0.005    0.001   0.000%     99.982%         Const  mixed_8/tower_1/conv_1/batchnorm/beta_min
    4.481    0.003    0.001   0.000%     99.982%         Const  mixed_8/tower_1/conv_1/batchnorm/beta_quint8_const
    4.474    0.002    0.001   0.000%     99.982%         Const  mixed_8/tower_1/conv_1/batchnorm/moving_variance_min
    4.452    0.003    0.001   0.000%     99.982%         Const  mixed_8/tower_1/conv_1/conv2d_params_max
    4.324    0.002    0.001   0.000%     99.982%         Const  mixed_8/tower/conv/batchnorm/beta_min
    4.314    0.004    0.001   0.000%     99.982%         Const  mixed_8/tower/conv/batchnorm/moving_variance_max
    4.284    0.003    0.001   0.000%     99.982%         Const  mixed_8/tower/conv/conv2d_params_min
    4.152    0.002    0.001   0.000%     99.982%         Const  mixed_7/tower_1/conv_3/batchnorm/moving_variance_max
    4.012    0.003    0.001   0.000%     99.982%         Const  mixed_7/tower_1/conv_1/conv2d_params_min
    3.976    0.002    0.001   0.000%     99.982%         Const  mixed_7/tower_1/conv/batchnorm/moving_mean_max
    3.950    0.003    0.001   0.000%     99.983%         Const  mixed_7/tower/conv_2/batchnorm/beta_max
    3.892    0.002    0.001   0.000%     99.983%         Const  mixed_7/tower/conv_1/batchnorm/beta_min
    3.862    0.003    0.001   0.000%     99.983%         Const  mixed_7/tower/conv_1/batchnorm/moving_mean_min
    3.781    0.002    0.001   0.000%     99.983%         Const  mixed_7/conv/batchnorm/beta_min
    3.755    0.002    0.001   0.000%     99.983%         Const  mixed_7/conv/batchnorm/moving_mean_min
    3.665    0.002    0.001   0.000%     99.983%         Const  mixed_6/tower_1/conv_4/batchnorm/moving_variance_max
    3.594    0.002    0.001   0.000%     99.983%         Const  mixed_6/tower_1/conv_3/batchnorm/moving_mean_max
    3.496    0.002    0.001   0.000%     99.983%         Const  mixed_6/tower_1/conv_1/batchnorm/moving_variance_min
    3.455    0.003    0.001   0.000%     99.983%         Const  mixed_6/tower_1/conv/batchnorm/beta_min
    3.391    0.004    0.001   0.000%     99.983%         Const  mixed_6/tower/conv_2/batchnorm/moving_variance_max
    3.379    0.002    0.001   0.000%     99.983%         Const  mixed_6/tower/conv_2/batchnorm/moving_mean_max
    3.233    0.002    0.001   0.000%     99.983%         Const  mixed_6/conv/batchnorm/beta_quint8_const
    3.182    0.005    0.001   0.000%     99.983%         Const  mixed_5/tower_2/conv/batchnorm/beta_min
    2.949    0.003    0.001   0.000%     99.983%         Const  mixed_5/tower_1/conv_1/batchnorm/moving_variance_min
    2.850    0.002    0.001   0.000%     99.983%         Const  mixed_5/tower/conv_2/batchnorm/beta_min
    2.795    0.002    0.001   0.000%     99.983%         Const  mixed_5/tower/conv_1/batchnorm/beta_min
    2.644    0.002    0.001   0.000%     99.983%         Const  mixed_5/conv/batchnorm/moving_mean_max
    2.541    0.002    0.001   0.000%     99.983%         Const  mixed_4/tower_1/conv_4/batchnorm/moving_variance_min
    2.363    0.004    0.001   0.000%     99.983%         Const  mixed_4/tower_1/conv_1/batchnorm/moving_mean_max
    2.275    0.002    0.001   0.000%     99.983%         Const  mixed_4/tower/conv_2/batchnorm/beta_min
    2.261    0.003    0.001   0.000%     99.983%         Const  mixed_4/tower/conv_2/batchnorm/moving_variance_min
    2.250    0.003    0.001   0.000%     99.983%         Const  mixed_4/tower/conv_2/batchnorm/moving_mean_min
    2.136    0.002    0.001   0.000%     99.983%         Const  mixed_4/tower/conv/batchnorm/moving_mean_max
    2.132    0.002    0.001   0.000%     99.984%         Const  mixed_4/tower/conv/batchnorm/moving_mean_min
    2.006    0.003    0.001   0.000%     99.984%         Const  mixed_3/tower/conv_2/conv2d_params_max
    1.980    0.006    0.001   0.000%     99.984%         Const  mixed_3/tower/conv_1/batchnorm/beta_min
    1.916    0.002    0.001   0.000%     99.984%         Const  mixed_3/tower/conv/batchnorm/moving_variance_min
    1.773    0.003    0.001   0.000%     99.984%         Const  mixed_2/tower_2/conv/batchnorm/moving_mean_max
    1.745    0.002    0.001   0.000%     99.984%         Const  mixed_2/tower_1/conv_2/batchnorm/beta_max
    1.705    0.002    0.001   0.000%     99.984%         Const  mixed_2/tower_1/conv_2/conv2d_params_max
    1.612    0.002    0.001   0.000%     99.984%         Const  mixed_2/tower_1/conv/batchnorm/moving_mean_max
    1.583    0.006    0.001   0.000%     99.984%         Const  mixed_2/tower/conv_1/batchnorm/beta_max
    1.573    0.002    0.001   0.000%     99.984%         Const  mixed_2/tower/conv_1/batchnorm/moving_variance_max
    1.505    0.003    0.001   0.000%     99.984%         Const  mixed_2/tower/conv/batchnorm/moving_variance_quint8_const
    1.443    0.002    0.001   0.000%     99.984%         Const  mixed_2/conv/batchnorm/moving_mean_min
    1.389    0.003    0.001   0.000%     99.984%         Const  mixed_1/tower_2/conv/batchnorm/moving_mean_min
    1.352    0.003    0.001   0.000%     99.984%         Const  mixed_1/tower_1/conv_2/batchnorm/moving_variance_max
    1.294    0.002    0.001   0.000%     99.984%         Const  mixed_1/tower_1/conv_1/batchnorm/moving_variance_min
    1.216    0.002    0.001   0.000%     99.984%         Const  mixed_1/tower_1/conv/conv2d_params_max
    1.198    0.002    0.001   0.000%     99.984%         Const  mixed_1/tower/conv_1/batchnorm/beta_min
    1.093    0.003    0.001   0.000%     99.984%         Const  mixed_1/tower/conv/conv2d_params_max
    1.019    0.002    0.001   0.000%     99.984%         Const  mixed/tower_2/conv/batchnorm/beta_max
    0.843    0.003    0.001   0.000%     99.984%         Const  mixed/tower_1/conv_1/conv2d_params_min
    0.698    0.004    0.001   0.000%     99.984%         Const  mixed/tower/conv_1/batchnorm/moving_mean_min
    0.685    0.004    0.001   0.000%     99.984%         Const  mixed/tower/conv_1/conv2d_params_max
    0.622    0.002    0.001   0.000%     99.985%         Const  mixed/tower/conv/conv2d_params_max
    0.437    0.003    0.001   0.000%     99.985%         Const  conv_2/batchnorm/beta_max
    0.411    0.003    0.001   0.000%     99.985%         Const  conv_2/batchnorm/moving_mean_max
    0.277    0.002    0.001   0.000%     99.985%         Const  conv/conv2d_params_max
    3.427    0.002    0.001   0.000%     99.985%         Const  mixed_6/tower_1/conv/batchnorm/moving_mean_min
    5.544    0.003    0.001   0.000%     99.985%         Const  mixed_9/tower_1/conv/batchnorm/moving_variance_min
    5.457    0.002    0.001   0.000%     99.985%         Const  mixed_9/tower/mixed/conv_1/conv2d_params_min
    5.412    0.002    0.001   0.000%     99.985%         Const  mixed_9/tower/mixed/conv/batchnorm/moving_mean_min
    5.251    0.006    0.001   0.000%     99.985%         Const  mixed_10/tower_2/conv/batchnorm/moving_mean_max
    5.185    0.003    0.001   0.000%     99.985%         Const  mixed_10/tower_1/mixed/conv_1/conv2d_params_max
    5.146    0.006    0.001   0.000%     99.985%         Const  mixed_10/tower_1/mixed/conv/batchnorm/moving_variance_max
    5.073    0.002    0.001   0.000%     99.985%         Const  mixed_10/tower_1/conv_1/batchnorm/moving_mean_max
    4.949    0.006    0.001   0.000%     99.985%         Const  mixed_10/tower/mixed/conv_1/batchnorm/moving_mean_min
    4.783    0.003    0.001   0.000%     99.985%         Const  mixed_10/conv/batchnorm/beta_min
    4.582    0.002    0.001   0.000%     99.985%         Const  mixed_8/tower_1/conv_3/batchnorm/moving_variance_min
    4.535    0.002    0.001   0.000%     99.985%         Const  mixed_8/tower_1/conv_2/batchnorm/beta_quint8_const
    4.528    0.002    0.001   0.000%     99.985%         Const  mixed_8/tower_1/conv_2/batchnorm/moving_variance_min
    4.470    0.003    0.001   0.000%     99.985%         Const  mixed_8/tower_1/conv_1/batchnorm/moving_variance_quint8_const
    4.460    0.003    0.001   0.000%     99.985%         Const  mixed_8/tower_1/conv_1/batchnorm/moving_mean_min
    4.338    0.002    0.001   0.000%     99.985%         Const  mixed_8/tower/conv_1/conv2d_params_min
    4.306    0.003    0.001   0.000%     99.985%         Const  mixed_8/tower/conv/batchnorm/moving_variance_quint8_const
    4.302    0.003    0.001   0.000%     99.985%         Const  mixed_8/tower/conv/batchnorm/moving_mean_max
    4.260    0.005    0.001   0.000%     99.985%         Const  mixed_7/tower_2/conv/batchnorm/moving_variance_max
    4.234    0.002    0.001   0.000%     99.986%         Const  mixed_7/tower_2/conv/conv2d_params_max
    4.160    0.002    0.001   0.000%     99.986%         Const  mixed_7/tower_1/conv_3/batchnorm/beta_min
    4.027    0.002    0.001   0.000%     99.986%         Const  mixed_7/tower_1/conv_1/batchnorm/moving_mean_min
    4.001    0.002    0.001   0.000%     99.986%         Const  mixed_7/tower_1/conv/batchnorm/beta_min
    3.937    0.002    0.001   0.000%     99.986%         Const  mixed_7/tower/conv_2/batchnorm/moving_variance_max
    3.818    0.002    0.001   0.000%     99.986%         Const  mixed_7/tower/conv/batchnorm/moving_variance_quint8_const
    3.702    0.002    0.001   0.000%     99.986%         Const  mixed_6/tower_2/conv/batchnorm/moving_mean_min
    3.687    0.002    0.001   0.000%     99.986%         Const  mixed_6/tower_2/conv/conv2d_params_min
    3.637    0.005    0.001   0.000%     99.986%         Const  mixed_6/tower_1/conv_4/conv2d_params_max
    3.564    0.002    0.001   0.000%     99.986%         Const  mixed_6/tower_1/conv_2/batchnorm/beta_min
    3.554    0.002    0.001   0.000%     99.986%         Const  mixed_6/tower_1/conv_2/batchnorm/moving_variance_max
    3.467    0.005    0.001   0.000%     99.986%         Const  mixed_6/tower_1/conv_1/conv2d_params_min
    3.334    0.005    0.001   0.000%     99.986%         Const  mixed_6/tower/conv_1/batchnorm/moving_variance_max
    3.330    0.003    0.001   0.000%     99.986%         Const  mixed_6/tower/conv_1/batchnorm/moving_variance_min
    3.201    0.002    0.001   0.000%     99.986%         Const  mixed_6/conv/conv2d_params_max
    3.161    0.002    0.001   0.000%     99.986%         Const  mixed_5/tower_2/conv/batchnorm/moving_mean_max
    3.121    0.002    0.001   0.000%     99.986%         Const  mixed_5/tower_1/conv_4/batchnorm/moving_variance_max
    3.049    0.002    0.001   0.000%     99.986%         Const  mixed_5/tower_1/conv_3/batchnorm/moving_mean_min
    3.009    0.002    0.001   0.000%     99.986%         Const  mixed_5/tower_1/conv_2/batchnorm/moving_variance_max
    2.995    0.005    0.001   0.000%     99.986%         Const  mixed_5/tower_1/conv_2/batchnorm/moving_mean_max
    2.914    0.002    0.001   0.000%     99.986%         Const  mixed_5/tower_1/conv/batchnorm/beta_max
    2.720    0.002    0.001   0.000%     99.986%         Const  mixed_5/tower/conv/batchnorm/beta_min
    2.605    0.003    0.001   0.000%     99.986%         Const  mixed_4/tower_2/conv/batchnorm/beta_min
    2.594    0.003    0.001   0.000%     99.987%         Const  mixed_4/tower_2/conv/batchnorm/moving_variance_min
    2.584    0.003    0.001   0.000%     99.987%         Const  mixed_4/tower_2/conv/batchnorm/moving_mean_max
    2.580    0.003    0.001   0.000%     99.987%         Const  mixed_4/tower_2/conv/batchnorm/moving_mean_min
    2.570    0.005    0.001   0.000%     99.987%         Const  mixed_4/tower_2/conv/conv2d_params_max
    2.548    0.002    0.001   0.000%     99.987%         Const  mixed_4/tower_1/conv_4/batchnorm/beta_quint8_const
    2.527    0.002    0.001   0.000%     99.987%         Const  mixed_4/tower_1/conv_4/batchnorm/moving_mean_min
    2.501    0.002    0.001   0.000%     99.987%         Const  mixed_4/tower_1/conv_3/batchnorm/beta_min
    2.121    0.002    0.001   0.000%     99.987%         Const  mixed_4/tower/conv/conv2d_params_max
    2.062    0.002    0.001   0.000%     99.987%         Const  mixed_4/conv/conv2d_params_max
    1.901    0.002    0.001   0.000%     99.987%         Const  mixed_3/tower/conv/batchnorm/moving_mean_min
    1.839    0.002    0.001   0.000%     99.987%         Const  mixed_3/conv/batchnorm/moving_variance_max
    1.821    0.002    0.001   0.000%     99.987%         Const  mixed_3/conv/batchnorm/moving_mean_min
    1.759    0.003    0.001   0.000%     99.987%         Const  mixed_2/tower_2/conv/conv2d_params_max
    1.731    0.005    0.001   0.000%     99.987%         Const  mixed_2/tower_1/conv_2/batchnorm/moving_variance_max
    1.672    0.002    0.001   0.000%     99.987%         Const  mixed_2/tower_1/conv_1/batchnorm/moving_variance_min
    1.598    0.002    0.001   0.000%     99.987%         Const  mixed_2/tower_1/conv/conv2d_params_max
    1.462    0.002    0.001   0.000%     99.987%         Const  mixed_2/conv/batchnorm/moving_variance_max
    1.432    0.003    0.001   0.000%     99.987%         Const  mixed_2/conv/conv2d_params_max
    1.418    0.005    0.001   0.000%     99.987%         Const  mixed_1/tower_2/conv/batchnorm/beta_max
    0.828    0.002    0.001   0.000%     99.987%         Const  mixed/tower_1/conv/batchnorm/beta_min
    0.679    0.004    0.001   0.000%     99.987%         Const  mixed/tower/conv_1/conv2d_params_min
    0.659    0.003    0.001   0.000%     99.987%         Const  mixed/tower/conv/batchnorm/beta_max
    0.481    0.003    0.001   0.000%     99.987%         Const  conv_3/batchnorm/moving_variance_min
    5.656    0.003    0.001   0.000%     99.988%         Const  mixed_9/tower_1/mixed/conv/batchnorm/moving_variance_min
    5.601    0.003    0.001   0.000%     99.988%         Const  mixed_9/tower_1/conv_1/batchnorm/moving_variance_min
    5.571    0.002    0.001   0.000%     99.988%         Const  mixed_9/tower_1/conv_1/conv2d_params_min
    5.514    0.006    0.001   0.000%     99.988%         Const  mixed_9/tower_1/conv/conv2d_params_min
    5.319    0.003    0.001   0.000%     99.988%         Const  softmax/biases_quint8_const
    5.258    0.003    0.001   0.000%     99.988%         Const  mixed_10/tower_2/conv/batchnorm/moving_variance_quint8_const
    5.247    0.003    0.001   0.000%     99.988%         Const  mixed_10/tower_2/conv/batchnorm/moving_mean_min
    5.134    0.003    0.001   0.000%     99.988%         Const  mixed_10/tower_1/mixed/conv/batchnorm/moving_mean_max
    5.037    0.002    0.001   0.000%     99.988%         Const  mixed_10/tower_1/conv/batchnorm/beta_max
    4.815    0.005    0.001   0.000%     99.988%         Const  mixed_10/tower/conv/batchnorm/moving_mean_min
    4.747    0.002    0.001   0.000%     99.988%         Const  mixed_10/conv/conv2d_params_max
    4.622    0.002    0.001   0.000%     99.988%         Const  mixed_9/conv/batchnorm/moving_mean_min
    4.575    0.002    0.001   0.000%     99.988%         Const  mixed_8/tower_1/conv_3/batchnorm/moving_variance_quint8_const
    4.368    0.002    0.001   0.000%     99.988%         Const  mixed_8/tower/conv_1/batchnorm/moving_variance_max
    4.357    0.002    0.001   0.000%     99.988%         Const  mixed_8/tower/conv_1/batchnorm/moving_mean_max
    4.350    0.002    0.001   0.000%     99.988%         Const  mixed_8/tower/conv_1/batchnorm/moving_mean_min
    4.327    0.002    0.001   0.000%     99.988%         Const  mixed_8/tower/conv/batchnorm/beta_max
    4.296    0.005    0.001   0.000%     99.988%         Const  mixed_8/tower/conv/batchnorm/moving_mean_min
    4.218    0.003    0.001   0.000%     99.988%         Const  mixed_7/tower_1/conv_4/batchnorm/beta_max
    4.120    0.002    0.001   0.000%     99.988%         Const  mixed_7/tower_1/conv_3/conv2d_params_min
    4.084    0.002    0.001   0.000%     99.988%         Const  mixed_7/tower_1/conv_2/batchnorm/moving_mean_max
    3.972    0.003    0.001   0.000%     99.988%         Const  mixed_7/tower_1/conv/batchnorm/moving_mean_min
    3.730    0.005    0.001   0.000%     99.988%         Const  mixed_6/tower_2/conv/batchnorm/beta_max
    3.691    0.002    0.001   0.000%     99.989%         Const  mixed_6/tower_2/conv/conv2d_params_max
    3.550    0.002    0.001   0.000%     99.989%         Const  mixed_6/tower_1/conv_2/batchnorm/moving_variance_min
    3.536    0.002    0.001   0.000%     99.989%         Const  mixed_6/tower_1/conv_2/batchnorm/moving_mean_min
    3.499    0.003    0.001   0.000%     99.989%         Const  mixed_6/tower_1/conv_1/batchnorm/moving_variance_max
    3.387    0.002    0.001   0.000%     99.989%         Const  mixed_6/tower/conv_2/batchnorm/moving_variance_min
    3.308    0.002    0.001   0.000%     99.989%         Const  mixed_6/tower/conv_1/conv2d_params_max
    3.189    0.002    0.001   0.000%     99.989%         Const  mixed_5/tower_2/conv/batchnorm/beta_max
    3.171    0.002    0.001   0.000%     99.989%         Const  mixed_5/tower_2/conv/batchnorm/moving_variance_min
    3.129    0.004    0.001   0.000%     99.989%         Const  mixed_5/tower_1/conv_4/batchnorm/beta_min
    3.107    0.002    0.001   0.000%     99.989%         Const  mixed_5/tower_1/conv_4/batchnorm/moving_mean_max
    3.066    0.003    0.001   0.000%     99.989%         Const  mixed_5/tower_1/conv_3/batchnorm/moving_variance_max
    3.059    0.002    0.001   0.000%     99.989%         Const  mixed_5/tower_1/conv_3/batchnorm/moving_variance_quint8_const
    2.960    0.002    0.001   0.000%     99.989%         Const  mixed_5/tower_1/conv_1/batchnorm/beta_quint8_const
    2.910    0.003    0.001   0.000%     99.989%         Const  mixed_5/tower_1/conv/batchnorm/beta_min
    2.889    0.002    0.001   0.000%     99.989%         Const  mixed_5/tower_1/conv/batchnorm/moving_mean_max
    2.869    0.004    0.001   0.000%     99.989%         Const  mixed_5/tower_1/conv/conv2d_params_min
    2.842    0.003    0.001   0.000%     99.989%         Const  mixed_5/tower/conv_2/batchnorm/moving_variance_max
    2.666    0.002    0.001   0.000%     99.989%         Const  mixed_5/conv/batchnorm/beta_min
    2.629    0.002    0.001   0.000%     99.989%         Const  mixed_5/conv/conv2d_params_max
    2.615    0.004    0.001   0.000%     99.989%         Const  mixed_4/tower_2/conv/batchnorm/beta_max
    2.598    0.002    0.001   0.000%     99.989%         Const  mixed_4/tower_2/conv/batchnorm/moving_variance_max
    2.505    0.002    0.001   0.000%     99.989%         Const  mixed_4/tower_1/conv_3/batchnorm/beta_max
    2.432    0.002    0.001   0.000%     99.989%         Const  mixed_4/tower_1/conv_2/batchnorm/moving_variance_max
    2.235    0.004    0.001   0.000%     99.990%         Const  mixed_4/tower/conv_2/conv2d_params_max
    2.215    0.003    0.001   0.000%     99.990%         Const  mixed_4/tower/conv_1/batchnorm/beta_min
    2.177    0.002    0.001   0.000%     99.990%         Const  mixed_4/tower/conv_1/conv2d_params_min
    1.930    0.002    0.001   0.000%     99.990%         Const  mixed_3/tower/conv/batchnorm/beta_min
    1.781    0.002    0.001   0.000%     99.990%         Const  mixed_2/tower_2/conv/batchnorm/moving_variance_min
    1.728    0.002    0.001   0.000%     99.990%         Const  mixed_2/tower_1/conv_2/batchnorm/moving_variance_min
    1.661    0.003    0.001   0.000%     99.990%         Const  mixed_2/tower_1/conv_1/batchnorm/moving_mean_min
    1.651    0.002    0.001   0.000%     99.990%         Const  mixed_2/tower_1/conv_1/conv2d_params_max
    1.647    0.002    0.001   0.000%     99.990%         Const  mixed_2/tower_1/conv_1/conv2d_params_min
    1.633    0.002    0.001   0.000%     99.990%         Const  mixed_2/tower_1/conv/batchnorm/beta_min
    1.608    0.002    0.001   0.000%     99.990%         Const  mixed_2/tower_1/conv/batchnorm/moving_mean_min
    1.580    0.002    0.001   0.000%     99.990%         Const  mixed_2/tower/conv_1/batchnorm/beta_min
    1.569    0.002    0.001   0.000%     99.990%         Const  mixed_2/tower/conv_1/batchnorm/moving_variance_min
    1.544    0.004    0.001   0.000%     99.990%         Const  mixed_2/tower/conv_1/conv2d_params_max
    1.532    0.002    0.001   0.000%     99.990%         Const  mixed_2/tower/conv/batchnorm/beta_max
    1.446    0.003    0.001   0.000%     99.990%         Const  mixed_2/conv/batchnorm/moving_mean_max
    1.360    0.002    0.001   0.000%     99.990%         Const  mixed_1/tower_1/conv_2/batchnorm/beta_min
    1.349    0.002    0.001   0.000%     99.990%         Const  mixed_1/tower_1/conv_2/batchnorm/moving_variance_min
    1.015    0.003    0.001   0.000%     99.990%         Const  mixed/tower_2/conv/batchnorm/beta_min
    0.977    0.002    0.001   0.000%     99.990%         Const  mixed/tower_2/conv/conv2d_params_max
    0.718    0.004    0.001   0.000%     99.990%         Const  mixed/tower/conv_1/batchnorm/moving_variance_min
    0.609    0.002    0.001   0.000%     99.990%         Const  mixed/conv/batchnorm/beta_max
    5.699    0.002    0.001   0.000%     99.990%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/moving_mean_min
    5.591    0.002    0.001   0.000%     99.990%         Const  mixed_9/tower_1/conv_1/batchnorm/moving_mean_max
    5.533    0.005    0.001   0.000%     99.991%         Const  mixed_9/tower_1/conv/batchnorm/moving_mean_max
    5.529    0.002    0.001   0.000%     99.991%         Const  mixed_9/tower_1/conv/batchnorm/moving_mean_min
    5.490    0.002    0.001   0.000%     99.991%         Const  mixed_9/tower/mixed/conv_1/batchnorm/moving_variance_max
    5.361    0.003    0.001   0.000%     99.991%         Const  mixed_9/tower/conv/batchnorm/moving_mean_max
    5.347    0.002    0.001   0.000%     99.991%         Const  mixed_9/tower/conv/conv2d_params_max
    5.327    0.003    0.001   0.000%     99.991%         Const  softmax/biases_max
    5.239    0.003    0.001   0.000%     99.991%         Const  mixed_10/tower_2/conv/conv2d_params_max
    5.219    0.003    0.001   0.000%     99.991%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/beta_min
    5.194    0.003    0.001   0.000%     99.991%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/moving_mean_min
    5.117    0.004    0.001   0.000%     99.991%         Const  mixed_10/tower_1/mixed/conv/conv2d_params_max
    5.111    0.004    0.001   0.000%     99.991%         Const  mixed_10/tower_1/mixed/conv/conv2d_params_min
    5.095    0.002    0.001   0.000%     99.991%         Const  mixed_10/tower_1/conv_1/batchnorm/beta_min
    5.091    0.003    0.001   0.000%     99.991%         Const  mixed_10/tower_1/conv_1/batchnorm/beta_quint8_const
    5.057    0.002    0.001   0.000%     99.991%         Const  mixed_10/tower_1/conv_1/conv2d_params_max
    4.982    0.002    0.001   0.000%     99.991%         Const  mixed_10/tower/mixed/conv_1/batchnorm/beta_max
    4.968    0.005    0.001   0.000%     99.991%         Const  mixed_10/tower/mixed/conv_1/batchnorm/moving_variance_max
    4.941    0.002    0.001   0.000%     99.991%         Const  mixed_10/tower/mixed/conv_1/conv2d_params_max
    4.922    0.003    0.001   0.000%     99.991%         Const  mixed_10/tower/mixed/conv/batchnorm/beta_min
    4.906    0.002    0.001   0.000%     99.991%         Const  mixed_10/tower/mixed/conv/batchnorm/moving_variance_min
    4.503    0.005    0.001   0.000%     99.991%         Const  mixed_8/tower_1/conv_2/conv2d_params_max
    4.446    0.002    0.001   0.000%     99.991%         Const  mixed_8/tower_1/conv_1/conv2d_params_min
    4.435    0.002    0.001   0.000%     99.991%         Const  mixed_8/tower_1/conv/batchnorm/beta_min
    4.425    0.002    0.001   0.000%     99.991%         Const  mixed_8/tower_1/conv/batchnorm/moving_variance_max
    4.342    0.002    0.001   0.000%     99.992%         Const  mixed_8/tower/conv_1/conv2d_params_max
    4.207    0.002    0.001   0.000%     99.992%         Const  mixed_7/tower_1/conv_4/batchnorm/moving_variance_max
    4.178    0.002    0.001   0.000%     99.992%         Const  mixed_7/tower_1/conv_4/conv2d_params_max
    3.987    0.002    0.001   0.000%     99.992%         Const  mixed_7/tower_1/conv/batchnorm/moving_variance_min
    3.933    0.002    0.001   0.000%     99.992%         Const  mixed_7/tower/conv_2/batchnorm/moving_variance_min
    3.814    0.003    0.001   0.000%     99.992%         Const  mixed_7/tower/conv/batchnorm/moving_mean_max
    3.745    0.002    0.001   0.000%     99.992%         Const  mixed_7/conv/conv2d_params_max
    3.528    0.003    0.001   0.000%     99.992%         Const  mixed_6/tower_1/conv_2/conv2d_params_max
    3.510    0.002    0.001   0.000%     99.992%         Const  mixed_6/tower_1/conv_1/batchnorm/beta_min
    3.433    0.003    0.001   0.000%     99.992%         Const  mixed_6/tower_1/conv/batchnorm/moving_mean_max
    3.404    0.002    0.001   0.000%     99.992%         Const  mixed_6/tower/conv_2/batchnorm/beta_max
    3.229    0.002    0.001   0.000%     99.992%         Const  mixed_6/conv/batchnorm/moving_variance_max
    3.147    0.005    0.001   0.000%     99.992%         Const  mixed_5/tower_2/conv/conv2d_params_max
    3.135    0.002    0.001   0.000%     99.992%         Const  mixed_5/tower_1/conv_4/batchnorm/beta_max
    3.076    0.003    0.001   0.000%     99.992%         Const  mixed_5/tower_1/conv_3/batchnorm/beta_min
    2.991    0.003    0.001   0.000%     99.992%         Const  mixed_5/tower_1/conv_2/batchnorm/moving_mean_min
    2.983    0.002    0.001   0.000%     99.992%         Const  mixed_5/tower_1/conv_2/conv2d_params_max
    2.928    0.003    0.001   0.000%     99.992%         Const  mixed_5/tower_1/conv_1/conv2d_params_max
    2.684    0.002    0.001   0.000%     99.992%         Const  mixed_5/tower/conv/conv2d_params_max
    2.640    0.002    0.001   0.000%     99.992%         Const  mixed_5/conv/batchnorm/moving_mean_min
    2.481    0.003    0.001   0.000%     99.992%         Const  mixed_4/tower_1/conv_3/batchnorm/moving_variance_min
    2.471    0.002    0.001   0.000%     99.992%         Const  mixed_4/tower_1/conv_3/batchnorm/moving_mean_max
    2.460    0.002    0.001   0.000%     99.992%         Const  mixed_4/tower_1/conv_3/conv2d_params_max
    2.414    0.002    0.001   0.000%     99.992%         Const  mixed_4/tower_1/conv_2/batchnorm/moving_mean_min
    2.392    0.002    0.001   0.000%     99.993%         Const  mixed_4/tower_1/conv_1/batchnorm/beta_max
    2.374    0.002    0.001   0.000%     99.993%         Const  mixed_4/tower_1/conv_1/batchnorm/moving_variance_min
    2.332    0.002    0.001   0.000%     99.993%         Const  mixed_4/tower_1/conv/batchnorm/beta_max
    2.208    0.002    0.001   0.000%     99.993%         Const  mixed_4/tower/conv_1/batchnorm/moving_variance_max
    2.101    0.003    0.001   0.000%     99.993%         Const  mixed_4/conv/batchnorm/beta_max
    2.084    0.005    0.001   0.000%     99.993%         Const  mixed_4/conv/batchnorm/moving_variance_min
    1.973    0.002    0.001   0.000%     99.993%         Const  mixed_3/tower/conv_1/batchnorm/moving_variance_max
    1.849    0.002    0.001   0.000%     99.993%         Const  mixed_3/conv/batchnorm/beta_min
    1.686    0.003    0.001   0.000%     99.993%         Const  mixed_2/tower_1/conv_1/batchnorm/beta_min
    1.679    0.002    0.001   0.000%     99.993%         Const  mixed_2/tower_1/conv_1/batchnorm/moving_variance_max
    1.665    0.002    0.001   0.000%     99.993%         Const  mixed_2/tower_1/conv_1/batchnorm/moving_mean_max
    1.594    0.003    0.001   0.000%     99.993%         Const  mixed_2/tower_1/conv/conv2d_params_min
    1.541    0.002    0.001   0.000%     99.993%         Const  mixed_2/tower/conv_1/conv2d_params_min
    1.509    0.003    0.001   0.000%     99.993%         Const  mixed_2/tower/conv/batchnorm/moving_variance_min
    1.469    0.002    0.001   0.000%     99.993%         Const  mixed_2/conv/batchnorm/beta_min
    1.466    0.002    0.001   0.000%     99.993%         Const  mixed_2/conv/batchnorm/beta_quint8_const
    1.393    0.002    0.001   0.000%     99.993%         Const  mixed_1/tower_2/conv/batchnorm/moving_mean_max
    1.224    0.002    0.001   0.000%     99.993%         Const  mixed_1/tower_1/conv/batchnorm/moving_mean_min
    1.168    0.002    0.001   0.000%     99.993%         Const  mixed_1/tower/conv_1/batchnorm/moving_mean_min
    0.564    0.002    0.001   0.000%     99.993%         Const  mixed/conv/conv2d_params_min
    5.669    0.005    0.001   0.000%     99.993%         Const  mixed_9/tower_1/mixed/conv/batchnorm/beta_min
    5.612    0.006    0.001   0.000%     99.993%         Const  mixed_9/tower_1/conv_1/batchnorm/beta_min
    5.469    0.002    0.001   0.000%     99.993%         Const  mixed_9/tower/mixed/conv_1/batchnorm/moving_mean_min
    5.461    0.002    0.001   0.000%     99.994%         Const  mixed_9/tower/mixed/conv_1/conv2d_params_max
    5.301    0.003    0.001   0.000%     99.994%         Const  softmax/weights_max
    5.262    0.002    0.001   0.000%     99.994%         Const  mixed_10/tower_2/conv/batchnorm/moving_variance_min
    5.211    0.002    0.001   0.000%     99.994%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/moving_variance_max
    5.206    0.003    0.001   0.000%     99.994%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/moving_variance_min
    5.080    0.003    0.001   0.000%     99.994%         Const  mixed_10/tower_1/conv_1/batchnorm/moving_variance_min
    5.019    0.002    0.001   0.000%     99.994%         Const  mixed_10/tower_1/conv/batchnorm/moving_variance_min
    4.844    0.003    0.001   0.000%     99.994%         Const  mixed_10/tower/conv/batchnorm/beta_min
    4.646    0.002    0.001   0.000%     99.994%         Const  mixed_9/conv/batchnorm/beta_min
    4.557    0.003    0.001   0.000%     99.994%         Const  mixed_8/tower_1/conv_3/conv2d_params_max
    4.531    0.003    0.001   0.000%     99.994%         Const  mixed_8/tower_1/conv_2/batchnorm/moving_variance_max
    4.478    0.002    0.001   0.000%     99.994%         Const  mixed_8/tower_1/conv_1/batchnorm/moving_variance_max
    4.421    0.002    0.001   0.000%     99.994%         Const  mixed_8/tower_1/conv/batchnorm/moving_variance_min
    4.256    0.002    0.001   0.000%     99.994%         Const  mixed_7/tower_2/conv/batchnorm/moving_variance_min
    4.094    0.002    0.001   0.000%     99.994%         Const  mixed_7/tower_1/conv_2/batchnorm/moving_variance_min
    4.041    0.002    0.001   0.000%     99.994%         Const  mixed_7/tower_1/conv_1/batchnorm/moving_variance_min
    3.965    0.002    0.001   0.000%     99.994%         Const  mixed_7/tower_1/conv/conv2d_params_max
    3.719    0.002    0.001   0.000%     99.994%         Const  mixed_6/tower_2/conv/batchnorm/moving_variance_max
    3.608    0.002    0.001   0.000%     99.994%         Const  mixed_6/tower_1/conv_3/batchnorm/moving_variance_max
    3.419    0.002    0.001   0.000%     99.994%         Const  mixed_6/tower_1/conv/conv2d_params_max
    3.401    0.002    0.001   0.000%     99.994%         Const  mixed_6/tower/conv_2/batchnorm/beta_min
    3.348    0.003    0.001   0.000%     99.994%         Const  mixed_6/tower/conv_1/batchnorm/beta_max
    3.264    0.003    0.001   0.000%     99.994%         Const  mixed_6/tower/conv/batchnorm/moving_mean_min
    3.236    0.002    0.001   0.000%     99.994%         Const  mixed_6/conv/batchnorm/beta_min
    3.211    0.003    0.001   0.000%     99.995%         Const  mixed_6/conv/batchnorm/moving_mean_min
    3.125    0.002    0.001   0.000%     99.995%         Const  mixed_5/tower_1/conv_4/batchnorm/beta_quint8_const
    3.117    0.003    0.001   0.000%     99.995%         Const  mixed_5/tower_1/conv_4/batchnorm/moving_variance_min
    3.080    0.002    0.001   0.000%     99.995%         Const  mixed_5/tower_1/conv_3/batchnorm/beta_max
    2.953    0.002    0.001   0.000%     99.995%         Const  mixed_5/tower_1/conv_1/batchnorm/moving_variance_max
    2.874    0.002    0.001   0.000%     99.995%         Const  mixed_5/tower_1/conv/conv2d_params_max
    2.824    0.003    0.001   0.000%     99.995%         Const  mixed_5/tower/conv_2/batchnorm/moving_mean_min
    2.658    0.003    0.001   0.000%     99.995%         Const  mixed_5/conv/batchnorm/moving_variance_max
    2.519    0.003    0.001   0.000%     99.995%         Const  mixed_4/tower_1/conv_4/conv2d_params_max
    2.486    0.004    0.001   0.000%     99.995%         Const  mixed_4/tower_1/conv_3/batchnorm/moving_variance_max
    2.467    0.003    0.001   0.000%     99.995%         Const  mixed_4/tower_1/conv_3/batchnorm/moving_mean_min
    2.388    0.002    0.001   0.000%     99.995%         Const  mixed_4/tower_1/conv_1/batchnorm/beta_min
    2.377    0.002    0.001   0.000%     99.995%         Const  mixed_4/tower_1/conv_1/batchnorm/moving_variance_max
    2.219    0.002    0.001   0.000%     99.995%         Const  mixed_4/tower/conv_1/batchnorm/beta_max
    2.044    0.002    0.001   0.000%     99.995%         Const  mixed_3/tower/conv_2/batchnorm/beta_min
    2.032    0.003    0.001   0.000%     99.995%         Const  mixed_3/tower/conv_2/batchnorm/moving_variance_min
    1.756    0.002    0.001   0.000%     99.995%         Const  mixed_2/tower_2/conv/conv2d_params_min
    1.502    0.002    0.001   0.000%     99.995%         Const  mixed_2/tower/conv/batchnorm/moving_mean_max
    1.078    0.002    0.001   0.000%     99.995%         Const  mixed_1/conv/batchnorm/beta_max
    0.957    0.002    0.001   0.000%     99.995%         Const  mixed/tower_1/conv_2/batchnorm/beta_min
    0.925    0.003    0.001   0.000%     99.995%         Const  mixed/tower_1/conv_2/batchnorm/moving_mean_min
    5.733    0.002    0.001   0.000%     99.995%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/beta_max
    5.703    0.002    0.001   0.000%     99.995%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/moving_mean_max
    5.688    0.003    0.001   0.000%     99.995%         Const  mixed_9/tower_1/mixed/conv_1/conv2d_params_max
    5.646    0.002    0.001   0.000%     99.996%         Const  mixed_9/tower_1/mixed/conv/batchnorm/moving_mean_max
    5.587    0.002    0.001   0.000%     99.996%         Const  mixed_9/tower_1/conv_1/batchnorm/moving_mean_min
    5.548    0.003    0.001   0.000%     99.996%         Const  mixed_9/tower_1/conv/batchnorm/moving_variance_max
    5.502    0.002    0.001   0.000%     99.996%         Const  mixed_9/tower/mixed/conv_1/batchnorm/beta_min
    5.445    0.002    0.001   0.000%     99.996%         Const  mixed_9/tower/mixed/conv/batchnorm/beta_max
    5.430    0.002    0.001   0.000%     99.996%         Const  mixed_9/tower/mixed/conv/batchnorm/moving_variance_max
    5.418    0.003    0.001   0.000%     99.996%         Const  mixed_9/tower/mixed/conv/batchnorm/moving_mean_max
    5.266    0.002    0.001   0.000%     99.996%         Const  mixed_10/tower_2/conv/batchnorm/moving_variance_max
    5.098    0.003    0.001   0.000%     99.996%         Const  mixed_10/tower_1/conv_1/batchnorm/beta_max
    5.022    0.002    0.001   0.000%     99.996%         Const  mixed_10/tower_1/conv/batchnorm/moving_variance_max
    4.926    0.002    0.001   0.000%     99.996%         Const  mixed_10/tower/mixed/conv/batchnorm/beta_max
    4.848    0.002    0.001   0.000%     99.996%         Const  mixed_10/tower/conv/batchnorm/beta_max
    4.807    0.003    0.001   0.000%     99.996%         Const  mixed_10/tower/conv/conv2d_params_max
    4.612    0.002    0.001   0.000%     99.996%         Const  mixed_9/conv/conv2d_params_max
    4.571    0.002    0.001   0.000%     99.996%         Const  mixed_8/tower_1/conv_3/batchnorm/moving_mean_max
    4.545    0.002    0.001   0.000%     99.996%         Const  mixed_8/tower_1/conv_2/batchnorm/beta_max
    4.514    0.002    0.001   0.000%     99.996%         Const  mixed_8/tower_1/conv_2/batchnorm/moving_mean_min
    4.492    0.002    0.001   0.000%     99.996%         Const  mixed_8/tower_1/conv_1/batchnorm/beta_max
    4.464    0.002    0.001   0.000%     99.996%         Const  mixed_8/tower_1/conv_1/batchnorm/moving_mean_max
    4.438    0.002    0.001   0.000%     99.996%         Const  mixed_8/tower_1/conv/batchnorm/beta_max
    4.397    0.003    0.001   0.000%     99.996%         Const  mixed_8/tower_1/conv/conv2d_params_max
    4.288    0.002    0.001   0.000%     99.996%         Const  mixed_8/tower/conv/conv2d_params_max
    4.270    0.002    0.001   0.000%     99.996%         Const  mixed_7/tower_2/conv/batchnorm/beta_min
    4.163    0.005    0.001   0.000%     99.996%         Const  mixed_7/tower_1/conv_3/batchnorm/beta_max
    4.137    0.002    0.001   0.000%     99.996%         Const  mixed_7/tower_1/conv_3/batchnorm/moving_mean_max
    4.123    0.002    0.001   0.000%     99.997%         Const  mixed_7/tower_1/conv_3/conv2d_params_max
    4.044    0.003    0.001   0.000%     99.997%         Const  mixed_7/tower_1/conv_1/batchnorm/moving_variance_max
    4.019    0.002    0.001   0.000%     99.997%         Const  mixed_7/tower_1/conv_1/conv2d_params_max
    3.947    0.002    0.001   0.000%     99.997%         Const  mixed_7/tower/conv_2/batchnorm/beta_min
    3.918    0.003    0.001   0.000%     99.997%         Const  mixed_7/tower/conv_2/batchnorm/moving_mean_min
    3.911    0.002    0.001   0.000%     99.997%         Const  mixed_7/tower/conv_2/conv2d_params_max
    3.895    0.003    0.001   0.000%     99.997%         Const  mixed_7/tower/conv_1/batchnorm/beta_max
    3.679    0.002    0.001   0.000%     99.997%         Const  mixed_6/tower_1/conv_4/batchnorm/beta_max
    3.661    0.003    0.001   0.000%     99.997%         Const  mixed_6/tower_1/conv_4/batchnorm/moving_variance_min
    3.626    0.002    0.001   0.000%     99.997%         Const  mixed_6/tower_1/conv_3/batchnorm/beta_max
    3.474    0.002    0.001   0.000%     99.997%         Const  mixed_6/tower_1/conv_1/conv2d_params_max
    3.459    0.002    0.001   0.000%     99.997%         Const  mixed_6/tower_1/conv/batchnorm/beta_max
    3.345    0.002    0.001   0.000%     99.997%         Const  mixed_6/tower/conv_1/batchnorm/beta_min
    3.020    0.002    0.001   0.000%     99.997%         Const  mixed_5/tower_1/conv_2/batchnorm/beta_min
    2.968    0.002    0.001   0.000%     99.997%         Const  mixed_5/tower_1/conv_1/batchnorm/beta_max
    2.964    0.002    0.001   0.000%     99.997%         Const  mixed_5/tower_1/conv_1/batchnorm/beta_min
    2.809    0.002    0.001   0.000%     99.997%         Const  mixed_5/tower/conv_2/conv2d_params_min
    2.418    0.005    0.001   0.000%     99.997%         Const  mixed_4/tower_1/conv_2/batchnorm/moving_mean_max
    2.294    0.002    0.001   0.000%     99.997%         Const  mixed_4/tower_1/conv/conv2d_params_max
    2.279    0.002    0.001   0.000%     99.997%         Const  mixed_4/tower/conv_2/batchnorm/beta_max
    2.036    0.002    0.001   0.000%     99.997%         Const  mixed_3/tower/conv_2/batchnorm/moving_variance_max
    1.955    0.003    0.001   0.000%     99.997%         Const  mixed_3/tower/conv_1/batchnorm/moving_mean_min
    1.795    0.002    0.001   0.000%     99.997%         Const  mixed_2/tower_2/conv/batchnorm/beta_min
    1.555    0.002    0.001   0.000%     99.997%         Const  mixed_2/tower/conv_1/batchnorm/moving_mean_min
    1.454    0.002    0.001   0.000%     99.998%         Const  mixed_2/conv/batchnorm/moving_variance_min
    1.334    0.003    0.001   0.000%     99.998%         Const  mixed_1/tower_1/conv_2/batchnorm/moving_mean_min
    0.856    0.002    0.001   0.000%     99.998%         Const  mixed/tower_1/conv_1/batchnorm/moving_mean_min
    5.785    0.002    0.001   0.000%     99.998%         Const  mixed_9/tower_2/conv/batchnorm/beta_min
    5.675    0.003    0.001   0.000%     99.998%         Const  mixed_9/tower_1/mixed/conv/batchnorm/beta_max
    5.632    0.005    0.001   0.000%     99.998%         Const  mixed_9/tower_1/mixed/conv/conv2d_params_max
    5.628    0.002    0.001   0.000%     99.998%         Const  mixed_9/tower_1/mixed/conv/conv2d_params_min
    5.559    0.002    0.001   0.000%     99.998%         Const  mixed_9/tower_1/conv/batchnorm/beta_min
    5.505    0.003    0.001   0.000%     99.998%         Const  mixed_9/tower/mixed/conv_1/batchnorm/beta_max
    5.198    0.003    0.001   0.000%     99.998%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/moving_mean_max
    5.066    0.005    0.001   0.000%     99.998%         Const  mixed_10/tower_1/conv_1/batchnorm/moving_mean_min
    4.978    0.003    0.001   0.000%     99.998%         Const  mixed_10/tower/mixed/conv_1/batchnorm/beta_min
    4.833    0.002    0.001   0.000%     99.998%         Const  mixed_10/tower/conv/batchnorm/moving_variance_max
    4.625    0.002    0.001   0.000%     99.998%         Const  mixed_9/conv/batchnorm/moving_mean_max
    4.382    0.002    0.001   0.000%     99.998%         Const  mixed_8/tower/conv_1/batchnorm/beta_max
    4.378    0.003    0.001   0.000%     99.998%         Const  mixed_8/tower/conv_1/batchnorm/beta_min
    4.249    0.002    0.001   0.000%     99.998%         Const  mixed_7/tower_2/conv/batchnorm/moving_mean_max
    4.245    0.002    0.001   0.000%     99.998%         Const  mixed_7/tower_2/conv/batchnorm/moving_mean_min
    4.215    0.002    0.001   0.000%     99.998%         Const  mixed_7/tower_1/conv_4/batchnorm/beta_min
    4.192    0.002    0.001   0.000%     99.998%         Const  mixed_7/tower_1/conv_4/batchnorm/moving_mean_max
    3.785    0.002    0.001   0.000%     99.998%         Const  mixed_7/conv/batchnorm/beta_max
    3.651    0.003    0.001   0.000%     99.998%         Const  mixed_6/tower_1/conv_4/batchnorm/moving_mean_max
    3.282    0.002    0.001   0.000%     99.998%         Const  mixed_6/tower/conv/batchnorm/moving_variance_max
    3.254    0.002    0.001   0.000%     99.998%         Const  mixed_6/tower/conv/conv2d_params_max
    3.103    0.002    0.001   0.000%     99.998%         Const  mixed_5/tower_1/conv_4/batchnorm/moving_mean_min
    2.531    0.002    0.001   0.000%     99.999%         Const  mixed_4/tower_1/conv_4/batchnorm/moving_mean_max
    2.442    0.002    0.001   0.000%     99.999%         Const  mixed_4/tower_1/conv_2/batchnorm/beta_min
    2.308    0.002    0.001   0.000%     99.999%         Const  mixed_4/tower_1/conv/batchnorm/moving_mean_max
    1.905    0.002    0.001   0.000%     99.999%         Const  mixed_3/tower/conv/batchnorm/moving_mean_max
    1.473    0.002    0.001   0.000%     99.999%         Const  mixed_2/conv/batchnorm/beta_max
    0.988    0.002    0.001   0.000%     99.999%         Const  mixed/tower_2/conv/batchnorm/moving_mean_min
    5.777    0.002    0.001   0.000%     99.999%         Const  mixed_9/tower_2/conv/batchnorm/moving_variance_max
    5.579    0.002    0.001   0.000%     99.999%         Const  mixed_9/tower_1/conv_1/conv2d_params_max
    5.521    0.002    0.001   0.000%     99.999%         Const  mixed_9/tower_1/conv/conv2d_params_max
    5.404    0.002    0.001   0.000%     99.999%         Const  mixed_9/tower/mixed/conv/conv2d_params_max
    5.157    0.002    0.001   0.000%     99.999%         Const  mixed_10/tower_1/mixed/conv/batchnorm/beta_min
    4.405    0.003    0.001   0.000%     99.999%         Const  mixed_8/tower_1/conv/batchnorm/moving_mean_min
    4.112    0.002    0.001   0.000%     99.999%         Const  mixed_7/tower_1/conv_2/batchnorm/beta_max
    3.822    0.002    0.001   0.000%     99.999%         Const  mixed_7/tower/conv/batchnorm/moving_variance_min
    3.096    0.002    0.001   0.000%     99.999%         Const  mixed_5/tower_1/conv_4/conv2d_params_max
    2.544    0.002    0.001   0.000%     99.999%         Const  mixed_4/tower_1/conv_4/batchnorm/moving_variance_max
    1.919    0.003    0.001   0.000%     99.999%         Const  mixed_3/tower/conv/batchnorm/moving_variance_max
    1.894    0.002    0.001   0.000%     99.999%         Const  mixed_3/tower/conv/conv2d_params_max
    1.487    0.002    0.001   0.000%     99.999%         Const  mixed_2/tower/conv/conv2d_params_max
    1.483    0.003    0.001   0.000%     99.999%         Const  mixed_2/tower/conv/conv2d_params_min
    5.714    0.003    0.001   0.000%     99.999%         Const  mixed_9/tower_1/mixed/conv_1/batchnorm/moving_variance_min
    5.642    0.003    0.001   0.000%     99.999%         Const  mixed_9/tower_1/mixed/conv/batchnorm/moving_mean_min
    5.620    0.002    0.001   0.000%     99.999%         Const  mixed_9/tower_1/conv_1/batchnorm/beta_max
    5.223    0.003    0.001   0.000%     99.999%         Const  mixed_10/tower_1/mixed/conv_1/batchnorm/beta_max
    5.005    0.002    0.001   0.000%     99.999%         Const  mixed_10/tower_1/conv/batchnorm/moving_mean_min
    4.997    0.002    0.001   0.000%    100.000%         Const  mixed_10/tower_1/conv/conv2d_params_max
    2.191    0.002    0.001   0.000%    100.000%         Const  mixed_4/tower/conv_1/batchnorm/moving_mean_min
    5.762    0.002    0.001   0.000%    100.000%         Const  mixed_9/tower_2/conv/batchnorm/moving_mean_max
    5.746    0.003    0.001   0.000%    100.000%         Const  mixed_9/tower_2/conv/conv2d_params_max
    5.323    0.003    0.001   0.000%    100.000%         Const  softmax/biases_min
    5.276    0.003    0.001   0.000%    100.000%         Const  mixed_10/tower_2/conv/batchnorm/beta_min
    4.894    0.002    0.001   0.000%    100.000%         Const  mixed_10/tower/mixed/conv/batchnorm/moving_mean_min
    4.585    0.002    0.001   0.000%    100.000%         Const  mixed_8/tower_1/conv_3/batchnorm/moving_variance_max
    3.869    0.002    0.001   0.000%    100.000%         Const  mixed_7/tower/conv_1/batchnorm/moving_mean_max
    4.593    0.002    0.001   0.000%    100.000%         Const  mixed_8/tower_1/conv_3/batchnorm/beta_min
    3.364    0.002    0.001   0.000%    100.000%         Const  mixed_6/tower/conv_2/conv2d_params_max
    2.760    0.002    0.001   0.000%    100.000%         Const  mixed_5/tower/conv_1/conv2d_params_max
    2.315    0.002    0.001   0.000%    100.000%         Const  mixed_4/tower_1/conv/batchnorm/moving_variance_min
    5.605    0.002    0.001   0.000%    100.000%         Const  mixed_9/tower_1/conv_1/batchnorm/moving_variance_max

I tensorflow/core/util/stat_summarizer.cc:263] 
"
4152,Not able to get NV profile for tensorflow(inception),"### Environment info

Operating System: Ubuntu 14.04
Machine: x86
Installed version of CUDA and cuDNN: 
CUDA 7.5, cuDNN 5.1.3
Installed from source:
1. The commit hash (`git rev-parse HEAD`)
9454b9015567ae0641250004fd2fafccede54a93
2. The output of `bazel version`
(tensorflow)asis@ws:~$ bazel version
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Jan 01 00:00:00 1970 (0)
Build timestamp: Thu Jan 01 00:00:00 1970 (0)
Build timestamp as int: 0
### Getting NV Profile for the following sample code:

```
(tensorflow)asis@ws:~$ cat test.py
import tensorflow as tf
c = []
for d in ['/gpu:0', '/gpu:1']:
  with tf.device(d):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])
    c.append(tf.matmul(a, b))
with tf.device('/cpu:0'):
  sum = tf.add_n(c)
sess = tf.Session()
print sess.run(sum)

(tensorflow)asis@ws:~$ nvprof -o nvtest_x86 python test.py
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5.1.3 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally
==41815== NVPROF is profiling process 41815, command: python test.py
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.562
pciBusID 0000:84:00.0
Total memory: 12.00GiB
Free memory: 11.87GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x313af00
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties:
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.562
pciBusID 0000:85:00.0
Total memory: 12.00GiB
Free memory: 11.87GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:84:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:85:00.0)
[[  44.   56.]
 [  98.  128.]]
==41815== Generated result file: /home/asis/samples/asis/nvtest_x86
(tensorflow)asis@ws:~$
```

**Generated result file: /home/asis/samples/asis/nvtest_x86**
### Not getting VN profile for the following TensorFlow(Inception)

```
(tensorflow)asis@ws:~$ nvprof -o nvprof_bs64_2gpu_3 bazel-bin/inception/imagenet_train --max_steps=3 --num_gpus=2 --batch_size=64 --train_dir=/home/asis/googlenet/test/train_bs64_2gpu_20160901_041429 --data_dir=/home/asis/inception_output_data
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5.1.3 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:84:00.0
Total memory: 12.00GiB
Free memory: 11.88GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7eb0000
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties:
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:85:00.0
Total memory: 12.00GiB
Free memory: 11.88GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:84:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:85:00.0)
WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7536 get requests, put_count=4071 evicted_count=1000 eviction_rate=0.24564 and unsatisfied allocation rate=0.605759
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110
2016-09-01 07:00:14.565170: step 0, loss = 13.07 (0.6 examples/sec; 104.984 sec/batch)
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=2012 evicted_count=2000 eviction_rate=0.994036 and unsatisfied allocation rate=0
======== Warning: No CUDA application was profiled, exiting
```

**======== Warning: No CUDA application was profiled, exiting**
### Problem

For the Sample code we are getting nv profile, but for the tensorflow(inception) code we are not getting nv profile and getting the following warning.
**Warning: No CUDA application was profiled, exiting**

How can we get nv profile on x86 for tensorflow(inception)
"
4151,Memory leak when continuously run assign op,"### Environment info

Operating System:
Ubuntu 16.04
Installed version of CUDA and cuDNN: 
8.0 RC, 5.1 (on GTX 1080)
If installed from source, provide 
HEAD: a8512e24deeeebe57eb1be14726634e7e6c23545
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Jan 01 00:00:00 1970 (0)
Build timestamp: Thu Jan 01 00:00:00 1970 (0)
Build timestamp as int: 0
### Problem:

In my application, I need to change value of some variable, and run the minimize in a loop, thus I have to run assign op continuously, which may add new op to graph every time, thus the program become very slow and memory explode.

```
sess = tf.Session()
a = tf.Variable(np.ones((5, 10000, 10000, 3)))
sess.run(tf.initialize_all_variables())

t0 = time.time()
for i in range(10000):
    sess.run(tf.assign(a,np.ones((5, 10000, 10000, 3)) ))
    t1 = time.time()
    print(t1-t0)
    t0 = t1
```

So is there a method to change the value of Variables without adding an op to graph? Or a way to remove it after(I have a big graph defined before the loop, I don't want to reset all of them and define again with reset_default_graph)?
And since the value assigned to the variable is different all the time, I cannot define the op before the for loop.
"
4149,Wide and Deep Learning tutorial example fails on Python 3.4,"When using the ""deep"" functionality in the Wide and Deep Learning tutorial on Python 3.4, I get the following error:

```
ValueError: Duplicate feature column key found for column: education_embedding. This usually means that the column is almost identical to another column, and one must be discarded.
```

Looks like a bug in tensorflow/contrib/layers/python/layers/feature_column.py in the _EmbeddingColumn class. The key(self) property is plagued by this bug: https://bugs.python.org/issue24931

So instead of coming out with a nice unique key, we get the following key for all _EmbeddingColumn instances: '_EmbeddingColumn()'

This causes the feature_column_ops.py's check_feature_columns() function to determine that the second _EmbeddingColumn instance is a duplicate since they keys of all of them are the same.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

I found this StackOverflow thread http://stackoverflow.com/questions/39249704/tensorflow-valueerror-duplicate-feature-column-key-found-for-column/ where someone had the same problem, with no answers.  So I did the debugging and answered the question.
### Environment info

Operating System: Fedora Core 23

Installed version of CUDA and cuDNN: 8 and 5.1
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

```
[jpangburn@localhost examples]$ ls /usr/local/cuda/lib64/libcud*
/usr/local/cuda/lib64/libcudadevrt.a
/usr/local/cuda/lib64/libcudart.so
/usr/local/cuda/lib64/libcudart.so.8.0
/usr/local/cuda/lib64/libcudart.so.8.0.27
/usr/local/cuda/lib64/libcudart_static.a
/usr/local/cuda/lib64/libcudnn.so
/usr/local/cuda/lib64/libcudnn.so.5
/usr/local/cuda/lib64/libcudnn.so.5.1.5
/usr/local/cuda/lib64/libcudnn_static.a

```

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)

```
[jpangburn@localhost examples]$ pwd
/home/jpangburn/open_source/tensorflow/tensorflow/examples
[jpangburn@localhost examples]$ git rev-parse HEAD
17d0e46e6cc31af0bcf6e80ff4c5670d233b4940

```
1. The output of `bazel version`

```
[jpangburn@localhost examples]$ bazel version
.
Build label: 0.3.1
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jul 29 09:09:52 2016 (1469783392)
Build timestamp: 1469783392
Build timestamp as int: 1469783392

```
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Be in the ""tensorflow/tensorflow/examples/learn"" directory.  Patch the wide_n_deep_tutorial.py to work in Python 3 by replacing ""urllib"" instances with ""urllib.request"".  Then run it ""python3 wide_n_deep_tutorial_py3.py --train_data=train_data --test_data=test_data --model_type=wide_n_deep"", obviously replacing the train_data and test_data files with your own copies, or omitting those switches altogether to download the files fresh.
### What other attempted solutions have you tried?

I worked around the problem by creating a subclass of _EmbeddedColumn and using that, but not sure my key(self) property implementation is working because the accuracy result with wide_n_deep is worse than with just ""wide"".  You can see this in my answer to the StackOverflow question at http://stackoverflow.com/questions/39249704/tensorflow-valueerror-duplicate-feature-column-key-found-for-column/39268045#39268045
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).

```
Traceback (most recent call last):
  File ""wide_n_deep_tutorial_py3.py"", line 220, in <module>
    tf.app.run()
  File ""/usr/lib/python3.4/site-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""wide_n_deep_tutorial_py3.py"", line 216, in main
    train_and_eval()
  File ""wide_n_deep_tutorial_py3.py"", line 204, in train_and_eval
    m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)
  File ""/usr/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 219, in fit
    max_steps=max_steps)
  File ""/usr/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 479, in _train_model
    train_op, loss_op = self._get_train_ops(features, targets)
  File ""/usr/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py"", line 166, in _get_train_ops
    logits = self._logits(features, is_training=True)
  File ""/usr/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py"", line 244, in _logits
    dnn_feature_columns = self._get_dnn_feature_columns()
  File ""/usr/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py"", line 208, in _get_dnn_feature_columns
    feature_column_ops.check_feature_columns(self._dnn_feature_columns)
  File ""/usr/lib/python3.4/site-packages/tensorflow/contrib/layers/python/layers/feature_column_ops.py"", line 318, in check_feature_columns
    f.name))
ValueError: Duplicate feature column key found for column: education_embedding. This usually means that the column is almost identical to another column, and one must be discarded.

```
"
4148,Build ERROR: undeclared inclusion(s) in rule '//tensorflow/core/kernels:batchtospace_op_gpu':,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
### Environment info

Operating System: Linux RHL 7.2

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): cuda 7.5 + cudnn 5

If installed from binary pip package, provide:
1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

./configure (select to support GPU)
bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package  --verbose_failures
### What other attempted solutions have you tried?

Rebuild from scratch, still repro.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).

ERROR: /data/tools/tensorflow/core/kernels/BUILD:1509:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:batchtospace_op_gpu':
this rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/batchtospace_op_gpu.cu.cc':
  '/usr/local/cuda-7.5/include/cuda_runtime.h'
  '/usr/local/cuda-7.5/include/host_config.h'
  '/usr/local/cuda-7.5/include/builtin_types.h'
  '/usr/local/cuda-7.5/include/device_types.h'
  '/usr/local/cuda-7.5/include/host_defines.h'
  '/usr/local/cuda-7.5/include/driver_types.h'
  '/usr/local/cuda-7.5/include/surface_types.h'
  '/usr/local/cuda-7.5/include/texture_types.h'
  '/usr/local/cuda-7.5/include/vector_types.h'
  '/usr/local/cuda-7.5/include/channel_descriptor.h'
  '/usr/local/cuda-7.5/include/cuda_runtime_api.h'
  '/usr/local/cuda-7.5/include/cuda_device_runtime_api.h'
  '/usr/local/cuda-7.5/include/driver_functions.h'
  '/usr/local/cuda-7.5/include/vector_functions.h'
  '/usr/local/cuda-7.5/include/vector_functions.hpp'
  '/usr/local/cuda-7.5/include/common_functions.h'
  '/usr/local/cuda-7.5/include/math_functions.h'
  '/usr/local/cuda-7.5/include/math_functions.hpp'
  '/usr/local/cuda-7.5/include/math_functions_dbl_ptx3.h'
  '/usr/local/cuda-7.5/include/math_functions_dbl_ptx3.hpp'
  '/usr/local/cuda-7.5/include/cuda_surface_types.h'
  '/usr/local/cuda-7.5/include/cuda_texture_types.h'
...
"
4147,`math_grad._ProdGrad` fails to handle cases where reduction_indices is scalar,"```
x = tf.zeros([10])
tf.gradients(tf.reduce_prod(x, 0), [x])
```

Gives

```
more traceback

/home/***/.local/lib/python2.7/site-packages/tensorflow/python/ops/math_grad.pyc in _ProdGrad(op, grad)
    128   reduced = math_ops.cast(op.inputs[1], dtypes.int32)
    129   idx = math_ops.range(0, array_ops.rank(op.inputs[0]))
--> 130   other, _ = array_ops.listdiff(idx, reduced)
    131   perm = array_ops.concat(0, [reduced, other])
    132   reduced_num = math_ops.reduce_prod(array_ops.gather(input_shape, reduced))

more traceback
```

In line 128, `op.inputs[1]` could be a scalar, which will cause a shape mismatch when passed to `array_ops.listdiff` in line 130.

TF version: master branch a week ago
"
4146,Provide generic android JNI interface,"In the Android example, the developer need to write some customized C++ code for the JNI call, this extra effort is not needed especially in the prototyping step.

Could we provide a generic JNI interface (just like generic inference serving, with input/output as a map of tensors, using serialized protobuf bytes or customized serialization), then the developer will not need to write and BUILD C++ code anymore.

Of course, the developers are free to optimize the code by writing the customized JNI calls. This feature will definitely boost the development process.
"
4139,Add the mention bot to tensorflow's github,"Integrate Mention-bot with the tensorflow github pull request system

https://github.com/facebook/mention-bot 

> The mention bot will automatically mention potential reviewers on pull requests. It helps getting faster turnaround on pull requests by involving the right people early on.
"
4138,seq2seq model is not efficient when using sampled softmax,"Hi, 
my commit number is
70de76e696c21da617fd2e6435cf7fedab220db8

I want to try sampled softmax to speed up my lm training. I'm changing slightly on the ptb training example code. I'm working on a CPU machine.
First, on ptb(vocab 10k), I'm using the ""small"" config, I see some speed up gain:
normal softmax 896wps

sample_softmax h256L1ba20 sample512 1975wps

Then, I move to a larger set, and using 100k vocab, I'm still setting the sample number to 512, I think the speed should be similar to the ptb case. But I got the speed to be about 300 wps, even if I set the sample number as small as 4. I don't understand why it should be so slower than the ptb case. Do you know how can I make it faster?

Here's some of the related code:

```
      output = tf.reshape(tf.concat(1, outputs), [-1, size])
      softmax_w = tf.get_variable(""softmax_w"", [size, vocab_size])
      softmax_w_t = tf.transpose(softmax_w)
      softmax_b = tf.get_variable(""softmax_b"", [vocab_size])

    if use_sample_softmax == True:
      loss = tf.nn.sampled_softmax_loss(softmax_w_t, softmax_b, output, tf.reshape(self._targets, [-1, 1]), num_samples, vocab_size) #todo
      self._cost = cost = tf.reduce_sum(loss) / batch_size
    else:
      logits = tf.matmul(output, softmax_w) + softmax_b
      loss = tf.nn.seq2seq.sequence_loss_by_example(
          [logits],
          [tf.reshape(self._targets, [-1])],
          [tf.ones([batch_size * num_steps])])
      self._cost = cost = tf.reduce_sum(loss) / batch_size


```

Thanks!
Goose
"
4135,"Seg fault when computing gradient of 3D convolution filter with (1,1,1) kernel","Minimal example to reproduce the bug:

``` python
import numpy as np
import tensorflow as tf

sess = tf.Session()
with sess.as_default():
    # Input: [batch, height, width, depth, input_channels]
    x_shape = [3, 85, 65, 83, 8]
    # Filter: [kernel_height, kernel_width, depth, input_channels, output_channels]
    f_shape = [1, 1, 1, x_shape[-1], 32]
    # Output: [batch, height, width, depth, output_channels]
    y_shape = [3, 85, 65, 83, f_shape[-1]]

    np.random.seed(1)  # Make it reproducible.
    x_val = np.random.random_sample(x_shape).astype(np.float32)
    f_val = np.random.random_sample(f_shape).astype(np.float32)
    output_val = np.random.random_sample(y_shape).astype(np.float32)
    x = tf.constant(x_val, name=""x"", dtype=tf.float32)
    f = tf.constant(f_val, name=""f"", dtype=tf.float32)

    output = tf.nn.conv3d(x, f, strides=(1,1,1,1,1), padding=""SAME"")

    r = tf.gradients(output, f)
    print(r[0].eval())
```

this leads to a seg fault, tested on a Mac with CPU, both with tensorflow 0.9.0 (binary release) and when compiled from source (last commit https://github.com/tensorflow/tensorflow/commit/ad4f02a69162abe5d242b7d94f62138849aec9ab ).

For certain values of `(x_shape[â€“1], f_shape[-1])`, for instance (2,4) or (8,32), the seg fault occurs, but for other values (2,2) or (16,32), there is no seg fault.

Inserting some `std::cout <<` print statements in the code shows that the segmentation fault occurs during this function call in Conv3DBackpropFilterOp::Compute https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_grad_ops_3d.cc#L315
"
4133,Compile error: invalid use of incomplete type 'class perftools::gputools::StreamExecutor' in stream_executor/kernel.h,"#### Issue

Building tensorflow from source using gcc 7.0.0 results in the below compile error.

The error is clearly enough because the structure of class StreamExecutor is not known from kernel.h.

```
$ bazel build --verbose_failures -c opt //tensorflow/tools/pip_package:build_pip_package
...
ERROR: /opt/tensorflow-r0.10/tensorflow/stream_executor/BUILD:5:1: C++ compilation of rule '//tensorflow/stream_executor:stream_executor' failed: gcc failed: error executing command 
  (cd /home/foreese/.cache/bazel/_bazel_foreese/2c35d3fa162a38720aa5307b2053cde8/execroot/tensorflow-r0.10 && \
  exec env - \
    LD_LIBRARY_PATH=/opt/gcc-dev/lib64:/opt/gcc-dev/lib:/opt/support/lib:/opt/support/lib32:/opt/gdb-7.7.1/lib \
    PATH=/opt/gcc-dev/bin:/opt/gcc-dev/libexec/gcc/x86_64-pc-linux-gnu/7.0.0:/home/foreese/bin:/home/foreese/.local/bin:/opt/python-3.5/bin:/opt/support/bin:/usr/local/bin:/usr/bin:/bin: \
  /opt/gcc-dev/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/opt/gcc-dev/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-canonical-system-headers -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/local-py3-opt/bin/tensorflow/stream_executor/_objs/stream_executor/tensorflow/stream_executor/machine_manager.pic.d '-frandom-seed=bazel-out/local-py3-opt/bin/tensorflow/stream_executor/_objs/stream_executor/tensorflow/stream_executor/machine_manager.pic.o' -fPIC -DHAVE_CONFIG_H -iquote . -iquote bazel-out/local-py3-opt/genfiles -iquote external/protobuf -iquote bazel-out/local-py3-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/local-py3-opt/genfiles/external/bazel_tools -iquote external/farmhash_archive -iquote bazel-out/local-py3-opt/genfiles/external/farmhash_archive -iquote external/gif_archive -iquote bazel-out/local-py3-opt/genfiles/external/gif_archive -iquote external/highwayhash -iquote bazel-out/local-py3-opt/genfiles/external/highwayhash -iquote external/jpeg_archive -iquote bazel-out/local-py3-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local-py3-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local-py3-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local-py3-opt/genfiles/external/eigen_archive -iquote external/zlib_archive -iquote bazel-out/local-py3-opt/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/local-py3-opt/genfiles/external/local_config_cuda -isystem external/protobuf/src -isystem bazel-out/local-py3-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 -isystem bazel-out/local-py3-opt/genfiles/external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 -isystem external/gif_archive/giflib-5.1.4/lib -isystem bazel-out/local-py3-opt/genfiles/external/gif_archive/giflib-5.1.4/lib -isystem external/highwayhash -isystem bazel-out/local-py3-opt/genfiles/external/highwayhash -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local-py3-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local-py3-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local-py3-opt/genfiles/external/re2 -isystem external/eigen_archive -isystem bazel-out/local-py3-opt/genfiles/external/eigen_archive -isystem external/zlib_archive/zlib-1.2.8 -isystem bazel-out/local-py3-opt/genfiles/external/zlib_archive/zlib-1.2.8 -isystem external/local_config_cuda/cuda -isystem bazel-out/local-py3-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/include -isystem bazel-out/local-py3-opt/genfiles/external/local_config_cuda/cuda/include -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/stream_executor/machine_manager.cc -o bazel-out/local-py3-opt/bin/tensorflow/stream_executor/_objs/stream_executor/tensorflow/stream_executor/machine_manager.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
In file included from ./tensorflow/stream_executor/stream.h:33:0,
                 from ./tensorflow/stream_executor/machine_manager.h:51,
                 from tensorflow/stream_executor/machine_manager.cc:16:
./tensorflow/stream_executor/kernel.h: In member function 'void perftools::gputools::TypedKernel<Params>::PackOneParam(std::vector<perftools::gputools::KernelArg>*, const T&, typename std::enable_if<perftools::gputools::IsDeviceMemoryValueLike<T>::value>::type*) const':
./tensorflow/stream_executor/kernel.h:358:32: error: invalid use of incomplete type 'class perftools::gputools::StreamExecutor'
     args->emplace_back(parent()->DeviceMemoryToKernelArg(arg));
                                ^~
In file included from ./tensorflow/stream_executor/stream.h:29:0,
                 from ./tensorflow/stream_executor/machine_manager.h:51,
                 from tensorflow/stream_executor/machine_manager.cc:16:
./tensorflow/stream_executor/device_memory.h:35:7: note: forward declaration of 'class perftools::gputools::StreamExecutor'
 class StreamExecutor;
       ^~~~~~~~~~~~~~
In file included from ./tensorflow/stream_executor/stream.h:33:0,
                 from ./tensorflow/stream_executor/machine_manager.h:51,
                 from tensorflow/stream_executor/machine_manager.cc:16:
./tensorflow/stream_executor/kernel.h: In member function 'void perftools::gputools::TypedKernel<Params>::PackOneParam(std::vector<perftools::gputools::KernelArg>*, T, typename std::enable_if<perftools::gputools::IsDeviceMemoryPointer<T>::value>::type*) const':
./tensorflow/stream_executor/kernel.h:368:32: error: invalid use of incomplete type 'class perftools::gputools::StreamExecutor'
     args->emplace_back(parent()->DeviceMemoryToKernelArg(*ptr));
                                ^~
In file included from ./tensorflow/stream_executor/stream.h:29:0,
                 from ./tensorflow/stream_executor/machine_manager.h:51,
                 from tensorflow/stream_executor/machine_manager.cc:16:
./tensorflow/stream_executor/device_memory.h:35:7: note: forward declaration of 'class perftools::gputools::StreamExecutor'
 class StreamExecutor;
       ^~~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
#### System details
- OS: CentOS 6.8 x86_64-redhat-linux using
- Compiler:  gcc 7.0.0 (I do this because my system gcc is 4.4.7 which is too old to build bazel properly with -std=c++11)
- CUDA/cuDNN: None
- tensorflow: branch r0.10 6ce5b5c8298273e3861a75fb6ccde63b9dd157c5
- bazel:

```
Build label: 0.3.1-2016-08-31 (@1b2071f)
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Aug 31 16:52:51 2016 (1472662371)
Build timestamp: 1472662371
Build timestamp as int: 1472662371
```
"
4132,Unexpected performance changes as a function of batch size,"I am observing unexpected performance from tensorflow as I change the batch size that I feed to the session.

![image](https://cloud.githubusercontent.com/assets/966348/18139518/cfa80cc0-6fa9-11e6-9b7f-20bbfb30ddc9.png)

I have created a [small jupyter](https://gist.github.com/tillahoffmann/63cbe9fce331df75a6b57420c48b7c36) notebook to demonstrate the issue. Errors bars correspond to the standard deviation of the mean over multiple runs.

In some of our more complex models, the jump in runtime occurs at small batch sizes (around 200 images of 40 by 80 pixels).
"
4131,reduce_max and maximum give different results for negative infinity,"Using `tf.maximum` with negative inf inputs as follows:

```
tf.maximum(-math.inf, -math.inf).eval()
```

gives the expected result `-inf`

However, `tf.reduce_max`, on the same inputs:

```
tf.reduce_max([-math.inf, -math.inf]).eval()
```

gives: `-3.40282e+38` which is the min float32.

For positive infinity inputs, both functions result in `inf`.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

I posted this as an SO question first:
http://stackoverflow.com/questions/39211546/bug-in-tensorflow-reduce-max-for-negative-infinity
### Environment info

Operating System: Ubuntu 16.04

Installed version of CUDA and cuDNN: 

```
-rw-r--r-- 1 root root   560184 May 25 17:48 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 May 25 17:52 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 May 25 17:52 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
-rw-r--r-- 1 root root   394472 May 25 17:48 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 root root   737516 May 25 17:48 /usr/local/cuda-8.0/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 Jul  4 16:22 /usr/local/cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       17 Jul  4 16:22 /usr/local/cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.0.5
-rwxr-xr-x 1 root root 78065952 Jul  4 16:22 /usr/local/cuda-8.0/lib64/libcudnn.so.5.0.5
-rw-r--r-- 1 root root 68709594 Jul  4 16:22 /usr/local/cuda-8.0/lib64/libcudnn_static.a

```

Installed from source.

Commit hash: 3cb39956e622b322e43547cf2b6e337020643f21

Bazel:

```
Build label: 0.3.0
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jun 10 11:38:23 2016 (1465558703)
Build timestamp: 1465558703
Build timestamp as int: 1465558703

```
"
4130,Training a neural net in C++,"1. There are examples in the repo for loading a pre-trained model in C++. Can someone add an example to train a simple 1 hidden layer fully connected neural net (or even logistic regression using SGD) in C++? 
2. I realize there is no publicly available support for auto differentiation in C++. Are there any plans to do so?
"
4128,Docs for Inception Model,"TensorFlow 0.10.0rc0 CPU on Linux.

This is about the Inception model and example that is included with TensorFlow:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/imagenet/classify_image.py

I am using this version of the Inception model:

http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz

I'm not sure whether to ask these questions here or on StackOverflow, but there are some related questions there that haven't been answered, and this is also sort of a request for improving the TensorFlow docs, so I hope it's OK that I ask the questions here.

My questions are:

(1) The softmax classifier apparently outputs an array of length 1008 - but there is only 1000 classes in the data-set. Why is there a difference? How should I adjust for this difference?

(2) What is the size of the input JPEG-images supposed to be, and is the Inception model rescaling them automatically?

(3) I get a deprecation warning. I don't know what is causing this. Does it mean that this Inception model will stop working in the future?

> /home/foo/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py:1811: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future
>  result_shape.insert(dim, 1)
> W tensorflow/core/framework/op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().

(4) The sample code uses `tf.gfile.Exists()` which just wraps `os.path.exists()` so it seems to be completely redundant. Why is it used?

(5) The sample code also uses `tf.gfile.FastGFile()` which seems to be the non-thread-safe version of TensorFlow's file-class. Why not use Python's built-in `open()` instead?

(6) The function `run_inference_on_image()` creates a new TensorFlow session for each image that is being processed. Is the TensorFlow session so lightweight that this is the preferred way of doing it? Or is it better to create a single session and use it repeatedly?

Thanks.
"
4127,Tensorflow build from source fails with C++ compilation of rule '//tensorflow/core:framework_internal' failed: crosstool_wrapper_driver_is_not_gcc failed:,"I am trying to build tensorflow from source (without external git access), can someone please let me know how to get this going, I have built most of the dependencies in a local system directory but end up with the following:

ERROR: /usr/local/tensorflow/tensorflow/core/BUILD:927:1: C++ compilation of rule '//tensorflow/core:framework_internal' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command
(cd /root/.cache/bazel/bazel_root/e86f6fce5559de9e3e13fb6adb66b858/execroot/tensorflow && \
exec env - \
PATH=/usr/local/tensorflow/protobuf/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/bin \
third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIC -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' '-frandom-seed=bazel-out/host/bin/tensorflow/core/objs/framework_internal/tensorflow/core/example/feature_util.o' -iquote . -iquote bazel-out/host/genfiles -iquote external/protobuf -iquote bazel-out/host/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote external/re2 -iquote bazel-out/host/genfiles/external/re2 -isystem external/protobuf/src -isystem bazel-out/host/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/highwayhash -isystem bazel-out/host/genfiles/external/highwayhash -isystem third_party/gpus/cuda/include -isystem bazel-out/host/genfiles/third_party/gpus/cuda/include -fno-exceptions -DEIGEN_AVOID_STL_ARRAY '-DGOOGLE_CUDA=1' -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -MD -MF bazel-out/host/bin/tensorflow/core/_objs/framework_internal/tensorflow/core/example/feature_util.d -c tensorflow/core/example/feature_util.cc -o bazel-out/host/bin/tensorflow/core/_objs/framework_internal/tensorflow/core/example/feature_util.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
tensorflow/core/example/feature_util.cc: In function 'const typename tensorflow::internal::RepeatedFieldTrait::Type& tensorflow::GetFeatureValues(const string&, const tensorflow::Example&) [with FeatureType = long long int; typename tensorflow::internal::RepeatedFieldTrait::Type = google::protobuf::RepeatedField; std::string = std::basic_string]':
tensorflow/core/example/feature_util.cc:54:67: error: invalid initialization of reference of type 'const google::protobuf::RepeatedField&' from expression of type 'const google::protobuf::RepeatedField'
return example.features().feature().at(name).int64_list().value();
tensorflow/core/example/feature_util.cc: In function 'typename tensorflow::internal::RepeatedFieldTrait::Type\* tensorflow::GetFeatureValues(const string&, tensorflow::Example) [with FeatureType = long long int; typename tensorflow::internal::RepeatedFieldTrait::Type = google::protobuf::RepeatedField; std::string = std::basic_string]':
tensorflow/core/example/feature_util.cc:62:23: error: cannot convert 'google::protobuf::RepeatedField' to 'google::protobuf::RepeatedField' in return
->mutable_value();
^
tensorflow/core/example/feature_util.cc: In function 'const typename tensorflow::internal::RepeatedFieldTrait::Type& tensorflow::GetFeatureValues(const string&, const tensorflow::Example&) [with FeatureType = long long int; typename tensorflow::internal::RepeatedFieldTrait::Typ
e = google::protobuf::RepeatedField; std::string = std::basic_string]':
tensorflow/core/example/feature_util.cc:55:1: warning: control reaches end of non-void function [-Wreturn-type]
}
^
tensorflow/core/example/feature_util.cc: In function 'typename tensorflow::internal::RepeatedFieldTrait::Type tensorflow::GetFeatureValues
(const string&, tensorflow::Example*) [with FeatureType = long long int; typename tensorflow::internal::RepeatedFieldTrait::Type = google::
protobuf::RepeatedField; std::string = std::basic_string]':
tensorflow/core/example/feature_util.cc:63:1: warning: control reaches end of non-void function [-Wreturn-type]
}
^
Target //tensorflow/cc:tutorials_example_trainer failed to build

Any pointers is greatly appreciated, not sure what I am missing here.

bash-4.2# cd /usr/local/
bazel-0.3.0 jpeg-9a libpng-1.2.53 tensorflow
eigen-eigen-9e1b48c333aa giflib-5.1.4

gmock-1.7.0 six-1.10.0 zlib-1.2.8
farmhash-34c13ddfab0e35422f4c3979f360635a8c050260

/usr/local/tensorflow
bash-4.2# ls
ACKNOWLEDGMENTS bazel-genfiles boost/ bzip2.BUILD gif.BUILD jpeg.BUILD navbar.md re2/ third_party zlib.BUILD
AUTHORS bazel-out boost.BUILD configure grpc.BUILD jsoncpp.BUILD png.BUILD README.md tools
avro.BUILD bazel-tensorflow boringssl.BUILD CONTRIBUTING.md highwayhash/ LICENSE protobuf/ RELEASE.md util
bazel-bin bazel-testlogs bower.BUILD farmhash.BUILD ISSUE_TEMPLATE.md nanopb.BUILD tensorflow WORKSPACE

workspace.bzl and CROSSTOOL have all the necessary entries for the include libraries and packages.
"
4126,Device logging from C API not appearing,"Hello,
I'm using the C API and creating a session with the `log_device_placement` config parameter set to `true`, but am not actually seeing any logging output on STDOUT. Is there something special I need to be doing to capture and display the TensorFlow log from C?
"
4125, do tensorflow support lmdb or hdf5 data?,"Now I have lmdb and hdf5 data for image classification. And I want to use tensorflow to train some convnets. So do tensorflow support lmdb or hdf5 data?
"
