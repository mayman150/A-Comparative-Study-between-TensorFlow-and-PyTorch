Issue Number,Issue Title,Issue Body
10471,[TF:XLA] XLA does not recognize symbol Polly.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: ('v1.0.0-1783-g4c3bb1a', '1.0.0')
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: - 
- **GPU model and memory**: -
- **Exact command to reproduce**: -

### Problem description
I have written my custom build file to integrate Polly into XLA's LLVM build. I have included a dependency to ```@llvm//:polly``` in my [tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/BUILD](https://gitlab.com/annanay25/tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/BUILD#L294) file under the ```compiler_functor``` rule (Since this is the file with the LLVM PassManagerBuilder defined). 
I also added the header 
```
#include ""external/llvm/tools/polly/include/polly/RegisterPasses.h""
```
in this file.
Somehow, this does not seem to be enough for XLA to recognize Polly. When I add - 

```
polly::registerPollyPasses(...)
```
to this file, it fails to compile with 
```
ERROR: Symbol polly was not declared here
```

I am unable to fix this (possibly linking) issue.

@hawkinsp - Could you please take a look at this?

Please refer to #10288 for more relevant information."
10468,[feature] Use Core ML on iOS,"Apple announced [Core ML](https://developer.apple.com/documentation/coreml) which may be useful for abstracting away the complexities of the hardware platform:

![image](https://user-images.githubusercontent.com/51059/26841383-5226ef0c-4ab7-11e7-9ce9-61849a3c0cc9.png)

Related:
* https://github.com/tensorflow/tensorflow/issues/7958 (MPS)
* https://github.com/tensorflow/tensorflow/issues/3001 (BNNS)"
10464,"Expected dimension in the range [-1, 1), but got 1","the error

InvalidArgumentError                      Traceback (most recent call last)
C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1021     try:
-> 1022       return fn(*args)
   1023     except errors.OpError as e:

C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1003                                  feed_dict, fetch_list, target_list,
-> 1004                                  status, run_metadata)
   1005 

C:\Users\Owner\Anaconda3\envs\tensorflow\lib\contextlib.py in __exit__(self, type, value, traceback)
     65             try:
---> 66                 next(self.gen)
     67             except StopIteration:

C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\errors_impl.py in raise_exception_on_not_ok_status()
    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),
--> 466           pywrap_tensorflow.TF_GetCode(status))
    467   finally:

InvalidArgumentError: Expected dimension in the range [-1, 1), but got 1
	 [[Node: ArgMax_1 = ArgMax[T=DT_INT32, Tidx=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_Placeholder_2_0, ArgMax_1/dimension)]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-22-5a0d43c01cd7> in <module>()
      1 batch_xs, batch_ys = dm.loadbatch(151,""data/NoneNematic/validate/NoneNematic"",0)
----> 2 print(sess.run(accuracy, feed_dict={x: batch_xs, z_: batch_ys}))

C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    765     try:
    766       result = self._run(None, fetches, feed_dict, options_ptr,
--> 767                          run_metadata_ptr)
    768       if run_metadata:
    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
    963     if final_fetches or final_targets:
    964       results = self._do_run(handle, final_targets, final_fetches,
--> 965                              feed_dict_string, options, run_metadata)
    966     else:
    967       results = []

C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1013     if handle is None:
   1014       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
-> 1015                            target_list, options, run_metadata)
   1016     else:
   1017       return self._do_call(_prun_fn, self._session, handle, feed_dict,

C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1033         except KeyError:
   1034           pass
-> 1035       raise type(e)(node_def, op, message)
   1036 
   1037   def _extend_graph(self):

InvalidArgumentError: Expected dimension in the range [-1, 1), but got 1
	 [[Node: ArgMax_1 = ArgMax[T=DT_INT32, Tidx=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_Placeholder_2_0, ArgMax_1/dimension)]]

Caused by op 'ArgMax_1', defined at:
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\traitlets\config\application.py"", line 658, in launch_instance
    app.start()
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\zmq\eventloop\ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\tornado\ioloop.py"", line 887, in start
    handler_func(fd_obj, events)
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\tornado\stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\zmq\eventloop\zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\zmq\eventloop\zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\zmq\eventloop\zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\tornado\stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2717, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2821, in run_ast_nodes
    if self.run_code(code, result):
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-20-1310bf690995>"", line 1, in <module>
    correct_prediction = tf.equal(tf.argmax(z,1), tf.argmax(z_,1))
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 173, in argmax
    return gen_math_ops.arg_max(input, axis, name)
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 168, in arg_max
    name=name)
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 2327, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Users\Owner\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1226, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Expected dimension in the range [-1, 1), but got 1
	 [[Node: ArgMax_1 = ArgMax[T=DT_INT32, Tidx=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_Placeholder_2_0, ArgMax_1/dimension)]]


the code
STM Data intermediate brain
Import dataset and tensorflow
In [ ]:

import tensorflow as tf
import math
In [ ]:

import datamaker as dm
import matplotlib.pyplot as plt
import numpy as np
Define the Model
In [ ]:

Nhidden=1024
IMAGE_SIZE = 30
IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE
NUM_CLASSES = 2
In [ ]:

x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS])
In [ ]:

W = tf.Variable(tf.zeros([IMAGE_PIXELS, Nhidden]))
b = tf.Variable(tf.zeros([Nhidden]))
In [ ]:

y = tf.nn.relu(tf.matmul(x, W) + b)
In [ ]:

V = tf.Variable(tf.zeros([Nhidden,NUM_CLASSES]))
a = tf.Variable(tf.zeros([NUM_CLASSES]))
In [ ]:

z = tf.nn.softmax(tf.matmul(y,V)+a)
In [ ]:

y_ = tf.placeholder(tf.float32, [None, Nhidden])
In [ ]:

z_ = tf.placeholder(tf.int32, [100])
Define the cost/loss function and training algorithm
In [ ]:

#cross_entropy1 = tf.nn.softmax_cross_entropy_with_logits(labels=y_,logits=tf.matmul(x, W) + b)
In [ ]:

#train_step1 = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy1)
In [ ]:

cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=z_,logits=tf.matmul(y,V)+a)
In [ ]:

loss = tf.reduce_mean(cross_entropy)
In [ ]:

train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
Train the model
In [ ]:

sess = tf.InteractiveSession()
In [ ]:

tf.global_variables_initializer().run()
In [ ]:

for b in range(94):
  batch_xs, batch_ys = dm.loadbatch(b,""data/NoneNematic/train/NoneNematic"",0)
  sess.run(train_step, feed_dict={x: batch_xs, z_: batch_ys})
In [ ]:

for b in range(100):
  batch_xs, batch_ys = dm.loadbatch(b,""data/Nematic/train/Nematic"",1)
  sess.run(train_step, feed_dict={x: batch_xs, z_: batch_ys})
Evaluate the model
In [ ]:

correct_prediction = tf.equal(tf.argmax(z,1), tf.argmax(z_,1))
In [ ]:

t
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.int32))
In [ ]:

batch_xs, batch_ys = dm.loadbatch(151,""data/NoneNematic/validate/NoneNematic"",0)
print(sess.run(accuracy, feed_dict={x: batch_xs, z_: batch_ys}))
In [ ]:

accuracy.eval({x: batch_xs, z_: batch_ys})
 print(""Accuracy:"", accuracy.eval({x: batch_xs, z_: batch_ys}))
In [ ]:

print(sess.run(z, feed_dict={x:batch_xs}))
print(sess.run(z, feed_dict={x:batch_xs}))"
10463,Where are quantization dependencies?,"In https://www.tensorflow.org/performance/quantization tutorial, there is //tensorflow/contrib/quantization and //tensorflow/contrib/quantization/kernels required to use a quantized graph.

I cannot find this in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantization 

Has the location been updated?

```
# Note: You need to add the dependencies of the quantization operation to the
#       cc_binary in the BUILD file of the label_image program:
#
#     //tensorflow/contrib/quantization:cc_ops
#     //tensorflow/contrib/quantization/kernels:quantized_ops

bazel build tensorflow/examples/label_image:label_image
bazel-bin/tensorflow/examples/label_image/label_image \
--image=<input-image> \
--graph=/tmp/quantized_graph.pb \
--labels=/tmp/imagenet_synset_to_human_label_map.txt \
--input_width=299 \
--input_height=299 \
--input_mean=128 \
--input_std=128 \
--input_layer=""Mul:0"" \
--output_layer=""softmax:0""
```
"
10461,"Tensor names prepended with ""import/"" when loaded from protobuf","When I load a (frozen) Tensorflow model from disk using:

```
    graph = tf.Graph()
    with graph.as_default():
        f = gfile.FastGFile(""frozen_graph.pb"", ""rb"")
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
        tf.import_graph_def(graph_def)
```

It seems that all tensor names are prepended with `import/`.
This is the code I use to print the names:
```
with tf.Session(graph=graph) as sess:
        all_ops = sess.graph.get_operations()
        op_values =  [op.values() for op in all_ops]
        for values in op_values:
            for each in value:
                print each.name

```
Why? Is this some kind of default option that can be overriden? Or can I rely on this in my code? I could not find this documented anyhwere, can anybody point me to references regarding this?
"
10460,tf.import_graph_def() does not return list of tensors or operations,"I read the documentation for `tf.import_graph_def` [here](https://www.tensorflow.org/api_docs/python/tf/import_graph_def).

As the documentation says: 
Returns: A list of Operation and/or Tensor objects from the imported graph, corresponding to the names in return_elements.

I tried that with this sample code but the method returns `None` for me.

I had to run some tests using this feature. But stuck at this issue.

Sample code:

```python3
import tensorflow as tf
g = tf.Graph()
with g.as_default():
    c = tf.constant(30.0)
    b = tf.constant(20.0)
    a = tf.add(b, c)
gdef = g.as_graph_def()
print(tf.import_graph_def(gdef))```

"
10459,"//tensorflow/core/kernels:sparse_matmul_op_test_gpu is failing on ppc64le with ""Actual: false but Expected: true""","### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
   Ubuntu 16.04 (ppc64le)
- **TensorFlow installed from (source or binary)**:
    Installed from source (v1.0.1)
- **TensorFlow version (use command below)**:
    ('v1.0.1-0-ge895d5c-dirty', '1.0.1')
- **Bazel version (if compiling from source)**:
     0.4.4-2017-05-26 (@80a07b5)
- **CUDA/cuDNN version**:
     CUDA = 8.0 and cuDNN = 5.1
- **GPU model and memory**:
    GPU 0: Tesla P100-SXM2-16GB
    GPU 1: Tesla P100-SXM2-16GB
- **Exact command to reproduce**:
     bazel test --config=opt --config=cuda //tensorflow/core/kernels:sparse_matmul_op_test_gpu

### Describe the problem
This test is passing successfully on X86, however getting failure on ppc64le with following errors:

### Source code / logs
```
$  bazel test --config=opt --config=cuda //tensorflow/core/kernels:sparse_matmul_op_test_gpu

exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
-----------------------------------------------------------------------------
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
Running main() from test_main.cc
[==========] Running 4 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 4 tests from SparseMatmulOpTest
[ RUN      ] SparseMatmulOpTest.BroadcastPacketTest
[0.170094 0.170094 0.170094 0.170094] != [  0.170094    0.14922 -0.0823886   0.026985], differences: [         0 -0.0208738  -0.252482  -0.143109]
tensorflow/core/kernels/sparse_matmul_op_test.cc:257: Failure
Value of: areApprox(ref, data2, PacketSize)
  Actual: false
Expected: true
[  FAILED  ] SparseMatmulOpTest.BroadcastPacketTest (0 ms)
[ RUN      ] SparseMatmulOpTest.InterleavePacketTest
[       OK ] SparseMatmulOpTest.InterleavePacketTest (0 ms)
[ RUN      ] SparseMatmulOpTest.Bfloat16ExpandTest
[       OK ] SparseMatmulOpTest.Bfloat16ExpandTest (0 ms)
[ RUN      ] SparseMatmulOpTest.Bfloat16LoadTest
[       OK ] SparseMatmulOpTest.Bfloat16LoadTest (0 ms)
[----------] 4 tests from SparseMatmulOpTest (0 ms total)

[----------] Global test environment tear-down
[==========] 4 tests from 1 test case ran. (0 ms total)
[  PASSED  ] 3 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] SparseMatmulOpTest.BroadcastPacketTest

 1 FAILED TEST
```
I went through the file tensorflow/core/kernels/sparse_matmul_op_test.cc to find the root cause, but couldn't understand the code. Please provide your suggestions/comments on this.Thanks!
"
10458,Transform_graph android error ,"Hi,
One month back I generated my custom TF model (output_graph.pb ) using Tensor Flow 1.0.1. It was working fine after optimization it using optimize_for_interface.

Now I plan to reduce its size and improve execution speed, I downloaded Tensor Flow 1.2.0.  I used transform_graph as 
 bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=./output_graph.pb \
--out_graph=./transformed_graph.pb \
--inputs='Mul' \
--outputs='final_result' \
--transforms='
  add_default_attributes
  strip_unused_nodes(type=float, shape=""1,299,299,3"")
  remove_nodes(op=Identity, op=CheckNumerics)
  fold_constants(ignore_errors=true)
  fold_batch_norms
  fold_old_batch_norms
  quantize_weights
  quantize_nodes
  strip_unused_nodes
  sort_by_execution_order'

I built the APK and ran on a Lenovo Yoga 3 tablet.
It generated a run time error:

W/native  (24951): op_kernel.cc:1165 Invalid argument: computed output size would be negative
E/TensorFlowInferenceInterface(24951): Failed to run TensorFlow inference with inputs:[Mul], outputs:[final_result]
--------- beginning of crash
E/AndroidRuntime(24951): FATAL EXCEPTION: inference
E/AndroidRuntime(24951): Process: org.tensorflow.demo, PID: 24951
E/AndroidRuntime(24951): java.lang.IllegalArgumentException: computed output size would be negative
E/AndroidRuntime(24951): 	 [[Node: pool_3/eightbit = QuantizedAvgPool[T=DT_QUINT8, ksize=[1, 8, 8, 1], padding=""VALID"", strides=[1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""](mixed_10/join/eightbit, mixed_10/join/eightbit:1, mixed_10/join/eightbit:2)]]
E/AndroidRuntime(24951): 	at org.tensorflow.Session.run(Native Method)
E/AndroidRuntime(24951): 	at org.tensorflow.Session.access$100(Session.java:48)

In both the cases, the Bazel version is 0.4.5
Any help to solve this? 



"
10456,Default CUDA / cuDNN version not used in template string,"When leaving version for CUDA / cuDNN empty, to default to 8.0 / 6, they are not properly used in the template at marks **_blank_**

Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]:
Please specify the location where CUDA **_blank_** toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:
Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]:
Please specify the location where cuDNN **_blank_** library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:

Instead, there is just a two-space gap.
The script seems to miss a proper fallback for `TF_CUDA_VERSION` / `TF_CUDNN_VERSION` unset."
10454,SavedModelBundle.java's load() method is not working with RandomForest Saved Model ,"------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS 10.12
- **TensorFlow installed from (source or binary)**: Binary 
- **TensorFlow version (use command below)**: Tried with both 1.1.0 and 1.2.0-rc1
- **CUDA/cuDNN version**: Running on CPU

I have SavedModel using TensorForestEstimator (RandomForest) by using export_savedmodel() it was saved successfully and able to load from SavedModel-cli too. But when trying to load using 

```
val SAVED_MODEL_PATH =
      ""/Users/Documents/tensordata/1496652787"";
    val bundle = SavedModelBundle.load(SAVED_MODEL_PATH, ""serve"");
```
 It's giving following error -:

```
Exception in thread ""main"" org.tensorflow.TensorFlowException: Op type not registered 'TreePredictions'
	at org.tensorflow.SavedModelBundle.load(Native Method)
	at org.tensorflow.SavedModelBundle.load(SavedModelBundle.java:38)
	at ai.tenten.ml.ClassifyITCase$.main(ClassifyITCase.scala:59)
```
I am able to load Other saved models like HalfPlusTwo Regression and mnist dense Predict model successfully. Only in RandomForest It's giving error I thing some op is missing from build or Java API is not updated."
10451,how to restore a tensor defined in a dictionary?,"I wrote the following definitions in my program.
`first_layer = {'w_1': tf.Variable(tf.truncated_normal([600, 300]), name='w_1'), 'b_1':tf.Variable(tf.truncated_normal([300]), name='b_1')}
`
And I use the following two lines to save my trained model.
`   saver = tf.train.Saver()
    saver.save(sess, os.path.join(model, 'model'))`
How can I restore the tensor with the name w_1 ?
The following codes didn't work out.
`  saver = tf.train.import_meta_graph(os.path.join(c_dir, 'model.meta'))
    sess.run(tf.global_variables_initializer())
    saver.restore(sess, tf.train.latest_checkpoint(c_dir))
   graph = tf.get_default_graph()
    w_1 = graph.get_tensor_by_name(""w_1:0"")`"
10450,"//tensorflow/core:platform_profile_utils_cpu_utils_test is failing on ppc64le with ""Check failed: cpu_frequency > 0 (-1 vs. 0)""","### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
    Ubuntu 16.04 (ppc64le)
- **TensorFlow installed from (source or binary)**:
    Installed from source (v1.0.1)
- **TensorFlow version (use command below)**:
   ('v1.0.1-0-ge895d5c-dirty', '1.0.1')
- **Bazel version (if compiling from source)**:
    0.4.4-2017-05-26 (@80a07b5)
- **CUDA/cuDNN version**:
    CUDA = 8.0 and cuDNN = 5.1
- **GPU model and memory**:
    GPU 0: Tesla P100-SXM2-16GB
    GPU 1: Tesla P100-SXM2-16GB
- **Exact command to reproduce**:
   bazel test --config=opt --config=cuda  //tensorflow/core:platform_profile_utils_cpu_utils_test

### Describe the problem
This test is passing successfully on X86, however getting failure on ppc64le with following errors:

### Source code / logs
```
$ bazel test --config=opt --config=cuda  //tensorflow/core:platform_profile_utils_cpu_utils_test

exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
-----------------------------------------------------------------------------
Running main() from test_main.cc
[==========] Running 5 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 5 tests from CpuUtilsTest
[ RUN      ] CpuUtilsTest.SetUpTestCase
[       OK ] CpuUtilsTest.SetUpTestCase (0 ms)
[ RUN      ] CpuUtilsTest.TearDownTestCase
[       OK ] CpuUtilsTest.TearDownTestCase (0 ms)
[ RUN      ] CpuUtilsTest.CheckGetCurrentClockCycle
[       OK ] CpuUtilsTest.CheckGetCurrentClockCycle (0 ms)
[ RUN      ] CpuUtilsTest.CheckCycleCounterFrequency
F tensorflow/core/platform/profile_utils/cpu_utils_test.cc:56] Check failed: cpu_frequency > 0 (-1 vs. 0)
external/bazel_tools/tools/test/test-setup.sh: line 123:  7285 Aborted                 (core dumped) ""${TEST_PATH}"" ""$@""
```

Any comments/suggestions ?"
10449,[Feature]Adding automatic model average parallelism support in TF,"Model Average is a common paradigm for distributed DL training, also there are several papers regarding to it and its variants:

(https://arxiv.org/pdf/1410.7455v8.pdf)
(http://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/0005880.pdf)

Do we have any plan to bring this support into TF? Actually we have already implemented model average support and benchmarked it on several in-house models, the speed up is good, about 3X(4 cards) to 9X(16 cards) convergence speed-up. Also we are working on making the model average mechanism as automatic as possible, so modeling guys could easily leverage this nice feature.

If TF community is fine with this feature/enhancement, we will be happy to merge our code into community.

Thanks.

"
10447,Tensorflow Errors with uint8 addition,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
8.0
- **GPU model and memory**:
Nvidia Titan X Pascal 
- **Exact command to reproduce**:
```
import tensorflow as tf
a=tf.constant([1],dtype=tf.uint8)
b=tf.constant([2],dtype=tf.uint8)
c=a+b
session=tf.Session()
with session.as_default():
    session.run(c)
```
### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

It appears as thought uint8 addition is not supported.  This is odd.  When running the above code I get the error:

```
InvalidArgumentError: No OpKernel was registered to support Op 'Add' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:
  device='CPU'; T in [DT_STRING]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_INT16]
  device='CPU'; T in [DT_INT8]
  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_COMPLEX64]
  device='GPU'; T in [DT_INT64]
  device='GPU'; T in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]

	 [[Node: add = Add[T=DT_UINT8](Const, Const_1)]]

```
"
10446,How to add a layer about activation function in tensorflow?,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
10443,How to do element wise operation for two Tensors?,"Say, I have two Tensors A and B, both with shape [-1, m, d]. How could I get a Tensor C with shape [-1, m*(m-1)/2, d] so than C_ij = A_i + B_j? Where + is an element wise addition. I know there is an element-wise operation named tf.add, but how should I use it? Thanks a lot."
10439,Different timelines on QueueDequeManyV2 in cpu and gpu installations,"OS Platform and Distribution: Ubuntu 14.04
TensorFlow version: 
    cpu: ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')
    gpu:  ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')

CUDA/cuDNN: 8.0/5.0

env: https://pastebin.com/f70fQ9Mw

I'm trying to fill queue of input data in additional thread in python, but obtain very strange timings for this operation, different for cpu and gpu version in 20 times. as you can see in full script, i forced device to cpu 

full script i'm using to fill queue: https://pastebin.com/Ny1QVZh2

key part:
```
    enqueue_op = queue.enqueue_many([queue_input_data, queue_input_target])
    dequeue_op = queue.dequeue()
 
    data_batch, target_batch = tf.train.batch(dequeue_op, batch_size=batch_size, capacity=10 * batch_size)
 
    def enqueue(sess):
        while True:
            sess.run(enqueue_op, feed_dict={queue_input_data: data, queue_input_target: target})

...

    run_options = tf.RunOptions(timeout_in_ms=4000, trace_level=tf.RunOptions.FULL_TRACE)
    run_metadata = tf.RunMetadata()
    sess.run(
        target_batch,
        options=run_options,
        run_metadata=run_metadata)
 
    tl = timeline.Timeline(run_metadata.step_stats)
    ctf = tl.generate_chrome_trace_format()
    with open('timeline.json', 'w') as f:
        f.write(ctf)
```
Steps to reproduce:
```
# cpu only version
pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.1.0-cp27-none-linux_x86_64.whl
python read_feed_dict.py
mv timeline.json timeline_cpu_version.json

pip uninstall tensorflow

# gpu version
pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.1.0-cp27-none-linux_x86_64.whl
python read_feed_dict.py
mv timeline.json timeline_gpu_version.json
```
cpu timeline:
![cpu](https://cloud.githubusercontent.com/assets/1593310/26796368/f636b37e-4a31-11e7-8851-863e74643697.png)

gpu timeline:
![gpu](https://cloud.githubusercontent.com/assets/1593310/26796369/f640d688-4a31-11e7-989d-197a7f2499e7.png)




"
10438,Android TF Classify: Reduce input size ,"I'm trying to reduce input size in ClassifierActivity, but I get ` java.lang.IllegalArgumentException: Input to reshape is a tensor with 128 values, but the requested shape requires a multiple of 2048`

```
  private static final int INPUT_SIZE = 96; // original 224
  private static final int IMAGE_MEAN = 117;
  private static final float IMAGE_STD = 1;
  private static final String INPUT_NAME = ""input"";
  private static final String OUTPUT_NAME = ""output"";
```

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/ClassifierActivity.java#L61

What should I do to reduce it? (my cropped image is also 96x96)

Also do I have to change `IMAGE_MEAN` ? Not sure what it means"
10437,Enabling TF to build LLVM AMDGPU backend for XLA,"Hi,
When I enable XLA during configure stage and run `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`, it starts pulling [llvm](https://github.com/tensorflow/tensorflow/blob/master/third_party/llvm/llvm.BUILD) and build for ARM, PPC, X86 and NVPTX backends. Is there a way to enable AMDGPU backend? (Matter of fact, all backends supported by LLVM?)
"
10436,"Failed to build on Mac OSX: ""Could not find python binary""","I am trying to compile tensorflow from source but cannot build `master`. The build for the pip package fails with the following error message

```
ERROR: /Users/till/git/tensorflow/tensorflow/tensorboard/components/vz_sorting/BUILD:8:1: Compiling 2 TypeScript files failed: execrooter failed: error executing command 
  (cd /private/var/tmp/_bazel_till/3e885964b28c274cf7e8652af4ed1911/execroot/tensorflow && \
  exec env - \
  bazel-out/host/bin/tensorflow/tensorboard/scripts/execrooter bazel-out/local-py3-opt/bin/tensorflow/tensorboard/components/vz_sorting/vz_sorting-tsc-execroot.json): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
Traceback (most recent call last):
  File ""bazel-out/host/bin/tensorflow/tensorboard/scripts/execrooter"", line 168, in <module>
    Main()
  File ""bazel-out/host/bin/tensorflow/tensorboard/scripts/execrooter"", line 148, in Main
    raise AssertionError('Could not find python binary: ' + PYTHON_BINARY)
AssertionError: Could not find python binary: python3
```

I am running Mac OSX 10.12.4, bazel 0.4.5, python 3.6.1 in a conda virtual environment, Apple LLVM version 8.1.0 (clang-802.0.42). This might be related to https://github.com/bazelbuild/bazel/issues/2752.

Any suggestions would be greatly appreciated."
10435,build optimize_for_inference fail,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
    No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
    Windows 7 x64
- **TensorFlow installed from (source or binary)**:
    binary (pip3 install....)
- **TensorFlow version (use command below)**:
    tensorflow 1.1.0 CPU only
- **Bazel version (if compiling from source)**:
    Build label: 0.5.0
    Build target: bazel-out/msys_x64-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
    Build time: Fri May 26 12:12:09 2017 (1495800729)
    Build timestamp: 1495800729
    Build timestamp as int: 1495800729
  
    $ protoc --version
    libprotoc 3.2.0
- **CUDA/cuDNN version**:
    CPU Only

### Describe the problem
I want to build optimize_for_inference and optimize and would like to optimize my graph by the optimizer.But when I run ` bazel build tensorflow/python/tools:optimize_for_inference `.It will get me a problem--` Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//'`.The problem occurs when i try to build anything with tensorflow as dependancy,like:`convert_graphdef_memmapped_format`.  
I've seen all the issues from github or stack overflow.And I tried all the fixes from issues or stackoverflow.But the problem still exists.
### Source code / logs
Source code:
``` 
bazel build tensorflow/python/tools:optimize_for_inference && bazel-bin/tensorflow/python/tools/optimize_for_inference --input=frozen_inception_graph.pb --output=optimized_inception_graph.pb --frozen_graph=True --input_names=Mul --output_names=softmax
```
logs:
``` 
D:\tensorflow-r1.2>bazel build tensorflow/python/tools:optimize_for_inference
[32mINFO: [0mLoading complete.  Analyzing...
[1A[K[32mINFO: [0mDownloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 257,202 bytes
[1A[K[32mINFO: [0mDownloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 501,196 bytes
[1A[K[32mINFO: [0mDownloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 951,646 bytes
[1A[K[32mINFO: [0mDownloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 1,433,556 bytes
[1A[K[32mINFO: [0mDownloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 2,197,176 bytes
[1A[K[32mINFO: [0mDownloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 2,997,976 bytes
[1A[K[31m[1mERROR: [0mD:/tensorflow-r1.2/tensorflow/python/tools/BUILD:133:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such packa
ge '@protobuf//': Traceback (most recent call last):
        File ""D:/tensorflow-r1.2/tensorflow/workspace.bzl"", line 117
                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)
        File ""D:/tensorflow-r1.2/tensorflow/workspace.bzl"", line 108, in _apply_patch
                _execute_and_check_ret_code(repo_ctx, cmd)
        File ""D:/tensorflow-r1.2/tensorflow/workspace.bzl"", line 92, in _execute_and_check_ret_code
                fail(""Non-zero return code({1}) when ..., <2 more arguments>))
Non-zero return code(3) when executing 'c:/tools/msys64/usr/bin/bash.exe -c patch -p1 -d C:/users/liba/appdata/local/temp/_bazel_liba/mffpt2ks/external/protobuf -i D:/tensorflow-r1.2/third_party/proto
buf/add_noinlines.patch':
Stdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc

Stderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354

This application has requested the Runtime to terminate it in an unusual way.
Please contact the application's support team for more information.
 and referenced by '//tensorflow/python/tools:optimize_for_inference'.
[31m[1mERROR: [0mAnalysis of target '//tensorflow/python/tools:optimize_for_inference' failed; build aborted.
[32mINFO: [0mElapsed time: 12.754s
```
"
10434,Building TensorFlow 1.2 from source results in catastrophic error?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 LTS
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.2
- **Bazel version (if compiling from source)**: 4.5.1
- **CUDA/cuDNN version**: 8.0/5.1
- **GPU model and memory**: GTX 860M
- **Exact command to reproduce**: `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures`

### Describe the problem
I have successfully installed TensorFlow 1.2 by upgrading the 1.1 version via Pip, but I wanted to install 1.2 from source to get the full benefits. However, strangely I received this error when building the pip package from bazel.

INFO: From Compiling tensorflow/core/kernels/batch_norm_op_gpu.cu.cc:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h(271): internal error: assertion failed at: ""/dvs/p4/build/sw/rel/gpu_drv/r361/r361_00/drivers/compiler/edg/EDG_4.10/src/folding.c"", line 9819


```
1 catastrophic error detected in the compilation of ""/tmp/tmpxft_00007cc2_00000000-7_batch_norm_op_gpu.cu.cpp1.ii"".
Compilation aborted.
Aborted (core dumped)
ERROR: /home/kwotsin/tensorflow_official/tensorflow/tensorflow/core/kernels/BUILD:2730:1: output 'tensorflow/core/kernels/_objs/batch_norm_op_gpu/tensorflow/core/kernels/batch_norm_op_gpu.cu.pic.o' was not created.
ERROR: /home/kwotsin/tensorflow_official/tensorflow/tensorflow/core/kernels/BUILD:2730:1: not all outputs were created or valid.
Target //tensorflow/tools/pip_package:build_pip_package failed to build

```

I've not seen this error before - is it because I did not git check out the 1.2 version but instead the master branch?
"
10433,BeamsearchDecoder w/ AttentionWrapper ,"Ubuntu 16.04
TF version; 1.2.0.rc1

Hi Team, I'm implementing seq2seq framework using BeamSearchDecoder and got an error when using with AttentionWrapper that has no predefined time-steps.

Here's my rough code

--------------------------------------------------------------------------------------------------------------------------
....
self.encoder_cell = build_encoder_cell()
self.encoder_inputs = tf.placeholder(dtype=tf.int32, shape=(None, None), name='encoder_inputs')
self.encoder_inputs_length = tf.placeholder(dtype=tf.int32, shape=(None,), name='encoder_inputs_length')
....
self.encoder_outputs, self.encoder_last_state = tf.nn.dynamic_rnn(
                cell=self.encoder_cell, inputs=self.encoder_inputs_embedded,
                sequence_length=self.encoder_inputs_length, dtype=self.dtype,
                time_major=False)
....
self.decoder_cell = build_decoder_cell()
self.decoder_cell = seq2seq.AttentionWrapper(
            cell=self.decoder_cell,
            attention_mechanism=self.attention_mechanism,
            attention_layer_size=self.hidden_units,
            cell_input_fn=attn_decoder_input_fn,
            initial_cell_state=encoder_last_state,
            alignment_history=False,
            name='Attention_Wrapper')

tiled_batch_size = self.batch_size * self.beam_width
self.decoder_initial_state = self.decoder_cell.zero_state(batch_size=tiled_batch_size)
....
inference_decoder = beam_search_decoder.BeamSearchDecoder(cell=self.decoder_cell,
                                                               embedding=embed_and_input_proj,
                                                               start_tokens=start_tokens,
                                                               end_token=end_token,
                                                               initial_state=self.decoder_initial_state,
                                                               beam_width=self.beam_width,
                                                               output_layer=output_layer,)

--------------------------------------------------------------------------------------------------------------------------

running the code made an error as below,

--------------------------------------------------------------------------------------------------------------------------

/home/pjh/ml_tutorials/tensorflow/tf-nmt/beam_search_decoder.pyc in __init__(self, cell, embedding, start_tokens, end_token, initial_state, beam_width, output_layer, length_penalty_weight)
    173     self._initial_cell_state = nest.map_structure(
    174         self._maybe_split_batch_beams,
--> 175         initial_state, self._cell.state_size)
    176     self._start_tokens = array_ops.tile(
    177         array_ops.expand_dims(self._start_tokens, 1), [1, self._beam_width])

/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/nest.pyc in map_structure(func, *structure, **check_types_dict)
    323 
    324   return pack_sequence_as(
--> 325       structure[0], [func(*x) for x in entries])
    326 
    327 

/home/pjh/ml_tutorials/tensorflow/tf-nmt/beam_search_decoder.pyc in _maybe_split_batch_beams(self, t, s)
    363       print t
    364       print s
--> 365       return self._split_batch_beams(t, s)
    366     else:
    367       return t

/home/pjh/ml_tutorials/tensorflow/tf-nmt/beam_search_decoder.pyc in _split_batch_beams(self, t, s)
    314     print s
    315     if isinstance(s, ops.Tensor):
--> 316       s = tensor_util.constant_value_as_shape(s)
    317     else:
    318       s = tensor_shape.TensorShape(s)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.pyc in constant_value_as_shape(tensor)
    732     A `TensorShape` based on the constant value of the given `tensor`.
    733   """"""
--> 734   shape = tensor.get_shape().with_rank(1)
    735   if tensor.get_shape() == [0]:
    736     return tensor_shape.scalar()

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc in with_rank(self, rank)
    630       return self.merge_with(unknown_shape(ndims=rank))
    631     except ValueError:
--> 632       raise ValueError(""Shape %s must have rank %d"" % (self, rank))
    633 
    634   def with_rank_at_least(self, rank):

ValueError: Shape () must have rank 1

--------------------------------------------------------------------------------------------------------------------------

I added a log to track the tensor argument (t) and the shape argument (s) of the _split_batch_beams function
In the log, 1024 is the size of hidden units. batch_size is 80 and beam_width is 12. For each cell, I used tf.contrib.rnn.LSTMCell()

--------------------------------------------------------------------------------------------------------------------------

Tensor(""decoder/AttentionWrapperZeroState/checked_cell_state:0"", shape=(?, 1024), dtype=float32) (t)
1024 (s) 
1024 (s)
Tensor(""decoder/AttentionWrapperZeroState/checked_cell_state_1:0"", shape=(?, 1024), dtype=float32)
1024
1024
Tensor(""decoder/AttentionWrapperZeroState/zeros_1:0"", shape=(960, 1024), dtype=float32)
1024
1024
Tensor(""decoder/AttentionWrapperZeroState/zeros_2:0"", shape=(960, ?), dtype=float32)
Tensor(""decoder/LuongAttention/strided_slice_2:0"", shape=(), dtype=int32)
Tensor(""decoder/LuongAttention/strided_slice_2:0"", shape=(), dtype=int32)

--------------------------------------------------------------------------------------------------------------------------

It turned out that the reason of the error was 'None' argument in the second dimension of self.encoder_inputs placeholder (which denotes max-time-steps).
I implemented self.encoder_inputs placeholder to have (None, None) shape to support dynamic batch_size and time-steps of input feeds.

When i hardcoded max-time-steps to be 80, the code worked well.
(self.encoder_inputs = tf.placeholder(tf.int32, shape=(None, """"80"""")), 

The input argument of _split_batch_beams was like this

Tensor(""decoder/AttentionWrapperZeroState/checked_cell_state:0"", shape=(?, 1024), dtype=float32)
1024
1024
Tensor(""decoder/AttentionWrapperZeroState/checked_cell_state_1:0"", shape=(?, 1024), dtype=float32)
1024
1024
Tensor(""decoder/AttentionWrapperZeroState/zeros_1:0"", shape=(960, 1024), dtype=float32)
1024
1024
Tensor(""decoder/AttentionWrapperZeroState/zeros_2:0"", shape=(960, """"80""""), dtype=float32)
""""80""""
""""80""""

--------------------------------------------------------------------------------------------------------------------------
Also the error did not occur if i used normal decoder cell (not with AttentionWrapper)
I'm wondering if this is a bug or a strict design rule for BeamSearchDecoder to support only AttentionWrapper with static_time_steps "
10432,No `data_format` option for slim.separable_convolution2d ?,"When I use slim.separable_convolution2d, according to the [CODE](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L1868)
`data_format` is not an argument.
Inside the separable_convolution2d function, if num_output is not None, `data_format` is always 'channel_last' when calling convolutional_layers.SeparableConvolution2D (Alias SeparableConvolution2D = SeparableConv2D)
But according to the SeparableConv2D [CODE](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L765), I think `data_format` is supported."
10429,Separate tensorboard from tensorflow (core),"I suggest to separate tensorboard from tensorflow.

This could mean:
* break tensorboard to its own repos (under the same organization)
* publish a separate tensorboard wheel / package on pypi. 

The advantages of this are:
* simpler building of tensorboard which will make it easier for non core tensorflow members to contribute to the development of the project.
* tensorboard to operate on its own release cycle
* simpler installation of tensorboard (it can be installed on its own without needing full install of tensorflow)"
10428,TensorBoard graph key does not match documentation,"
The key in the TensorBoard UI indicates a ""Reference edge"" as a single-headed arrow:

[![enter image description here][1]][1]

while the documentation shows these as double-headed arrows:

[![enter image description here][2]][2]

Moreover, it appears that the edges indicated as references edges in the UI (according to the key there) are not in fact such edges. For example neither 

    cs = tf.constant([1,2,3], name='const_share')
    vs = tf.Variable([1,2,3], name='var_share')
    tf.add(cs, vs, name='opVS1')
    tf.add(vs, cs, name='opVS2')

<img src=""https://i.stack.imgur.com/8z1Bn.png"" height=""350"">

note

    tf.add([4],[3], name='opA')

[![enter image description here][3]][3]

should include reference edges (should they?). But in both cases the key in the UI says that they do.

  [1]: https://i.stack.imgur.com/5MhdF.png
  [2]: https://i.stack.imgur.com/fWRZL.png
  [3]: https://i.stack.imgur.com/eZHmm.png"
10425,Feature request -forwardsolve and backsolve,"I would like to use OLS to estimate parameters in some parts of my models. We're most of the way there today: QR decomposition is already implemented; we just need a backsolve command. And while we're at it, we might as well add a forwardsolve command. For example, one solution would just be calls to SGETRS	CGETRS	DGETRS	ZGETRS in LAPACK and their transpose equivalents."
10424,Unable to train restore or save when running lamp server,"Running apache mysql python server on port 80, I am unable to save my train data or restore. "
10422,Ability to add to an existing named scope,"Currently it is impossible to use `tf.name_scope` to add additional elements to an existing scope. Using the same name a second time creates a new scope (with an appended index). This means that any elements that need to be in the same scope must be defined close together in source, which is not always consistent with the logic of the code.

A desirable feature would be the ability to add to an existing named scope, either with a new named parameter for `tf.name_scope` (e.g. Boolean `add_to_existing`), or with a new related function (e.g., `tf.existing_name_scope`)."
10421,Specify which scopes are removed from TensorBoard main graph by default,"The heuristics used by TensorBoard to determine which scopes are added to the ""main graph"" and which are not by default often results in scopes being included that obscure the underlying network structure.

It would be nice to have an additional named parameter for `tf.name_scope` that indicated whether that scope should be removed (or included) in the main graph by default."
10420,"I think your linux python3.5 1.2rc whl is actually installing TF 1.1.0, the Python 3.6 one works","Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0rc1-cp35-cp35m-linux_x86_64.whl
- **TensorFlow version (use command below)**:
was trying to install 1.2rc but got 1.1.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
8.0 
- **GPU model and memory**:
Titan X
- **Exact command to reproduce**:

THIS CREATES THE PROBLEM:
```conda create --name tfpy35 python=3.5
source activate tfpy35
pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0rc1-cp35-cp35m-linux_x86_64.whl
python
>>> import tensorflow as tf
>>> print(tf.__version__)
1.1.0
IT SHOULD BE 1.2.0-RC1
```

THIS WORKS:
```
conda create --name tensorflow python=3
source activate tensorflow
pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0rc1-cp36-cp36m-linux_x86_64.whl
python
>>> import tensorflow as tf; print(tf.__version__)
1.2.0-rc1
```


### Describe the problem
I am using anaconda and created a py3.5 env
I installed TF GPU as per https://www.tensorflow.org/versions/r1.2/install/install_linux#the_url_of_the_tensorflow_python_package
for python 3.5
But when I run python then import tensorflow as tf; print(tf.__version__)
I get 1.1.0 

If I do exactly the same thing but create a python 3 enviornment and then load the 3.6 gpu whl it works




### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.


```
paul@SPEED:~$ conda create --name tfpy35 python=3.5
Fetching package metadata ...........
Solving package specifications: .

Package plan for installation in environment /home/paul/anaconda3/envs/tfpy35:

The following NEW packages will be INSTALLED:

    openssl:    1.0.2l-0     
    pip:        9.0.1-py35_1 
    python:     3.5.3-1      
    readline:   6.2-2        
    setuptools: 27.2.0-py35_0
    sqlite:     3.13.0-0     
    tk:         8.5.18-0     
    wheel:      0.29.0-py35_0
    xz:         5.2.2-1      
    zlib:       1.2.8-3      

Proceed ([y]/n)? y

python-3.5.3-1 100% |################################| Time: 0:00:03   4.96 MB/s
setuptools-27. 100% |################################| Time: 0:00:00   5.59 MB/s
wheel-0.29.0-p 100% |################################| Time: 0:00:00   7.59 MB/s
#
# To activate this environment, use:
# > source activate tfpy35
#
# To deactivate this environment, use:
# > source deactivate tfpy35

paul@SPEED:~$ source activate tfpy35

(tfpy35) paul@SPEED:~$ pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0rc1-cp35-cp35m-linux_x86_64.whl
Collecting tensorflow-gpu==1.2.0rc1 from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0rc1-cp35-cp35m-linux_x86_64.whl
  Using cached https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0rc1-cp35-cp35m-linux_x86_64.whl
Collecting markdown==2.2.0 (from tensorflow-gpu==1.2.0rc1)
Collecting wheel>=0.26 (from tensorflow-gpu==1.2.0rc1)
  Using cached wheel-0.29.0-py2.py3-none-any.whl
Collecting numpy>=1.11.0 (from tensorflow-gpu==1.2.0rc1)
  Using cached numpy-1.12.1-cp35-cp35m-manylinux1_x86_64.whl
Collecting protobuf>=3.2.0 (from tensorflow-gpu==1.2.0rc1)
  Using cached protobuf-3.3.0-cp35-cp35m-manylinux1_x86_64.whl
Collecting werkzeug>=0.11.10 (from tensorflow-gpu==1.2.0rc1)
  Using cached Werkzeug-0.12.2-py2.py3-none-any.whl
Collecting bleach==1.5.0 (from tensorflow-gpu==1.2.0rc1)
  Using cached bleach-1.5.0-py2.py3-none-any.whl
Collecting html5lib==0.9999999 (from tensorflow-gpu==1.2.0rc1)
Collecting six>=1.10.0 (from tensorflow-gpu==1.2.0rc1)
  Using cached six-1.10.0-py2.py3-none-any.whl
Collecting setuptools (from protobuf>=3.2.0->tensorflow-gpu==1.2.0rc1)
  Using cached setuptools-36.0.1-py2.py3-none-any.whl
Installing collected packages: markdown, wheel, numpy, six, setuptools, protobuf, werkzeug, html5lib, bleach, tensorflow-gpu
Successfully installed bleach-1.5.0 html5lib-0.9999999 markdown-2.2.0 numpy-1.12.1 protobuf-3.3.0 setuptools-36.0.1 six-1.10.0 tensorflow-gpu-1.2.0rc1 werkzeug-0.12.2 wheel-0.29.0

(tfpy35) paul@SPEED:~$ python
Python 3.5.3 |Continuum Analytics, Inc.| (default, Mar  6 2017, 11:58:13) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> print(tf.__version__)
1.1.0
```




"
10419,[Feature Request] A guide for creating custom DeepDream models,"The current DeepDream guide located [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/deepdream), uses the inception5h model. From what I can tell, it does not appear very straightforward in terms of how to fine tune the model in order to create different DeepDream hallucinations from a custom data set. It also does not appear to be relatively easy to change the model that the guide uses. 

I think that an additional guide which shows individuals how to fine tune a model for the purposes of DeepDream would be useful for those trying to explore the artistic and visual aspects of TensorFlow models. I haven't been able to find any guide for creating custom DeepDream models in Tensorflow, so I am not sure where to start.


"
10416,Printing TensorFlow Library Logs in Android Studio,"I have built Tensor Flow using Bazel  and am using the generated TensorFlow Libraries in Android Studio
for ndk.

But I do not find the logs of Tensor Flow being printed in Android Studio.
Can I know how do I acheive this .

I have tried using Android/log.h in TensorFlow code but of no use
"
10415,ValueError: Attempt to have a second RNNCell use the weights of a variable scope that already has weights: ,"'generator/rnn_cell/rnn/basic_rnn_cell'; and the cell was not constructed as BasicRNNCell(..., reuse=True).  To share the weights of an RNNCell, simply reuse it in your second calculation, or create a new one with the argument reuse=True."
10413,Random crop a patch from a various-sized image read in TFRecord,"I have posted my question on stackoverflow but no ones answered me. [here](https://stackoverflow.com/questions/44152661/random-crop-a-patch-from-a-various-sized-image-read-in-tfrecord)
I found another similar question posted more than one year ago on stackoverflow as well, [How to read images with different size in a TFRecord file.
](https://stackoverflow.com/questions/35028173/how-to-read-images-with-different-size-in-a-tfrecord-file)
I tried the method someone suggested in the page, image = tf.reshape(image_data, tf.pack([image_rows, image_cols, 3])), but it still fails.

And I read the comments given by  Yaroslav Bulatov, performing randomly crop and resize them to a fixed size before converting them to TFRecord. It should be available, but it may increase the size of TFRecord file for saving a lot of cropped images. 
But what I'm curious about is that if the image size can't be utilized during training, why the [example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py) provided by the tutorial document of Tensorflow would write the width, height and depth of the image in TFRecord? That really confuses me. So is it possible to read images with different size in a TFRecord file? If it's possible, how to do it? Thanks"
10410,Could not create Tensorflow Graph: Invalid argument: No OpKernel was registered to support Op 'Const' with these attrs,"I am try to create to Graph by running below command
tensorflow::Status s = session->Create(tensorflow_graph);
But I get below error 

Could not create Tensorflow Graph: Invalid argument: No OpKernel was registered to support Op 'Const' with these attrs.  Registered devices: [CPU], Registered kernels: \<no registered kernels\>

"
10408,Memory leak ,"I have a memory leak with TensorFlow. I refered to https://stackoverflow.com/questions/35695183/tensorflow-memory-leak-even-while-closing-session to address my issue, and I followed the advices of the answer, that seemed to have solved the problem. However it does not work here. 

In order to recreate the memory leak, I have created a simple example. First, I use this function (that I got here : https://stackoverflow.com/questions/276052/how-to-get-current-cpu-and-ram-usage-in-python) to check the memory use of the python process : 

    def memory():
        import os
        import psutil
        pid = os.getpid()
        py = psutil.Process(pid)
        memoryUse = py.memory_info()[0]/2.**30  # memory use in GB...I think
        print('memory use:', memoryUse)

Then, everytime I call the `build_model` function, the use of memory increases.

Here is the `build_model` function that has a memory leak : 
 
    def build_model():

        '''Model'''

        tf.reset_default_graph()


        with tf.Graph().as_default(), tf.Session() as sess:
            tf.contrib.keras.backend.set_session(sess)

            labels = tf.placeholder(tf.float32, shape=(None, 1))
            input = tf.placeholder(tf.float32, shape=(None, 1))

            x = tf.contrib.keras.layers.Dense(30, activation='relu', name='dense1')(input)
            x1 = tf.contrib.keras.layers.Dropout(0.5)(x)
            x2 = tf.contrib.keras.layers.Dense(30, activation='relu', name='dense2')(x1)
            y = tf.contrib.keras.layers.Dense(1, activation='sigmoid', name='dense3')(x2)


            loss = tf.reduce_mean(tf.contrib.keras.losses.binary_crossentropy(labels, y))

            train_step = tf.train.AdamOptimizer(0.004).minimize(loss)

            #Initialize all variables
            init_op = tf.global_variables_initializer()
            sess.run(init_op)

            sess.close()

        tf.reset_default_graph()

        return 

 I would have thought that using the block ` with tf.Graph().as_default(), tf.Session() as sess: ` and then **closing the session** and **calling `tf.reset_default_graph`** would clear all the memory used by TensorFlow. Apparently it does not.

The memory leak can be recreated as following : 

    memory()
    build_model()
    memory()
    build_model()
    memory()

The output of this is (for my computer) :

    memory use: 0.1794891357421875
    memory use: 0.184417724609375
    memory use: 0.18923568725585938

Clearly we can see that all the memory used by TensorFlow is not freed afterwards. Why?

I hope I made myself clear."
10406,Build Error gcc: error: ”-DEIGEN_USE_VML”: No such file or directory,"$ bazel build --config=mkl --copt=”-DEIGEN_USE_VML” -c opt //tensorflow/tools/pip_package:build_pip_package
WARNING: /home/mstf/workspace/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': Use SavedModel Builder instead.
WARNING: /home/mstf/workspace/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': Use SavedModel instead.
INFO: Found 1 target...
ERROR: /home/mstf/.cache/bazel/_bazel_mstf/ac8273660b2bd94e42901d7fff45bcd5/external/grpc/BUILD.bazel:71:1: C++ compilation of rule '@grpc//:gpr' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 34 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
gcc: error: ”-DEIGEN_USE_VML”: No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 119.271s, Critical Path: 14.01s
"
10405,"Operation functions return operations, not Tensors","The way the return value for many TensorFlow operations is described in the documentation is a potential (and actual, from experience) source of confusion. 

For example the docs for `tf.acos` say (like many ops): 

> ""Returns: A Tensor. ..."" 

But that's not true, is it: Tensors are graph *edges* (pipes through with values flow), while `tf.acos` is a graph *node* (an operation that needs to be executed to put a value into the pipe). Right?

Shouldn't the documentation say that these functions return _operations_, which, when evaluated, produce a Tensor? (And if not why does the TensorFlow documentation say that these operations return Tensors?)"
10403,"contrib/learn/python/learn_io/data_feeder.py : ""y_is_dict"" instead of ""x_is_dict"" at line 325","version r1.2
I was trying to create a regressor with X as dict, and y as ndarray. Got error that at line mentioned in title saying ndarray object as no attribute items.
When I checked the code, it look like

```
    x_is_dict, y_is_dict = isinstance(x, dict), y is not None and isinstance(y, dict)
    if isinstance(y, list):
      y = np.array(y)

    self._x = dict([(k, check_array(v, v.dtype)) for k, v in list(x.items())
                   ]) if x_is_dict else check_array(x, x.dtype)
    self._y = None if y is None else \
      dict([(k, check_array(v, v.dtype)) for k, v in list(y.items())]) if x_is_dict else check_array(y, y.dtype)
```
This **`x_is_dict`** in the last line should be `y_is_dict`!! Or do I have to send both as dict??"
10399,export step of Experiment fails in python 3.5 (likely due to string-type mismatch),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yep
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX
- **TensorFlow installed from (source or binary)**: binary 1.1.0
- **TensorFlow version (use command below)**: 1.1.0
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**:
git clone https://github.com/amygdala/tensorflow-workshop
python tensorflow-workshop/workshop_sections/wide_n_deep/widendeep/model.py


### Describe the problem

When running Experiment stuff, it works fine in 2.7 but breaks in 3.5. From what I can tell, it's happening in gc.py, which is being called _after_ the model has been exported and everything is done. 

My best guess from looking at similar bugs on SO is that there's a mismatch somewhere between 'byte' strings and python strings, possibly because the base folder path of the export is generated (e.g. /exports/Servo/123456789/saved_model.py), so the somehow the concat isn't happy with this.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

File ""/Users/yufengg/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/utils/gc.py"", line 202, in get_paths
    p = parser(Path(os.path.join(base_dir, r), None))
  File ""/Users/yufengg/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/posixpath.py"", line 89, in join
    genericpath._check_arg_types('join', a, *p)
  File ""/Users/yufengg/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/genericpath.py"", line 145, in _check_arg_types
    raise TypeError(""Can't mix strings and bytes in path components"") from None
TypeError: Can't mix strings and bytes in path components
"
10398,"[Docs] ""arg_scope"" overrides ""defined in"" paths","I found that some ops have incorrect ""defined in"" paths in the [`contrib.layers` docs](https://www.tensorflow.org/api_docs/python/tf/contrib/layers).

Examples: `avg_pool2d, batch_norm, bias_add, conv2d, conv2d_in_plane, conv2d_transpose, dropout, flatten, fully_connected, layer_norm, one_hot_encoding, separable_conv2d, softmax`

It always uses [`tensorflow/contrib/framework/python/ops/arg_scope.py`](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/framework/python/ops/arg_scope.py) which is clearly outright wrong.

This seems to be the case for all ops defined in [`tensorflow/contrib/layers/python/layers/layers.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py) which are annotated with `@add_arg_scope`"
10397,Tensorboard Regex does not work - Docker Container (non-gpu),"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Tensorflow
- **TensorFlow installed from (source or binary)**:
from docker image: tensorflow/tensorflow
- **TensorFlow version (use command below)**:
from docker image: tensorflow/tensorflow
- **Bazel version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
Deployed to a kubernetes cluster; below is .yaml file.

apiVersion: v1
kind: Service
metadata:
  labels:
    app: tensorboard
  name: tensorboard
spec:
  ports:
  - port: 80
    targetPort: 6006
  selector:
    app: tensorboard
  type: LoadBalancer
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: tensorboard
  name: tensorboard
spec:
  template:
    metadata:
      labels:
        app: tensorboard    
      containers:
      - name: tensorboard
        command: [""/bin/sh"", ""-c""]
        args: [""tensorboard --logdir /tensorboard""]
        image: tensorflow/tensorflow
        ports:
        - containerPort: 6006

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
![tensorboard_regex_bug](https://cloud.githubusercontent.com/assets/6099287/26727906/65192646-4776-11e7-93f2-c2a78accbb83.png)

regex just simply is not working.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
10396,inference require the old training datasets exist,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 1604
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**: r1.1
- **Bazel version (if compiling from source)**: /
- **CUDA/cuDNN version**: 8.0/5.1
- **GPU model and memory**: GTX1080/8G
 
**Problem description**
The exact problem is Inference requires training dataset 
This may because some of the stuff in import_meta_graph(). After you create the training graph and op train_input/ReaderRead would save the 'file names' as const in the graph. And then use saver to save the model. The next time you retrain your model, the import_meta_graph() would requires training dataset existing, or you may found the error below.

For example, such a case:

when I build my graph, I use such code to generate input tensor:

  ```
logging.info(""Using batch size of "" + str(batch_size) + "" for training."")
  with tf.name_scope(""train_input""):
    files = gfile.Glob(data_pattern)
    if not files:
      raise IOError(""Unable to find training files. data_pattern='"" +
                    data_pattern + ""'."")
    logging.info(""Number of training files: %s."", str(len(files)))
    filename_queue = tf.train.string_input_producer(
        files, num_epochs=num_epochs, shuffle=True)
    training_data = [
        reader.prepare_reader(filename_queue) for _ in range(num_readers)
    ]

    return tf.train.shuffle_batch_join(
        training_data,
        batch_size=batch_size,
        capacity=batch_size * 5,
        min_after_dequeue=batch_size,
        allow_smaller_final_batch=True,
        enqueue_many=True)
```

After trained the model , I want to inference the model then I rebuild my input tensor and recover my model like this:

```
saver = tf.train.import_meta_graph(meta_graph_location, clear_devices=True)

   saver.restore(sess, latest_checkpoint)
```

**Everything is ok when the training datasets could be found**, However, after I train the model I remove the training data,  inference.py got error like this: 
```
Traceback (most recent call last):
  File ""inference.py"", line 197, in <module>
    app.run()
  File ""/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""inference.py"", line 193, in main
    FLAGS.output_file, FLAGS.batch_size, FLAGS.top_k)
  File ""inference.py"", line 166, in inference
    coord.join(threads)
  File ""/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 234, in _run
    sess.run(enqueue_op)
  File ""/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 778, in run
    run_metadata_ptr)
  File ""/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 982, in _run
    feed_dict_string, options, run_metadata)
  File ""/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1032, in _do_run
    target_list, options, run_metadata)
  File ""/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1052, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: /data_tmp/train1Z.tfrecord
	 [[Node: train_input/ReaderReadV2_5 = ReaderReadV2[_device=""/job:localhost/replica:0/task:0/cpu:0""](train_input/TFRecordReaderV2_5, train_input/input_producer)]]
	 [[Node: train_input/DecodeRaw_11/_143 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_18_train_input/DecodeRaw_11"", tensor_type=DT_UINT8, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

Caused by op u'train_input/ReaderReadV2_1', defined at:
  File ""inference.py"", line 197, in <module>
    app.run()
  File ""/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""inference.py"", line 193, in main
    FLAGS.output_file, FLAGS.batch_size, FLAGS.top_k)
  File ""inference.py"", line 122, in inference
    saver = tf.train.import_meta_graph(meta_graph_location, clear_devices=True)
  File ""/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1595, in import_meta_graph
    **kwargs)
  File ""/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py"", line 499, in import_scoped_meta_graph
    producer_op_list=producer_op_list)
  File ""/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 308, in import_graph_def
    op_def=op_def)
  File ""/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2336, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1228, in __init__
    self._traceback = _extract_stack()

NotFoundError (see above for traceback): 
/data_tmp/traintY.tfrecord
	 [[Node: train_input/ReaderReadV2_1 = ReaderReadV2[_device=""/job:localhost/replica:0/task:0/cpu:0""](train_input/TFRecordReaderV2_1, train_input/input_producer)]]
	 [[Node: train_input/DecodeRaw_2/_127 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_47_train_input/DecodeRaw_2"", tensor_type=DT_UINT8, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
```

 

I wonder why train.import_meta_graph() does such check? Because after restore the weights I will run the graph by feeding a new input tensor, for example:
```
video_id_batch_val, video_batch_val,num_frames_batch_val = sess.run([video_id_batch, video_batch, num_frames_batch])
          predictions_val, = sess.run([predictions_tensor], feed_dict={input_tensor: video_batch_val, num_frames_tensor: num_frames_batch_val})
```
when it is inferencing, nothing concerns about the 'old input producer' in training section, however , when the 'old files for training' changed in the environment, the error arise and I have to rebuild the whole graph again instead of using import_meta_graph().  It's really confused.

 
"
10394,How to disable checkpoints?,"This is a classifier:
classifier = SKCompat(tf.contrib.learn.DNNClassifier(feature_columns=feature_columns, 
                                            hidden_units=[7, 4, 2], 
                                            n_classes=2,
                                            optimizer=tf.train.GradientDescentOptimizer(0.08)
                                                    )
                     )

and when i try to fit model, i get this:
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Saving checkpoints for 1 into C:\Users\home\AppData\Local\Temp\tmprit6vryq\model.ckpt. 
INFO:tensorflow:loss = 0.71007, step = 1
INFO:tensorflow:Loss for final step: 0.71007.
WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.
INFO:tensorflow:Restoring parameters from C:\Users\home\AppData\Local\Temp\tmprit6vryq\model.ckpt-1

How to disable saving? It takes 3-4 seconds. It's too long for me. Thanks for helping!

I want to fit my model ""online"" (1 example, 1 step). "
10393,I want read train or test data in next_batch by tf.cond,"```
import tensorflow as tf
import matplotlib.pyplot as plt
import os
batch_size = 4
im_w = 32
im_h = 32
im_d = 3
label_bytes = 1
im_bytes = im_w*im_h*im_d

def next_path(data_dir, is_train, batch_size, shuffle):
    # if is_train:
    #     filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in range(1, 6)]
    # else:
    #     filenames = [os.path.join(data_dir, 'test_batch.bin')]

    a = tf.cast([os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in range(1, 6)], tf.string)
    b = tf.cast([os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in range(1, 6)], tf.string)
    filenames = tf.cond(is_train, lambda: a, lambda: b)

    # filenames = tf.cast([os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in range(1, 6)], tf.string)
    filenames_queue = tf.train.string_input_producer(filenames)
    reader = tf.FixedLengthRecordReader(label_bytes+im_bytes)
    key, value = reader.read(filenames_queue)
    record_bytes = tf.decode_raw(value, tf.uint8)
    label = tf.slice(record_bytes, [0], [label_bytes])
    label = tf.cast(label, tf.int32)

    im_raw = tf.slice(record_bytes, [label_bytes], [im_bytes])
    im_raw = tf.reshape(im_raw, [im_d, im_h, im_w])
    im = tf.transpose(im_raw, [1, 2, 0])
    im = tf.cast(im, tf.float32)

    # im = tf.image.per_image_standardization(im)
    if shuffle:
        images, labels = tf.train.shuffle_batch([im, label],
                                                batch_size=batch_size,
                                                capacity=1000,
                                                num_threads=16,
                                                min_after_dequeue=100)
    else:
        images, labels = tf.train.batch([im, label], batch_size=batch_size, capacity=1000, num_threads=16)

    labels = tf.reshape(labels, [batch_size])

    return images, labels

is_train = tf.placeholder(tf.bool)
images, labels = next_path('cifar-10-batches-bin', is_train, batch_size, True)


with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess=sess, coord=coord)
    for step in range(1):
        images_, _ = sess.run([images, labels], feed_dict={is_train:True})
        plt.imshow(images_[0, :,:,:])
        plt.show()

    coord.request_stop()
    coord.join(threads=threads)
```
```

a = tf.cast([os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in range(1, 6)], tf.string)
b = tf.cast([os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in range(1, 6)], tf.string)
filenames = tf.cond(is_train, lambda: a, lambda: b)
```
    
this is error ,but below is ok, why and how to do
   ` filenames = tf.cast([os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in range(1, 6)], tf.string)
`"
10392,transfer problem,"How to transfer model.ckpt-152000.meta into .pb file, we will apply to graph_transfer?
Thank you!!"
10390,error ，a configure error! help!,"
➜  tensorflow git:(master) ✗ ./configure
Please specify the location of python. [Default is /usr/bin/python]: 
Found possible Python library paths:
  /opt/ros/kinetic/lib/python2.7/dist-packages
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/opt/ros/kinetic/lib/python2.7/dist-packages]

Using python library path: /opt/ros/kinetic/lib/python2.7/dist-packages
Do you wish to build TensorFlow with MKL support? [y/N] 
No MKL support will be enabled for TensorFlow
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 
Do you wish to use jemalloc as the malloc implementation? [Y/n] 
jemalloc enabled
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] 
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] 
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] 
No XLA support will be enabled for TensorFlow
Do you wish to build TensorFlow with VERBS support? [y/N] 
No VERBS support will be enabled for TensorFlow
Do you wish to build TensorFlow with OpenCL support? [y/N] 
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] u
Invalid selection:  u
Do you wish to build TensorFlow with CUDA support? [y/N] y
CUDA support will be enabled for TensorFlow
Do you want to use clang as CUDA compiler? [y/N] n
nvcc will be used as CUDA compiler
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Please specify the cuDNN version you want to use. [Leave empty to use system default]: 
Please specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 
Do you wish to build TensorFlow with MPI support? [y/N] n
MPI support will not be enabled for TensorFlow
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=80
INFO: Reading options for 'clean' from /home/lulin/tensorflow/tools/bazel.rc:
  Inherited 'build' options: --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --genrule_strategy=standalone -c opt
INFO: Reading options for 'clean' from /home/lulin/tensorflow/.tf_configure.bazelrc:
  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python --action_env PYTHON_LIB_PATH=/opt/ros/kinetic/lib/python2.7/dist-packages --define PYTHON_BIN_PATH=/usr/bin/python --define PYTHON_LIB_PATH=/opt/ros/kinetic/lib/python2.7/dist-packages --force_python=py2 --host_force_python=py2 --python2_path=/usr/bin/python --define with_jemalloc=true --action_env TF_NEED_CUDA=1 --action_env TF_NEED_OPENCL=0 --action_env TF_CUDA_CLANG=0 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda --action_env TF_CUDA_VERSION=8.0 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/gcc --action_env TF_CUDNN_VERSION= --action_env CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2
Unrecognized option: --action_env


bazel is 0.3.0;cuda8.0;cudnn5.0.

"
10389,slim.conv2d Error: Input has undefined `axis` dimension.,"Here is my code:
```
import tensorflow as tf
slim = tf.contrib.slim
data_format='NCHW'

input = tf.placeholder(dtype=tf.float32, shape=[2, 4, None, None])
with slim.arg_scope([slim.batch_norm], data_format=data_format):
    net = slim.conv2d(input, 4, kernel_size=3, stride=1, padding='SAME', rate=2,
                      data_format=data_format, normalizer_fn=slim.batch_norm)
```

Here is the logs:
```
Traceback (most recent call last):
  File ""/home/zzy/workspace/tf-models/mind_slim/examples/test.py"", line 9, in <module>
    net = slim.conv2d(input, 4, kernel_size=3, stride=1, padding='SAME', rate=2, data_format=data_format, normalizer_fn=slim.batch_norm)
  File ""/home/zzy/anaconda2/envs/tf-gpu-py27-source/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args
    return func(*args, **current_args)
  File ""/home/zzy/anaconda2/envs/tf-gpu-py27-source/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 927, in convolution
    outputs = normalizer_fn(outputs, **normalizer_params)
  File ""/home/zzy/anaconda2/envs/tf-gpu-py27-source/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args
    return func(*args, **current_args)
  File ""/home/zzy/anaconda2/envs/tf-gpu-py27-source/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 528, in batch_norm
    outputs = layer.apply(inputs, training=is_training)
  File ""/home/zzy/anaconda2/envs/tf-gpu-py27-source/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 320, in apply
    return self.__call__(inputs, **kwargs)
  File ""/home/zzy/anaconda2/envs/tf-gpu-py27-source/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 286, in __call__
    self.build(input_shapes[0])
  File ""/home/zzy/anaconda2/envs/tf-gpu-py27-source/lib/python2.7/site-packages/tensorflow/python/layers/normalization.py"", line 118, in build
    input_shape)
ValueError: ('Input has undefined `axis` dimension. Input shape: ', TensorShape([Dimension(2), Dimension(None), Dimension(None), Dimension(None)]))
```

When I change the rate to 1
Or I change the data_format to 'NHWC' and input to [2, None, None, 4]
It works without any error.
My tensorflow version: 1.1"
10383,error， I use tensorboard with pandas error,"
hello , during this days I was learning use tensorflow to build NN, when I use the tensorboard .In my code I use:
`self.writer = tf.summary.FileWriter('./board', self.graph)`\
to create the board file. and use qurery ,and It successfully have a file called events.out.tfevents.1496363479.dyy there. the Iuse query :tensorboard --logdir=board
then I got the error info.
`panda@dyy:~/code/number_test$ tensorboard --logdir=board
Traceback (most recent call last):
  File ""/home/panda/anaconda3/bin/tensorboard"", line 7, in <module>
    from tensorflow.tensorboard.tensorboard import main
  File ""/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/tensorboard/tensorboard.py"", line 33, in <module>
    from tensorflow.tensorboard.backend import application
  File ""/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/tensorboard/backend/application.py"", line 47, in <module>
    from tensorflow.tensorboard.plugins.projector import projector_plugin
  File ""/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/tensorboard/plugins/projector/projector_plugin.py"", line 28, in <module>
    from tensorflow.contrib.tensorboard.plugins.projector import PROJECTOR_FILENAME
  File ""/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/__init__.py"", line 30, in <module>
    from tensorflow.contrib import factorization
  File ""/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/factorization/__init__.py"", line 24, in <module>
    from tensorflow.contrib.factorization.python.ops.gmm import *
  File ""/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/factorization/python/ops/gmm.py"", line 27, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import estimator
  File ""/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/__init__.py"", line 87, in <module>
    from tensorflow.contrib.learn.python.learn import *
  File ""/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/__init__.py"", line 23, in <module>
    from tensorflow.contrib.learn.python.learn import *
  File ""/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/__init__.py"", line 25, in <module>
    from tensorflow.contrib.learn.python.learn import estimators
  File ""/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/__init__.py"", line 297, in <module>
    from tensorflow.contrib.learn.python.learn.estimators.dnn import DNNClassifier
  File ""/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py"", line 29, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import dnn_linear_combined
  File ""/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py"", line 31, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import estimator
  File ""/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 49, in <module>
    from tensorflow.contrib.learn.python.learn.learn_io import data_feeder
  File ""/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_io/__init__.py"", line 21, in <module>
    from tensorflow.contrib.learn.python.learn.learn_io.dask_io import extract_dask_data
  File ""/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_io/dask_io.py"", line 26, in <module>
    import dask.dataframe as dd
  File ""/home/panda/anaconda3/lib/python3.6/site-packages/dask/dataframe/__init__.py"", line 3, in <module>
    from .core import (DataFrame, Series, Index, _Frame, map_partitions,
  File ""/home/panda/anaconda3/lib/python3.6/site-packages/dask/dataframe/core.py"", line 38, in <module>
    pd.computation.expressions.set_use_numexpr(False)
AttributeError: module 'pandas' has no attribute 'computation'
`
my python is 3.5.2 (in anaconda 4.2.0 )and my tensorflow is 1.1, and the pandas is 0.20.1（at first it is 0.18.1,but the error was about pandas so I try to update it）,this is run in ubuntu 15 with x64。"
10382,"Cant start work with tensorflow, import error","### System information
Windows 7 64.
Installed CUDA, cuDNN 8.0.
Installed TensorFlow gpu from pip.
Pyrhon 3.5 by anaconda.

Try inpute import tensorflow and get this

ImportError                               Traceback (most recent call last)
C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     17         try:
---> 18             return importlib.import_module(mname)
     19         except ImportError:

C:\Anaconda3\lib\importlib\__init__.py in import_module(name, package)
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127 

C:\Anaconda3\lib\importlib\_bootstrap.py in _gcd_import(name, package, level)

C:\Anaconda3\lib\importlib\_bootstrap.py in _find_and_load(name, import_)

C:\Anaconda3\lib\importlib\_bootstrap.py in _find_and_load_unlocked(name, import_)

C:\Anaconda3\lib\importlib\_bootstrap.py in _load_unlocked(spec)

C:\Anaconda3\lib\importlib\_bootstrap.py in module_from_spec(spec)

C:\Anaconda3\lib\importlib\_bootstrap_external.py in create_module(self, spec)

C:\Anaconda3\lib\importlib\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)

ImportError: DLL load failed: Specified module not found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     40     sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_GLOBAL)
---> 41   from tensorflow.python.pywrap_tensorflow_internal import *
     42   from tensorflow.python.pywrap_tensorflow_internal import __version__

C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
     20             return importlib.import_module('_pywrap_tensorflow_internal')
---> 21     _pywrap_tensorflow_internal = swig_import_helper()
     22     del swig_import_helper

C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     19         except ImportError:
---> 20             return importlib.import_module('_pywrap_tensorflow_internal')
     21     _pywrap_tensorflow_internal = swig_import_helper()

C:\Anaconda3\lib\importlib\__init__.py in import_module(name, package)
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127 

ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-a649b509054f> in <module>()
----> 1 import tensorflow

C:\Anaconda3\lib\site-packages\tensorflow\__init__.py in <module>()
     22 
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26 

C:\Anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>()
     49 import numpy as np
     50 
---> 51 from tensorflow.python import pywrap_tensorflow
     52 
     53 # Protocol buffers

C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     50 for some common reasons and solutions.  Include the entire stack trace
     51 above this error message when asking for help."""""" % traceback.format_exc()
---> 52   raise ImportError(msg)
     53 
     54 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 919, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Не найден указанный модуль.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
10378,What are the 'from' and 'to' dimensions of transition_params in tf.contrib.crf.crf_log_likelihood?,"On TensorFlow, I want to pass a transition_params matrix as argument to tf.contrib.crf.crf_log_likelihood (https://www.tensorflow.org/api_docs/python/tf/contrib/crf/crf_log_likelihood) in order to initialize the transitions matrix of the CRF. Although, in the documentation, it doesn't state which dimension of this matrix corresponds to the first tag of the transition and which dimension corresponds to the second.

So, let T be the transitions matrix, does T[i,j] represent the score of the transition from tag i to tag j, or is it the other way around?

"
10377,tf.contrib.data.Dataset fails to batch elements that are tuples,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution**: Linux Ubuntu 16.04.2 LTS
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.2.0-rc0-312-g0b72359', '1.2.0-rc0')
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: TITAN X (Pascal) / 12186MiB

### Issue

With the new `tf.contrib.data.Dataset` API, it seems not possible to batch the outputs of a Dataset instance whose elements are tuples.  Code demonstrating problem:

```
ds = tf.contrib.data.Dataset.range(100).map(lambda x: (x, 2*x))
batched_ds = ds.batch(4)
batched_iter = batched_ds.make_one_shot_iterator()
nxt = batched_iter.get_next()

with tf.Session() as sess:
    val = sess.run(nxt)
    print(type(val))  # => <type 'tuple'>
    print(len(val))  # => 2
```
It seems that `sess.run(nxt)` should provide an object having 4 elements."
10376,Lack support of qint32 in tf.nn.tanh,"According to the [doc](https://www.tensorflow.org/versions/master/api_docs/python/tf/tanh), the `tanh` operation supports floating point inputs as well as fixed point inputs of type qint32. However, in the latest master, a `TypeError` raised when running following code:
```
import tensorflow as tf
sess = tf.InteractiveSession()
x = tf.constant([1.,2.,3.], dtype=tf.float32)

from tensorflow.python.ops.gen_array_ops import quantize_v2
x_quant = quantize_v2(x, min_range=0., max_range=4., T=tf.qint32)
y_quant = tf.nn.tanh(x_quant[0])
```
The complete error message is 
```
TypeError: Value passed to parameter 'x' has DataType qint32 not in list of allowed values: float16, float32, float64, complex64, complex128
```

According to the backend function `_tanh` in `gen_math_ops.py`:
```
def _tanh(x, name=None):
  r""""""Computes hyperbolic tangent of `x` element-wise.

  Args:
    x: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`, `complex64`, `complex128`.
    name: A name for the operation (optional).
```
It shows that it doesn't support qint32.
"
10375,[Feature] sparse_tensor_dense_matmul with two SparseTensor,"Would it be possible to implement matrix multiplication between two `SparseTensor`? 

Currently, it's possible to multiply two dense `Tensor` with many zeros using the `X_is_sparse` parameter of `tf.matmul`, and a `SparseTensor` with a `Tensor` using `tf.sparse_tensor_dense_matmul`. 

However, with some datasets it is impossible to store dense matrices in memory and it would be great if we could have the option of fully sparse multiplication.

Thanks,
Daniele
"
10374,Weird bugs after successful running until reboot computer,"- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
r0.9
- **Bazel version (if compiling from source)**:
0.2.3
- **CUDA/cuDNN version**:
v4
- **GPU model and memory**:
k80

Due to need debug some old Tensorflow code, i need to install TF0.9 version, after i installed, it could run code, everything goes okey. But after rebooting computer, run tensorflow code once, it can not run again, it trapped in tf.Session() funtion.

"
10373,Limit the number of images in a summary [feature request],"When working on dense prediction tasks it is very convenient to show the predictions of the network in TensorBoard to evaluate them qualitatively during training. Even if TensorBoard shows only a subset of these images, all the summaries that have been saved since the beginning of times are kept on disk. The occupied space grows with the size of the dataset, the sampling frequency and the number of experiments, often resulting in a huge waste of space. When the disk space is limited, this can heavily limit the number of experiments logs one can run simultaneously and/or keep stored.

It would be great to have a way to either limit the number of images saved on disk or to remove some of them (e.g., for old experiments). Here is what I suggest:

- Add an argument to `tf.Summary.Image` to define how many images should be kept on disk (e.g., `buffer_size` or `steps_to_retain`). Also add an extra argument (e.g., `retain_strategy`) to select the strategy to define which images should be kept (e.g., keep latest, sample uniformly, ..).
- Add a function to remove some of the stored images programmatically.

This feature request is partially related to https://github.com/tensorflow/tensorflow/issues/5039
Also related to this SO thread: https://stackoverflow.com/questions/41543954/remove-image-outputs-from-tensorboard/41690170
"
10371,//tensorflow/tools/test:cast_op_benchmark  and //tensorflow/tools/test:rnn_op_benchmark tests are failing with KeyError: 'l2_cache_size' on ppc64le,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
      Ubuntu 16.04 (ppc64le)
- **TensorFlow installed from (source or binary)**:
     Installed from source (v1.0.1)
- **TensorFlow version (use command below)**:
     ('v1.0.1-0-ge895d5c-dirty', '1.0.1')
- **Bazel version (if compiling from source)**:
     0.4.4-2017-05-26 (@80a07b5)
- **CUDA/cuDNN version**:
     CUDA = 8.0 and cuDNN = 5.1
- **GPU model and memory**:
     GPU 0: Tesla P100-SXM2-16GB
     GPU 1: Tesla P100-SXM2-16GB
- **Exact command to reproduce**:
    bazel test --config=opt --config=cuda //tensorflow/tools/test:cast_op_benchmark 
                                                           or
    bazel test --config=opt --config=cuda //tensorflow/tools/test:rnn_op_benchmark

### Describe the problem
Both tests are passing on x86, however getting failure on ppc64le with following errors :

### Source code / logs
```
$   bazel test --config=opt --config=cuda //tensorflow/tools/test:rnn_op_benchmark

exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
-----------------------------------------------------------------------------
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
..
----------------------------------------------------------------------
Ran 2 tests in 0.008s

OK
Traceback (most recent call last):
  File ""/home/amit/.cache/bazel/_bazel_amit/24cb277fcedebf1cf5a3d7b8e713f578/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/test/rnn_op_benchmark.runfiles/org_tensorflow/tensorflow/tools/test/run_and_gather_logs.py"", line 99, in <module>
    app.run()
  File ""/home/amit/.cache/bazel/_bazel_amit/24cb277fcedebf1cf5a3d7b8e713f578/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/test/rnn_op_benchmark.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/home/amit/.cache/bazel/_bazel_amit/24cb277fcedebf1cf5a3d7b8e713f578/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/test/rnn_op_benchmark.runfiles/org_tensorflow/tensorflow/tools/test/run_and_gather_logs.py"", line 78, in main
    test_args)
  File ""/home/amit/.cache/bazel/_bazel_amit/24cb277fcedebf1cf5a3d7b8e713f578/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/test/rnn_op_benchmark.runfiles/org_tensorflow/tensorflow/tools/test/run_and_gather_logs_lib.py"", line 145, in run_and_gather_logs
    log_files=log_files), mangled_test_name)
  File ""/home/amit/.cache/bazel/_bazel_amit/24cb277fcedebf1cf5a3d7b8e713f578/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/test/rnn_op_benchmark.runfiles/org_tensorflow/tensorflow/tools/test/run_and_gather_logs_lib.py"", line 76, in process_test_logs
    system_info_lib.gather_machine_configuration())
  File ""/home/amit/.cache/bazel/_bazel_amit/24cb277fcedebf1cf5a3d7b8e713f578/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/test/rnn_op_benchmark.runfiles/org_tensorflow/tensorflow/tools/test/system_info_lib.py"", line 44, in gather_machine_configuration
    config.cpu_info.CopyFrom(gather_cpu_info())
  File ""/home/amit/.cache/bazel/_bazel_amit/24cb277fcedebf1cf5a3d7b8e713f578/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/test/rnn_op_benchmark.runfiles/org_tensorflow/tensorflow/tools/test/system_info_lib.py"", line 98, in gather_cpu_info
    l2_cache_size = re.match(r'(\d+)', str(info['l2_cache_size']))
KeyError: 'l2_cache_size'

```

Any comments/suggestions ?"
10370,Bug: ProtoBuf tokenizer crashes when loading single_image_random_dot_stereograms OP,"### System information
- No custom code
- Ubuntu 17.04 (also confirmed on 16.04)
- TensorFlow installed from source
- TensorFlow version: v1.2.0-rc0-486-g95d90ab2e 1.2.0-rc1
- Bazel version: 0.5.0
- Python version: 3.5.3 (also confirmed on 2.7.12):
- Not tested with GPU support:

Reproduction
------------------

    import tensorflow as tf
    regressor = tf.contrib.learn.LinearRegressor(feature_columns=[])

Alternative:

    import keras

Reference: [Stackoverflow](https://stackoverflow.com/questions/44291072/google-protobuf-text-format-parseerror-when-instantiating-a-tensorflow-model-wit)

Manifestation of the error
---------------------------------

The first method to reproduce should cause an assert due to the empty `feature_columns`.
Instead, the protobuf tokenizer crashes with:

> Traceback (most recent call last):
>  File ""<stdin>"", line 1, in <module>
>  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/lazy_loader.py"", line 53, in __getattr__
>    module = self._load()
>  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/lazy_loader.py"", line 42, in _load
>    module = importlib.import_module(self.__name__)
>  File ""/usr/lib/python3.5/importlib/__init__.py"", line 126, in import_module
>    return _bootstrap._gcd_import(name[level:], package, level)
>  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
>  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
>  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
>  File ""<frozen importlib._bootstrap>"", line 673, in _load_unlocked
>  File ""<frozen importlib._bootstrap_external>"", line 673, in exec_module
>  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
>  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/__init__.py"", line 35, in <module>
>    from tensorflow.contrib import image
>  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/image/__init__.py"", line 40, in <module>
>    from tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms
>  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/image/python/ops/single_image_random_dot_stereograms.py"", line 26, in <module>
>    ""_single_image_random_dot_stereograms.so""))
>  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/util/loader.py"", line 55, in load_op_library
>    ret = load_library.load_op_library(path)
>  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/load_library.py"", line 84, in load_op_library
>    exec(wrappers, module.__dict__)
>  File ""<string>"", line 248, in <module>
>  File ""<string>"", line 114, in _InitOpDefLibrary
>  File ""/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py"", line 481, in Merge
>    descriptor_pool=descriptor_pool)
>  File ""/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py"", line 535, in MergeLines
>    return parser.MergeLines(lines, message)
>  File ""/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py"", line 568, in MergeLines
>    self._ParseOrMerge(lines, message)
>  File ""/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py"", line 583, in _ParseOrMerge
>    self._MergeField(tokenizer, message)
>  File ""/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py"", line 684, in _MergeField
>    merger(tokenizer, message, field)
>  File ""/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py"", line 773, in _MergeMessageField
>    self._MergeField(tokenizer, sub_message)
>  File ""/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py"", line 684, in _MergeField
>    merger(tokenizer, message, field)
>  File ""/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py"", line 773, in _MergeMessageField
>    self._MergeField(tokenizer, sub_message)
>  File ""/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py"", line 684, in _MergeField
>    merger(tokenizer, message, field)
>  File ""/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py"", line 773, in _MergeMessageField
>    self._MergeField(tokenizer, sub_message)
>  File ""/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py"", line 652, in _MergeField
>    (message_descriptor.full_name, name))
>google.protobuf.text_format.ParseError: 48:12 : Message type ""tensorflow.AttrValue"" has no field named ""5"".

What causes this exception?
--------------------------------------

The problem is the information extracted from the `_single_image_random_dot_stereograms.so` library file from `/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/image/python/ops/`
This file contains encoded information passed to protobuf.

The problem occurs when a line with a `float` default is parsed.

In this case, it is the sequence

    eye_separation: float = 2.5

at offset *0xa3b4* in `emphasized _single_image_random_dot_stereograms.so`

Somehow, the parser replaces decimal points with commas. In the end, this is created:

    attr {\n'
      name: ""eye_separation""\n'
      type: ""float""\n'
      default_value {\n'
        f: 2,5\n'
      }\n'
    }\n'

The tokenizer (at `google/protobuf/text_format.py`) gets confused by the `,` in the default value and thinks that `5` is a separate field.

Root of the error
---------------------

The error occurs during the execution of [load_library.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/load_library.py#L72).

    op_list_str = py_tf.TF_GetOpList(lib_handle)
    op_list = op_def_pb2.OpList()
    op_list.ParseFromString(compat.as_bytes(op_list_str))
    wrappers = py_tf.GetPythonWrappers(op_list_str)

`op_list` contains the correct default value of `2.5`, whereas `wrappers`, the wrapped list returned from [python_op_gen.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/python_op_gen.cc#L762) contains a `2,5`.

This is what `GetPythonWrappers` does:

    string GetPythonWrappers(const char* op_list_buf, size_t op_list_len) {
      string op_list_str(op_list_buf, op_list_len);
      OpList ops;
      ops.ParseFromString(op_list_str);
      return GetPythonOps(ops, {}, false);
    }

The appended files include the contents of the `op_list` and the `wrappers` variable:
[op_list.txt](https://github.com/tensorflow/tensorflow/files/1044775/op_list.txt)
[wrapper.txt](https://github.com/tensorflow/tensorflow/files/1044777/wrapper.txt)

"
10369,Deadlock in MapDataset,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04

- **TensorFlow installed from (source or binary)**: source

- **TensorFlow version (use command below)**:
95d90ab2e0994127ffc42b80e16a3f532895cf6d

- **Bazel version (if compiling from source)**:
0.5.0

- **CUDA/cuDNN version**:
None

- **GPU model and memory**:
None
- **Exact command to reproduce**:
python map_dataset_op_test.py


### Describe the problem
The process hangs forever

### Source code / logs
Below is from map_dataset_op_test.py:

	""""""Tests for the experimental input pipeline ops.""""""
	from __future__ import absolute_import
	from __future__ import division
	from __future__ import print_function

	import numpy as np

	from tensorflow.contrib.data.python.ops import dataset_ops
	from tensorflow.python.framework import constant_op
	from tensorflow.python.framework import dtypes
	from tensorflow.python.framework import errors
	from tensorflow.python.ops import array_ops
	from tensorflow.python.ops import data_flow_ops
	from tensorflow.python.ops import lookup_ops
	from tensorflow.python.ops import math_ops
	from tensorflow.python.ops import random_ops
	from tensorflow.python.ops import string_ops
	from tensorflow.python.ops import variable_scope
	from tensorflow.python.platform import test


	class MapDatasetTest(test.TestCase):

	  def _buildParallelMapDataset(self, components, count, num_threads,
								   output_buffer_size):
		def _map_fn(x, y, z):
		  return math_ops.square(x), math_ops.square(y), math_ops.square(z)
		return (dataset_ops.Dataset.from_tensor_slices(components).map(
			_map_fn, num_threads=num_threads, output_buffer_size=output_buffer_size)
				.repeat(count))

	  def testParallelMapDataset(self):
		""""""Test an dataset that maps a TF function across its input elements.""""""
		# The pipeline is TensorSliceDataset -> ParallelMapDataset(square_3) ->
		# RepeatDataset(count).
		components = [np.arange(7),
					  np.array([[1, 2, 3]]) * np.arange(7)[:, np.newaxis],
					  np.array(37.0) * np.arange(7)]
		count = array_ops.placeholder(dtypes.int64, shape=[])
		num_threads = array_ops.placeholder(dtypes.int32, shape=[])
		output_buffer_size = array_ops.placeholder(dtypes.int64, shape=[])

		dataset = self._buildParallelMapDataset(components, count, num_threads,
												output_buffer_size)
		iterator = dataset.make_initializable_iterator()
		init_op = iterator.initializer
		get_next = iterator.get_next()

		self.assertEqual([c.shape[1:] for c in components],
						 [t.shape for t in get_next])

		with self.test_session() as sess:
		  def do_test(num_threads_val, output_buffer_size_val):
			# Test single-threaded access to the iterator.
			sess.run(init_op, feed_dict={
				count: 14,
				num_threads: num_threads_val,
				output_buffer_size: output_buffer_size_val})
			for _ in range(14):
			  for i in range(7):
				result = sess.run(get_next)
				for component, result_component in zip(components, result):
				  self.assertAllEqual(component[i]**2, result_component)
			with self.assertRaises(errors.OutOfRangeError):
			  sess.run(get_next)

			# Test multi-threaded access to the same iterator.
			sess.run(init_op, feed_dict={
				count: 18,
				num_threads: num_threads_val,
				output_buffer_size: output_buffer_size_val})
			results = []
			def iterator_thread():
			  while True:
				try:
				  results.append(sess.run(get_next))
				except errors.OutOfRangeError:
				  return
			threads = [self.checkedThread(target=iterator_thread) for _ in range(8)]
			for t in threads:
			  t.start()
			for t in threads:
			  t.join()

			# `results` will contain the same elements components**2
			# repeated 18 times, but in a non-deterministic order. Sort the
			# results, and assert that each element of components**2 is
			# produced 18 times.
			results.sort(key=lambda x: x[0])
			for i in range(7):
			  for j in range(18):
				for component, result_component in zip(components,
													   results[i * 18 + j]):
				  self.assertAllEqual(component[i]**2, result_component)

		  for num_threads_val, output_buffer_size_val in [
			  (1, 1), (1, 2), (2, 2), (2, 4), (8, 8), (8, 16)]:
			do_test(num_threads_val, output_buffer_size_val)

	if __name__ == ""__main__"":
	  test.main()

[bt.txt](https://github.com/tensorflow/tensorflow/files/1044353/s.txt)

	
"
10367,Error building TensorFlow from source,"### System information

- No custom code.
- Ubuntu 16.04.
- Trying to compile Tensorflow r1.1 from source.
- Bazel 0.5.0
- CUDA 8.0, cuDNN 5
- K80 GPU (12GB memory)

### Describe the problem

I'm trying to build TensorFlow from source, with CUDA support. My CUDA and cuDNN are installed successfully, as I can run a TensorFlow program fine (with pre-built TensorFlow GPU image).

The command I used was:

```sh
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --copt=-mavx --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both
```

I specified CUDA 8.0 and cuDNN 5 explicitly in `./configure` before running the above. The error is the following:

```sh
ERROR: /home/danqing/.cache/bazel/_bazel_danqing/f5e3ebf5f39671e6e38e6235913f00e0/external/protobuf/BUILD:241:1: C++ compilation of rule '@protobuf//:js_embed' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command
  (cd /home/danqing/.cache/bazel/_bazel_danqing/f5e3ebf5f39671e6e38e6235913f00e0/execroot/tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64 \
    PATH='/home/danqing/.pyenv/plugins/pyenv-virtualenv/shims:/home/danqing/.pyenv/shims:~/.pyenv/bin:/home/danqing/bin:/home/danqing/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin:/usr/local/cuda/bin:/usr/local/cuda/bin' \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -g0 -MD -MF bazel-out/host/bin/external/protobuf/_objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.d '-frandom-seed=bazel-out/host/bin/external/protobuf/_objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.o' -iquote external/protobuf -iquote bazel-out/host/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c external/protobuf/src/google/protobuf/compiler/js/embed.cc -o bazel-out/host/bin/external/protobuf/_objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 2.
python: can't open file 'external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc': [Errno 2] No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```

However, the file actually exists:

```sh
sudo find / -name 'crosstool_wrapper_driver_is_not_gcc'
/home/danqing/.cache/bazel/_bazel_danqing/f5e3ebf5f39671e6e38e6235913f00e0/external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc
```

(Also exists in `bazel-tensorflow/external/local_config_cuda/crosstool`)

Thanks!
"
10364,worker.pb.h file not found,"I am trying to find the file worker.ph.h file which is included in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpc/grpc_remote_worker.cc

Can I know if need to run any command like below one to generate the file
**bazel build tensorflow/cc:cc_ops** as indicated in https://github.com/tensorflow/tensorflow/issues/3017 to generate the respective ops files
"
10363,TensorBoard: Graph Visualization failed Runtime statistics,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 8 (jessie)
- **TensorFlow installed from (source or binary)**: using virtualenv, pip install
- **TensorFlow version (use command below)**: 1.1.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA 7.5, cuDNN 4.0
- **GPU model and memory**: GeForce GTX TITAN X 12GB
- **Exact command to reproduce**: Run this https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py scripts.

### Describe the problem
I am following the tutorials in https://www.tensorflow.org/get_started/graph_viz, downloading and running the mnist_with_summaries.py scripts.

All the Tensorboard visualization works, except the Runtime statistics. In the GRAPHS tab, the Session runs shows (0) items available. And it says 'None'. "
10362,dataset.output_shapes returns demension(none) after batching,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.2.0-rc0
- **CUDA/cuDNN version**:8.0
- **GPU model and memory**:GTX 1080 8G

### Describe the problem
I'm using the Dataset API for input pipelines in tensorflow r1.2
I build my own dataset and batch it with batch size = 128 and then input it into RNN.
but the dataset.output_shape returns dimension(none) in the first dimension, so the RNN raises a error:
```
Traceback (most recent call last):
  File ""untitled1.py"", line 188, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/harold/anaconda2/envs/tensorflow_py2.7/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""untitled1.py"", line 121, in main
    run_training()
  File ""untitled1.py"", line 57, in run_training
    is_training=True)
  File ""/home/harold/huawei/ConvLSTM/ConvLSTM.py"", line 216, in inference
    initial_state=initial_state)
  File ""/home/harold/anaconda2/envs/tensorflow_py2.7/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 566, in dynamic_rnn
    dtype=dtype)
  File ""/home/harold/anaconda2/envs/tensorflow_py2.7/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 636, in _dynamic_rnn_loop
    ""Input size (depth of inputs) must be accessible via shape inference,""
ValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.
```
I think this error is caused by the shape of input, the first dimension should be batch size but not none.

### Source code / logs
```
origin_dataset = Dataset.BetweenS_Dataset(FLAGS.data_path)
train_dataset = origin_dataset.train_dataset
test_dataset = origin_dataset.test_dataset
shuffle_train_dataset = train_dataset.shuffle(buffer_size=10000)
shuffle_batch_train_dataset = shuffle_train_dataset.batch(128)
batch_test_dataset = test_dataset.batch(FLAGS.batch_size)

iterator = tf.contrib.data.Iterator.from_structure(
                           shuffle_batch_train_dataset.output_types,
                            shuffle_batch_train_dataset.output_shapes)
(images, labels) = iterator.get_next()

training_init_op = iterator.make_initializer(shuffle_batch_train_dataset)
test_init_op = iterator.make_initializer(batch_test_dataset)

print(shuffle_batch_test_dataset.output_shapes)
```
I print output_shapes and it gives:
`(TensorShape([Dimension(None), Dimension(36), Dimension(100)]), TensorShape([Dimension(None)]))`

I suppose that it should be 128, because I have batched dataset:
`(TensorShape([Dimension(128), Dimension(36), Dimension(100)]), TensorShape([Dimension(128)]))`"
10361,"Excuse me,I know this is a basic problem, but I don't know how to solve, time is tight, trouble to give some guidance, how to find the lost files, such as libprotobuf - lite. a, libprotobuf. a?","Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
10360,How to compile a 32-bit shared library with bazel on a 64-bit machine ?,
10357,TensorFlow build: protobuf/pyext/_message.so' failed,"I'm trying to build TensorFlow with Bazel on Windows.

But i'm getting this error:

    ERROR: C:/users/spenh/appdata/local/temp/_bazel_spen/icnq02mb/external/protobuf/BUILD:623:1: 
    C++ compilation of rule '@protobuf//:python/google/protobuf/pyext/_message.so' failed: 
    msvc_cl.bat failed: error executing command 
    external/local_config_cc/wrapper/bin/msvc_cl.bat /DOS_WINDOWS=OS_WINDOWS /DCOMPILER_MSVC /DNOGDI /DNOMINMAX /DPRAGMA_SUPPORTED /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS ... (remaining 42 argument(s) skipped): 
    com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 2.
    ...
    C:\users\spenh\appdata\local\temp\_bazel_spen\icnq02mb\execroot\org_tensorflow\external\protobuf\python\google\protobuf\pyext\repeated_scalar_container.cc(307): fatal error C1057: Unerwartetes Dateiende bei der Erweiterung eines Makros
    C:\users\spenh\appdata\local\temp\_bazel_spen\icnq02mb\execroot\org_tensorflow\external\protobuf\python\google\protobuf\pyext\repeated_scalar_container.cc(307): fatal error C1057: Unerwartetes Dateiende bei der Erweiterung eines Makros
        
    Target //tensorflow/tools/pip_package:build_pip_package failed to build



Any idea what i can try to solve this? Or what information i could provide you to solve this?

 - Bazel: git/master (30.05.2017)
 - TensorFlow: git/master (30.05.2017) 
 - Python 3.6.1 x64
 - Visual Studio 2017 v15.0.26430.6 x64
 - Cuda: 8.0
 - Java JDK 1.8.0_60
 - Windows 10
 - Nvidia 1080

Build command:

    bazel build -c opt --config=win-cuda --cpu=x64_windows_msvc --host_cpu=x64_windows_msvc --copt=-w --host_copt=-w tensorflow/tools/pip_package:build_pip_package


Stackoverflow link: https://stackoverflow.com/questions/44264801/tensorflow-bazel-build-protobuf-pyext-message-so-failed"
10355,confusing warning of rank of input Tensor should be the same as output_rank ,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.2.0-rc0 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
8.0
- **GPU model and memory**:
- **Exact command to reproduce**:

cod (mostly from [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/input_fn/boston.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/input_fn/boston.py))
```
from tensorflow.contrib.learn.python.learn.learn_io import pandas_io
import pandas as pd
import tensorflow as tf
import pdb

tf.logging.set_verbosity(tf.logging.INFO)

COLUMNS = [""crim"", ""zn"", ""indus"", ""nox"", ""rm"", ""age"",
           ""dis"", ""tax"", ""ptratio"", ""medv""]
FEATURES = [""crim"", ""zn"", ""indus"", ""nox"", ""rm"",
            ""age"", ""dis"", ""tax"", ""ptratio""]
LABEL = ""medv""

training_set = pd.read_csv(""boston_train.csv"", skipinitialspace=True,
                           skiprows=1, names=COLUMNS)
# test_set = pd.read_csv(""boston_test.csv"", skipinitialspace=True,
#                        skiprows=1, names=COLUMNS)
# prediction_set = pd.read_csv(""boston_predict.csv"", skipinitialspace=True,
#                              skiprows=1, names=COLUMNS)

feature_cols = [tf.contrib.layers.real_valued_column(k) for k in FEATURES]

regressor = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,
                                          hidden_units=[10, 10],
                                          model_dir=""/tmp/boston_model"")

pd_input_fn = pandas_io.pandas_input_fn(training_set[FEATURES], y=training_set[LABEL],
                                        batch_size=128,target_column='medv')

regressor.fit(input_fn=pd_input_fn, steps=5000)
```


### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request. 

I saw a stakoervlow question [here](https://stackoverflow.com/questions/41273182/tensorflow-0-12-tutorials-produce-warning-rank-of-input-tensor-should-be-the-s), but I don't think that applies to me. 

I have the following confusing warnings.  As a result, I am not sure that everything is right. 
```
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
WA
```

### Source code / logs
Full output:

```
INFO:tensorflow:Using default config.
INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5a3710eba8>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {
  per_process_gpu_memory_fraction: 1.0
}
, '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/boston_model'}
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
WARNING:tensorflow:From /home/jiqiang/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.
Instructions for updating:
Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Restoring parameters from /tmp/boston_model/model.ckpt-16
INFO:tensorflow:Saving checkpoints for 17 into /tmp/boston_model/model.ckpt.
INFO:tensorflow:loss = 115.183, step = 17
INFO:tensorflow:Saving checkpoints for 20 into /tmp/boston_model/model.ckpt.
INFO:tensorflow:Loss for final step: 161.23.

```

"
10352,Feature request: Update OpDef proto to ease 1-based indexing,"It would be nice if the `OpDef` proto included information on which inputs and attributes are indices, so that TensorFlow bindings for index-from-1 languages (like Julia) could automatically subtract 1 from the parameters of client calls that refer to those parameters. 

Currently, the Julia binding has to rely on [rough heuristics](https://github.com/malmaud/TensorFlow.jl/blob/40d963f010bd394258d4a950069db85401430050/src/ops.jl#L232), like checking if the operation's input's type attribute is called ""Tidx"", to provide the conversion. "
10350,"Build Android demo, ambiguous python reference","Hi, while I was building the android demo with the command `bazel build -c opt //tensorflow/examples/android:tensorflow_demo --verbose_failures`,  it throw the error: 

```
ERROR: /home/damian/ai/tensorflow/tensorflow/examples/android/BUILD:63:1: Extracting Java resources from deploy jar for split Java resource apk failed: resource_extractor failed: error executing command 
  (cd /home/damian/.cache/bazel/_bazel_damian/80fc8f2224411108cd5fffb32690db73/execroot/tensorflow && \
  exec env - \
  bazel-out/host/bin/external/bazel_tools/tools/android/resource_extractor bazel-out/local-opt/bin/tensorflow/examples/android/tensorflow_demo_deploy.jar bazel-out/local-opt/bin/tensorflow/examples/android/_dx/tensorflow_demo/extracted_tensorflow_demo_deploy.jar): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
  File ""/home/damian/.cache/bazel/_bazel_damian/80fc8f2224411108cd5fffb32690db73/execroot/tensorflow/bazel-out/host/bin/external/bazel_tools/tools/android/resource_extractor.runfiles/org_tensorflow/../bazel_tools/tools/android/resource_extractor.py"", line 98
    print USAGE
              ^
SyntaxError: Missing parentheses in call to print

```

I debuged the error and the cause is in the file `bazel-out/host/bin/external/bazel_tools/tools/android/resource_extractor` around the line 35:  

```
PYTHON_BINARY = 'python'
if IsWindows() and not PYTHON_BINARY.endswith('.exe'):
  PYTHON_BINARY = PYTHON_BINARY + '.exe'

# Find a file in a given search path.
def SearchPath(name):
  search_path = os.getenv('PATH', os.defpath).split(os.pathsep)
  for directory in search_path:
    if directory == '': continue
    path = os.path.join(directory, name)
    if os.path.isfile(path) and os.access(path, os.X_OK):
      return path
  return None

def IsRunningFromZip():
  return False

# Find the real Python binary if it's not a normal absolute path
def FindPythonBinary():
  if PYTHON_BINARY.startswith('//'):
    # Case 1: Path is a label. Not supported yet.
    raise AssertionError(
      'Bazel does not support execution of Python interpreters via labels yet')
  elif PYTHON_BINARY.startswith('/'):
    # Case 2: Absolute path.
    return PYTHON_BINARY
  elif '/' in PYTHON_BINARY:
    # Case 3: Path is relative to current working directory.
    return os.path.join(os.getcwd(), PYTHON_BINARY)
  else:
    # Case 4: Path has to be looked up in the search path.
    return SearchPath(PYTHON_BINARY)

```

the script  need and assumes the default /usr/bin/python is linked to python 2.x wich in my case is not,
My current work around is a simple `ln -sf /usr/bin/python2 /usr/bin/python`.

I would like to patch but I dont have idea where is the file `bazel-out/host/bin/external/bazel_tools/tools/android/resource_extractor` because is an autogen script, I tried look for it in the bazel repo but I can't find it "
10348,Android Detect App does not draw overlay correctly.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: [No] Further troubleshooting shows a minor change may cause this.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android 7.1
- **TensorFlow installed from (source or binary)**: N/A (Nightly apk)
- **TensorFlow version (use command below)**: 1.1,1.2
- **Phone Hardware**: Pixel C Tablet
- **Exact command to reproduce**: Use the Detect App and move your target off centre

### Describe the problem
Due to how the Detect act appears to process it's imagery and camera fragments etc it means that the overlay that's drawn drifts from the target when off centre. (Will try to post pics when I can.)

Part of this issue appears to be the fact that camera preview shown in the app is oddly cropped and squished from what the app actually sees (from debug view). Because of this, when the overlays are drawn they do not match the camera feed below. (Noticeable when your target object is towards the edge of the input)

### Source code / logs
To reproduce, install the app, load up an image of a human on google images and move your camera so it's on the edge. It doesn't line up with the camera preview. Moving this from side to side helps show the problem.

I have spent many hours at work trying to understand why this is. Unfortunately I find the source code very difficult to work with and understand how it at all works. I've had to start again but I came across this bug.

Thanks"
10342,Compilation error when install with CUDA 7.0: image_ops_gpu.cu.pic.o was not created,"Similar to 
[https://github.com/tensorflow/tensorflow/issues/10258](url) 
and 
[https://stackoverflow.com/questions/44116381/error-when-install-tensorflow-from-source](url), 
no working solutions yet.
When I'm trying to install tensorflow r1.1 from source with CUDA 7.0, this error comes up:
```
1 error detected in the compilation of ""/tmp/tmpxft_00005478_00000000-10_image_ops_gpu.cu.compute_52.cpp1.ii"".
ERROR: /root/workspace/tensorflow/tensorflow/contrib/image/BUILD:20:1: output 'tensorflow/contrib/image/_objs/python/ops/_image_ops_gpu/tensorflow/contrib/image/kernels/image_ops_gpu.cu.pic.o' was not created.
ERROR: /root/workspace/tensorflow/tensorflow/contrib/image/BUILD:20:1: not all outputs were created or valid.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
Environment is Ubuntu 14.04, CUDA 7.0, CUDNN 4, gcc 4.8.4, bazel 0.4.5, tensorflow r1.1.
Because I don't have the privilege to upgrade the cuda driver version 346.46, which does not support CUDA 7.5 or 8.0, I have to stay on CUDA 7.0.
Could anyone help me? Thanks! "
10338,tensorflow.Session() crashes if executed after importing scipy.optimize and pytorch,"### Configuration

Python version: 3.5.2 (same for Python 3.6)
SciPy version: 0.19.0
PyTorch version: 0.1.12_2
TensorFlow version: 1.1.0
Host system: Ubuntu 16.04

### Dockerfile to reproduce my setup:

```
FROM nvidia/cuda:8.0-cudnn5-devel-ubuntu16.04

RUN apt-get update -qq \
 && apt-get install -yq -qq --no-install-recommends \
    python3 \
    python3-dev \
    curl \
    ca-certificates
RUN curl -O https://bootstrap.pypa.io/get-pip.py && python3 get-pip.py && rm get-pip.py
RUN pip3 install numpy scipy
RUN pip3 install --no-cache-dir tensorflow-gpu
RUN pip3 install --no-cache-dir http://download.pytorch.org/whl/cu80/torch-0.1.12.post2-cp35-cp35m-linux_x86_64.whl
```

### Describe the problem

```
Python 3.5.2 (default, Nov 17 2016, 17:05:23) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import scipy.optimize
>>> import torch
>>> import tensorflow
>>> tensorflow.Session()
*** Error in `python3': free(): invalid pointer: 0x00007f28329efac0 ***
```

### Output

The above error message is followed by a huge backtrace that starts like this ...

```
======= Backtrace: =========
/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f28796ed7e5]
/lib/x86_64-linux-gnu/libc.so.6(+0x7fe0a)[0x7f28796f5e0a]
/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f28796f998c]
/usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt15basic_stringbufIcSt11char_traitsIcESaIcEE8overflowEi+0x181)[0x7f28377aefa1]
/usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt15basic_streambufIcSt11char_traitsIcEE6xsputnEPKcl+0x89)[0x7f2837805e79]
/usr/local/lib/python3.5/dist-packages/torch/lib/libshm.so(_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l+0x1c5)[0x7f2832764235]
/usr/local/lib/python3.5/dist-packages/torch/lib/libshm.so(_ZStlsISt11char_traitsIcEERSt13basic_ostreamIcT_ES5_PKc+0x27)[0x7f28327644f7]
/usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow20BaseGPUDeviceFactory17GetValidDeviceIdsERKSsPSt6vectorIiSaIiEE+0x7c7)[0x7f27fb0a8127]
/usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow20BaseGPUDeviceFactory13CreateDevicesERKNS_14SessionOptionsERKSsPSt6vectorIPNS_6DeviceESaIS8_EE+0x15a)[0x7f27fb0a9dca]
/usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow13DeviceFactory10AddDevicesERKNS_14SessionOptionsERKSsPSt6vectorIPNS_6DeviceESaIS8_EE+0x17d)[0x7f27fb0d2bad]
/usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow20DirectSessionFactory10NewSessionERKNS_14SessionOptionsE+0x98)[0x7f27fb0904c8]
/usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow10NewSessionERKNS_14SessionOptionsEPPNS_7SessionE+0x127)[0x7f27fb103c07]
/usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(+0x114c081)[0x7f27f968e081]
/usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(+0xe8670b)[0x7f27f93c870b]
python3(PyCFunction_Call+0x4f)[0x4e9b9f]
python3(PyEval_EvalFrameEx+0x614)[0x524414]
python3[0x52d2e3]
python3(PyEval_EvalFrameEx+0x50ee)[0x528eee]
python3(PyEval_EvalCodeEx+0x88a)[0x52e87a]
python3[0x4ebd38]
python3(PyObject_Call+0x47)[0x5b7167]
python3[0x4f413e]

...
```

and ends like this

```
7f2879cf3000-7f2879cf4000 rw-s cfdf2000 00:2e 14                         /dev/nvidia0
7f2879cf4000-7f2879cf5000 rw-s 18da776000 00:2e 12                       /dev/nvidiactl
7f2879cf5000-7f2879e7a000 rw-p 00000000 00:00 0 
7f2879e7a000-7f2879e7b000 rw-s cfdf2000 00:2e 14                         /dev/nvidia0
7f2879e7b000-7f2879e7c000 rw-s 1ec635d000 00:2e 12                       /dev/nvidiactl
7f2879e7c000-7f2879e7d000 rw-s cfdf2000 00:2e 14                         /dev/nvidia0
7f2879e7d000-7f2879e7e000 rw-s 1ffafd3000 00:2e 12                       /dev/nvidiactl
7f2879e7e000-7f2879e7f000 rwxp 00000000 00:00 0 
7f2879e7f000-7f2879e81000 rw-p 00000000 00:00 0 
7f2879e81000-7f2879e82000 r--p 00025000 08:01 3973057                    /lib/x86_64-linux-gnu/ld-2.23.so
7f2879e82000-7f2879e83000 rw-p 00026000 08:01 3973057                    /lib/x86_64-linux-gnu/ld-2.23.so
7f2879e83000-7f2879e84000 rw-p 00000000 00:00 0 
7ffe1932c000-7ffe1934d000 rw-p 00000000 00:00 0                          [stack]
7ffe193bf000-7ffe193c1000 r--p 00000000 00:00 0                          [vvar]
7ffe193c1000-7ffe193c3000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]
Aborted (core dumped)
```"
10332,[Feature request] dynamically catch exceptions in TensorFlow as part of the graph execution,"Via [this StackOverflow question](https://stackoverflow.com/questions/44137542/dynamically-catch-exceptions-in-tensorflow-as-part-of-the-graph-execution):

E.g. the `QueueBase.dequeue` function can raise an `OutOfRangeError` exception which I will receive in Python from the `Session.run` call. It would be nice to catch the exception inside the graph, similar as `tf.cond`. E.g. something like:

    result = tf.on_exception(queue.dequeue(), lambda: 42)

Maybe also the first argument would need to be a `lambda` such that it can properly set the context.
To make this work, like in `tf.cond`, the result from both arguments would need to be of the same type.
"
10329,Problem exporting the weight matrix for each layer along with all the hyper parameters of each layer and their connectivity.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

Yes, I have modified Tensorflow before but not w.r.t. this problem. (I used the binaries provided by conda and pip)

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

macOS 10.12 Sierra for iMac

- **TensorFlow installed from (source or binary)**:

1.1.+

- **TensorFlow version (use command below)**:

pip and pip version

- **Bazel version (if compiling from source)**:

n/a

- **CUDA/cuDNN version**:
n/a
- **GPU model and memory**:
n/a
- **Exact command to reproduce**:

n/a

### Describe the problem

How do you extract the weights of each of the intermediate layers in tensorflow?  I use the /tmp/cifar-train but I couldn't find the exact location of the weight matrix along with the hyper parameters?  

![screen shot 2017-05-31 at 10 18 13 am](https://cloud.githubusercontent.com/assets/9545735/26639453/7ca8beb2-45ea-11e7-8ae6-d1fba7c190d5.png)

I see the following files above but there is no documentation that tells how to export the weight matrix along with the hyper parameters for the entire network and each layer for the exact configuration.

### Source code / logs

n/a"
10325,How to add cwise_op_floor_mod to build libtensorflow-core.a for ANDROID,"OS: Ubuntu 16.04 64bits
Android Version: 7.1 (Nougat)
NDK Version: android-ndk-r12b
HEXAGON SDK: 3.1

When i am trying to build benchmark app, using make build, floor mod operation is not getting added by default, 

### By default:- 
when I issue below command, cwise_op_floor_mod.cc is not getting compiled against most of the files getting compiled in core/kernel/ folder
`make -f tensorflow/contrib/makefile/Makefile TARGET=ANDROID NDK_ROOT=~/android-ndk-r12b`

### Changes tried to add floor mod but failed:-
tensorflow/core/kernels/cwise_op_floor_mode.cc - changes in bold 

--- snip ---

```
#include ""tensorflow/core/kernels/cwise_ops_common.h""
 
 namespace tensorflow {
 REGISTER2(BinaryOp, CPU, ""FloorMod"", functor::safe_floor_mod, int32, int64);
 REGISTER2(BinaryOp, CPU, ""FloorMod"", functor::floor_fmod, float, double);
 
 **REGISTER_KERNEL_BUILDER(Name(""FloorMod"")
                             .Device(DEVICE_CPU)
                             .HostMemory(""x"")
                             .HostMemory(""y"")
                             .HostMemory(""z"")
                             .TypeConstraint<int32>(""T""),
                         BinaryOp<CPUDevice, functor::safe_floor_mod<int32>>);**
 #if GOOGLE_CUDA
 // A special GPU kernel for int32.
 // TODO(b/25387198): Also enable int32 in device memory. This kernel
 // registration requires all int32 inputs and outputs to be in host memory.
 REGISTER_KERNEL_BUILDER(Name(""FloorMod"")
                             .Device(DEVICE_GPU)
                             .HostMemory(""x"")
                             .HostMemory(""y"")
                             .HostMemory(""z"")
                             .TypeConstraint<int32>(""T""),
                         BinaryOp<CPUDevice, functor::safe_floor_mod<int32>>);
 #endif

```
-- eo snip --

somebody help out adding floor mod in the build.
thanks
"
10317,mnist_with_summaries does not produce node statistics,"### System information
- Windows 7 Enterprise 64
- TF 1.1.0 installed from pip
- CUDA 8.0/cuDNN 5.1.5
- GeForce GTX TITAN X
- command: python mnist_with_summaries.py

### Describe the problem
In https://www.tensorflow.org/get_started/graph_viz, mnist_with_summaries is given as an illustration of how to use tensorboard to show node statistics. However, it does not produce node statistics on my computer. Everything else is working correctly (the graph is there, summaries as well, the training goes smoothly).

### Source code / logs
https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py
"
10315,tf.image.central_crop returns zero dimension shaped tensors ,"* I have written custom code.
* OS platform
c++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4
Copyright (C) 2013 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
* Tensorflow version
1.1.0-rc2
* GPU model
Wed May 31 11:49:29 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro K2200        On   | 0000:04:00.0      On |                  N/A |
| 42%   37C    P8     1W /  39W |    422MiB /  4032MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      6147    G   /usr/lib/xorg/Xorg                             213MiB |
|    0     16477    G   cinnamon                                        81MiB |
+-----------------------------------------------------------------------------+

Problem definition:
Given an image of shape (240, 320, 3), and a central_fraction of 0.33
tf.image.central_crop returns an image of shape (0, 0, 3), which is wrong.
Moreover, such tensors of zero dimension results in the cryptic error

could not set cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM

when these zero sized tensors are passed to downstream ops.

Source code to reproduce the problem:

```
import numpy as np
import tensorflow as tf

img_shape = (240, 320, 3)

img_ph = tf.placeholder(shape=img_shape, dtype=tf.uint8)
central_fraction = 0.33
crop_op = tf.image.central_crop(img_ph, central_fraction)

with tf.Session() as sess:
  img = np.zeros((240, 320, 3), dtype=np.uint8)
  feed_dict = {img_ph: img}
  cropped = sess.run(crop_op, feed_dict=feed_dict)
  print(""cropped shape: {}"".format(cropped.shape))
```

"
10311,Different arg names for conv1d,"### System information
- **TensorFlow version (use command below)**: r1.1

### Describe the problem
In latest verion, tf.nn.conv1d and tf.nn.conv2d has different arg names for filter and stride.
tf.nn.conv1d: 
    filters: A 3D Tensor. Must have the same type as input.
    stride: An integer. The number of entries by which the filter is moved right at each step.
tf.nn.conv2d:
    filter: A Tensor. Must have the same type as input. A 4-D tensor of shape [filter_height, filter_width, in_channels, out_channels]
    strides: A list of ints. 1-D tensor of length 4. The stride of the sliding window for each dimension of input. The dimension order is determined by the value of data_format, see below for details.

Should it be unified to same name?"
10306,Build fails on ppc64le,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.04 
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 359d6f9716c0bb9bd8201ce600da98b0481a8049
- **Bazel version (if compiling from source)**:  0.4.5-2017-05-25 (@255953740)
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: `bazel build --verbose_failures --show_package_location  //tensorflow/tools/pip_package:build_pip_package`

### Describe the problem
On a ppc64le machine running Ubuntu 17.04 I am not able to build tensorflow.

### Source code / logs
```
ERROR: /home/brosa/.cache/bazel/_bazel_brosa/141a2b9f209d04ad1bc4d9433836a54c/external/io_bazel_rules_closure/java/io/bazel/rules/closure/webfiles/server/BUILD:54:1: Generating SOY v2 Java files @io_bazel_rules_closure//java/io/bazel/rules/closure/webfiles/server:listing_files failed: bash failed: error executing command
  (cd /home/brosa/.cache/bazel/_bazel_brosa/141a2b9f209d04ad1bc4d9433836a54c/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/brosa/bazel/output:/home/brosa/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/external/com_google_template_soy/SoyParseInfoGenerator --outputDirectory=bazel-out/host/genfiles/external/io_bazel_rules_closure/java/io/bazel/rules/closure/webfiles/server --javaPackage=io.bazel.rules.closure.webfiles.server --javaClassNameSource=filename --allowExternalCalls=1 $(cat bazel-out/host/genfiles/external/io_bazel_rules_closure/java/io/bazel/rules/closure/webfiles/server/listing_files__srcs) $(cat bazel-out/host/genfiles/external/io_bazel_rules_closure/java/io/bazel/rules/closure/webfiles/server/listing_files__deps)'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
Unrecognized option: -client
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
Target //tensorflow/tools/pip_package:build_pip_package failed to build


```"
10303,TensorflowDebugger does not dump Stack/Pack/Concat nodes,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
8.0/5.1.5
- **GPU model and memory**:
Titan X Pascal
- **Exact command to reproduce**:
```
import sys
import tensorflow as tf
from tensorflow.python import debug as tf_debug

base = tf.ones([10], dtype=tf.float32, name='base')
stacked = tf.stack([base, base], name='stacked')
concat = tf.concat([[base], [base]], axis=0, name='concat')

session = tf.Session()
session = tf_debug.LocalCLIDebugWrapperSession(session)

with session.as_default():
    res = session.run([ stacked, concat])
print res
```

### Describe the problem

When using the TensorflowDebugger with stacked/concated, the stacked/concated nodes do not appear in the set of dumped nodes once a run has completed.  In addition nodes that fed into these nodes are not dumped."
10301,"TensorBoard Embeddings fail if a ""+"" character is in LOG_DIR subdirectory filename","### System information
- **Occurs in Tutorial**: DandelionMane's TF Dev Summit 2017 TensorBoard Tutorial
  - If learning_rate >= 1 (not that you would want to do that)
- **OS Platform**: Windows 10
- **TensorFlow install**: Binary
- **TensorFlow version**: 1.1.0
- **Exact command to reproduce**: Use a plus sign in the filename of one of LOG_DIR's subdirectories

### Problem
The tensorboard embeddings tab throws an ""Error fetching projector config"" if a + sign is used in a log_dir subdirectory; that is, if the projector_config.pbtxt is in that subdirectory."
10299,document how to use selective_registration and the use of __ANDROID_FULL_TYPE__,"Every developer wants to use his own model. 
Most app built for Mobile will crash at runtime, because those model won't just use the sparse subset of ops shipped with the aar available from jcenter 

Which means, that mobile developers need a way to easily and painlessly cross compile tenserflow with the right ops for their custom models. 

The print_header_for_selecrtive_registration.py  script is a good step in the right direction for this but it is not documented. 

The documentation is completely lacking. Please document how to cross-compile tenserflow for mobile with the right types, and the right ops. With want command ? what files should we modify, where ? 

I spent days looking at the tensorflow code/build files trying things and I still could not build a binary that would make that annoying ""Op wasn't registered issue"" away

Fix this ! This is not an individual problem, look at the amount of questions and issues about ""tenserflow and Op wasn't registered"" on the internet.

btw, the tensorflow aar from jcenter is completely useless as it can only be used to build the demo app.
It would be better to put the demo apk on Google play, it would not mislead developers into thinking they can easily build apps with it
 

"
10297,docker tensorflow python - FileNotFoundError: [WinError 3] The system cannot find the path specified: '',"getting below error while running following command on docker. kindly assist..

$ python retrain.py \
> --bottleneck_dir= tf_files/bottlenecks \
> --how_many_training_steps=500 \
> --model_dir=tf_files/inception \
> --summaries_dir=tf_files/training_summaries/basic \
> --output_graph=tf_files/retrained_graph.pb \
> --output_labels=tf_files/retrained_labels.txt \
> --image_dir=tf_files/images/
Looking for images in 'cat'
Looking for images in 'dog'


Traceback (most recent call last):
  File ""retrain.py"", line 1063, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""C:\Users\Sharjeel.Riaz\AppData\Local\Programs\Python\Python35\Lib\site-packages\tensorflow\python\platform\app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""retrain.py"", line 809, in main
    jpeg_data_tensor, bottleneck_tensor)
  File ""retrain.py"", line 434, in cache_bottlenecks
    ensure_dir_exists(bottleneck_dir)
  File ""retrain.py"", line 316, in ensure_dir_exists
    os.makedirs(dir_name)
  File ""C:\Users\Sharjeel.Riaz\AppData\Local\Programs\Python\Python35\lib\os.py"", line 241, in makedirs
    mkdir(name, mode)
FileNotFoundError: [WinError 3] The system cannot find the path specified: ''"
10296,Test CMake entries against filetree,There were some invalid entries in `tensorflow/contrib/cmake/tf_python.cmake` which are removed as of #10294 and which were discovered alongside #10264 @drpngx where I proposed extracting all file glob entries from all CMake files for better management as well as automated testing for their validity to keep things safe and sound.
10292,FEATURE: Label scalar summaries,"Often I log scalar summaries at intervals that are meaningful in their own right — e.g. at epochs or min-batch boundaries — and report this, rather than the global step, as the x-value with something like

    some_writer.add_summary(a_value, epoch_number)

or

    some_writer.add_summary(a_value, batch_number)

But the API has no way of indicating that these x-values are not ""steps"", and reports them in the TensorBoard UI as ""Steps"", which they are not.

_It would be nice to have a way of changing the ""Steps"" label to some other string (here, for example ""Epoch"" or ""Batch"")._"
10291,Memory violation when adding an new op,"-----------------------
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
commit 3bee923c9
- **Bazel version (if compiling from source)**:
0.4.5
- **CUDA/cuDNN version**:
cuda 8.0/cudnn 5.1.5
- **GPU model and memory**:
Tesla P40 
- **Exact command to reproduce**:

### Describe the problem
I created a custom operation, it works well with bazel test  --run_under=valgrind. However, when I uses the custom operation using python api, memory violation is happend. Even though no modification of the input tensor, input tensor error comes out.

### Source code / logs
```
namespace tensorflow {

class InGraphAutoParallelOp : public OpKernel {
 public:
  explicit InGraphAutoParallelOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* ctx) override {
    // Get graph definition
    const Tensor* meta_graph_proto_tensor;
    OP_REQUIRES_OK(ctx, ctx->input(""meta_graph_def_str"", &meta_graph_proto_tensor));
    MetaGraphDef meta_graph_def;
    meta_graph_def.ParseFromString(meta_graph_proto_tensor->scalar<string>()());

    // Get num replicas
    const Tensor* num_replicas_tensor;
    OP_REQUIRES_OK(ctx, ctx->input(""num_replicas"", &num_replicas_tensor));
    int num_replicas = num_replicas_tensor->flat<int>()(0);

    MetaGraphDef out_meta_graph_def;
    if (num_replicas == 1) {
      out_meta_graph_def = meta_graph_def;
    } else {
      rdag::grappler::AutoParallel auto_parallel(num_replicas);
      auto_parallel.BuildGraph(meta_graph_def, out_meta_graph_def.mutable_graph_def());
    }

    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(ctx,
        ctx->allocate_output(0, TensorShape({}), &output_tensor));
    CHECK(out_meta_graph_def.SerializeToString(&output_tensor->scalar<string>()()));
  }
};

REGISTER_KERNEL_BUILDER(Name(""InGraphAutoParallel"").Device(DEVICE_CPU), InGraphAutoParallelOp);

 
```

Below is the modification of auto_parallel code in TensorFlow.

```
#include ""tensorflow_rdag/grappler/auto_parallel.h""

#include ""tensorflow_rdag/grappler/grappler_item_builder.h""
#include ""tensorflow_rdag/grappler/op_types.h""
#include ""tensorflow_rdag/grappler/utils.h""

namespace tensorflow {
namespace rdag {
namespace grappler {

static const std::set<std::string> APPLY_GRADIENT_OPS = {""ApplyGradientDescent"",
                                                     ""ApplyProximalGradientDescent"",
                                                     ""ApplyAdadelta"",
                                                     ""ApplyAdagrad"",
                                                     ""ApplyProximalAdagrad"",
                                                     ""ApplyAdagradDA"",
                                                     ""ApplyFtrl"",
                                                     ""ApplyMomentum"",
                                                     ""ApplyAdam"",
                                                     ""ApplyRMSProp"",
                                                     ""ApplyCenteredRMSProp""};
static std::map<std::string, int> GRADIENT_POS = {{""ApplyGradientDescent"", 2},
                                             {""ApplyProximalGradientDescent"", 4},
                                             {""ApplyAdadelta"", 6},
                                             {""ApplyAdagrad"", 3},
                                             {""ApplyProximalAdagrad"", 5},
                                             {""ApplyAdagradDA"", 3},
                                             {""ApplyFtrl"", 3},
                                             {""ApplyMomentum"", 3},
                                             {""ApplyAdam"", 9},
                                             {""ApplyRMSProp"", 7},
                                             {""ApplyCenteredRMSProp"", 8}};
const char kAutoParallelPrefix[] = ""AutoParallel"";

std::string AutoParallel::GetGradientNodeName(const std::string& apply_gradient_node_name) {
  auto apply_gradients_node = all_nodes_[apply_gradient_node_name];
  return apply_gradients_node->input(GRADIENT_POS[apply_gradients_node->op()]);
}

Status AutoParallel::Initialize(const GrapplerItem& item) {
  LOG(INFO) << ""Number of replicas: "" << num_replicas_;
  item_ = &item;
  graph_ = item.graph;
  LOG(INFO) << ""Original graph size: "" << item.graph.node_size();
  if (item.fetch.empty()) {
    return Status(error::INVALID_ARGUMENT, ""No fetch nodes provided."");
  }

  if (item.MainVariables().empty()) {
    return Status(error::INVALID_ARGUMENT, ""No variables provided."");
  }

  for (const auto& init : item.init_ops) {
    VLOG(1) << ""Init node: "" << init;
  }

  for (const auto& fetch : item.fetch) {
    VLOG(1) << ""Fetch node: "" << fetch;
  }

  for (const auto& var : item.MainVariables()) {
    VLOG(2) << ""Variable: "" << var->name();
  }

  for (QueueRunnerDef def : item.queue_runners) {
    queue_runners_.insert(std::make_pair(def.queue_name(), def));
  }

  for (VariableDef def : item.variables) {
    variables_.insert(std::make_pair(def.variable_name(), def));
  }

  std::vector<std::string> queue_nodes;
  for (int i = 0; i < graph_.node_size(); i++) {
    all_nodes_.insert(
        std::make_pair(graph_.node(i).name(), graph_.mutable_node(i)));
    if (APPLY_GRADIENT_OPS.find(graph_.node(i).op()) !=
      APPLY_GRADIENT_OPS.end()) {
      apply_gradients_nodes_.insert(graph_.node(i).name());
      VLOG(2) << ""Apply gradients node: "" << graph_.node(i).name();
    }
  }

  std::set<std::string> dont_replicate_nodes;
  for (const auto& variable : item.MainVariables()) {
    dont_replicate_nodes.insert(variable->name());
    VariableDef def = variables_[variable->name()];
    dont_replicate_nodes.insert(def.initializer_name());
    dont_replicate_nodes.insert(def.snapshot_name());
  }

  std::vector<std::string> gradient_nodes;
  for (const auto& apply_gradient_node_name : apply_gradients_nodes_) {
    gradient_nodes.push_back(GetGradientNodeName(apply_gradient_node_name));
  }

  auto train_nodes = ComputeTransitiveFanin(graph_, gradient_nodes);
  LOG(INFO) << ""Number of training nodes: "" << train_nodes.size();

  for (const auto& fetch_node_name : item.fetch) {
    dont_replicate_nodes.insert(fetch_node_name);
  }

  std::vector<const NodeDef*> enqueue_dequeue_nodes;
  std::vector<const NodeDef*> visitied;
  for (const auto& node : train_nodes) {
    if (dont_replicate_nodes.find(node->name()) == dont_replicate_nodes.end()) {
      replica_nodes_.insert(node->name());
      if (IsDequeueOp(*node)) {
        enqueue_dequeue_nodes.push_back(node);
      }
    }
  }
  LOG(INFO) << ""Number of replica nodes: "" << replica_nodes_.size();

  std::vector<const NodeDef*> input_nodes;
  while (!enqueue_dequeue_nodes.empty()) {
    // Pop first node in eneque_dequeue_nodes.
    const NodeDef* enqueue_dequeue_node = *enqueue_dequeue_nodes.begin();
    enqueue_dequeue_nodes.erase(enqueue_dequeue_nodes.begin());
    if(std::find(visitied.begin(), visitied.end(), enqueue_dequeue_node) != visitied.end())
      continue;
    visitied.push_back(enqueue_dequeue_node);

    auto temp_input_nodes = ComputeTransitiveFanin(graph_, {enqueue_dequeue_node->name()});

    for (const NodeDef* input_node : temp_input_nodes) {
      input_nodes.push_back(input_node);

      if (IsQueueOp(*input_node)) {
        QueueRunnerDef def = queue_runners_[input_node->name()];
        for (int i = 0; i < def.enqueue_op_name_size(); i++) {
          const auto& enqueue_op = all_nodes_[def.enqueue_op_name(i)];
          input_nodes.push_back(enqueue_op);
          enqueue_dequeue_nodes.push_back(enqueue_op);
        }
        input_nodes.push_back(all_nodes_[def.close_op_name()]);
        input_nodes.push_back(all_nodes_[def.cancel_op_name()]);
      } else if (IsDequeueOp(*input_node)) {
        enqueue_dequeue_nodes.push_back(input_node);
      }
    }
  }

  LOG(INFO) << ""Number of input nodes: "" << input_nodes.size();

  // Replicate all input pipeline nodes
  for (const auto& input_node : input_nodes) {
    replica_nodes_.insert(input_node->name());
  }

  for (const auto& node : all_nodes_) {
    if (replica_nodes_.find(node.first) == replica_nodes_.end()) {
      shared_nodes_.insert(node.first);
    }
  }
  LOG(INFO) << ""Number of shared nodes: "" << shared_nodes_.size();
  return Status::OK();
}

bool AutoParallel::NotSharedNode(const std::string& name) {
  return shared_nodes_.find(name) == shared_nodes_.end();
}

void AutoParallel::AddSharedNodes(GraphDef* graph) {
  std::string prefix = strings::StrCat(kAutoParallelPrefix, ""-Replica-"", 0);
  for (const auto& node : shared_nodes_) {
    auto new_node = graph->add_node();
    *new_node = *all_nodes_[node];
    for (int i = 0; i < new_node->input_size(); i++) {
      if (NotSharedNode(NodeName(new_node->input(i)))) {
        std::string new_name = AddPrefixToNodeName(new_node->input(i), prefix);
        *new_node->mutable_input(i) = new_name;
      }
    }
  }
}

void AutoParallel::AddOneReplica(GraphDef* graph, int number) {
  std::string prefix = strings::StrCat(kAutoParallelPrefix, ""-Replica-"", number);
  for (const auto& node : replica_nodes_) {
    auto new_node = graph->add_node();
    *new_node = *all_nodes_[node];
    assert(new_node != all_nodes_[node]);
    if (NotSharedNode(new_node->name())) {
      new_node->set_name(AddPrefixToNodeName(new_node->name(), prefix));
      if (num_replicas_ > 0) {
        // TODO : keep previous device setting except gpu device
        // new_node->set_device(std::strings::StrCat(""/gpu:"", number % num_replicas_));
      }
      for (int i = 0; i < new_node->input_size(); i++) {
        if (NotSharedNode(NodeName(new_node->input(i)))) {
          std::string new_name = AddPrefixToNodeName(new_node->input(i), prefix);
          *new_node->mutable_input(i) = new_name;
        }
      }
    }
  }
}

NodeDef* AutoParallel::AddNodeDivConst(GraphDef* graph) {
  NodeDef* node = graph->add_node();
  node->set_name(strings::StrCat(kAutoParallelPrefix, ""-Div-Const""));
  node->set_op(""Const"");

  AttrValue attr_data_type;
  attr_data_type.set_type(DT_FLOAT);
  node->mutable_attr()->insert({""dtype"", attr_data_type});

  AttrValue attr_tensor;
  auto tensor = attr_tensor.mutable_tensor();
  tensor->add_float_val(static_cast<float>(num_replicas_));
  tensor->set_dtype(DT_FLOAT);
  node->mutable_attr()->insert({""value"", attr_tensor});
  return node;
}

NodeDef* AutoParallel::AddNodeDiv(GraphDef* graph, const std::string& name, const std::string& input_a,
                                  const std::string& input_b) {
  NodeDef* node = graph->add_node();
  node->set_name(strings::StrCat(kAutoParallelPrefix, ""-Div-"", name));
  node->set_op(""RealDiv"");
  node->add_input(input_a);
  node->add_input(input_b);
  AttrValue attr_type;
  attr_type.set_type(DT_FLOAT);
  node->mutable_attr()->insert({""T"", attr_type});
  return node;
}

void AutoParallel::AddApplyGradientDescent(GraphDef* graph) {
  std::map<std::string, NodeDef*> apply_gradients_nodes;
  for (int i = 0; i < graph->node_size(); i++) {
    if (APPLY_GRADIENT_OPS.find(graph->node(i).op()) !=
        APPLY_GRADIENT_OPS.end()) {
        apply_gradients_nodes.insert(
          std::make_pair(graph->node(i).name(), graph->mutable_node(i)));
    }
  }

  auto div_const_node = AddNodeDivConst(graph);

  for (const auto& apply_gradient_node_name : apply_gradients_nodes_) {
    auto apply_gradients_node = apply_gradients_nodes[apply_gradient_node_name];

    // Add all gradients
    NodeDef* add_node = graph->add_node();
    add_node->set_name(strings::StrCat(kAutoParallelPrefix, ""-Add-"", apply_gradient_node_name));
    add_node->set_op(""AddN"");
    const auto& input_name = GetGradientNodeName(apply_gradient_node_name);
    for (int i = 0; i < num_replicas_; i++) {
      add_node->add_input(strings::StrCat(kAutoParallelPrefix, ""-Replica-"", i, ""/"", input_name));
      LOG(INFO) << ""Gradient Node Name: "" << strings::StrCat(kAutoParallelPrefix, ""-Replica-"", i, ""/"", input_name);
    }
    AttrValue attr_type;
    attr_type.set_type(DT_FLOAT);
    add_node->mutable_attr()->insert({""T"", attr_type});
    AttrValue attr_type2;
    attr_type2.set_type(DT_INT32);
    attr_type2.set_i(num_replicas_);
    add_node->mutable_attr()->insert({""N"", attr_type2});

    // Divide by number of GPUs
    auto div_node = AddNodeDiv(
        graph,
        apply_gradient_node_name,
        add_node->name(),
        div_const_node->name());

    LOG(INFO) << ""Change gradient node as : "" << div_node->name();
    *apply_gradients_node->mutable_input(GRADIENT_POS[apply_gradients_node->op()]) =
        div_node->name();
  }
}

void AutoParallel::BuildGraph(const MetaGraphDef& meta_graph, GraphDef* graph) {
  ItemConfig cfg;
  std::unique_ptr<GrapplerItem> grappler_item = GrapplerItemFromMetaGraphDef(
      ""graph_to_optimize"", meta_graph, cfg);
  Initialize(*grappler_item);

  //GraphDef* out_graph_def = out_graph.mutable_graph_def();
  AddSharedNodes(graph);
  for (int i = 0; i < num_replicas_; i++) {
    AddOneReplica(graph, i);
  }
  AddApplyGradientDescent(graph);
  //
  *(graph->mutable_library()) = meta_graph.graph_def().library();
  // LOG(INFO) << ""Parallelized graph size: "" << out_graph_def->node_size();

  // SetMetaGraphForQueueRunners(out_graph, replicated_queues);
  // SetMetaGraphForVariables(out_graph, variables);
  //return graph;
}

}  // end namespace grappler
}  // end namespace rdag
}  // end namespace tensorflow

```"
10290,tf.split should allow negative axis arguments,"`tf.concat` does, for instance."
10289,Cannot bazel build tensorflow pip package from source (/usr/bin/env: 'python': No such file or directory),"------------------------
### System information
- OS Platform and Distribution: Linux Ubuntu 16.04
- CUDA Toolkit v8.0
- cuDNN 5
- gcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
- Bazel 0.5.0 (from repo)
- Anaconda Python 3.6
```
francesco@gpu-box:~/tensorflow$ python
Python 3.6.0 |Anaconda custom (64-bit)| (default, Dec 23 2016, 12:22:00) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> 
```


------------------------

Hello,

I am trying to build tensor flow pip package from source. I can install and use tensorflow with GPU using a precompiled binary just fine. However, in this case, I need to build from source.

Here is the following steps I take:
1. git clone https://github.com/tensorflow/tensorflow.git
2. cd tensorflow
3. ./configure

```
francesco@gpu-box:~/tensorflow$ ./configure 
Please specify the location of python. [Default is /home/francesco/anaconda3/bin/python]: 
Found possible Python library paths:
  /home/francesco/anaconda3/lib/python3.6/site-packages/
  /home/francesco/anaconda3/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/home/francesco/anaconda3/lib/python3.6/site-packages/]

Using python library path: /home/francesco/anaconda3/lib/python3.6/site-packages/
Do you wish to build TensorFlow with MKL support? [y/N] n
No MKL support will be enabled for TensorFlow
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 
Do you wish to use jemalloc as the malloc implementation? [Y/n] y
jemalloc enabled
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] n
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] n
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] n
No XLA JIT support will be enabled for TensorFlow
Do you wish to build TensorFlow with VERBS support? [y/N] n
No VERBS support will be enabled for TensorFlow
Do you wish to build TensorFlow with OpenCL support? [y/N] n
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] y
CUDA support will be enabled for TensorFlow
Do you want to use clang as CUDA compiler? [y/N] n
nvcc will be used as CUDA compiler
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Please specify the cuDNN version you want to use. [Leave empty to use system default]: 5
Please specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 6.1
Do you wish to build TensorFlow with MPI support? [y/N] n
MPI support will not be enabled for TensorFlow
INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.
Configuration finished

```

Unfortunately, when I then build with bazel, it runs into an error. It seems like it cannot find python?

```
francesco@gpu-box:~/tensorflow$ bazel build -c opt --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package
WARNING: /home/francesco/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': Use SavedModel Builder instead.
WARNING: /home/francesco/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': Use SavedModel instead.
INFO: Found 1 target...
INFO: From Compiling external/snappy/snappy-sinksource.cc [for host]:
cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
cc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'
INFO: From Compiling external/snappy/snappy-c.cc [for host]:
cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
cc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'
INFO: From Compiling external/snappy/snappy-stubs-internal.cc [for host]:
cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
cc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'
INFO: From Compiling external/snappy/snappy.cc [for host]:
cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
external/snappy/snappy.cc: In member function 'void snappy::SnappySinkAllocator::Flush(size_t)':
external/snappy/snappy.cc:1403:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (int i = 0; i < blocks_.size(); ++i) {
                       ^
In file included from external/snappy/snappy-internal.h:34:0,
                 from external/snappy/snappy.cc:30:
external/snappy/snappy.cc: In instantiation of 'bool snappy::SnappyScatteredWriter<Allocator>::AppendFromSelf(size_t, size_t) [with Allocator = snappy::SnappySinkAllocator; size_t = long unsigned int]':
external/snappy/snappy.cc:715:13:   required from 'void snappy::SnappyDecompressor::DecompressAllTags(Writer*) [with Writer = snappy::SnappyScatteredWriter<snappy::SnappySinkAllocator>]'
external/snappy/snappy.cc:799:3:   required from 'bool snappy::InternalUncompressAllTags(snappy::SnappyDecompressor*, Writer*, snappy::uint32) [with Writer = snappy::SnappyScatteredWriter<snappy::SnappySinkAllocator>; snappy::uint32 = unsigned int]'
external/snappy/snappy.cc:1460:78:   required from here
external/snappy/snappy.cc:1316:34: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (PREDICT_TRUE(offset - 1u < op_ptr_ - op_base_ && op_end <= op_limit_)) {
                                  ^
external/snappy/snappy-stubs-internal.h:80:25: note: in definition of macro 'PREDICT_TRUE'
 #define PREDICT_TRUE(x) x
                         ^
At global scope:
cc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'
ERROR: /home/francesco/.cache/bazel/_bazel_francesco/7b3bdb053a374c3fec955b526c0e6446/external/highwayhash/BUILD.bazel:8:1: C++ compilation of rule '@highwayhash//:sip_hash' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/francesco/.cache/bazel/_bazel_francesco/7b3bdb053a374c3fec955b526c0e6446/execroot/tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/francesco/anaconda3/bin/python \
    PYTHON_LIB_PATH=/home/francesco/anaconda3/lib/python3.6/site-packages/ \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_CUDA_VERSION=8.0 \
    TF_CUDNN_VERSION=5 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/local_linux-py3-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.pic.d '-frandom-seed=bazel-out/local_linux-py3-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.pic.o' -fPIC -iquote external/highwayhash -iquote bazel-out/local_linux-py3-opt/genfiles/external/highwayhash -iquote external/bazel_tools -iquote bazel-out/local_linux-py3-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c external/highwayhash/highwayhash/sip_hash.cc -o bazel-out/local_linux-py3-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127.
/usr/bin/env: 'python': No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 44.289s, Critical Path: 5.54s
```

Both my python_bin_path and python_lib_path are correct, so I do not understand.

Any advice?

Regards"
10288,[XLA][Feature] - Pass config flags for LLVM runtime.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**:  ('v1.0.0-1783-g4c3bb1a', '1.0.0')
- **Bazel version (if compiling from source)**:  0.4.5
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: -

### Problem description
As part of my Google Summer of Code project, I am trying to build TensorFlow with Polly-enabled LLVM. To do this, I have written my own BUILD file which runs TensorFlow with a custom repository of LLVM that has Polly checked out as well. I have managed to get a clean build and am now looking to incorporate Polly's passes in the Optimization pipeline of XLA. 

In XLA, the llvm module passes are registered [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/compiler_functor.cc#L214).

Polly register's its passes in LLVM through the following steps  

1)
```
static llvm::RegisterStandardPasses RegisterPollyOptimizerEarly(
    llvm::PassManagerBuilder::EP_ModuleOptimizerEarly,
    registerPollyEarlyAsPossiblePasses);
```
Corresponding file - ```<polly-src>/lib/Support/RegisterPasses.cpp```.  

2)

```
polly::initializePollyPasses(Registry); 
```

Corresponding file - ```<polly-src>/lib/Polly.cpp```

I have built the object files for both these files. But I want to check if Polly is actually being invoked in the pipeline, and so my question is -
- Are these steps enough to use Polly in the bazel build of TensorFlow?
- How can I pass configuration flags to LLVM in TensorFlow to check for Polly usage?

As a reference, please find my BUILD file [here](https://gitlab.com/annanay25/tensorflow/blob/master/third_party/llvm/llvm.BUILD).

cc @phawkins @eliben"
10287,Windows 8.1 Anaconda Tensorflow GPU -- BLACKSCREEN,"Followed instructions on https://www.tensorflow.org/install/install_windows
When I get to

activate tensorflow-gpu 
 $ python
>>> import tensorflow as tf
>>> hello = tf.constant('Hello, TensorFlow!')
>>> sess = tf.Session()
Screen goes Black. When I move the cursor it moves, and then resets to the center every 10 seconds or so and disappears into the blackness, and doesnt go back to normal screen.

### System information
- Windows 8.1 Pro
used
- pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.1.0-cp35-cp35m-win_amd64.whl 

- Cuda toolkit 8.0 
- Cudnn 5.1
- 4G NVidia GT 750M:

 python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
b'unkown' 1.1.0


Let me know what else I should provide
"
10286,Not able to run fusedgraph test on Hexagon,"OS: Ubuntu 16.04 64bits
Android Version: 7.1 (Nougat)
NDK Version: android-ndk-r12b
HEXAGON SDK: 3.1
nnlib source: https://source.codeaurora.org/quic/hexagon_nn/nnlib

Able to run runtime tf graph on hexagon **but not able to run fused graph** .
I got below error.

WARNING: linker: Warning: unable to normalize """"
Running main() from test_main.cc
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from GraphTransferer
[ RUN      ] GraphTransferer.RunInceptionV3OnHexagonExampleWithFusedGraph
native : hexagon_graph_execution_test.cc:474 Run inception v3 with fused graph
GetSocControllerVersion
native : hexagon_graph_execution_test.cc:72 Hexagon controller version is 90
native : hexagon_graph_execution_test.cc:122 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes
native : hexagon_graph_execution_test.cc:128 header size = 54
native : hexagon_graph_execution_test.cc:130 image size = 40
native : hexagon_graph_execution_test.cc:132 width = 299
native : hexagon_graph_execution_test.cc:134 height = -299
native : hexagon_graph_execution_test.cc:262 loading image finished.
native : hexagon_graph_execution_test.cc:170 loading image finished.
native : hexagon_graph_execution_test.cc:174 Copy data to tensor.
can't determine number of CPU cores: assuming 4
can't determine number of CPU cores: assuming 4
native : hexagon_graph_execution_test.cc:284 Run graph
tensorflow/core/kernels/hexagon/hexagon_graph_execution_test.cc:288: Failure
Value of: status.ok()
  Actual: false
Expected: true
[  FAILED  ] GraphTransferer.RunInceptionV3OnHexagonExampleWithFusedGraph (111 ms)
[----------] 1 test from GraphTransferer (111 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test case ran. (111 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] GraphTransferer.RunInceptionV3OnHexagonExampleWithFusedGraph

 1 FAILED TEST
  YOU HAVE 4 DISABLED TESTS


Where to get fused graph for inceptionv3q to run on hexagon?
what is the difference between TF runtime and TF fused graph(pointers? - operation wise)

"
10285,Tensorflow Windows Visual Studio Help,"Hi all,
I'm VietNamese, my English is not good.
I'm programming mobile robot with Aria ,C++ ,OpenCv and Tensorflow
I'm using Windows and Visual Studio 2012. I can't setup Tensorflow C++.
how to setup Tensorflow C++ and connect to VS2012
I hope everyone will train and help me. 
Thank you.


 "
10284,Using fixed_unigram_candidate_sampler + nce_loss with reserved_ids emits NaN outputs,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: From Source,
- **TensorFlow version (use command below)**: v1.1.0-rc2-773-g7fa0cf3
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: CUDA 8.0, cuDNN 5

### Describe the problem
Using `tf.nn.nce_loss` (to be precise, `_compute_sampled_logits` function with argument `subtract_log_q=True`) with `tf.nn.fixed_unigram_candidate_sampler(num_reserved_ids>0)` + inputs with reserved ids gives NaN/inf logit output.

The cause for this NaN seems to be the `true_expected_count` return value of `tf.nn.fixed_unigram_candidate_sampler` for ids in range `[0, num_reserved_ids)`, since sampler returns expected count 0.0 for these ids. When the `subtract_log_q` argument of `_compute_sampled_logits` is zero, log value of expected count for these ids become inf or NaN. I used reserved ids for UNK and PAD (since `nce_loss` does not support variable number of target classes yet), using these ids in input was inevitable.

Possible solution would be adding/cliiping log input by small epsilon. Will there be any better solution?

### Source code / logs
```
import tensorflow as tf
import numpy as np

batch_size = 3
num_true = 4
num_classes = 5
num_sampled = 5
embed_dim = 5

true_classes = tf.constant(
    np.array(
        [[3, 1, 2, 0],
         [2, 0, 0, 0],
         [2, 4, 3, 0]]),
    dtype=tf.int64)

sampled_values = tf.nn.fixed_unigram_candidate_sampler(
    true_classes=true_classes,
    num_true=num_true,
    num_sampled=num_sampled,
    unique=False,
    range_max=num_classes,
    num_reserved_ids=1,
    unigrams=[10, 10, 10, 10]
)
sampled_ids, true_expected_count, sampled_expected_count = sampled_values

loss = tf.reduce_mean(
    tf.nn.nce_loss(
        weights=tf.ones([num_classes, embed_dim], dtype=tf.float32),
        biases=tf.zeros([num_classes], dtype=tf.float32),
        labels=true_classes,
        inputs=tf.ones([batch_size, embed_dim], dtype=tf.float32),
        num_sampled=num_sampled,
        num_classes=num_classes,
        num_true=num_true,
        sampled_values=sampled_values
    )
)

with tf.Session() as sess:
    loss_value, true_count = sess.run([loss, true_expected_count])
    print('Loss: {:.4f}'.format(loss_value))
    print('Min True Count: {:.4f}'.format(np.amin(true_count)))

>>>>>>>>
Loss: nan
Min True Count: 0.0000
```"
10283,ImportError:_pywrap_tensorflow_internal.so: __sprintf_chk: symbol  not found,"# OS
Host: Windows 10 Professional  64bit
Docker container :  Alpine
`/ # cat /etc/issue`
Welcome to Alpine Linux 3.6
Kernel \r on an \m (\l)

`/ # uname -a`
Linux 3b851449cb60 4.9.27-moby #1 SMP Thu May 11 04:01:18 UTC 2017 x86_64 Linux

# Installation
- Part of my dockerfile

`FROM frolvlad/alpine-glibc`
`RUN apk update && apk add --no-cache \
        wget ca-certificates unzip vim git \
        gcc g++ python python-dev py-numpy-dev && \
    apk add --no-cache --virtual=build-dependencies \
        libffi-dev libressl-dev zlib-dev jpeg-dev freetype-dev libpng-dev `

`RUN wget https://bootstrap.pypa.io/get-pip.py && \
    python get-pip.py && rm get-pip.py && \
    ln -s /usr/include/locale.h /usr/include/xlocale.h && \
    pip --no-cache-dir install requests[security] ipykernel jupyter matplotlib scipy scikit-learn pandas seaborn \
    https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.0rc1-cp27-none-linux_x86_64.whl && \
    python -m ipykernel.kernelspec `

The dockerfile was built successfully. But when `import tensorflow` in a container , the following happened

>Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: Error relocating /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: __sprintf_chk: symbol
 not found
Failed to load the native TensorFlow runtime."
10282,Conflict between Defun and py_func,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS 10.12.5
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.1.0-rc0-61-g1ec6ed5 1.1.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A (run on CPU)
- **Exact command to reproduce**:

```
import tensorflow as tf
from tensorflow.python.framework import function

def f(x):
    return x

@function.Defun(tf.float32, func_name='f')
def f1(x): 
    return tf.py_func(f, [x], tf.float32)

with tf.Session() as sess:
    x = tf.constant(1.)
    print(sess.run(f1(x)))
```

### Describe the problem
The decorator `Defun` does not work with `py_func`, and generates the KeyError when attempting to call with the token `pyfunc_#`.  If one comments out the decorator `@function.Defun(...)`, or if one redefines `def f1(x): return x`, the error will disappear.

### Source code / logs
Error traceback:
```
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/script_ops.py in __call__(self, token, args)
     77   def __call__(self, token, args):
     78     """"""Calls the registered function for `token` with args.""""""
---> 79     func = self._funcs[token]
     80     if func is None:
     81       raise ValueError(""callback %s is not found"" % token)

KeyError: 'pyfunc_0'

---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1038     try:
-> 1039       return fn(*args)
   1040     except errors.OpError as e:

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1020                                  feed_dict, fetch_list, target_list,
-> 1021                                  status, run_metadata)
   1022 

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/contextlib.py in __exit__(self, type, value, traceback)
     65             try:
---> 66                 next(self.gen)
     67             except StopIteration:

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py in raise_exception_on_not_ok_status()
    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),
--> 466           pywrap_tensorflow.TF_GetCode(status))
    467   finally:

InternalError: Failed to run py callback pyfunc_0: see error log.
	 [[Node: n1 = PyFunc[Tin=[DT_FLOAT], Tout=[DT_FLOAT], token=""pyfunc_0""](n0)]]
	 [[Node: f = f[_device=""/job:localhost/replica:0/task:0/cpu:0""](Const)]]

During handling of the above exception, another exception occurred:

InternalError                             Traceback (most recent call last)
<ipython-input-1-0ed0f802b342> in <module>()
      8 with tf.Session() as sess:
      9     x = tf.constant(1.)
---> 10     print(sess.run(f1(x)))

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    776     try:
    777       result = self._run(None, fetches, feed_dict, options_ptr,
--> 778                          run_metadata_ptr)
    779       if run_metadata:
    780         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
    980     if final_fetches or final_targets:
    981       results = self._do_run(handle, final_targets, final_fetches,
--> 982                              feed_dict_string, options, run_metadata)
    983     else:
    984       results = []

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1030     if handle is None:
   1031       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
-> 1032                            target_list, options, run_metadata)
   1033     else:
   1034       return self._do_call(_prun_fn, self._session, handle, feed_dict,

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1050         except KeyError:
   1051           pass
-> 1052       raise type(e)(node_def, op, message)
   1053 
   1054   def _extend_graph(self):

InternalError: Failed to run py callback pyfunc_0: see error log.
	 [[Node: n1 = PyFunc[Tin=[DT_FLOAT], Tout=[DT_FLOAT], token=""pyfunc_0""](n0)]]
	 [[Node: f = f[_device=""/job:localhost/replica:0/task:0/cpu:0""](Const)]]
```"
10281,//tensorflow/python:function_test is passing without GPU support but failing with GPU support,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
     Ubuntu 16.04 (ppc64le)
- **TensorFlow installed from (source or binary)**:
     Installed from source (v1.0.1)
- **TensorFlow version (use command below)**:
    ('v1.0.1-0-ge895d5c-dirty', '1.0.1')
- **Bazel version (if compiling from source)**:
    0.4.4-2017-05-26 (@80a07b5)
- **CUDA/cuDNN version**:
     CUDA = 8.0 and cuDNN = 5.1
- **GPU model and memory**:  
     GPU 0: Tesla P100-SXM2-16GB 
     GPU 1: Tesla P100-SXM2-16GB
- **Exact command to reproduce**:
      bazel test --config=opt --config=cuda  //tensorflow/python:function_test

### Describe the problem
If we run this test without CUDA then its passing, however with CUDA getting failure i.e.  ""array mismatch 0.64697265625%"" error , see below for details : 

### Source code / logs
```
$ bazel test --config=opt --config=cuda //tensorflow/python:function_test

exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
-----------------------------------------------------------------------------
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: Tesla P100-SXM2-16GB
major: 6 minor: 0 memoryClockRate (GHz) 1.4805
pciBusID 0002:01:00.0
Total memory: 15.89GiB
Free memory: 15.61GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x10002b8ea90
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties:
name: Tesla P100-SXM2-16GB
major: 6 minor: 0 memoryClockRate (GHz) 1.4805
pciBusID 0006:01:00.0
Total memory: 15.89GiB
Free memory: 15.61GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0006:01:00.0)
I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 2 visible devices
I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 160 visible devices
I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:
I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>
I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 2 visible devices
I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 160 visible devices
I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform CUDA. Devices:
I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): Tesla P100-SXM2-16GB, Compute Capability 6.0
I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (1): Tesla P100-SXM2-16GB, Compute Capability 6.0
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcupti.so.8.0 locally
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0006:01:00.0)
..I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0006:01:00.0)
..I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0006:01:00.0)
.
.
.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0006:01:00.0)
F.I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0006:01:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0006:01:00.0)
..
======================================================================
FAIL: testUnrollLSTMGrad (__main__.UnrollLSTMTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/amit/.cache/bazel/_bazel_amit/c2069970ba4ea955300413b19a88640d/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/function_test.runfiles/org_tensorflow/tensorflow/python/framework/function_test.py"", line 832, in testUnrollLSTMGrad
    self.assertAllClose(d0, d1, rtol=1e-4)
  File ""/home/amit/.cache/bazel/_bazel_amit/c2069970ba4ea955300413b19a88640d/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/function_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 485, in assertAllClose
    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)
  File ""/usr/lib64/python2.7/site-packages/numpy/testing/utils.py"", line 1411, in assert_allclose
    verbose=verbose, header=header, equal_nan=equal_nan)
  File ""/usr/lib64/python2.7/site-packages/numpy/testing/utils.py"", line 796, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Not equal to tolerance rtol=0.0001, atol=1e-06

(mismatch 0.64697265625%)
 x: array([[[ 4.276238,  0.15792 ,  0.614052, ...,  2.567241,  0.883315,
          0.162565],
        [ 3.256324,  0.069458,  1.132122, ...,  2.211422,  0.491486,...
 y: array([[[ 4.276276,  0.15792 ,  0.614058, ...,  2.567239,  0.883316,
          0.162566],
        [ 3.256359,  0.069458,  1.132127, ...,  2.211414,  0.491488,...

----------------------------------------------------------------------
Ran 41 tests in 14.361s

FAILED (failures=1)
255.971 13.0395
255.971 13.0395
not close where =  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0]), array([ 0,  1,  4,  6,  6,  8,  8,  9, 11, 12, 12, 13, 13, 15, 18, 19, 26,
       26, 26, 28, 28, 29, 29, 30, 30, 31, 31, 32, 33, 34, 34, 34, 39, 41,
       41, 42, 42, 43, 45, 47, 48, 48, 50, 50, 50, 52, 55, 55, 55, 57, 59,
       61, 62]), array([110,  49,  28, 110, 117,  24,  45,  24, 117,  45,  85,  28, 110,
       104,  20,  13,  21,  28, 109,  24, 104, 110, 117,  45, 117, 104,
       110, 106,  23,   0,  77,  87,  13,  28,  49,  53,  89, 110,  49,
        28,  49, 122,  21,  23,  87,  45,  45,  87, 112, 124,  28,   7,  36]))
not close lhs =  [ 0.00964716  0.0108541  -0.02481478  0.01410314 -0.01004539 -0.00598446
 -0.06178713 -0.00234316  0.00250508  0.13677871  0.01347511  0.03004426
 -0.02344483  0.03969495 -0.00342371  0.06425443  0.04404789  0.01697282
  0.00173055  0.00629981 -0.07690701  0.01800087  0.00182568  0.10605431
 -0.00270458  0.01780143  0.00939897 -0.01084067 -0.00755548  0.00419208
 -0.00418841 -0.00287345  0.05488337 -0.03028315  0.00747772 -0.00952677
  0.01471797 -0.00804311  0.01273583  0.00947097  0.00700331 -0.00019016
  0.03132032  0.14848644 -0.00288739 -0.01046085  0.0042536   0.04803397
  0.0140109  -0.03605841 -0.00635294  0.16805029 -0.01475533]
not close rhs =  [ 0.0096446   0.01085714 -0.02482039  0.01410037 -0.01004928 -0.00598631
 -0.06180775 -0.00234506  0.00250115  0.13676071  0.01347163  0.03003997
 -0.02344918  0.03968659 -0.00342231  0.06426588  0.04405451  0.0169698
  0.00172922  0.00629799 -0.07691711  0.01799804  0.00182213  0.1060307
 -0.0027072   0.01779342  0.00939631 -0.01084429 -0.00755821  0.00418651
 -0.00418626 -0.00287483  0.05489143 -0.03027856  0.0074747  -0.00952475
  0.01471417 -0.00804621  0.01273203  0.00946853  0.00700712 -0.00019156
  0.03132723  0.14846958 -0.00288068 -0.01046896  0.00425826  0.04802619
  0.01401426 -0.03605241 -0.00635018  0.16807139 -0.01475281]
not close dif =  [  2.56299973e-06   3.04728746e-06   5.60283661e-06   2.77161598e-06
   3.88920307e-06   1.84774399e-06   2.06232071e-05   1.89989805e-06
   3.93390656e-06   1.80006027e-05   3.47942114e-06   4.29153442e-06
  4.35113907e-06   8.35955143e-06   1.40070915e-06   1.14440918e-05
   6.61611557e-06   3.02493572e-06   1.32620335e-06   1.81794167e-06
   1.01029873e-05   2.83122063e-06   3.54647636e-06   2.36034393e-05
   2.62260437e-06   8.01682472e-06   2.65240669e-06   3.62098217e-06
   2.72691250e-06   5.57303429e-06   2.15321779e-06   1.37463212e-06
   8.05780292e-06   4.58955765e-06   3.01748514e-06   2.02655792e-06
   3.79979610e-06   3.09944153e-06   3.79979610e-06   2.44379044e-06
   3.81469727e-06   1.40070915e-06   6.91413879e-06   1.68532133e-05
   6.71669841e-06   8.10623169e-06   4.66406345e-06   7.78585672e-06
   3.36393714e-06   5.99771738e-06   2.75671482e-06   2.11000443e-05
   2.51457095e-06]
not close tol =  [  1.96445990e-06   2.08571419e-06   3.48203866e-06   2.41003727e-06
   2.00492832e-06   1.59863112e-06   7.18077490e-06   1.23450559e-06
   1.25011445e-06   1.46760713e-05   2.34716254e-06   4.00399631e-06
   3.34491824e-06   4.96865869e-06   1.34223126e-06   7.42658722e-06
   5.40545079e-06   2.69698012e-06   1.17292200e-06   1.62979904e-06
   8.69171163e-06   2.79980395e-06   1.18221283e-06   1.16030706e-05
   1.27071985e-06   2.77934168e-06   1.93963137e-06   2.08442907e-06
   1.75582113e-06   1.41865110e-06   1.41862574e-06   1.28748252e-06
   6.48914238e-06   4.02785645e-06   1.74746981e-06   1.95247480e-06
   2.47141656e-06   1.80462098e-06   2.27320288e-06   1.94685254e-06
   1.70071212e-06   1.01915577e-06   4.13272301e-06   1.58469575e-05
   1.28806778e-06   2.04689604e-06   1.42582599e-06   5.80261849e-06
   2.40142572e-06   4.60524143e-06   1.63501818e-06   1.78071386e-05
   2.47528124e-06]
dtype = float32, shape = (1, 64, 128)
```
Any comments/suggestions ?"
10279,Windows GPU Nightly Build Failures,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

* Windows 10
* With GPU


### Describe the problem

The last stable nightly build for TF on Windows with GPU is currently # 149 and 1+ month old.
Is there a specific blocking issue, why a stable nightly build for this platform is not available for such an extended period?

Thanks for all the hard work!!! As always.

### Source code / logs

n/a
"
10278,Pool Allocator Problem (re-allocation after every batch!),"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.1
- **CUDA/cuDNN version**: 8.0/5.1
- **GPU model and memory**: Tesla K80 16GB

Hi there,
I am running a 2 layer bidirectional LSTM with 128 nodes in each layer, as well as some fully connected layers after. I am training in batch mode (16 inputs/batch) and before training every single batch I see this in the console. It seems to me that pool allocation for fixed batch size and fixed input dimension models should only have to happen once. Instead, as I said, it is happening every single batch. Here is the log: 

2017-05-29 20:46:08.759331: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 15978 get requests, put_count=32386 evicted_count=8000 eviction_rate=0.24702 and unsatisfied allocation rate=0
2017-05-29 20:46:08.963459: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 24574 get requests, put_count=50982 evicted_count=18000 eviction_rate=0.353066 and unsatisfied allocation rate=0
2017-05-29 20:46:09.234499: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 34004 get requests, put_count=70412 evicted_count=28000 eviction_rate=0.397659 and unsatisfied allocation rate=0
2017-05-29 20:46:09.458269: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 43726 get requests, put_count=90134 evicted_count=38000 eviction_rate=0.421595 and unsatisfied allocation rate=0
2017-05-29 20:46:09.684101: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 53450 get requests, put_count=109858 evicted_count=48000 eviction_rate=0.436928 and unsatisfied allocation rate=0
2017-05-29 20:46:09.911030: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 63171 get requests, put_count=129579 evicted_count=58000 eviction_rate=0.447603 and unsatisfied allocation rate=0
2017-05-29 20:46:10.138285: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 72892 get requests, put_count=149300 evicted_count=68000 eviction_rate=0.455459 and unsatisfied allocation rate=0
2017-05-29 20:46:10.365756: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 82614 get requests, put_count=169022 evicted_count=78000 eviction_rate=0.461478 and unsatisfied allocation rate=0
2017-05-29 20:46:10.593120: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 92388 get requests, put_count=188796 evicted_count=88000 eviction_rate=0.466112 and unsatisfied allocation rate=0
2017-05-29 20:46:10.819558: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 102115 get requests, put_count=208523 evicted_count=98000 eviction_rate=0.469972 and unsatisfied allocation rate=0
2017-05-29 20:46:11.045245: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 111836 get requests, put_count=228244 evicted_count=108000 eviction_rate=0.473178 and unsatisfied allocation rate=0
2017-05-29 20:46:11.274408: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 121562 get requests, put_count=247970 evicted_count=118000 eviction_rate=0.475864 and unsatisfied allocation rate=0
2017-05-29 20:46:11.496428: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 130910 get requests, put_count=267318 evicted_count=128000 eviction_rate=0.47883 and unsatisfied allocation rate=0
2017-05-29 20:46:18.634003: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 15314 get requests, put_count=31563 evicted_count=7000 eviction_rate=0.221779 and unsatisfied allocation rate=0
2017-05-29 20:46:18.895088: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 24076 get requests, put_count=50325 evicted_count=17000 eviction_rate=0.337804 and unsatisfied allocation rate=0
2017-05-29 20:46:19.119133: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 33852 get requests, put_count=70101 evicted_count=27000 eviction_rate=0.385159 and unsatisfied allocation rate=0
2017-05-29 20:46:19.343135: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 43576 get requests, put_count=89825 evicted_count=37000 eviction_rate=0.411912 and unsatisfied allocation rate=0
2017-05-29 20:46:19.567368: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 53300 get requests, put_count=109549 evicted_count=47000 eviction_rate=0.429032 and unsatisfied allocation rate=0
2017-05-29 20:46:19.790310: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 63026 get requests, put_count=129275 evicted_count=57000 eviction_rate=0.440921 and unsatisfied allocation rate=0
2017-05-29 20:46:20.012524: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 72746 get requests, put_count=148995 evicted_count=67000 eviction_rate=0.44968 and unsatisfied allocation rate=0
2017-05-29 20:46:20.235646: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 82468 get requests, put_count=168717 evicted_count=77000 eviction_rate=0.456386 and unsatisfied allocation rate=0
2017-05-29 20:46:20.459472: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 92188 get requests, put_count=188437 evicted_count=87000 eviction_rate=0.461693 and unsatisfied allocation rate=0
2017-05-29 20:46:20.685801: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 101917 get requests, put_count=208166 evicted_count=97000 eviction_rate=0.465974 and unsatisfied allocation rate=0
2017-05-29 20:46:20.908996: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 111685 get requests, put_count=227934 evicted_count=107000 eviction_rate=0.469434 and unsatisfied allocation rate=0
2017-05-29 20:46:21.133457: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 121412 get requests, put_count=247661 evicted_count=117000 eviction_rate=0.47242 and unsatisfied allocation rate=0
2017-05-29 20:46:28.415509: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 14283 get requests, put_count=30457 evicted_count=6000 eviction_rate=0.196999 and unsatisfied allocation rate=0
2017-05-29 20:46:28.639884: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 24059 get requests, put_count=50233 evicted_count=16000 eviction_rate=0.318516 and unsatisfied allocation rate=0
2017-05-29 20:46:28.864077: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 33783 get requests, put_count=69957 evicted_count=26000 eviction_rate=0.371657 and unsatisfied allocation rate=0
2017-05-29 20:46:29.089434: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 43507 get requests, put_count=89681 evicted_count=36000 eviction_rate=0.401423 and unsatisfied allocation rate=0
2017-05-29 20:46:29.318965: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 53233 get requests, put_count=109407 evicted_count=46000 eviction_rate=0.420448 and unsatisfied allocation rate=0
2017-05-29 20:46:29.545553: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 62953 get requests, put_count=129127 evicted_count=56000 eviction_rate=0.433682 and unsatisfied allocation rate=0
2017-05-29 20:46:29.772076: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 72675 get requests, put_count=148849 evicted_count=66000 eviction_rate=0.443402 and unsatisfied allocation rate=0
2017-05-29 20:46:29.998109: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 82395 get requests, put_count=168569 evicted_count=76000 eviction_rate=0.450854 and unsatisfied allocation rate=0
2017-05-29 20:46:30.223748: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 92118 get requests, put_count=188292 evicted_count=86000 eviction_rate=0.456737 and unsatisfied allocation rate=0
2017-05-29 20:46:30.451240: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 101892 get requests, put_count=208066 evicted_count=96000 eviction_rate=0.461392 and unsatisfied allocation rate=0
2017-05-29 20:46:30.678667: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 111619 get requests, put_count=227793 evicted_count=106000 eviction_rate=0.465335 and unsatisfied allocation rate=0
2017-05-29 20:46:38.292491: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 15761 get requests, put_count=31952 evicted_count=5000 eviction_rate=0.156485 and unsatisfied allocation rate=0
2017-05-29 20:46:38.517312: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 25488 get requests, put_count=51679 evicted_count=15000 eviction_rate=0.290253 and unsatisfied allocation rate=0
2017-05-29 20:46:38.741779: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 35210 get requests, put_count=71401 evicted_count=25000 eviction_rate=0.350135 and unsatisfied allocation rate=0
2017-05-29 20:46:38.969213: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 44934 get requests, put_count=91125 evicted_count=35000 eviction_rate=0.384088 and unsatisfied allocation rate=0
2017-05-29 20:46:39.199590: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 54655 get requests, put_count=110846 evicted_count=45000 eviction_rate=0.405969 and unsatisfied allocation rate=0
2017-05-29 20:46:39.429868: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 64376 get requests, put_count=130567 evicted_count=55000 eviction_rate=0.42124 and unsatisfied allocation rate=0
2017-05-29 20:46:39.660914: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 74097 get requests, put_count=150288 evicted_count=65000 eviction_rate=0.432503 and unsatisfied allocation rate=0
2017-05-29 20:46:39.895695: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 83872 get requests, put_count=170063 evicted_count=75000 eviction_rate=0.441013 and unsatisfied allocation rate=0
2017-05-29 20:46:40.121575: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 93597 get requests, put_count=189788 evicted_count=85000 eviction_rate=0.447868 and unsatisfied allocation rate=0
2017-05-29 20:46:40.350568: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 103320 get requests, put_count=209511 evicted_count=95000 eviction_rate=0.453437 and unsatisfied allocation rate=0
2017-05-29 20:46:48.175862: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 17797 get requests, put_count=36107 evicted_count=6000 eviction_rate=0.166173 and unsatisfied allocation rate=0
2017-05-29 20:46:48.402920: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 27518 get requests, put_count=55828 evicted_count=16000 eviction_rate=0.286595 and unsatisfied allocation rate=0
2017-05-29 20:46:48.636189: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 37239 get requests, put_count=75549 evicted_count=26000 eviction_rate=0.344148 and unsatisfied allocation rate=0
2017-05-29 20:46:48.868928: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 46961 get requests, put_count=95271 evicted_count=36000 eviction_rate=0.377869 and unsatisfied allocation rate=0
2017-05-29 20:46:49.100911: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 56736 get requests, put_count=115046 evicted_count=46000 eviction_rate=0.39984 and unsatisfied allocation rate=0
2017-05-29 20:46:49.332567: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 66462 get requests, put_count=134772 evicted_count=56000 eviction_rate=0.415517 and unsatisfied allocation rate=0
2017-05-29 20:46:49.563835: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 76185 get requests, put_count=154495 evicted_count=66000 eviction_rate=0.427198 and unsatisfied allocation rate=0
2017-05-29 20:46:49.794439: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 85909 get requests, put_count=174219 evicted_count=76000 eviction_rate=0.436233 and unsatisfied allocation rate=0
2017-05-29 20:46:50.005278: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 94453 get requests, put_count=192763 evicted_count=86000 eviction_rate=0.446144 and unsatisfied allocation rate=0
2017-05-29 20:46:58.209250: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 21897 get requests, put_count=44438 evicted_count=9000 eviction_rate=0.202529 and unsatisfied allocation rate=0
2017-05-29 20:46:58.422662: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 31619 get requests, put_count=64160 evicted_count=19000 eviction_rate=0.296135 and unsatisfied allocation rate=0
2017-05-29 20:46:58.653616: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 41394 get requests, put_count=83935 evicted_count=29000 eviction_rate=0.345505 and unsatisfied allocation rate=0
2017-05-29 20:46:58.886744: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 51121 get requests, put_count=103662 evicted_count=39000 eviction_rate=0.376223 and unsatisfied allocation rate=0
2017-05-29 20:46:59.117713: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 60843 get requests, put_count=123384 evicted_count=49000 eviction_rate=0.397134 and unsatisfied allocation rate=0
2017-05-29 20:46:59.353056: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 70567 get requests, put_count=143108 evicted_count=59000 eviction_rate=0.412276 and unsatisfied allocation rate=0
2017-05-29 20:46:59.582799: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 80287 get requests, put_count=162828 evicted_count=69000 eviction_rate=0.42376 and unsatisfied allocation rate=0
2017-05-29 20:47:07.939562: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 19361 get requests, put_count=39256 evicted_count=5000 eviction_rate=0.127369 and unsatisfied allocation rate=0
2017-05-29 20:47:08.175305: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 29088 get requests, put_count=58983 evicted_count=15000 eviction_rate=0.254311 and unsatisfied allocation rate=0
2017-05-29 20:47:08.408822: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 38810 get requests, put_count=78705 evicted_count=25000 eviction_rate=0.317642 and unsatisfied allocation rate=0
2017-05-29 20:47:08.641838: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 48534 get requests, put_count=98429 evicted_count=35000 eviction_rate=0.355586 and unsatisfied allocation rate=0
2017-05-29 20:47:08.874378: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 58255 get requests, put_count=118150 evicted_count=45000 eviction_rate=0.380872 and unsatisfied allocation rate=0
2017-05-29 20:47:09.102372: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 67976 get requests, put_count=137871 evicted_count=55000 eviction_rate=0.398924 and unsatisfied allocation rate=0
2017-05-29 20:47:18.179298: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 21753 get requests, put_count=44138 evicted_count=6000 eviction_rate=0.135937 and unsatisfied allocation rate=0
2017-05-29 20:47:18.407809: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 31473 get requests, put_count=63858 evicted_count=16000 eviction_rate=0.250556 and unsatisfied allocation rate=0
2017-05-29 20:47:18.642316: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 41196 get requests, put_count=83581 evicted_count=26000 eviction_rate=0.311075 and unsatisfied allocation rate=0
2017-05-29 20:47:18.873651: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 50970 get requests, put_count=103355 evicted_count=36000 eviction_rate=0.348314 and unsatisfied allocation rate=0
2017-05-29 20:47:28.081378: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 20448 get requests, put_count=41471 evicted_count=3000 eviction_rate=0.0723397 and unsatisfied allocation rate=0
2017-05-29 20:47:28.314172: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 30170 get requests, put_count=61193 evicted_count=13000 eviction_rate=0.212443 and unsatisfied allocation rate=0
2017-05-29 20:47:28.517176: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 38482 get requests, put_count=79505 evicted_count=23000 eviction_rate=0.28929 and unsatisfied allocation rate=0
---------------------------------------

As a result the model training is painfully slow. 

As an additional note: the GPU is only being utilized about 60% during training. There doesn't seem to be a bottleneck on CPU because we have 4 cores and they are 70% idle.

Thank you so much for your help.

Best,
Dylan
"
10277,Android demo app crash on x86 device(libavcodec.so text relocations),"The app start then,  complain about libavcodec.so text relocations
![screenshot_20170529-122626](https://cloud.githubusercontent.com/assets/4120796/26559275/f5468b90-446b-11e7-85bb-fb1eadcbf691.png)
here is the log:

`05-29 12:33:36.787 12014-12014/org.tensorflow.demo E/WindowManager: android.view.WindowLeaked: Activity org.tensorflow.demo.ClassifierActivity has leaked window DecorView@34ca6d3[] that was originally added here
                                                                        at android.view.ViewRootImpl.<init>(ViewRootImpl.java:418)
                                                                        at android.view.WindowManagerGlobal.addView(WindowManagerGlobal.java:331)
                                                                        at android.view.WindowManagerImpl.addView(WindowManagerImpl.java:94)
                                                                        at android.app.Dialog.show(Dialog.java:329)
                                                                        at android.app.AlertDialog$Builder.show(AlertDialog.java:1112)
                                                                        at android.app.Activity.performStart(Activity.java:6723)
                                                                        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2662)
                                                                        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2766)
                                                                        at android.app.ActivityThread.-wrap12(ActivityThread.java)
                                                                        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1507)
                                                                        at android.os.Handler.dispatchMessage(Handler.java:102)
                                                                        at android.os.Looper.loop(Looper.java:154)
                                                                        at android.app.ActivityThread.main(ActivityThread.java:6236)
                                                                        at java.lang.reflect.Method.invoke(Native Method)
                                                                        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:891)
                                                                        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:781)
05-29 12:33:36.864 12014-12014/org.tensorflow.demo E/Surface: dequeueBuffer failed (No such device)
05-29 12:33:36.865 12014-12014/org.tensorflow.demo E/ViewRootImpl[ClassifierActivity]: Could not lock surface
                                                                                       java.lang.IllegalArgumentException
                                                                                           at android.view.Surface.nativeLockCanvas(Native Method)
                                                                                           at android.view.Surface.lockCanvas(Surface.java:310)
                                                                                           at android.view.ViewRootImpl.drawSoftware(ViewRootImpl.java:2853)
                                                                                           at android.view.ViewRootImpl.draw(ViewRootImpl.java:2827)
                                                                                           at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2608)
                                                                                           at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2215)
                                                                                           at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1254)
                                                                                           at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:6344)
                                                                                           at android.view.Choreographer$CallbackRecord.run(Choreographer.java:874)
                                                                                           at android.view.Choreographer.doCallbacks(Choreographer.java:686)
                                                                                           at android.view.Choreographer.doFrame(Choreographer.java:621)
                                                                                           at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:860)
                                                                                           at android.os.Handler.handleCallback(Handler.java:751)
                                                                                           at android.os.Handler.dispatchMessage(Handler.java:95)
                                                                                           at android.os.Looper.loop(Looper.java:154)
                                                                                           at android.app.ActivityThread.main(ActivityThread.java:6236)
                                                                                           at java.lang.reflect.Method.invoke(Native Method)
                                                                                           at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:891)
                                                                                           at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:781)
`

The device is an Zenfone 2 with an intel atom processor, is it supported?"
10275,TF 1.2.0-rc1: Build with MKL failed,"Hi!
I just tried building TF 1.2.0-rc1 with MKL, using:

    bazel build --config=opt --config=cuda --config=mkl --copt=""-DEIGEN_USE_VML"" -c opt //tensorflow/tools/pip_package:build_pip_package

Where, in the `'./configure` step I said ""yes"" to both using MKL and to downloading it from the web. I get the following error:

    ERROR: /dataSSD/untom/tmp/tensorflow-1.2.0-rc1/tensorflow/core/BUILD:1544:1: undeclared inclusion(s) in rule '//tensorflow/core:core_cpu_base':
    this rule is missing dependency declarations for the following files included by 'tensorflow/core/graph/mkl_tfconversion_pass.cc':
      '/dataSSD/untom/tmp/tensorflow-1.2.0-rc1/tensorflow/core/common_runtime/function.h'
      '/dataSSD/untom/tmp/tensorflow-1.2.0-rc1/tensorflow/core/common_runtime/device_mgr.h'
      '/dataSSD/untom/tmp/tensorflow-1.2.0-rc1/tensorflow/core/common_runtime/optimization_registry.h'
      '/dataSSD/untom/tmp/tensorflow-1.2.0-rc1/tensorflow/core/common_runtime/device_set.h'.
    Target //tensorflow/tools/pip_package:build_pip_package failed to build
    Use --verbose_failures to see the command lines of failed build steps.


EDIT: I used bazel 0.4.5 and Python 3.6.1
"
10273,Android build error: tensorflow/core/framework/op_gen_lib.h:22:59: fatal error,"Hello,

I previously built (some earlier releases of tensorflow) succesfully for android platform (aarch64). However, I get this  following error ""./tensorflow/core/framework/op_gen_lib.h:22:59: fatal error: tensorflow/core/framework/op_gen_overrides.pb.h: No such file or directory
 #include ""tensorflow/core/framework/op_gen_overrides.pb.h""
"" incase of the latest release.

I did run ""download_dependencies.sh"", ""compile_android_protobuf.sh"" prior to the command ""make -f ./tensorflow/contrib/makefile/Makefile TARGET=ANDROID""

There is no file called ""op_gen_overrides.pb.h""!"
10270,Performance degradation with large lookup tables - optimizer._apply_sparse_duplicate_indices  (TF V1.0.1),"Hi,

I ran into this performance issue while trying to upgrade tensorflow from version 0.12.1  to 1.X.

We ran a network with large embedding lookup tables:
- 100K X 32 (for example, word embedding -  with 100K unique words)
- 300K X 128 (for example, categorical feature with cardinality of 300K unique items)

 After upgrading TF version to 1.0.1,  GPU usage dropped in from 60% to 30%.
Training time went up in 50%-200% (depends on how big is the embedding lookup table). 


This is the commit that caused the performance degradation:
https://github.com/tensorflow/tensorflow/commit/f9f56f9dc7fe41ef1128290a77ac88e889ea5229

The handling of unique indexes is very slow and does not run in parallel with others operations. 
Please note the big unique blocks in the middle.
![trace_unique](https://cloud.githubusercontent.com/assets/8734262/26542969/ab0f3740-4464-11e7-9dcb-f3ccd58dfc8a.png)

Here is a work around (not handling unique indexes ):
```
class MyOptimizer(tf.train.AdamOptimizer):
        def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8,
               use_locking=False, name=""Adam""):
                super(MyOptimizer,self).__init__(learning_rate,beta1, beta2, epsilon, use_locking,name)

        def _apply_sparse_duplicate_indices(self, grad, var):
                return self._apply_sparse(grad, var)
```


Thanks,
Erez
"
10269,Applying dropout in Tensorflow C++ API,"I have retrained Inception-v3 model using (Tensorflow) Python API and saved a standalone Graph in .pb form. I have also used a dropout layer before the final layer. I can successfully run inference on the graph in python. The code to generate predictions in python is as follows:

    softmax_tensor = sess.graph.get_tensor_by_name('final_layer/final_result/Softmax:0')
    predictions = sess.run(softmax_tensor, { 'DecodeJpeg/contents:0': image_data, 'final_layer/dropout/Placeholder:0': 1.})

The C++ counterpart of the python code is as follows:

    string input_layer = ""Mul""; 
    string output_layer = ""final_layer/dropout/Placeholder:0"";
    Status run_status = session->Run({{input_layer, resized_tensor}}, {output_layer}, {}, &outputs);

The C++ code ends up with the following error message:

`Running model failed: Invalid argument: You must feed a value for placeholder tensor 'final_layer/dropout/Placeholder'`

What should I change in the above C++ code to remove this error? In other words, how do I change a placeholder value in the C++ code as in python code."
10268,sparse_softmax_cross_entropy_with_logits gives NaN instead of error when using non-existent labels,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 14.04

- **TensorFlow installed from (source or binary)**:
Source

- **TensorFlow version (use command below)**:
v1.0.0-65-g4763edf-dirty
1.0.1

- **Bazel version (if compiling from source)**:
0.4.5

- **CUDA/cuDNN version**:
V8.0.61

- **GPU model and memory**:
NVIDIA GFORCE GTX 760 2GB

- **Exact command to reproduce**:
import tensorflow as tf
sess = tf.Session(0
sess.run(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=[ 100 ], logits=[[ 0.0, 1.0 ]]))
>>> array([ nan], dtype=float32)

### Describe the problem
Running above code gives NaN instead of an error when on Ubuntu but when I run the same code on Windows I correctly get an InvalidArgumentError error.

### Source code / logs
See code above."
10267,Unnecessary label division at tf.nn.nce_loss?,"As far as I know, NCE Loss is a sampling-based loss that converts large-scale multiclass loss into a sum of binary loss for sampled classes. Each binary classification infers a probability where the given context and word is from the proxy corpus (real distribution) or noise distribution. Therefore, I guess each binary classification of word should be hard binary classification with label 0.0 or 1.0.

However, the documentation and the implementation of `tf.nn.nce_loss` (https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss, https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L1044) indicates that *the target probability is assigned to `1 / num_true` to make target probabilities sum to 1*. Implementation of `tf.nn.nce_loss` uses the helper function `_compute_sampled_logits` in same file, which always returns true label divided by the number of true examples (Line 1044).

Is this division necessary for NCE Loss? Isn't the label of `<1.0` for positive examples generates the unnecessary opposite-direction loss `(1-y)log(1-y')`? Is there any other reason that I missed for this label division?
"
10266,QuantizedConv2D (inference) on iOS,"Hello,

I had some question and it would be nice to get quick help/reply.

I know, that accelerate framework (cblass_sgemm) is used for Conv operation (gemm_functors.h) incase of iOS platform. However, that is for float datatype. 

1) What happens incase of quantized op of inception5h graph. For example, which library is used for QuantizedConv2D operation incase of iOS mobile platform?
2) Is Accelerate framework is used also for QuantizedConv2D operation? If so, then is the comptuation actually happens for float datatype and not 8bit mult and 32bit accumulation?

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
10265,Possible Bug: tensorflow.cholesky_solve,"I wrote a script to compare the output of a very simple linear system with simple matrix inversion a la [tensorflow.matrix_inverse](https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops/matrix_math_functions#matrix_inverse), the non-cholesky based matrix equation solver [tensorflow.matrix_solve](https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops/matrix_math_functions#matrix_solve), and [tensorflow.cholesky_solve](https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops/matrix_math_functions#cholesky_solve).

According to my understanding of the docs I've linked, these three cases should all yield a solution of the `I/2`, but this is not the case for `tensorflow.cholesky_solve`. Perhaps I'm misunderstanding the docs? 

    import tensorflow as tf
    
    I = tf.eye(2, dtype=tf.float32)
    X = 2 * tf.eye(2, dtype=tf.float32)
    X_inv = tf.matrix_inverse(X)
    X_solve = tf.matrix_solve(X, I)
    X_chol_solve = tf.cholesky_solve(tf.cholesky(X), I)
    
    with tf.Session() as sess:
        for x in [X_inv, X_solve, X_chol_solve]:
            print('{}:\n{}'.format(x.name, sess.run(x)))
            print

yielding output:

    MatrixInverse:0:
    [[ 0.5  0. ]
     [ 0.   0.5]]
    
    MatrixSolve:0:
    [[ 0.5  0. ]
     [ 0.   0.5]]
    
    cholesky_solve/MatrixTriangularSolve_1:0:
    [[ 1.  0.]
     [ 0.  1.]]    
    
    Process finished with exit code 0

### System information
- **OS: Ubuntu 16.04 xenial**
- **Kernel: x86_64 Linux 4.8.0-52-generic**
- **TensorFlow installed from binary**
- **TensorFlow version ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')**
- **Cuda compilation tools, release 8.0, V8.0.61**
- **GTX 1070 8GB**



"
10263,Looks like GPU Mac builds are no longer building,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

404 error on Mac GPU download 

Link: https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.2.0rc0-py3-none-any.whl

Error:
HTTP ERROR 404

Problem accessing /view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.2.0rc0-py3-none-any.whl. Reason:

Not Found




### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
10262,tensorflow read tfrecord not synchronize,"i would like to read tfrecords with two feats, but when i read it from tfrecords, it not synchronize. my data is like

    a a_1
    b b_1
    c c_1
    d d_1
    e e_1
    f f_1
    g g_1
my code write this file to tfrecord is like this:


    import numpy as np
    import tensorflow as tf
    import sys,os
    
    def _int64_feature(value):
      return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))
    
    def _bytes_feature(value):
      return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
    def float32_feature(value):
      return tf.train.Feature(float_list=tf.train.FloatList(value=value))
    
    def _parse_line(line, writer):
      l = line.rstrip().split()
      feat1=l[0].strip()
      feat2=l[1].strip()
    
      example = tf.train.Example(features=tf.train.Features(feature={
          'feat1': _bytes_feature(feat1),
          'feat2': _bytes_feature(feat2)
        }))
      writer.write(example.SerializeToString())
    
    
    def convert_to(feat_file,output_file):
        f = open(feat_file).readlines()
        writer = tf.python_io.TFRecordWriter(output_file)
        for line in f:
            _parse_line(line, writer)
        return
    
    def main(argv):
        convert_to(sys.argv[1],sys.argv[2])
        
    
    if __name__ == '__main__':
        tf.app.run()
        pass

my code is train.py is like this:

    import tensorflow as tf
    import numpy as np
    import sys,os
    def read_and_decode(filename_queue):
      reader = tf.TFRecordReader()
      _, serialized_example = reader.read(filename_queue)
      features = tf.parse_single_example(
          serialized_example,
          features={
              'feat1': tf.FixedLenFeature([], tf.string),
              'feat2': tf.FixedLenFeature([], tf.string)
          })
      feat1=features['feat1']
      feat2=features['feat2']
      return feat1,feat2
    
    def batch_inputs():
        tf_record_pattern = os.path.join('./', '%s*' % 'record')
        data_files = tf.gfile.Glob(tf_record_pattern)
        print data_files
        filename_queue = tf.train.string_input_producer(data_files, num_epochs=1,shuffle=True)
        feat1,feat2 = read_and_decode(filename_queue)
        feats1,feats2 = tf.train.shuffle_batch([feat1,feat2],batch_size=1, num_threads=1,capacity=1090,min_after_dequeue=1000)
        return feats1,feats2
    with tf.Session() as sess:
        feat1,feat2=batch_inputs()
        init = tf.group(tf.global_variables_initializer(),
    	       tf.local_variables_initializer())
        sess.run(init)
        coord = tf.train.Coordinator()  
        threads = tf.train.start_queue_runners(sess=sess, coord=coord)
        print sess.run(feat1)
        print sess.run(feat2)

when i run this train.py. it outputs 
['b']
['d_1']
which i suppose  it should output,the feat1 is always corresponding feat2 like
['b']
['b_1']"
10260,Image and label decode from TFRecord are synchronized.,"The data I read from TFRecords has a problem, the image and label are synchronized. In other words, what we want is [a---1, b---2, c---3, d---4, e---5], but what I got is [a---2, b---3, c---4, d---5, e---1]. The version of TensorFlow I used is 1.1.0. Can someone help me? thanks a lot. Here is my code:

```
import tensorflow as tf

images = ['a', 'b', 'c', 'd', 'e']
labels = [1, 2, 3, 4, 5]


def convert_to_tfrecords(_images, _labels):
    path = 'test.tfrecords'
    writer = tf.python_io.TFRecordWriter(path)
    for [i, l] in zip(_images, _labels):
        example = tf.train.Example(features=tf.train.Features(feature={
            'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(i)])),
            'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[l]))
        }))
        writer.write(example.SerializeToString())
    writer.close()


def read_and_decode(filename_queue):
    reader = tf.TFRecordReader()
    _, serialized_example = reader.read(filename_queue)
    features = tf.parse_single_example(serialized_example, features={
        'image': tf.FixedLenFeature([], tf.string),
        'label': tf.FixedLenFeature([], tf.int64)
    })
    _image = tf.cast(features['image'], tf.string)
    _label = tf.cast(features['label'], tf.int64)
    return _image, _label


def main():
    convert_to_tfrecords(images, labels)
    filename = 'test.tfrecords'
    filename_queue = tf.train.string_input_producer([filename])
    img, lab = read_and_decode(filename_queue)
    with tf.Session() as sess:
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess=sess, coord=coord)
        for _ in range(5):
            print(sess.run(img), '---', sess.run(lab))
        coord.request_stop()
        coord.join(threads)


if __name__ == '__main__':
    main()
```"
10259,Retrain.py try download remote archive ,"I want manually set input directory, but retrain.py downloading remote archive. Why so?

`python retrain.py --image_dir=D:\123\flower_photos\`"
10258,Compilation error: image_ops_gpu.cu.pic.o was not created.,"The issue is similar to https://stackoverflow.com/questions/44116381/error-when-install-tensorflow-from-source
/cc @drpngx 
```
1 error detected in the compilation of ""/tmp/tmpxft_00004364_00000000-7_image_ops_gpu.cu.cpp1.ii"".
 ERROR:PATH/tensorflow_cuda75/tensorflow/contrib/image/BUILD:20:1: output 'tensorflow/contrib/image/_objs/python/ops/_image_ops_gpu/tensorflow/contrib/image/kernels/image_ops_gpu.cu.pic.o' was not created.
```

```
ERROR: PATH/tensorflow_cuda75/tensorflow/contrib/image/BUILD:20:1: 
not all outputs were created or valid.`
```
Selective logs before error:
```
 ./tensorflow/contrib/image/kernels/image_ops.h(69): error: Within a __device__/__global__ function, only __shared__ variables may be marked ""static""
          detected during:                                                                                                                                                 instantiation of ""Eigen::TensorEvaluator<const Eigen::TensorGeneratorOp<Generator, ArgType>, Device>::CoeffReturnType Eigen::TensorEvaluator<const Eigen::TensorGeneratorOp<Generator, ArgType>, Device>::coeff(Eigen::TensorEvaluator<const Eigen::TensorGeneratorOp<Generator, ArgType>, Device>::Index) const [with Generator=tensorflow::generator::ProjectiveGenerator<tensorflow::functor::GPUDevice, tensorflow::uint8>, ArgType=const Eigen::TensorMap<Eigen::Tensor<cons
t tensorflow::uint8, 4, 1, Eigen::DenseIndex>, 16, Eigen::MakePointer>, Device=Eigen::GpuDevice]""
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): here                                                                                        instantiation of ""void Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LeftArgType, RightArgType>, Device>::evalScalar(Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LeftArgType, RightArgType>, Device>::Index) [with LeftArgType=Eigen::TensorMap<Eigen::Tensor<tensorflow::uint8, 4, 1, Eigen::DenseInd
ex>, 16, Eigen::MakePointer>, RightArgType=const Eigen::TensorGeneratorOp<tensorflow::generator::ProjectiveGenerator<tensorflow::functor::GPUDevice, tensorflow
::uint8>, const Eigen::TensorMap<Eigen::Tensor<const tensorflow::uint8, 4, 1, Eigen::DenseIndex>, 16, Eigen::MakePointer>>, Device=Eigen::GpuDevice]""
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h(210): here


...... omit....

           instantiation of ""void Eigen::internal::EigenMetaKernelEval<Evaluator, Index, Vectorizable>::run(Evaluator &, Index, Index, Index) [with Evaluator=Eigen::TensorEvaluator<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<double, 4, 1, Eigen::DenseIndex>, 16, Eigen::MakePointer>, const Eigen::TensorGeneratorOp<tensorflow::generator::ProjectiveGenerator<tensorflow::functor::GPUDevice, double>, c
onst Eigen::TensorMap<Eigen::Tensor<const double, 4, 1, Eigen::DenseIndex>, 16, Eigen::MakePointer>>>, E
igen::GpuDevice>, Index=Eigen::DenseIndex, Vectorizable=false]""                                         external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h(243): here                               instantiation of ""void Eigen::internal::EigenMetaKernel(Evaluator, Index) [with Evaluator=Eigen::TensorEvaluator<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<double, 4, 1, Eigen::DenseIndex>, 16, Eigen::MakePointer>, const Eigen::TensorGeneratorOp<tensorflow::generator::ProjectiveGenerator<tensorflow::functor::GPUDevice, double>, const Eigen::TensorMap<Eigen::Tensor<const double, 4, 1, E
igen::DenseIndex>, 16, Eigen::MakePointer>>>, Eigen::GpuDevice>, Index=Eigen::DenseIndex]""
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h(260): here                               instantiation of ""void Eigen::internal::TensorExecutor<Expression, Eigen::GpuDevice, Vectorizable>::run(const Expression &, const Eigen::GpuDevice &) [with Expression=const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<double, 4, 1, Eigen::DenseIndex>, 16, Eigen::MakePointer>, const Eigen::Te
nsorGeneratorOp<tensorflow::generator::ProjectiveGenerator<tensorflow::functor::GPUDevice, double>, cons
t Eigen::TensorMap<Eigen::Tensor<const double, 4, 1, Eigen::DenseIndex>, 16, Eigen::MakePointer>>>, Vect
orizable=false]""
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h(35): here                                  instantiation of ""Eigen::TensorDevice<ExpressionType, DeviceType> &Eigen::TensorDevice<ExpressionType, DeviceType>::operator=(const OtherDerived &) [with ExpressionType=Eigen::TensorMap<Eigen::Ten
sor<double, 4, 1, Eigen::DenseIndex>, 16, Eigen::MakePointer>, DeviceType=tensorflow::functor::GPUDevice, OtherDerived=Eigen::TensorGeneratorOp<tensorflow::generator::ProjectiveGenerator<tensorflow::functor::
GPUDevice, double>, const Eigen::TensorMap<Eigen::Tensor<const double, 4, 1, Eigen::DenseIndex>, 16, Eig
en::MakePointer>>]""
(156): here
            instantiation of ""void tensorflow::functor::FillProjectiveTransform<Device, T>::operator()(c
onst Device &, tensorflow::functor::FillProjectiveTransform<Device, T>::OutputType *, const tensorflow::
functor::FillProjectiveTransform<Device, T>::InputType &, const tensorflow::functor::FillProjectiveTrans
form<Device, T>::TransformsType &) const [with Device=tensorflow::functor::GPUDevice, T=double]""
tensorflow/contrib/image/kernels/image_ops_gpu.cu.cc(36): here
``` 


Environment: centos 6.8  devtoolset-3 cuda 7.5 cudnn v4. gcc-4.9 bazel 4.5 TF r1.2


Any advice and suggestions will be appreciated!"
10257,Error during inference: Invalid argument: No OpKernel was registered to support Op 'DecodeJpeg' with these attrs,"I got this error message when I ran the app. It crash just a few seconds after app launched.

```
05-28 09:05:29.791 15453-15472/my.intellij.androidtensorflowbirdexample A/native: tensorflow_jni.cc:304 Error during inference: Invalid argument: No OpKernel was registered to support Op 'DecodeJpeg' with these attrs
                                                                                  	 [[Node: DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method="""", fancy_upscaling=true, ratio=1, try_recover_truncated=false](DecodeJpeg/contents)]]
05-28 09:05:29.791 15453-15472/my.intellij.androidtensorflowbirdexample A/libc: Fatal signal 6 (SIGABRT), code -6 in tid 15472 (InferenceThread)
                                                                                
                                                                                [ 05-28 09:05:29.796 15453:15503 E/         ]
                                                                                [android_ws] Format: 5, Width: 1080, Height: 1620
                                                                                
                                                                                
                                                                                [ 05-28 09:05:29.796 15453:15503 E/         ]
                                                                                [android_ws] Format: 5, Width: 1080, Height: 1620
```

Please advice. Thank you."
10256,string_input_producer possible race while enqueuing,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows-7-6.1.7601-SP1
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: b'unknown' 1.0.1

Python version `'3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)]'
`

### Describe the problem
When I run the script below in pycharm _most of the time_ (hence race) I get 

> ERROR:tensorflow:Exception in QueueRunner: Attempted to use a closed Session.

Running directly from cmd (that's the same command pycharm uses):

```
C:\...>C:\_\Python35\python.exe C:/Users/MrD/.PyCharm2017.1/config/scratches/scratch_40.py
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') f
or unknown op: BestSplits
...
b'C:\\Dropbox\\eclipse_workspaces\\python\\nn_nielsen\\resources\\tf_records_gap\\img_2013-01-01-00-02.tfrecords'
b'C:\\Dropbox\\eclipse_workspaces\\python\\nn_nielsen\\resources\\tf_records_gap\\img_2013-01-01-00-01.tfrecords'
b'C:\\Dropbox\\eclipse_workspaces\\python\\nn_nielsen\\resources\\tf_records_gap\\img_2013-01-01-00-00.tfrecords'
W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\kernels\queue_base.cc:294] _0_input_producer: Skipping cancelled enqueue attem
pt with queue not closed
```

### Source code / logs

```python
import os
import tensorflow as tf

data_dir = r""C:\...""
filenames = [os.path.join(data_dir, f) for f in os.listdir(data_dir)]
queue = tf.train.string_input_producer(filenames)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    tf.train.start_queue_runners(sess=sess)
    for _ in range(len(filenames)):
        print(queue.dequeue().eval())
```

Running this as a test inside pycharm (using PyCharm 2017.1.3, Build #PY-171.4424.42) as in:

```python

class TestFileNameQueue(unittest.TestCase):
    def test__get_filename_queue(self):
        data_dir = r""C:\...""
        filenames = [os.path.join(data_dir, f) for f in os.listdir(data_dir)]
        queue = tf.train.string_input_producer(filenames)
        with tf.Session() as sess:
            sess.run(tf.global_variables_initializer())
            tf.train.start_queue_runners(sess=sess)
            for _ in range(len(filenames)):
                print(queue.dequeue().eval())
```

I get a more detailed traceback:

```
C:\_\Python35\python.exe ""C:\_\JetBrains\PyCharm 2016.3\helpers\pydev\pydevd.py"" --multiproc --save-signatures --qt-support --client 127.0.0.1 --port 4524 --file ""C:\_\JetBrains\PyCharm 2016.3\helpers\pycharm\_jb_unittest_runner.py"" --target tf_test_sanity.TestFileNameQueue.test__get_filename_queue
Testing started at 11:41 PM ...
warning: Debugger speedups using cython not found. Run '""C:\_\Python35\python.exe"" ""C:\_\JetBrains\PyCharm 2016.3\helpers\pydev\setup_cython.py"" build_ext --inplace' to build.
pydev debugger: process 6148 is connecting

Connected to pydev debugger (build 171.4424.42)
Launching unittests with arguments python -m unittest tf_test_sanity.TestFileNameQueue.test__get_filename_queue in C:\Dropbox\eclipse_workspaces\python\nn_nielsen
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots
ERROR:tensorflow:Exception in QueueRunner: Attempted to use a closed Session.


b'resources/tf_records_no_gap\\img_2013-01-01-00-00.tfrecords'
b'resources/tf_records_no_gap\\img_2013-01-01-00-02.tfrecords'
b'resources/tf_records_no_gap\\img_2013-01-01-00-01.tfrecords'


Ran 1 test in 255.837s

OK
Exception in thread Thread-7:
Traceback (most recent call last):
  File ""C:\_\Python35\lib\threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""C:\_\Python35\lib\threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""C:\_\Python35\lib\site-packages\tensorflow\python\training\queue_runner_impl.py"", line 234, in _run
    sess.run(enqueue_op)
  File ""C:\_\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 767, in run
    run_metadata_ptr)
  File ""C:\_\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 903, in _run
    raise RuntimeError('Attempted to use a closed Session.')
RuntimeError: Attempted to use a closed Session.


Process finished with exit code 0
```"
10254,custom Android op configuration,"Why does Android lack support for so many ops (especially quantized/quantization)?
Although some of them get added after a while, this really gets into the way...

I know that the background is to lower the binary size...
But wouldn't it be possible to compile specifically for a given graph definition?

This could be offered in parallel to the current solution and provide even more fine-grained control."
10253,label_keys type error on DNNCLassifier Tensorflow,"I got the following error when I tried to embed an array of label_keys of type string into a DNNClassifier. At the first attempt I folowed the official documentation for instantiating a DNNClassifier.

`Traceback (most recent call last):
  File ""embedding_model_probe.py"", line 118, in <module>
    m.fit(input_fn=train_input_fn, steps=200)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 281, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 430, in fit
    loss = self._train_model(input_fn=input_fn, hooks=hooks)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 927, in _train_model
    model_fn_ops = self._get_train_ops(features, labels)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1132, in _get_train_ops
    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1103, in _call_model_fn
    model_fn_results = self._model_fn(features, labels, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py"", line 180, in _dnn_model_fn
    logits=logits)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py"", line 1004, in create_model_fn_ops
    labels = self._transform_labels(mode=mode, labels=labels)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py"", line 1033, in _transform_labels
    ""label_ids"": table.lookup(labels_tensor),
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lookup/lookup_ops.py"", line 179, in lookup
    (self._key_dtype, keys.dtype))
TypeError: Signature mismatch. Keys must be dtype <dtype: 'string'>, got <dtype: 'int64'>.
`

Here is the ""guilty"" piece of my code:

`  label_keys_values = [""satan"", ""ipsweep"", ""nmap"", ""portsweep""]    `
`  m = tf.contrib.learn.DNNClassifier(model_dir=model_dir,
	                    feature_columns=deep_columns,
	                    n_classes=4,
	                   hidden_units=[12, 4],
	                   label_keys=label_keys_values)`
`  m.fit(input_fn=train_input_fn, steps=200)`

On the other hand, if I make the `label_key_values` column a `numpy.array`
`label_keys_values = np.array([""satan"", ""ipsweep"", ""nmap"", ""portsweep""], dtype='string')`

I will get the following error:

`Traceback (most recent call last): File ""embedding_model_probe.py"", line 116, in label_keys=label_keys_values) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py"", line 337, in init label_keys=label_keys), File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py"", line 331, in multi_class_head label_keys=label_keys) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py"", line 986, in init if label_keys and len(label_keys) != n_classes: ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()`


"
10252,Feature Request:   Tensorflow for Python 3.6 ,"
Hi,

I was wondering if there are plans to release tensorflow for Python 3.6  and if so is there ETA?


Thank you. "
10251,Support for custom input producers,"This is a request for custom functions to feed examples into input pipelines. Existing ops are designed to read pre-defined formats. Although it is entirely possible to pre-process all the data beforehand, I felt like it shouldn't be necessary, especially in realtime environments. It'd be more convenient to pass TensorFlow a lambda to use to generate the batches instead of pre-processing them into TFRecords or dealing with nasty IO or using the not scalable option of python dicts.

[Please tell me if there already exists such a solution, I didn't find a way to do it.]"
10250,Error on importing metagraph that uses unbound input multiple times,"If I export a scoped metagraph which has multiple references to a tensor which will be unbound after exporting, I cannot re-import the metagraph. This problem can be reproduced with the following MWE.

```python
import tensorflow as tf

graph = tf.Graph()

with graph.as_default():
    inputs = tf.placeholder(shape=[], dtype=tf.float32, name=""inputs"")

    with tf.name_scope(""scope""):
        output = inputs + inputs

    tf.train.export_meta_graph(""./mwe.meta"",
                               export_scope=""scope"",
                               as_text=True)

graph = tf.Graph()

with graph.as_default():
    inputs = tf.constant(shape=[], value=1.0)

    tf.train.import_meta_graph(""./mwe.meta"",
                               import_scope=""scope"",
                               input_map={
                                   ""$unbound_inputs_inputs"": inputs
                               })

```
Running this code results in the following error.
```
Traceback (most recent call last):
  File ""<snip>\metagraph_bug_mwe.py"", line 23, in <module>
    ""$unbound_inputs_inputs"": inputs
  File ""<snip>\Python35\lib\site-packages\tensorflow\python\training\saver.py"", line 1595, in import_meta_graph
    **kwargs)
  File ""<snip>\Python35\lib\site-packages\tensorflow\python\framework\meta_graph.py"", line 479, in import_scoped_meta_graph
    "","".join([compat.as_str(v) for v in field.value
ValueError: Graph contains unbound inputs: $unbound_inputs_inputs,$unbound_inputs_inputs. Must provide these inputs through input_map.
```
Upon inspection of the generated metagraph file (see [mwe.meta](https://github.com/tensorflow/tensorflow/files/1033570/mwe.meta.txt), lines 67 - 75), I noticed that the `inputs` tensor actually has two entries in the `unbound_inputs` collection. Since the `import_scoped_meta_graph` function compares the full collection read from the proto to the `input_map` parameter, they do not match and the error is raised.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see above
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.1.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8.0/5.1.10
- **GPU model and memory**: Nvidia GeForce GTX 860M with 2GB VRAM + 4GB shared
- **Exact command to reproduce**: See MWE above

"
10247,Error arising when import tensorflow ,"alpine 3.5 python 2.7
Installing tensorflow using pip succesfully, but after `import` tensorflow` I got this

> ImportError: Error relocating /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal
.so: __sprintf_chk: symbol not found


"
10245,Implement Focused Online Learning which converges faster than SGD,"![image](https://cloud.githubusercontent.com/assets/9004594/26520117/ec115938-42fe-11e7-9350-8da77dc6488e.png)
Shai Shalev-Shwartz and Yonatan Wexler. Minimizing the Maximal Loss: How and Why?. ICML, 2016.
http://proceedings.mlr.press/v48/shalev-shwartzb16.pdf
http://arxiv.org/abs/1602.01690
https://www.cs.huji.ac.il/~shais/talks/FOL_talk.pdf"
10243,"UnimplementedError, if only a word as the input data","When the input tensor only contain a word, the program will raise the UnimplementedError.

`import numpy as np
import tensorflow as tf

# Data settings.
num_examples = 10
num_words = 1
num_features = 100
num_tags = 5

# Random features.
x = np.random.rand(num_examples, num_words, num_features).astype(np.float32)

# Random tag indices representing the gold sequence.
y = np.random.randint(num_tags, size=[num_examples, num_words]).astype(np.int32)

# All sequences in this example have the same length, but they can be variable in a real model.
sequence_lengths = np.full(num_examples, num_words - 1, dtype=np.int32)

# Train and evaluate the model.
with tf.Graph().as_default():
  with tf.Session() as session:
    # Add the data to the TensorFlow graph.
    x_t = tf.constant(x)
    y_t = tf.constant(y)
    sequence_lengths_t = tf.constant(sequence_lengths)

    # Compute unary scores from a linear layer.
    weights = tf.get_variable(""weights"", [num_features, num_tags])
    matricized_x_t = tf.reshape(x_t, [-1, num_features])
    matricized_unary_scores = tf.matmul(matricized_x_t, weights)
    unary_scores = tf.reshape(matricized_unary_scores,
                              [num_examples, num_words, num_tags])

    # Compute the log-likelihood of the gold sequences and keep the transition
    # params for inference at test time.
    log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(
        unary_scores, y_t, sequence_lengths_t)

    # Add a training op to tune the parameters.
    loss = tf.reduce_mean(-log_likelihood)
    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)

    # Train for a fixed number of iterations.
    session.run(tf.global_variables_initializer())
    for i in range(1000):
      tf_unary_scores, tf_transition_params, _ = session.run(
          [unary_scores, transition_params, train_op])
      if i % 100 == 0:
        correct_labels = 0
        total_labels = 0
        for tf_unary_scores_, y_, sequence_length_ in zip(tf_unary_scores, y,
                                                          sequence_lengths):
          # Remove padding from the scores and tag sequence.
          tf_unary_scores_ = tf_unary_scores_[:sequence_length_]
          y_ = y_[:sequence_length_]

          # Compute the highest scoring sequence.
          viterbi_sequence, _ = tf.contrib.crf.viterbi_decode(
              tf_unary_scores_, tf_transition_params)

          # Evaluate word-level accuracy.
          correct_labels += np.sum(np.equal(viterbi_sequence, y_))
          total_labels += sequence_length_
        accuracy = 100.0 * correct_labels / float(total_labels)
        print(""Accuracy: %.2f%%"" % accuracy)`

UnimplementedError (see above for traceback): TensorArray has size zero, but element shape <unknown> is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.
	 [[Node: gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGatherV3 = TensorArrayGatherV3[_class=[""loc:@rnn/TensorArray_1""], dtype=DT_FLOAT, element_shape=<unknown>, _device=""/job:localhost/replica:0/task:0/cpu:0""](gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/TensorArrayGradV3, rnn/TensorArrayUnstack/range, gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/gradient_flow)]]

"
10241,C++ Online Documentation codeblocks not formatting,"There are some codeblocks in C++ documentation , written with github-style fenced markdown, 
that are not rendering as `<code>`.

Eg: https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/depth-to-space

![screenshot of a page with docs not formatted as code](https://cloud.githubusercontent.com/assets/5127634/26518496/5c312d9e-42e4-11e7-856a-972268bcf757.png)

It looks like something is going wrong with the site generation,
that when translating markdown, it does not pickup these blocks.

In the pages I quickly checked it seems to occur in the Summary sections, eg in:

- https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/depth-to-space
- https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/batch-to-space
- https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/batch-to-space-n-d

"
10240,wide&deep tutorial for large data.,"When i apply the wide&deep tutorial  code to much larger dataset with millions of rows, I received 

[libprotobuf ERROR google/protobuf/io/zero_copy_stream_impl_lite.cc:173] Cannot allocate buffer larger than kint32max for StringOutputStream.

or sometimes


ValueError: GraphDef cannot be larger than 2GB.

Any simple fix?  I pretty much want to apply the combined classifier in the tutorial."
10239,can not run   “bazel build inception/download_and_preprocess_imagenet”,"it always give error:""no such target '//:inception/download_and_preprocess_imagenet': target 'inception/download_and_preprocess_imagenet' not declared in package '' defined by /home/hank/tensorflow/BUILD.""
the BUILD file is empty."
10238,ImportError with macOS,"when I run 
`from tensorflow.examples.tutorials.mnist import input_data`
in my terminal, following error occur:
```
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-5-7cee33d24aa5> in <module>()
----> 1 from tensorflow.examples.tutorials.mnist import input_data

/Users/zklgame/anaconda/lib/python2.7/site-packages/tensorflow/examples/tutorials/mnist/__init__.py in <module>()
     19 from __future__ import print_function
     20 
---> 21 from tensorflow.examples.tutorials.mnist import input_data
     22 from tensorflow.examples.tutorials.mnist import mnist

/Users/zklgame/anaconda/lib/python2.7/site-packages/tensorflow/examples/tutorials/mnist/input_data.py in <module>()
     27 from six.moves import xrange  # pylint: disable=redefined-builtin
     28 import tensorflow as tf
---> 29 from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets

/Users/zklgame/anaconda/lib/python2.7/site-packages/tensorflow/contrib/__init__.py in <module>()
     20 
     21 # Add projects here, they will show up under tf.contrib.
---> 22 from tensorflow.contrib import bayesflow
     23 from tensorflow.contrib import compiler
     24 from tensorflow.contrib import copy_graph

ImportError: cannot import name bayesflow
```

I have try it on both the pip-installed version and built-from-source version, and they both failed.
 Here is some infomation about my env:
```

== cat /etc/issue ===============================================
Darwin zklgamedeMacBook-Pro.local 16.6.0 Darwin Kernel Version 16.6.0: Fri Apr 14 16:21:16 PDT 2017; root:xnu-3789.60.24~6/RELEASE_X86_64 x86_64
Mac OS X 10.12.5

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 8.1.0 (clang-802.0.42)
Target: x86_64-apple-darwin16.6.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

== uname -a =====================================================
Darwin zklgamedeMacBook-Pro.local 16.6.0 Darwin Kernel Version 16.6.0: Fri Apr 14 16:21:16 PDT 2017; root:xnu-3789.60.24~6/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.12.1)
numpydoc (0.6.0)
protobuf (3.2.0)
tensorflow (1.0.1)
tensorflow-gpu (1.0.1)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.0.1
tf.GIT_VERSION = v1.0.1-3-g905662a1c-dirty
tf.COMPILER_VERSION = v1.0.1-3-g905662a1c-dirty
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib:

== nvidia-smi ===================================================

== cuda libs  ===================================================

```

Solution:
Just now, I found to update my dask package to 0.14.3 help me.
`conda update dask`

It's quite hard for me to found this solution and fix it..
"
10236,"Remove ""bazel clean"" after new release","In the new release of bazel, #8880 was fixed which required `bazel clean` at the end of `configure`.
```
# TODO(gunan): Remove once bazel correctly handles changes in remote repositories.
bazel clean
```
Could @gunan look if this is save to remove now?"
10234,Update tensorboard development instructions,"I want to be able to run tensorboard in development mode and make some changes.

from the [DEVELOPMENT document:](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/DEVELOPMENT.md)

> bazel run third_party/tensorflow/tensorboard/components/tf_tensorboard:demo

but this command seems to be obsolete from the directory structure (there is no target inside /third_party).

also seems there is a `.idea` folder committed in the repository. 
Would be nice to know how the core team setups their environment.
"
10233,Tensorboard does not support multiple google cloud directory as logdir,"I would like to launch tensorboard with

    tensorboard --logdir gs://path1,gs://path2

or even better
  
     tensorboard --logdir model1:gs://path/1,model2:gs://path/2

but this is not currently supported. 

It should be an easy addition and I can provide a patch if it make sense. 

"
10232,module 'tensorflow.contrib' has no attribute 'signal',"Previously, I got the `signal` package merged into `contrib`: #9236

But now that I recompiled it all after a while, I got this when trying to use it:
`AttributeError: module 'tensorflow.contrib' has no attribute 'signal'`

Since all the tests went through, how can it be it can't even be found?
Could I have missed adding yet another reference to the package somewhere?

`tf.contrib.layers.batch_norm` does work
`tf.contrib.signal.frames` does not work"
10227,Add an option to enable CORS in TensorBoard,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  b'unknown' 1.1.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 8, cuDNN 5.1
- **GPU model and memory**: 1080ti 11GB
- **Exact command to reproduce**: N/A

### Describe the problem

The [DeepDream notebook example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb) in TensorFlow provides a snippet that uses https://tensorboard.appspot.com to embed `<iframe>`s inside a Jupyter notebook. Changing this snippet to point to a local instance of TensorBoard fails with a CORS error

![](http://i.imgur.com/WogWPOR.png)

Having the option to enable CORS when starting TensorBoard, or enabling it by default, would make it really easy to use the snippet to visualize directly in Jupyter using a local instance of TensorBoard.

### Source code / logs

Here's the snippet in its whole, with modified URL to point to `http://localhost:6006` instead of `https://tensorboard.appspot.com`

```python
# TensorFlow Graph visualizer code
import numpy as np
from IPython.display import clear_output, Image, display, HTML

def strip_consts(graph_def, max_const_size=32):
    """"""Strip large constant values from graph_def.""""""
    strip_def = tf.GraphDef()
    for n0 in graph_def.node:
        n = strip_def.node.add() 
        n.MergeFrom(n0)
        if n.op == 'Const':
            tensor = n.attr['value'].tensor
            size = len(tensor.tensor_content)
            if size > max_const_size:
                tensor.tensor_content = ""<stripped %d bytes>""%size
    return strip_def

def show_graph(graph_def, max_const_size=32):
    """"""Visualize TensorFlow graph.""""""
    if hasattr(graph_def, 'as_graph_def'):
        graph_def = graph_def.as_graph_def()
    strip_def = strip_consts(graph_def, max_const_size=max_const_size)
    code = """"""
        <script src=""//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js""></script>
        <script>
          function load() {{
            document.getElementById(""{id}"").pbtxt = {data};
          }}
        </script>
        <link rel=""import"" href=""http://localhost:6006/tf-graph-basic.build.html"" onload=load()>
        <div style=""height:600px"">
          <tf-graph-basic id=""{id}""></tf-graph-basic>
        </div>
    """""".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))

    iframe = """"""
        <iframe seamless style=""width:1200px;height:620px;border:0"" srcdoc=""{}""></iframe>
    """""".format(code.replace('""', '&quot;'))
    display(HTML(iframe))
```"
10226,Inconsistent Tensor Initialization on Multiple GPUs,"### The problem (bug?):
I'm having trouble getting consistent initialization of variables across multiple GPUs, and it appears to be a bug. Below is a test that replicates the bug.

Basically, the test just sets up a tensor on each GPU and an initializer for each. After running initialization and grabbing the initialized tensors, they do not match despite the same initializer configurations. The problem exists across multiple platforms, any number of GPUs > 1, and multiple TF versions.

Gory details: The inconsistency is non-deterministic, occurring in about 50% of runs with 2 GPUs. Roughly 0.0002% of matrix values do not match. The matrix indices that do not match have significantly different values (i.e. greater than FP rounding errors). Assuming row-major tensor storage, the incorrect indices are in contiguous groups of 4 floats (16B), and the distance - in memory addresses - between these groups is consistent but platform dependent (e.g. 512kB stride between groups on GTX980 vs. 480kB between groups on K40m)

- **I have written custom code**:
```
import numpy as np
import os
import tensorflow as tf
from tensorflow.python.platform import test


class AllreduceTest(test.TestCase):
    def dumpFailure(self, my_rank, num_ranks, first_output, second_output):
        out_dims = first_output.shape
        assert(len(out_dims) == 2)
        for i in range(out_dims[0]):
            for j in range(out_dims[1]):
                if first_output[i][j] != second_output[i][j]:
                    print(""{}: [{}][{}]: {} {}""
                          .format(my_rank, i, j, first_output[i][j],
                                  second_output[i][j]),
                          flush=True)

    def test_mpi_allreduce(self):
        num_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
        gpu_indices = [index for index in range(num_gpus)]

        mat_dim = 3072

        outputs = []
        for index in gpu_indices:
            with tf.device(""/gpu:{}"".format(index)):
                initer = tf.random_uniform_initializer(-0.1, 0.1, seed=1234,
                                                       dtype=tf.float32)
                outputs.append(tf.get_variable(""outputs-{}"".format(index),
                                               shape=(mat_dim, mat_dim),
                                               dtype=tf.float32,
                                               initializer=initer))

        # Session to test initialization across multiple GPUs
        gpu_options = tf.GPUOptions(
            visible_device_list=','.join(str(idx) for idx in gpu_indices))
        config = tf.ConfigProto(gpu_options=gpu_options)
        with tf.Session(config=config) as sess:
            sess.run(tf.global_variables_initializer())
            output_result = sess.run(outputs)
            for index in gpu_indices:
                if not np.allclose(output_result[0], output_result[index]):
                    print(""CRAP: Init outputs 0 and {} do not match""
                          .format(index), flush=True)
                    self.dumpFailure(index, num_gpus, output_result[0],
                                     output_result[index])
                    assert(np.allclose(output_result[0],
                                       output_result[index]))

if __name__ == '__main__':
    test.main()
```

### System information
- **OS Platform and Distribution**: Linux Ubuntu 14.04.2
- **TensorFlow installed from**: source
- **TensorFlow version**: 1.0.1 (`tf.GIT_VERSION = b'v1.0.1-0-ge895d5c', tf.COMPILER_VERSION = b'v1.0.1-0-ge895d5c', protobuf = 3.1.0`) and 1.1.0-rc2 (`tf.GIT_VERSION = b'v1.1.0-rc2-1164-g1d993dd', tf.COMPILER_VERSION = b'v1.1.0-rc2-1164-g1d993dd', protobuf = 3.3.0`)
- **Bazel version**: 0.45
- **Numpy version**: 1.12.1
- **CUDA/cuDNN version**: cuda-8.0, cudnn-6
- **GPU model and memory**: GeForce GTX 980, TITAN X Maxwell, Tesla K40m, Tesla M40 24GB
"
10224,I can't run tensorflow at an atom.,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

-----------
I'm using window 10 and ANAconda.
I downloaded it using prompt.
I can use it at the cmd, however, I can't using it at the atom.
I also can't code complex program like linear regression.
I only can code easy thing hello tensorflow using prompt.
How can I solve this problem?
Please help me.
I uninstalled it and reinstalled it, however, it doesn't work.


import tensorflow as tf
hello = tf.constant('Hello,Tensorflow!')
sess = tf.Session()
print(sess.run(hello))




Python - test.py:2
Traceback (most recent call last):
  File ""F:\Python\test.py"", line 1, in <module>
    import tensorflow as tf
ModuleNotFoundError: No module named 'tensorflow'
[Finished in 0.126s]
hellotf.py5:1(4, 112)
CRLFUTF-8Python
"
10222,tf.nn.moments with tf.concat numerical ambiguity,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: custom code, see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 LTE
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: CUDA: 8.0 / cuDNN: 5.1
- **GPU model and memory**: GTX960m 4GB and GTX1080 8GB
- **Exact command to reproduce**: run the code below

### Describe the problem
**tf.nn.moments** GPU version produces different mean and variance values for numerically same input tensors. The only difference between the inputs is that they are the outputs of one and two **tf.concat** operations (see the code below). CPU version works well.

The bad output is:
```
input diff:0.0
mean diff:2.98023223877e-08
var diff:7.45058059692e-09
```
The correct output should be:
```
input diff:0.0
mean diff:0.0
var diff:0.0
```

### Source code / logs
```
import numpy as np
import tensorflow as tf

with tf.device(""/gpu:0""):
    input = tf.placeholder(shape=[16, 4, 4, 8], dtype=tf.float32)
    input1 = tf.concat([input], axis=0)
    input2 = tf.concat([tf.concat([input], axis=0)], axis=0)

    mean1, var1 = tf.nn.moments(input1, axes=[0,1,2])
    mean2, var2 = tf.nn.moments(input2, axes=[0,1,2])

    input_diff_max = tf.reduce_max(tf.abs(input1 - input2))
    mean_diff_max = tf.reduce_max(tf.abs(mean1 - mean2))
    var_diff_max = tf.reduce_max(tf.abs(var1 - var2))

    with tf.Session() as sess:
        i_v, m_v, v_v = sess.run([input_diff_max, mean_diff_max, var_diff_max], feed_dict={input: np.random.rand(16, 4, 4, 8)})
        print(""input diff:{}"".format(i_v))
        print(""mean diff:{}"".format(m_v))
        print(""var diff:{}"".format(v_v))
```
"
10220,Tensorflow crashes on build on Ubuntu 16.04 when building for skylake (avx512),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: building from source
- **TensorFlow version (use command below)**:  5ae244e
- **Bazel version (if compiling from source)**:  0.4.5
- **CUDA/cuDNN version**: CUDA 8.0.61, cudnn 6.0.21 (tried also 5.1)
- **GPU model and memory**: 2x Tesla P100-PCIE-12GB
- **Exact command to reproduce**: building
- **Additional information**: Intel(R) Xeon(R) CPU E7-4860 v2 @ 2.60GHz, gcc version 5.4.1 20170519 (Ubuntu 5.4.1-11ubuntu2~16.04)


### Describe the problem
On the regular rebuild of Tensorflow, the build crashes with bunch of `error: argument of type ""const void *"" is incompatible with parameter of type ""const something *""`

### Source code / logs
Crash log:
```
INFO: From Compiling tensorflow/core/kernels/scatter_functor_gpu.cu.cc:
/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9218): error: argument of type ""const void *"" is incompatible with parameter of type ""const float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9229): error: argument of type ""const void *"" is incompatible with parameter of type ""const float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9242): error: argument of type ""const void *"" is incompatible with parameter of type ""const double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9253): error: argument of type ""const void *"" is incompatible with parameter of type ""const double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9266): error: argument of type ""const void *"" is incompatible with parameter of type ""const float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9277): error: argument of type ""const void *"" is incompatible with parameter of type ""const float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9290): error: argument of type ""const void *"" is incompatible with parameter of type ""const double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9301): error: argument of type ""const void *"" is incompatible with parameter of type ""const double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9314): error: argument of type ""const void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9325): error: argument of type ""const void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9338): error: argument of type ""const void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9350): error: argument of type ""const void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9363): error: argument of type ""const void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9374): error: argument of type ""const void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9387): error: argument of type ""const void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9399): error: argument of type ""const void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9408): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9417): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9426): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9435): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9443): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9452): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9461): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9470): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9479): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9488): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9497): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9506): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9515): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9524): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9533): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9542): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(54): error: argument of type ""const void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(62): error: argument of type ""const void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(70): error: argument of type ""const void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(78): error: argument of type ""const void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(86): error: argument of type ""void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(95): error: argument of type ""void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(104): error: argument of type ""void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(112): error: argument of type ""void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(120): error: argument of type ""void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(129): error: argument of type ""void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(138): error: argument of type ""void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(146): error: argument of type ""void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10223): error: argument of type ""const void *"" is incompatible with parameter of type ""const float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10235): error: argument of type ""const void *"" is incompatible with parameter of type ""const float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10247): error: argument of type ""const void *"" is incompatible with parameter of type ""const double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10259): error: argument of type ""const void *"" is incompatible with parameter of type ""const double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10271): error: argument of type ""const void *"" is incompatible with parameter of type ""const float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10283): error: argument of type ""const void *"" is incompatible with parameter of type ""const float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10295): error: argument of type ""const void *"" is incompatible with parameter of type ""const double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10307): error: argument of type ""const void *"" is incompatible with parameter of type ""const double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10319): error: argument of type ""const void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10331): error: argument of type ""const void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10343): error: argument of type ""const void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10355): error: argument of type ""const void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10367): error: argument of type ""const void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10379): error: argument of type ""const void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10391): error: argument of type ""const void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10403): error: argument of type ""const void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10413): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10424): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10433): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10444): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10453): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10464): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10473): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10484): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10493): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10504): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10513): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10524): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10533): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10544): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10553): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10564): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10573): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10584): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10593): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10604): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10613): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10624): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10633): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10644): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10653): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10664): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10673): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10684): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10693): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10704): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10713): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10724): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

92 errors detected in the compilation of ""/tmp/tmpxft_00008f12_00000000-7_scatter_functor_gpu.cu.cpp1.ii"".
ERROR: /scratch/chaimb/tensorflow/tensorflow/core/kernels/BUILD:1140:1: output 'tensorflow/core/kernels/_objs/scatter_functor_gpu/tensorflow/core/kernels/scatter_functor_gpu.cu.pic.o' was not created.
ERROR: /scratch/chaimb/tensorflow/tensorflow/core/kernels/BUILD:1140:1: not all outputs were created or valid.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 147.888s, Critical Path: 69.54s
```


I've tried disabling most of the options (MKL, architecture optimizations, computability) but the crash happens even with full-default (except CUDA and XLA) configuration."
10218,time and memory cost more and more while using keras.backend.ctc_decoder(),"### memory not released bug with tf.python.ops.gen_ctc_ops  both with ctc_greedy_decoder and ctc_beam_search_decoder...


```python
import time

from keras import backend as K

start_time = time.time()
while True:
     y_pred  = K.ctc_decode(args)
     cost_time = time.time()-start_time
     print 'cost time', cost_time   # time cost more and more with the same input args. why ??!! 
 ...
```
when using  K.ctc_decode() in a large loop, time cost longer  loop become larger...

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source ( pip install -U tensorflow)
- **TensorFlow version (use command below)**: (1.1.0)
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: None ( same problem occured if has CUDA7.5)
- **GPU model and memory**: None ( same problem occured with GPU of 32G)
- **Exact command to reproduce**:

####  function in file  tensorflow.python.ops.ctc_ops.py

Please help, thank you very much!"
10216,tf.nn.max_pool_with_argmax bug with padding,"There is a bug in the indices returned from `tf.nn.max_pool_with_argmax` when a padding is applied. The indices with be based on the shape supplied to `max_pool_with_argmax`, instead of the shape+padding.

Simple code example to reproduce:
```
import tensorflow as tf

def main():
    with tf.Session() as session:
        input = tf.get_variable('weights',
                                  shape=[1, 301, 201, 1],
                                  initializer=tf.truncated_normal_initializer(stddev=0.5, dtype=tf.float32),
                                  dtype=tf.float32)

        val, idx = tf.nn.max_pool_with_argmax(input, [1, 2, 2, 1], [1, 2, 2, 1], padding=""SAME"") #padding will turn dimensions to 302x202

        y1 = idx // 201
        x1 = idx % 201

        y2 = idx // 202
        x2 = idx % 202

        max_x1 = tf.reduce_max(x1)
        max_y1 = tf.reduce_max(y1)
        max_x2 = tf.reduce_max(x2)
        max_y2 = tf.reduce_max(y2)


        session.run(tf.global_variables_initializer())
        m_x1, m_y1, m_x2, m_y2 = session.run([max_x1, max_y1, max_x2, max_y2])

        print(""%d, %d, %d, %d""%(m_x1, m_y1, m_x2, m_y2))

if __name__ == ""__main__"":
    main()
```

This prints `""200, 300, 201, 299""`. As you can see, the padding would increase the dimensions of the tensor to `302x202`, so the maximum `y` coordinate should be `301` and `x` should be `201`. But if we unravel the argmax indices with a width of `202`, we get a maximum `x` of `201`, but maximum `y` of only `299`. If we instead use `201` as width for unraveling (the unpadded width of the input tensor), we get `200` and `300`, respectively, which are the correct values for the unpadded input tensor.

So the `((b * height + y) * width + x) * channels + c` formula for `tf.nn.max_pool_with_argmax` uses the input tensor dimensions for `width`, not the input+padding dimensions.

This is relevant if you then use the indices to unpool/reverse the max_pool, since often you'll multiple the dimensions of the max_pool output by 2 to get the input dimensions, which would be off with a naive implementation like this.

When using this to implement an unpooling operation (for instance for SegNet), this will cause every line of the image to shift (by 1 pixel) if padding is applied to the width of an image,  basically slightly tilting an image. It's especially obvious with multiple argmax&unpool in succession.

It also means that, with zero-padding, if the whole tensor is negative values (in which case the zero-padding would be the highest value in the tensor), it won't return the coordinates of the padding. Basically, it doesn't actually add any padding to the input tensor, it just pretends it does to make dimensions line up, but doesn't consider the actual values/zeros in the padding. This might be intended behaviour, though it would strike me as odd given the usual understanding of padding. If so, the documentation should be changed to make it clear that this is the way the operation works. But I think this would be against the principle of least astonishment, since I assume most people would think that an op talking about padding would actually add padding values.

See also #2169 for a use-case for this with additional discussion"
10214,Definition of session -> run() in c++,"Can I know the location of the code which contains the function definition of session-> run(const std::vector<std::pair<string, Tensor> >& inputs,
                     const std::vector<string>& output_tensor_names,
                     const std::vector<string>& target_node_names,
                     std::vector<Tensor>* outputs)

of session.h

After looking at docs , I found that its a part of TensorFlow C++ api whose code is present in tensorflow/code location, but I couldn't find exact location.
I could find only function declaration in session.h file in tensorflow/core/public but not its definition

I would like to modify the implementation of session -> run() and use it."
10213,BasicLSTMCell zero_state() raise error in TF1.2 but works in TF1.1,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux version 4.4.0-75-generic (buildd@lgw01-21) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) ) #96-Ubuntu SMP Thu Apr 20 09:56:33 UTC 2017
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
('v1.2.0-rc0-312-g0b72359', '1.2.0-rc0')
- **Bazel version (if compiling from source)**:
NA
- **CUDA/cuDNN version**:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2016 NVIDIA Corporation
Built on Sun_Sep__4_22:14:01_CDT_2016
Cuda compilation tools, release 8.0, V8.0.44
- **GPU model and memory**:
Quadro K620 
- **Exact command to reproduce**:



### Describe the problem
When I try to create zero state tensor for a BasicLSTMCell, a error is raised as below. The same code can work in tensorflow 1.1

File ""/home/wangyao/PersonalCode/GAN/ganForTimeSeq.py"", line 204, in discriminator
    init_state = lstmCell.zero_state(self.batch_size_t, dtype=tf.float32)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 229, in zero_state
    return _zero_state_tensors(state_size, batch_size, dtype)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 130, in _zero_state_tensors
    return nest.map_structure(get_state_shape, state_size)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/nest.py"", line 317, in map_structure
    structure[0], [func(*x) for x in entries])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 125, in get_state_shape
    c = _concat(batch_size, s)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 90, in _concat
    ""but saw tensor: %s"" % p)
ValueError: prefix tensor must be either a scalar or vector, but saw tensor: Tensor(""Placeholder:0"", dtype=int32)


### Source code / logs
self.batch_size_t = tf.placeholder(tf.int32, None)

with tf.variable_scope('d_rnn'):
                lstmCell = tf.contrib.rnn.BasicLSTMCell(num_units_in_LSTMCell,reuse=reuseParam)
                init_state = lstmCell.zero_state(self.batch_size_t, dtype=tf.float32)
                raw_output, final_state = tf.nn.dynamic_rnn(lstmCell, inputTensor, initial_state=init_state)
"
10211,Eye Iris Identification using Tensorflow,"Hello, 

In my android app, I'm looking to build eye iris identification for each user. Can I do this with Tensorflow lite? The user would use their native camera to put infront of their eyes, Tensorflow will detect if this is identified or not from our database.

Thank you,
Kind Regards,
"
10210,tf.summary.image bug with input_queues,"### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: Windows 10, Linux Ubuntu 14.04.02
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.1.0
- **CUDA/cuDNN version**: 8.0
- **Exact command to reproduce**: python error_reading_with_summary.py

### Describe the problem
When creating `tf.summary.image()` summaries when using `tf.train.slice_input_producer()` for generating the images, the input queue gets emptied very fast and I end up with less data for training... 
For example: When having 5 images, the first image gets read but when I try collecting the second image I get a `tf.errors.OutOfRangeError` error. This is not a problem with how I input the images because when I leave out the summaries I can read all images.
I get multiple warnings like
```
2017-05-26 08:33:43.017260: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:1152] Out of range: FIFOQueue '_6_input_producer/input_producer' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: input_producer/input_producer_Dequeue = QueueDequeueV2[component_types=[DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](input_producer/input_producer)]]
```
when I create the summary operation and then run the summary operation.
Interestingly enough, I also get this warning when I run the summary operation without any summaries created. But at least then all my images get read.
When I don't have any summaries and don't run any summary operation this warning doesn't appear at all.

The attached archive contains a minimal replicating example. The `tf.summary.image()` is at line 15-16 in the `get_batch()` function and the `merged_summary_op` is created at line 31 and run at line 45 in the `test_read_images_summary()` function.
Run `python error_reading_with_summaries` to execute all 4 cases: 
1. When the summary isn't created and the summary operation isn't executed.
2. When the summary is created but the summary operation isn't executed.
3. When the summary isn't created but the summary operation is executed.
4. When the summary is created and the summary operation is executed.

Use the option `-s` for only creating the summary and `-r` only for executing the summary operation and use both `-r -s` for creating and executing the summary operation.

### Source code / logs
Source code, traceback and images are included: [error_reading_with_summary.zip](https://github.com/tensorflow/tensorflow/files/1031025/error_reading_with_summary.zip)
Run `python error_reading_with_summaries.py` or `python error_reading_with_summaries.py -h` for help."
10208,TypeError: eval() got multiple values for argument 'feed_dict',
10207,SVG or HTML Summary,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: 5ae244ed0b702e50fd6bb7bc73d45b9188c4239d
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

TensorBoard supports image summaries, which is great for visualization. However, TensorFlow doesn't have rich operators for drawing, and people have to rely on Python libraries like `PIL` or `cv2` for rendering. Those libraries typically use software rendering and are not very fast.

Considering TensorBoard is a web app, and browsers already have rich support for rendering, why don't we offload the rendering to browsers? For example, SVG is a simple XML format, which supports vector graphics, text and embedded bitmap images. Outputting an SVG seems to be a fairly easy job with some text templates. "
10204,tf.summary.text fails keeping summaries,"I got following issues when I use `tf.summary.text` and view the summaries on tensorboard.

- It shows me text summaries in random order.
- It randomly removes existing summaries and show me only a few (Is there a configuration for maximum number of summaries to keep?)
- I can usually see only around 5 summaries on tensorboard even if I added summaries 100+ times.
- Other summaries work properly when I use summaries like below.
```
summary_op = tf.summary.merge(summaries) # Other scalar, distribution, histogram summaries
valid_summary_op = tf.summary.merge([valid_sentence_summary]) # text summary with tf.summary.text
```


I can reproduce this problem in two different environments.

1. Ubuntu 14.04 / CUDA 8.0 / Cudnn 5.1 / TF 1.1.0rc2 / Bazel 0.4.5 / GPU TITAN X Pascal (use 0 gpus~4gpus)
2. Mac OSx Sierra / TF 1.1.0rc2 / Bazel 0.4.5 / No GPU

Below is sample code to reproduce this issue.

```
import tensorflow as tf

text_list = ['this is the first text', 'this is 2nd text', 'this is random text']
id2sent = {id:sent for id, sent in enumerate(text_list)}
sent2id = {sent:id for id, sent in id2sent.items()}

tf.reset_default_graph()    

outer_string = tf.convert_to_tensor('This is string outside inner scope.')
outer_summary = tf.summary.text('outside_summary', outer_string)

with tf.name_scope('validation_sentences') as scope:
    id_list = tf.placeholder(tf.int32, shape=[3], name='sent_ids')

    valid_placeholder = tf.placeholder(tf.string, name='valid_summaries')

    inner_summary = tf.summary.text('sent_summary', valid_placeholder)
    summaries = [outer_summary, inner_summary]
    summary_op = tf.summary.merge(summaries)
        
sess = tf.Session()
summary_writer = tf.summary.FileWriter(logdir='./text_summary', graph=sess.graph)

for step in range(10):

    predicted_sents_ids = sess.run(
        id_list,
        feed_dict={
            id_list: [0, 1, 2]
        })

    # list of string
    predicted_sents = [id2sent[id] for id in predicted_sents_ids]

    valid_summary = sess.run(summary_op, feed_dict={
        valid_placeholder: predicted_sents
    })

    summary_writer.add_summary(valid_summary, global_step=step)
    # summary_writer.flush()
# summary_writer.flush()
# flush() didn't help..
```

And below is the result on tensorboard.

![image](https://cloud.githubusercontent.com/assets/18069263/26475951/2030a548-41f6-11e7-8898-ca7ac12aa137.png)
"
10200,Docker.gpu build fail: http 404,"------------------------

### System information
- Only change: in Docker.gpu I added `apt-get python-tk`
- Linux 14.04
- Docker
- Tesla k80
-  what causes problem: ` sudo nvidia-docker build -t with_tk -f Dockerfile.gpu . `

### Describe the problem
When editing the dockerfile to simply add python-tk the build says:
```
Collecting tensorflow-gpu==0.0.0 from http://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.0.0-cp27-none-linux_x86_64.whl
  HTTP error 404 while getting http://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.0.0-cp27-none-linux_x86_64.whl
  Could not install requirement tensorflow-gpu==0.0.0 from http://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.0.0-cp27-none-linux_x86_64.whl because of error 404 Client Error: Not Found for url: http://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.0.0-cp27-none-linux_x86_64.whl
Could not install requirement tensorflow-gpu==0.0.0 from http://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.0.0-cp27-none-linux_x86_64.whl because of HTTP error 404 Client Error: Not Found for url: http://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.0.0-cp27-none-linux_x86_64.whl for URL http://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.0.0-cp27-none-linux_x86_64.whl
The command '/bin/sh -c pip --no-cache-dir install     http://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.0.0-cp27-none-linux_x86_64.whl' returned a non-zero code: 1
```

To fix I simply declared gpu-1.0.0 instead of 0.0.0, but I am not supposed to write in those lines!

"
10199,Built tensorflow CPU mode with SIMD_OPTIONS but when when opening session it warns it wasn't compiled to use SSE,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10. Intel Core i7-6600U
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**:1.2.0-rc0
- **Bazel version (if compiling from source)**: No
- **CUDA/cuDNN version**:No
- **GPU model and memory**:No
- **Exact command to reproduce**:
I built tensorflow on Windows following the instructions from 
https://github.com/tensorflow/tensorflow/tree/r0.12/tensorflow/contrib/cmake
My goal was to enable the SIMD options for performance improvements.

1) Set up toolchain for for 64-bit: vcvarsall amd64
2) Invoked CMAKE
C:\Projects\tensorflow\tensorflow\contrib\cmake\build>cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:/tools/swigwin-3.0.12\swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=C:\Users\sergio.murillo\AppData\Local\Programs\Python\Python35/PYTHON.EXE -DPYTHON_LIBRARIES=C:\Users\sergio.murillo\AppData\Local\Programs\Python\Python35\libs\python35.lib -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX
3) To build the PIP package
MSBuild /p:Configuration=Release /filelogger tf_python_build_pip_package.vcxproj
It build with no errors.
4) Install whl
pip install .\tf_python\dist\tensorflow-1.2.0rc0-cp35-cp35m-win_amd64.whl
5) Validate installation
>> import tensorflow as tf
>>> hello = tf.constant('Hello, Tensorflow!')
>>> sess = tf.Session()

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I built tensorflow on Windows following the instructions from 
https://github.com/tensorflow/tensorflow/tree/r0.12/tensorflow/contrib/cmake
My goal was to enable the SIMD options for performance improvements. It builds without errors, but when I open a tensorflow session it still shows the warnings telling that SIMD was not enabled.

1) Set up toolchain for for 64-bit: vcvarsall amd64
2) Invoked CMAKE
C:\Projects\tensorflow\tensorflow\contrib\cmake\build>cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:/tools/swigwin-3.0.12\swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=C:\Users\sergio.murillo\AppData\Local\Programs\Python\Python35/PYTHON.EXE -DPYTHON_LIBRARIES=C:\Users\sergio.murillo\AppData\Local\Programs\Python\Python35\libs\python35.lib -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX
3) To build the PIP package
MSBuild /p:Configuration=Release /filelogger tf_python_build_pip_package.vcxproj
It build with no errors.
4) Install whl
pip install .\tf_python\dist\tensorflow-1.2.0rc0-cp35-cp35m-win_amd64.whl
5) Validate installation: This is when despite compiling and building without errors, I see the warnings about the SIMD instructions that should have been enabled in CMAKE with -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX

>>> import tensorflow as tf
>>> hello = tf.constant('Hello, Tensorflow!')
>>> sess = tf.Session()
2017-05-25 14:23:37.872523: W c:\projects\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2017-05-25 14:23:37.873758: W c:\projects\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-25 14:23:37.873870: W c:\projects\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-25 14:23:37.874002: W c:\projects\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-25 14:23:37.874191: W c:\projects\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-25 14:23:37.874379: W c:\projects\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-25 14:23:37.874565: W c:\projects\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.

I followed the instructions, but it seems that SIMD options were not build/compiled. Did I missed something?
Thanks!

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

[msbuild.zip](https://github.com/tensorflow/tensorflow/files/1030085/msbuild.zip)


"
10197,Error when serializing LAYER_NAME_UIDS,"### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**: source
- **TensorFlow version**: v1.2.0-rc0-312-g0b72359 1.2.0-rc0
- **Bazel version**: 0.4.5
- **CUDA/cuDNN version**: 8.0/5.1
- **GPU model and memory**: NVIDIA GeForce GTX 750 Ti

After recompiling TensorFlow I got this strange log message.
I doesn't seem to affect any of the computation.
All of the other tools behave as expected.

```
WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.
Type is unsupported, or the types of the items don't match field type in CollectionDef.
'dict' object has no attribute 'name'
```"
10196,tf.contrib.ffmpeg.decode_audio causes kernel crash w/ multi-threading,"- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow version (use command below)**: v1.1.0 - gpu
- **CUDA/cuDNN version**: 8.0
- **Exact command to reproduce**:

I have noticed that attempting to run the 'tf.contrib.ffmpeg.decode_audio' function with multiple threads causes the kernel to crash. This occurs when trying to create batches of data from audio binaries. 

The code underlying 'tf.contrib.ffmpeg.decode_audio' appears to a basic reference to functions outside of tensorflow so I am unsure there is a solution inside of the tensorflow domain. Nonetheless I wanted to bring this up in case someone had a solution. This is probably not a bug report and more of a low priority feature request. 

The code below will run without error when **num_threads=1** for tf.train.batch but the kernel will crash for **num_threads=2** or more. 

`
```
graph = tf.Graph()

with graph.as_default():

    
    batch_size=2

    queue = tf.train.slice_input_producer([paths, labels], 
                                          num_epochs=2, shuffle=True, capacity=32)
    
    
    audio_binary = tf.read_file(queue[0])
    signal = tf.contrib.ffmpeg.decode_audio(audio_binary, file_format='mp3', 
                                        samples_per_second=22500,  
                                        channel_count=1)[:450000,0]
    y_ = tf.one_hot(queue[1], 16, dtype=tf.float32)


    batch_sig, batch_y_ = tf.train.batch([signal, y_], batch_size=batch_size, 
                                         shapes=[(450000,), (16,)], 
                                         num_threads=1, capacity=64)
    
    
    with tf.Session(config=tf.ConfigProto(operation_timeout_in_ms=500)) as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())

        
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess=sess, coord=coord)
        
        
        for i in range(2):
            print(tf.reduce_max(batch_sig, axis=1).eval())
            
        coord.request_stop()
        coord.join(threads)            
        `


"
10195,Use freeze_graph only with an input checkpoint,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.2.0-rc0

`freeze_graph` method from `tensorflow.python.tools` should be able to work just with an input checkpoint, it needn't a graph definition from a protobuf file. Just restoring the metagraph and using the graph from the session lets you get rid of the graph def file.

Also, as you have provided from 1.2.0-rc0 a method to freeze from code without loading the files ([`freeze_graph_with_def_protos`](https://github.com/tensorflow/tensorflow/blob/v1.2.0-rc0/tensorflow/python/tools/freeze_graph.py#L58)), it should be able to work without a checkpoint but just with a session.

These will make freezing way simpler."
10194,Undefined references in android studio for libandroid_tensorflow_kernels.lo,"I am using the prebuilt binaries and am using them in Android studio in windows 10 and have set it up 
according to https://github.com/Qualeams/Android-Face-Recognition-with-Deep-Learning-Library/issues/1
When I build the app , i get below errors

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':facerecognitionlibrary:externalNativeBuildRelease'.
> Build command failed.
Error while executing process C:\Users\H242018\AppData\Local\Android\Sdk\ndk-bundle\ndk-build.cmd with arguments {NDK_PROJECT_PATH=null APP_BUILD_SCRIPT=C:\Projects\Android-Test1\facerecognitionlibrary\jni-build\jni\Android.mk NDK_APPLICATION_MK=C:\Projects\Android-Test1\facerecognitionlibrary\jni-build\jni\Application.mk APP_ABI=armeabi-v7a NDK_ALL_ABIS=armeabi-v7a NDK_DEBUG=0 APP_PLATFORM=android-21 NDK_OUT=C:/Projects/Android-Test1/facerecognitionlibrary/build/intermediates/ndkBuild/release/obj NDK_LIBS_OUT=C:\Projects\Android-Test1\facerecognitionlibrary\build\intermediates\ndkBuild\release\lib C:/Projects/Android-Test1/facerecognitionlibrary/build/intermediates/ndkBuild/release/obj/local/armeabi-v7a/libtensorflow.so}
Android NDK: WARNING:C:\Projects\Android-Test1\facerecognitionlibrary\jni-build\jni\Android.mk:tensorflow: non-system libraries in linker flags: C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libprotos_all_cc.a C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libprotobuf.a C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libprotobuf_lite.a C:/Users/H242018/AppData/Local/Android/sdk/ndk-bundle/build//../sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libgnustl_static.a C:/Users/H242018/AppData/Local/Android/sdk/ndk-bundle/build//../sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libsupc++.a    
Android NDK:     This is likely to result in incorrect builds. Try using LOCAL_STATIC_LIBRARIES    
Android NDK:     or LOCAL_SHARED_LIBRARIES instead to list the library dependencies of the    
Android NDK:     current module    
[armeabi-v7a] SharedLibrary  : libtensorflow.so
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(bias_op.o):bias_op.cc:function tensorflow::(anonymous namespace)::GetBiasValueDims(tensorflow::Tensor const&, tensorflow::TensorFormat, int*, int*, int*, int*): error: undefined reference to 'tensorflow::TensorShape::dim_size(int) const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(bias_op.o):bias_op.cc:function tensorflow::(anonymous namespace)::GetBiasValueDims(tensorflow::Tensor const&, tensorflow::TensorFormat, int*, int*, int*, int*): error: undefined reference to 'tensorflow::TensorShape::dim_size(int) const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(bias_op.o):bias_op.cc:function tensorflow::(anonymous namespace)::GetBiasValueDims(tensorflow::Tensor const&, tensorflow::TensorFormat, int*, int*, int*, int*): error: undefined reference to 'tensorflow::TensorShape::dim_size(int) const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(bias_op.o):bias_op.cc:function tensorflow::(anonymous namespace)::GetBiasValueDims(tensorflow::Tensor const&, tensorflow::TensorFormat, int*, int*, int*, int*): error: undefined reference to 'tensorflow::TensorShape::dim_size(int) const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(bias_op.o):bias_op.cc:function tensorflow::BiasOp<Eigen::ThreadPoolDevice, int>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::DebugString() const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(bias_op.o):bias_op.cc:function tensorflow::BiasOp<Eigen::ThreadPoolDevice, int>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::DebugString() const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(bias_op.o):bias_op.cc:function tensorflow::BiasOp<Eigen::ThreadPoolDevice, int>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::DebugString() const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(bias_op.o):bias_op.cc:function tensorflow::BiasOp<Eigen::ThreadPoolDevice, int>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::DebugString() const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(bias_op.o):bias_op.cc:function tensorflow::BiasGradOp<Eigen::ThreadPoolDevice, int>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::TensorShape(tensorflow::gtl::ArraySlice<long long>)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(bias_op.o):bias_op.cc:function tensorflow::BiasGradOp<Eigen::ThreadPoolDevice, int>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::DestructorOutOfLine()'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(bias_op.o):bias_op.cc:function tensorflow::BiasGradOp<Eigen::ThreadPoolDevice, float>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::TensorShape(tensorflow::gtl::ArraySlice<long long>)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(bias_op.o):bias_op.cc:function tensorflow::BiasGradOp<Eigen::ThreadPoolDevice, float>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::DestructorOutOfLine()'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(concat_op.o):concat_op.cc:function tensorflow::ConcatOffsetOp::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::TensorShape(tensorflow::gtl::ArraySlice<long long>)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(concat_op.o):concat_op.cc:function tensorflow::ConcatOffsetOp::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::DestructorOutOfLine()'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(concat_op.o):concat_op.cc:function tensorflow::ConcatOp<Eigen::ThreadPoolDevice, int>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::AddDim(long long)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(concat_op.o):concat_op.cc:function tensorflow::ConcatOp<Eigen::ThreadPoolDevice, int>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::DestructorOutOfLine()'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(concat_op.o):concat_op.cc:function tensorflow::ConcatOp<Eigen::ThreadPoolDevice, int>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::SlowCopyFrom(tensorflow::TensorShape const&)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(concat_op.o):concat_op.cc:function tensorflow::ConcatOp<Eigen::ThreadPoolDevice, int>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::set_dim(int, long long)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(concat_op.o):concat_op.cc:function tensorflow::ConcatOp<Eigen::ThreadPoolDevice, int>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::SlowCopyFrom(tensorflow::TensorShape const&)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(concat_op.o):concat_op.cc:function tensorflow::ConcatOp<Eigen::ThreadPoolDevice, float>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::AddDim(long long)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(concat_op.o):concat_op.cc:function tensorflow::ConcatOp<Eigen::ThreadPoolDevice, float>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::SlowCopyFrom(tensorflow::TensorShape const&)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(concat_op.o):concat_op.cc:function tensorflow::ConcatOp<Eigen::ThreadPoolDevice, float>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::set_dim(int, long long)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(concat_op.o):concat_op.cc:function tensorflow::ConcatOp<Eigen::ThreadPoolDevice, float>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::SlowCopyFrom(tensorflow::TensorShape const&)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(concat_op.o):concat_op.cc:function tensorflow::ConcatOp<Eigen::ThreadPoolDevice, Eigen::QUInt16>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::AddDim(long long)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(concat_op.o):concat_op.cc:function tensorflow::ConcatOp<Eigen::ThreadPoolDevice, Eigen::QUInt16>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::set_dim(int, long long)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(concat_op.o):concat_op.cc:function tensorflow::ConcatOp<Eigen::ThreadPoolDevice, Eigen::QInt8>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::AddDim(long long)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(concat_op.o):concat_op.cc:function tensorflow::ConcatOp<Eigen::ThreadPoolDevice, Eigen::QInt8>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::set_dim(int, long long)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(constant_op.o):constant_op.cc:function tensorflow::{lambda(tensorflow::OpKernelConstruction*)#7}::_FUN(tensorflow::OpKernelConstruction*): error: undefined reference to 'tensorflow::TensorShape::TensorShape()'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(constant_op.o):constant_op.cc:function tensorflow::{lambda(tensorflow::OpKernelConstruction*)#6}::_FUN(tensorflow::OpKernelConstruction*): error: undefined reference to 'tensorflow::TensorShape::TensorShape()'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(constant_op.o):constant_op.cc:function tensorflow::FillOp<Eigen::ThreadPoolDevice, int>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::TensorShape()'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(constant_op.o):constant_op.cc:function tensorflow::FillOp<Eigen::ThreadPoolDevice, float>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::TensorShape()'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(example_parsing_ops.o):example_parsing_ops.cc:function tensorflow::SingleSequenceExampleParserOp::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::Features_default_instance_'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(example_parsing_ops.o):example_parsing_ops.cc:function tensorflow::SingleSequenceExampleParserOp::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::dim_sizes() const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(example_parsing_ops.o):example_parsing_ops.cc:function tensorflow::SingleSequenceExampleParserOp::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::TensorShape(tensorflow::gtl::ArraySlice<long long>)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(example_parsing_ops.o):example_parsing_ops.cc:function tensorflow::SingleSequenceExampleParserOp::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::FeatureLists_default_instance_'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(example_parsing_ops.o):example_parsing_ops.cc:function tensorflow::SingleSequenceExampleParserOp::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::dim_sizes() const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(gather_op.o):gather_op.cc:function tensorflow::TTypes<int, 2u, int>::ConstTensor tensorflow::Tensor::flat_outer_dims<int, 2u>() const: error: undefined reference to 'tensorflow::Tensor::ComputeFlatOuterDims(long long) const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(gather_op.o):gather_op.cc:function tensorflow::TTypes<float, 2u, int>::ConstTensor tensorflow::Tensor::flat_outer_dims<float, 2u>() const: error: undefined reference to 'tensorflow::Tensor::ComputeFlatOuterDims(long long) const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(pack_op.o):pack_op.cc:function tensorflow::PackOp<Eigen::ThreadPoolDevice, int>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::InsertDim(int, long long)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(pack_op.o):pack_op.cc:function tensorflow::PackOp<Eigen::ThreadPoolDevice, float>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::InsertDim(int, long long)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(shape_ops.o):shape_ops.cc:function tensorflow::SqueezeOp::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::dim_sizes() const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(shape_ops.o):shape_ops.cc:function tensorflow::ExpandDimsOp::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::dim_sizes() const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(strided_slice_op.o):strided_slice_op.cc:function tensorflow::StridedSliceOp<Eigen::ThreadPoolDevice, int>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::ValidateStridedSliceOp(tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::ShapeReadWriteInterface const&, int, int, int, int, int, tensorflow::ShapeReadWriteInterface*, tensorflow::ShapeReadWriteInterface*, bool*, bool*, bool*, tensorflow::gtl::InlinedVector<long long, 4>*, tensorflow::gtl::InlinedVector<long long, 4>*, tensorflow::gtl::InlinedVector<long long, 4>*)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(strided_slice_op.o):strided_slice_op.cc:function tensorflow::StridedSliceGradOp<Eigen::ThreadPoolDevice, int>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::ValidateStridedSliceOp(tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::ShapeReadWriteInterface const&, int, int, int, int, int, tensorflow::ShapeReadWriteInterface*, tensorflow::ShapeReadWriteInterface*, bool*, bool*, bool*, tensorflow::gtl::InlinedVector<long long, 4>*, tensorflow::gtl::InlinedVector<long long, 4>*, tensorflow::gtl::InlinedVector<long long, 4>*)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(strided_slice_op.o):strided_slice_op.cc:function tensorflow::StridedSliceAssignOp<Eigen::ThreadPoolDevice, int>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::ValidateStridedSliceOp(tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::ShapeReadWriteInterface const&, int, int, int, int, int, tensorflow::ShapeReadWriteInterface*, tensorflow::ShapeReadWriteInterface*, bool*, bool*, bool*, tensorflow::gtl::InlinedVector<long long, 4>*, tensorflow::gtl::InlinedVector<long long, 4>*, tensorflow::gtl::InlinedVector<long long, 4>*)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(strided_slice_op.o):strided_slice_op.cc:function tensorflow::StridedSliceAssignOp<Eigen::ThreadPoolDevice, float>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::ValidateStridedSliceOp(tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::ShapeReadWriteInterface const&, int, int, int, int, int, tensorflow::ShapeReadWriteInterface*, tensorflow::ShapeReadWriteInterface*, bool*, bool*, bool*, tensorflow::gtl::InlinedVector<long long, 4>*, tensorflow::gtl::InlinedVector<long long, 4>*, tensorflow::gtl::InlinedVector<long long, 4>*)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(unpack_op.o):unpack_op.cc:function tensorflow::UnpackOp<Eigen::ThreadPoolDevice, int>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::RemoveDim(int)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(unpack_op.o):unpack_op.cc:function tensorflow::UnpackOp<Eigen::ThreadPoolDevice, float>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::RemoveDim(int)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(cwise_op_select.o):cwise_op_select.cc:function tensorflow::TTypes<int, 2u, int>::Tensor tensorflow::Tensor::flat_outer_dims<int, 2u>(): error: undefined reference to 'tensorflow::Tensor::ComputeFlatOuterDims(long long) const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(cwise_op_select.o):cwise_op_select.cc:function tensorflow::TTypes<float, 2u, int>::Tensor tensorflow::Tensor::flat_outer_dims<float, 2u>(): error: undefined reference to 'tensorflow::Tensor::ComputeFlatOuterDims(long long) const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(maxpooling_op.o):maxpooling_op.cc:function tensorflow::MaxPoolingGradOp<Eigen::ThreadPoolDevice, float>::MaxPoolingGradOp(tensorflow::OpKernelConstruction*): error: undefined reference to 'tensorflow::DeviceTypeString(tensorflow::DeviceType)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(maxpooling_op.o):maxpooling_op.cc:function tensorflow::MaxPoolingGradOp<Eigen::ThreadPoolDevice, Eigen::half>::MaxPoolingGradOp(tensorflow::OpKernelConstruction*): error: undefined reference to 'tensorflow::DeviceTypeString(tensorflow::DeviceType)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(random_op.o):random_op.cc:function tensorflow::(anonymous namespace)::RandomGammaOp<float>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::AppendShape(tensorflow::TensorShape const&)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(save_restore_tensor.o):save_restore_tensor.cc:function tensorflow::Status tensorflow::checkpoint::TensorSliceWriter::Add<int>(std::string const&, tensorflow::TensorShape const&, tensorflow::TensorSlice const&, int const*): error: undefined reference to 'tensorflow::TensorShape::TensorShape(tensorflow::TensorShapeProto const&)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(save_restore_tensor.o):save_restore_tensor.cc:function tensorflow::Status tensorflow::checkpoint::TensorSliceWriter::Add<int>(std::string const&, tensorflow::TensorShape const&, tensorflow::TensorSlice const&, int const*): error: undefined reference to 'tensorflow::TensorShape::AsProto(tensorflow::TensorShapeProto*) const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(save_restore_tensor.o):save_restore_tensor.cc:function tensorflow::Status tensorflow::checkpoint::TensorSliceWriter::Add<int>(std::string const&, tensorflow::TensorShape const&, tensorflow::TensorSlice const&, int const*): error: undefined reference to 'tensorflow::TensorShape::TensorShape(tensorflow::TensorShapeProto const&)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(save_restore_tensor.o):save_restore_tensor.cc:function tensorflow::Status tensorflow::checkpoint::TensorSliceWriter::Add<int>(std::string const&, tensorflow::TensorShape const&, tensorflow::TensorSlice const&, int const*): error: undefined reference to 'tensorflow::SavedTensorSliceMeta_default_instance_'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(save_restore_tensor.o):save_restore_tensor.cc:function tensorflow::Status tensorflow::checkpoint::TensorSliceWriter::Add<int>(std::string const&, tensorflow::TensorShape const&, tensorflow::TensorSlice const&, int const*): error: undefined reference to 'tensorflow::TensorShapeProto_default_instance_'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(save_restore_tensor.o):save_restore_tensor.cc:function tensorflow::Status tensorflow::checkpoint::TensorSliceWriter::Add<float>(std::string const&, tensorflow::TensorShape const&, tensorflow::TensorSlice const&, float const*): error: undefined reference to 'tensorflow::TensorShape::TensorShape(tensorflow::TensorShapeProto const&)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(save_restore_tensor.o):save_restore_tensor.cc:function tensorflow::Status tensorflow::checkpoint::TensorSliceWriter::Add<float>(std::string const&, tensorflow::TensorShape const&, tensorflow::TensorSlice const&, float const*): error: undefined reference to 'tensorflow::TensorShape::AsProto(tensorflow::TensorShapeProto*) const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(save_restore_tensor.o):save_restore_tensor.cc:function tensorflow::Status tensorflow::checkpoint::TensorSliceWriter::Add<float>(std::string const&, tensorflow::TensorShape const&, tensorflow::TensorSlice const&, float const*): error: undefined reference to 'tensorflow::TensorShape::TensorShape(tensorflow::TensorShapeProto const&)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(save_restore_tensor.o):save_restore_tensor.cc:function tensorflow::Status tensorflow::checkpoint::TensorSliceWriter::Add<float>(std::string const&, tensorflow::TensorShape const&, tensorflow::TensorSlice const&, float const*): error: undefined reference to 'tensorflow::SavedTensorSliceMeta_default_instance_'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(save_restore_tensor.o):save_restore_tensor.cc:function tensorflow::Status tensorflow::checkpoint::TensorSliceWriter::Add<float>(std::string const&, tensorflow::TensorShape const&, tensorflow::TensorSlice const&, float const*): error: undefined reference to 'tensorflow::TensorShapeProto_default_instance_'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(save_restore_tensor.o):save_restore_tensor.cc:function bool tensorflow::checkpoint::TensorSliceReader::CopySliceData<int>(std::string const&, tensorflow::TensorSlice const&, int*) const: error: undefined reference to 'tensorflow::TensorProto_default_instance_'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(save_restore_tensor.o):save_restore_tensor.cc:function bool tensorflow::checkpoint::TensorSliceReader::CopySliceData<int>(std::string const&, tensorflow::TensorSlice const&, int*) const: error: undefined reference to 'tensorflow::SavedSlice_default_instance_'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(save_restore_tensor.o):save_restore_tensor.cc:function bool tensorflow::checkpoint::TensorSliceReader::CopySliceData<float>(std::string const&, tensorflow::TensorSlice const&, float*) const: error: undefined reference to 'tensorflow::TensorProto_default_instance_'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(save_restore_tensor.o):save_restore_tensor.cc:function bool tensorflow::checkpoint::TensorSliceReader::CopySliceData<float>(std::string const&, tensorflow::TensorSlice const&, float*) const: error: undefined reference to 'tensorflow::SavedSlice_default_instance_'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(save_restore_v2_ops.o):save_restore_v2_ops.cc:function tensorflow::SaveV2::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::BundleWriter::~BundleWriter()'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(tensor_array_ops.o):tensor_array_ops.cc:function tensorflow::TensorArrayPackOrGatherOp<Eigen::ThreadPoolDevice, int, false>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::PartialTensorShape::IsFullyDefined() const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(tensor_array_ops.o):tensor_array_ops.cc:function tensorflow::TensorArrayPackOrGatherOp<Eigen::ThreadPoolDevice, int, false>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::PartialTensorShape::DebugString() const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(tensor_array_ops.o):tensor_array_ops.cc:function tensorflow::TensorArrayPackOrGatherOp<Eigen::ThreadPoolDevice, int, false>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::InsertDim(int, long long)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(tensor_array_ops.o):tensor_array_ops.cc:function tensorflow::TensorArrayPackOrGatherOp<Eigen::ThreadPoolDevice, int, false>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::PartialTensorShape::IsCompatibleWith(tensorflow::TensorShape const&) const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(tensor_array_ops.o):tensor_array_ops.cc:function tensorflow::TensorArrayPackOrGatherOp<Eigen::ThreadPoolDevice, int, false>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::InsertDim(int, long long)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(tensor_array_ops.o):tensor_array_ops.cc:function tensorflow::TensorArrayPackOrGatherOp<Eigen::ThreadPoolDevice, int, false>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::PartialTensorShape::DebugString() const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(tensor_array_ops.o):tensor_array_ops.cc:function tensorflow::TensorArrayPackOrGatherOp<Eigen::ThreadPoolDevice, Eigen::QInt32, true>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::PartialTensorShape::IsFullyDefined() const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(tensor_array_ops.o):tensor_array_ops.cc:function tensorflow::TensorArrayPackOrGatherOp<Eigen::ThreadPoolDevice, Eigen::QInt32, true>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::PartialTensorShape::DebugString() const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(tensor_array_ops.o):tensor_array_ops.cc:function tensorflow::TensorArrayPackOrGatherOp<Eigen::ThreadPoolDevice, Eigen::QInt32, true>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::PartialTensorShape::IsCompatibleWith(tensorflow::TensorShape const&) const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(tensor_array_ops.o):tensor_array_ops.cc:function tensorflow::TensorArrayPackOrGatherOp<Eigen::ThreadPoolDevice, Eigen::QInt32, true>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::PartialTensorShape::DebugString() const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(tensor_array_ops.o):tensor_array_ops.cc:function tensorflow::TensorArrayPackOrGatherOp<Eigen::ThreadPoolDevice, tensorflow::bfloat16, true>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::PartialTensorShape::IsFullyDefined() const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(tensor_array_ops.o):tensor_array_ops.cc:function tensorflow::TensorArrayPackOrGatherOp<Eigen::ThreadPoolDevice, tensorflow::bfloat16, true>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::PartialTensorShape::IsCompatibleWith(tensorflow::TensorShape const&) const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(tensor_array_ops.o):tensor_array_ops.cc:function tensorflow::TensorArrayPackOrGatherOp<Eigen::ThreadPoolDevice, int, true>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::PartialTensorShape::IsFullyDefined() const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(tensor_array_ops.o):tensor_array_ops.cc:function tensorflow::TensorArrayPackOrGatherOp<Eigen::ThreadPoolDevice, int, true>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::PartialTensorShape::IsCompatibleWith(tensorflow::TensorShape const&) const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(tensor_array_ops.o):tensor_array_ops.cc:function tensorflow::TensorArrayConcatOp<Eigen::ThreadPoolDevice, Eigen::QUInt8>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::RemoveDim(int)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(tensor_array_ops.o):tensor_array_ops.cc:function tensorflow::TensorArrayConcatOp<Eigen::ThreadPoolDevice, int>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShape::RemoveDim(int)'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(topk_op.o):topk_op.cc:function tensorflow::TTypes<int, 2u, int>::ConstTensor tensorflow::Tensor::flat_inner_dims<int, 2u>() const: error: undefined reference to 'tensorflow::Tensor::ComputeFlatInnerDims(long long) const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(topk_op.o):topk_op.cc:function tensorflow::TTypes<float, 2u, int>::ConstTensor tensorflow::Tensor::flat_inner_dims<float, 2u>() const: error: undefined reference to 'tensorflow::Tensor::ComputeFlatInnerDims(long long) const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(topk_op.o):topk_op.cc:function tensorflow::TTypes<int, 2u, int>::Tensor tensorflow::Tensor::flat_inner_dims<int, 2u>(): error: undefined reference to 'tensorflow::Tensor::ComputeFlatInnerDims(long long) const'
C:/Projects/Android-Test1/facerecognitionlibrary/jni-build/jni/libs/armeabi-v7a/libandroid_tensorflow_kernels.lo(topk_op.o):topk_op.cc:function tensorflow::TTypes<float, 2u, int>::Tensor tensorflow::Tensor::flat_inner_dims<float, 2u>(): error: undefined reference to 'tensorflow::Tensor::ComputeFlatInnerDims(long long) const'
tensorflow/core/framework/reader_base.cc:226: error: undefined reference to 'tensorflow::ReaderBaseState::Clear()'
collect2.exe: error: ld returned 1 exit status
make: *** [C:/Projects/Android-Test1/facerecognitionlibrary/build/intermediates/ndkBuild/release/obj/local/armeabi-v7a/libtensorflow.so] Error 1


* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED"
10193,When could it be upgraded to 1.2 from pip? ,"I noticed that 1.2 has released in official website. I tried to upgrade to 1.2 via pip, but it is still 1.1.
How could I upgrade to 1.2?"
10192,How to know what commands and types are supported in iOS build?,"According to this [comment](https://github.com/tensorflow/tensorflow/issues/9934#issuecomment-302817142)  and other related issues currently there is some commands and types that iOS users can't load from `frozen.pb` graph. So we could use any TF API in python but not in iOS. It is hard to guess what python API will not been supported in iOS. So is there any documentation or instructions of how to write solution using python API and what functions and types could be used to make `frozen.pb` graph be fully supported by iOS API?

"
10190,tf.contrib.keras Conv2DTranspose output shape undefined,"The output shape for Conv2DTranspose is not fixed even when the input shape is fixed. The output shape for simple convolutioncomes to be correct.

```
import tensorflow as tf
L = tf.contrib.keras.layers.Conv2DTranspose( 512, (4,4), strides=(4, 4), padding='valid', data_format='channels_first' )
L2 = tf.contrib.keras.layers.Conv2D( 512, (4,4), strides=(4, 4), padding='valid', data_format='channels_first' )

img = tf.placeholder(tf.float32, shape=( 16 , 3 , 64 , 64 ))

x1 = L(img)
x2 = L2(img)

print x1.get_shape() # why ???
print x2.get_shape() # this is fine
```
Outputs : 
```
(?, 512, ?, ?)
(16, 512, 16, 16)
```"
10189,[Tensorboard] AssertionError: Cannot find .runfiles directory for /usr/bin/tensorboard,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 25
- **PIP Version**: 9.0.1
- **Tensorboard Version**: 1.0.0a7


### Describe the problem
When installing Tensorboard with pip, it is not executeable and producing following error.
I could reproduce this on my laptop and Tower both running Fedora 25.
Is there some issue with the tensorboard whl?

### Install

```
$ sudo pip3 install tensorboard
Collecting tensorboard
  Using cached tensorboard-1.0.0a7-cp35-cp35m-manylinux1_x86_64.whl
Requirement already satisfied: protobuf>=3.1.0 in /usr/lib64/python3.5/site-packages (from tensorboard)
Requirement already satisfied: six>=1.10.0 in /usr/lib/python3.5/site-packages (from tensorboard)
Requirement already satisfied: Pillow>=4.0.0 in /usr/lib64/python3.5/site-packages (from tensorboard)
Requirement already satisfied: numpy>=1.11.0 in /usr/lib64/python3.5/site-packages (from tensorboard)
Requirement already satisfied: wheel>=0.26 in /usr/lib/python3.5/site-packages (from tensorboard)
Requirement already satisfied: werkzeug>=0.11.10 in /usr/lib64/python3.5/site-packages (from tensorboard)
Requirement already satisfied: setuptools in /usr/lib/python3.5/site-packages (from protobuf>=3.1.0->tensorboard)
Requirement already satisfied: olefile in /usr/lib/python3.5/site-packages (from Pillow>=4.0.0->tensorboard)
Requirement already satisfied: packaging>=16.8 in /usr/lib/python3.5/site-packages (from setuptools->protobuf>=3.1.0->tensorboard)
Requirement already satisfied: appdirs>=1.4.0 in /usr/lib/python3.5/site-packages (from setuptools->protobuf>=3.1.0->tensorboard)
Requirement already satisfied: pyparsing in /usr/lib/python3.5/site-packages (from packaging>=16.8->setuptools->protobuf>=3.1.0->tensorboard)
Installing collected packages: tensorboard
Successfully installed tensorboard-1.0.0a7
```

### Starting tensorboard

Starting by just running ```tesorboard``` in terminal:

```
Traceback (most recent call last):
  File ""/usr/bin/tensorboard"", line 160, in <module>
    Main()
  File ""/usr/bin/tensorboard"", line 110, in Main
    module_space = FindModuleSpace()
  File ""/usr/bin/tensorboard"", line 91, in FindModuleSpace
    sys.argv[0])
AssertionError: Cannot find .runfiles directory for /usr/bin/tensorboard
```
"
10187,inter_op_parallelism_threads on Window for GPU models,"I need to limit CPU usage of tensorflow on Windows by setting inter_op_parallelism_threads = 1 , ( intra_op_parallelism_threads = 1  also for CPU)

When I run a model on CPU, inter_op_parallelism_threads = 1  works perfectly, and only one logical core is used.

But when I run a model on GPU,  inter_op_parallelism_threads =1 doesn't work,  tensorflow still uses all the available logical cores. "
10186,Bug in r1.2rc,"[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/image/kernels/image_ops.h#L69](url)

static will cause a compile error(Its a TODO:) )"
10185,Unable to create so file ,"I have got the build file in my` tensorflow-master/tensorflow` directory. Following is the build file contents

```
cc_binary(
    name = ""libtensorflow.so"",
    copts = tf_copts(),
    linkshared = 1,
    linkopts = [
        ""-lpthread"",
        ""-lm"",
    ],
    deps = [
        "":cc_ops"",
        ""//tensorflow/core:kernels"",
        ""//tensorflow/core:tensorflow"",
    ],
)
```

Executing this build file, throws the following error
```
/Users/Johnny/Downloads/tensorflow-master/tensorflow/BUILD: line 1: syntax error near unexpected token `newline'
/Users/Johnny/Downloads/tensorflow-master/tensorflow/BUILD: line 1: `cc_binary('
logout
```"
10184,the performance is Unexpectedly in iOS,"today i test Tensorflow(TF) iOS example with my iPhone 6S , according to the introduction in TF Website and source code , i know it use Apple's Accelerate framework , i build the protobuf , and TF's source code in my Mac , then run iOS example , i record the time with the code
```
tensorflow::Status run_status = tf_session->Run(
        {{input_layer_name, image_tensor}}, {output_layer_name}, {}, &outputs);
```
and the time is fast, only 90ms, i know TF's iOS example use the Google Inception V1 Model , and i test Apple's example which use Google Inception V3 Model , the time is 120ms, metal is more slow than Accelerate framework ? i can not understand . i do not think there is too much different feature that affect performance between inception V1 and V3... so how to explain it ?"
10183,tf.nn.dynamic_rnn seems not working when given sequence_length,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.1
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.2.0rc0
- **CUDA/cuDNN version**: CUDA: 8.0 cuDNN: 5.1
- **GPU model and memory**: GTX 1080 8GB

### Describe the problem
I tried to migrate my code from tensorflow r1.1.0 to tensorflow r1.2.0rc0.
When using tf.nn.dynamic_rnn, if sequence_length is not None, It got an error(traceback pasted below).
However, if I set sequence_length to None, it works properly as before.
Is this a bug or I got something wrong?

### Source code / logs
Source code:
``` python
tf.nn.dynamic_rnn(
    cell,
    inputs,
    dtype=tf.float32,
    sequence_length=lens)
```
where lens is a Tensor:
```
Tensor(""Gather_DequeueMany:0"", shape=(256,), dtype=int32)
```

Traceback:
```
Traceback (most recent call last):
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py"", line 671, in _call_cpp_shape_fn_impl
    input_tensors_as_shapes, status)
  File ""/usr/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes must be equal rank, but are 0 and 1 for 'rnn/while/Select_4' (op: 'Select') with input shapes: [256], [], [].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""main.py"", line 74, in <module>
    tf.app.run()
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""main.py"", line 70, in main
    trainer = ModelTrainer(config)
  File ""/home/carbon/Codes/seq2seqmapmatching/model_trainer.py"", line 25, in __init__
    self.model.construct(self.train_reader.queue, self.valid_reader.queue, self.global_step)
  File ""/home/carbon/Codes/seq2seqmapmatching/seq2seq_model.py"", line 35, in construct
    global_step=global_step)
  File ""/home/carbon/Codes/seq2seqmapmatching/seq2seq_model.py"", line 118, in build_model
    outputs, decoder_state = self.build_decoder(encoder_outputs, inputs, per_h, lens, pts_lens)
  File ""/home/carbon/Codes/seq2seqmapmatching/seq2seq_model.py"", line 185, in build_decoder
    sequence_length=lens)
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 566, in dynamic_rnn
    dtype=dtype)
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 729, in _dynamic_rnn_loop
    swap_memory=swap_memory)
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2766, in while_loop
    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2595, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2545, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 712, in _time_step
    skip_conditionals=True)
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 202, in _rnn_step
    final_output_and_state = _copy_some_through(new_output, new_state)
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 174, in _copy_some_through
    for state, new_state in zip(flat_state, flat_new_state)]
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 174, in <listcomp>
    for state, new_state in zip(flat_state, flat_new_state)]
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 163, in _copy_one_through
    return array_ops.where(copy_cond, output, new_output)
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py"", line 2328, in where
    return gen_math_ops._select(condition=condition, t=x, e=y, name=name)
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 2145, in _select
    name=name)
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2508, in create_op
    set_shapes_for_outputs(ret)
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1873, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1823, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py"", line 610, in call_cpp_shape_fn
    debug_python_shape_fn, require_shape_fn)
  File ""/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py"", line 676, in _call_cpp_shape_fn_impl
    raise ValueError(err.message)
ValueError: Shapes must be equal rank, but are 0 and 1 for 'rnn/while/Select_4' (op: 'Select') with input shapes: [256], [], [].
```"
10182,tensorflow: How to access and reuse feature column values?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution Linux Ubuntu 16.04**:
- **TensorFlow installed from pip**:
- **TensorFlow version 1.1.0**:

### problem

In tensorflow's wide & deep model tutorial, it constructed some feature columns like:

```
wide_columns = [gender, native_country, education, occupation, workclass,
              relationship, age_buckets,
              tf.contrib.layers.crossed_column([education, occupation],
                                               hash_bucket_size=int(1e4)),
              tf.contrib.layers.crossed_column(
                  [age_buckets, education, occupation],
                  hash_bucket_size=int(1e6)),
              tf.contrib.layers.crossed_column([native_country, occupation],
                                               hash_bucket_size=int(1e4))]
```
I want to directly reuse his columns result/values to construct my wide & deep model with another deep learning library, but how can I access these values after feed inputs ? or how to save into file ? （I didn't found any method that export true data）"
10179,Support python3 on Docker image tensorflow/tensorflow:latest,"### System information

I'm running the `tensorflow/tensorflow:latest` Docker image

```bash
$ docker run -it --rm tensorflow/tensorflow python3
Python 3.5.2 (default, Nov 17 2016, 17:05:23)
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: No module named 'tensorflow'
```

Note that the same works perfectly for Python 2:

```bash
✗ docker run -it --rm tensorflow/tensorflow python
Python 2.7.12 (default, Nov 19 2016, 06:48:10)
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
>>>
```

### Describe the problem

Importing tensorflow from python3 fails for the tensorflow/tensorflow Docker image
This is surprising because python3 itself is installed, so the solution is:

a) make tensorflow be importable from python3
b) remote python3 so it's obvious you need to use a different tag (point at it, maybe)"
10178,Setter for Tensor.shape?,"`tensor.shape` is equivalent to `tensor.get_shape()` in TF 1.0 (see https://github.com/tensorflow/tensorflow/issues/586).

Should we also make `tensor.shape = new_shape` equivalent to `tensor.set_shape(new_shape)`? This feels natural and results in slightly more idiomatic Python.

This would be straightforwardly done with a setter method, e.g.,
```
@shape.setter
def shape(self, new_shape):
    self.set_shape(new_shape)
```"
10173,about cudnn version,"which version of cud should I use?
I saw cudnn has developed to v8, but the tensorflow recommend https://www.tensorflow.org/install/install_mac (V5.1)
should I use a new one"
10171,tf.contrib.rnn.decoder does not require explicitly build encoder?,"As a beginner to tensorflow, I need to build a LSTM encoder decoder framework for images.
The tf.contrib.seq2seq.decoder does not require explicitly build encoder? Does it take the output from previous step as input? Thus the decoder_inputs are actually the input for encoder?"
10170,re,
10169,tf.resize_images return image full of noise but work on resize_image_with_crop_or_pad,"- **OS OSX 10.12**:
- **TensorFlow installed from: source**:
- **TensorFlow version 1.2.0rc**:
- **Bazel version: 0.45**:
- **CUDA: CPU-only**:
- **GPU: CPU-only**:
```
img = tf.image.decode_jpeg(tf.read_file(path), channels=3)
img = tf.image.resize_images(img, [200, 200])
sess.run(img)
```
![damaged!](https://cloud.githubusercontent.com/assets/18662395/26419617/de547202-40f2-11e7-82c4-3640990d625c.png)


```
img = tf.image.decode_jpeg(tf.read_file(path), channels=3)
img = tf.image.resize_image_with_crop_or_pad(img, 200, 200)
sess.run(img)
```

![worked](https://cloud.githubusercontent.com/assets/18662395/26419629/ec09f1a6-40f2-11e7-9f44-0807633f60da.png)
"
10167,Ability to completelly reset tensorflow state (Session.close() does not do that),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: parallels ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.1.0
- **Bazel version (if compiling from source)**: 0.4.5-jdk7
- **CUDA/cuDNN version**: no cuda
- **GPU model and memory**: no gpu
- **Exact command to reproduce**: see the code below

### Describe the problem

Bug:
session.close() seems to imply that all resources would be freed and tensorflow state would be reset.
or Feature request:
If I'm wrong about session.close(), then there should be a way to reset tensorflow state. 

When I first time create a session, tensorflow seem to initialize certain variables that I see as a printout like ""platform Host present with 8 visible devices"". However after I close the session I expect these resources to be freed. At least there would be nice to maybe have an additional command to close the device. After running this ""complete_device_close()"" command, I expect that a new session will initialize device again with the same printout. 

The lack of this behavior currently does not allow to run tensorflow sessions after a fork even in the case where I know that the parent process is never going to use tensorflow again. The following code hangs:
```python
   import multiprocessing
   import tensorflow as tf

    def _runner():
        sess = tf.Session()
        sess.run(tf.Variable(0.).initializer)  # this hangs after fork
        sess.close()  # here all resources should be cleared

    _runner()

    p = multiprocessing.Process(target=_runner)
    p.start()
    p.join()
```
There are multiple issues on ""hanging after fork"" topic:
https://github.com/tensorflow/tensorflow/issues/5448
https://github.com/tensorflow/tensorflow/issues/2448
with a solution to create a server or not to fork at all. However in my case parent process does not want to use tensorflow anymore so I expect to close it forever and the parent process and reopen it in a child. Suggested workarounds do not work for me because parent tensorflow use and a use in a child after fork happens in totally unrelated parts of the system and at different times and creating any coupling between them would be really ugly. We would better establish a practice to ""properly close tensorflow after you done"".

Notice that initializing devices in multiple children after fork works fine if parent is not involved:
```python
   import multiprocessing
   import tensorflow as tf

    def _runner_2(index):
        sess = tf.Session()
        for i in range(10):
            import time
            time.sleep(0.1)
            print('running %d' % index)
            sess.run(tf.Variable(0.).initializer)
        sess.close()

    for j in range(10):
        p = multiprocessing.Process(target=_runner_2, args=(j,))
        p.start()
```
This code prints ""platform Host present with 8 visible devices"" 10 times.
This suggests that there is no underlying reason to prevent sharing of CPU or any other resources in this case (I do not have GPU).

Alternative phrasing of the feature request would be:
""Make tensorflow fork-safe after certain full deinitialization command""

### Source code / logs
see above
"
10166,OOM error when initializing float tensors 1/2 the size of the VRAM (in bytes),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04 (64 bit)
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu
- **TensorFlow version (use command below)**:  ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8.0/5.1.10-1
- **GPU model and memory**: K40 / 12 GB (also GTX 980 / 4 GB)
- **Exact command to reproduce**:

`time CUDA_VISIBLE_DEVICES=1 python memory_usage.py`

TF encounters OOM when I try to initialize an array half the size of the VRAM, but only when it's a FLOAT type (float16, float32, float64). This happens even when `trainable=False`.

I checked via `nvidia-smi` that no other process is using the GPU.

```
import tensorflow as tf

vram = 12 * 1024 ** 3 # 12 GB

def use_half_vram(dtype):
    n = vram // dtype.size // 2
    x = tf.get_variable('x', shape=[n], dtype=dtype, \
        initializer=tf.constant_initializer(7), trainable=False)
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())

# use_half_vram(tf.uint8) # OK
# use_half_vram(tf.int32) # OK
# use_half_vram(tf.int64) # OK
# use_half_vram(tf.float16) # OOM
use_half_vram(tf.float32) # OOM
# use_half_vram(tf.float64) # OOM
```

"
10163,Custom Poets Models Run Slow on Android,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Not really.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android/Windows
- **TensorFlow installed from (source or binary)**: Binary (From android nightly
- **TensorFlow version (use command below)**: 1.2

### Describe the problem
I've noticed that using a retrained inception model within the demo app, following the guidelines suggested, is awfully slow. Shouldn't the custom models generated in the style of Tensorflow for poets be pretty similar to the inception model that the demo comes with? I have noticed inference times to be around 5 times as slow on two devices. (Nexus 6P and Pixel C) compared to the original demo.
Even when the graphs are quantized I am getting no apparent performance increase (apart from model size). If anything it's actually slower.
Is this normal behaviour? I'm aware of the image size is different (224 vs 299) but is that enough to haemorrhage the performance?

### Source code / logs
Avg. ms for Conv2D is 1366ms 
Inference time ~1700ms (Pixel C) ~3500 (Nexus 6P)

Model building steps: Normal Model via Tensorflow for poets etc. --> strip nodes --> quantize --> replace in apk.
Same performance regardless of quantization.

@andrewharp this was what I referred to in the windows/android thread. Can move to s/o if preferred."
10162,ResourceExhaustedError,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom adaptation of tensorflow doc example
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS 10.11.3 El Capitan
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.1.0-rc0-61-g1ec6ed5 1.1.0 / python 3.5.2
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: No CUDA-compliant GPU on Mac Book Pro
- **Exact command to reproduce**:  see full code below

### Describe the problem

The gist below fails reproducibly with the following error both from a jupyter notebook or the command line : 
```
ResourceExhaustedError (see above for traceback): ./models/m9-6/model.ckpt-826.data-00000-of-00001
	 [[Node: save/RestoreV2_6 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_6/tensor_names, save/RestoreV2_6/shape_and_slices)]]
```
I added some explict `del` and explicit garbage collection but that doesn't seem to change anything. I'm not sure if this is an OOM per se or if it is a different kind of resource exhaustion. In any case I wasn't expecting fitting a fresh model `m` repeatedly in a loop to be an issue so I'm reporting this in case it happens to be a TF issue.

### Source code / logs

https://gist.github.com/lelayf/f81b078a197b30490d6d52ba3f02f0a4
"
10157,bazel build failure of tensorflow with mkl and specific eigen3 flags,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Don't have code yet

- **OS Platform and Distribution
Linux Ubuntu 16.04

- **TensorFlow installed from (source or binary)**:
git clone latest revision (TensorFlow 1.1)
- **TensorFlow version (use command below)**:

- **Bazel version (if compiling from source)**:
Build label: 0.4.5
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Mar 16 12:19:38 2017 (1489666778)
Build timestamp: 1489666778
Build timestamp as int: 1489666778

- **CUDA/cuDNN version**:
None
- **GPU model and memory**:
None

- **Exact command to reproduce**:
I don't understand what the above sentence refers to?

### Describe the problem

Bazel failed to build/compile tensor flow with mkl support
I added these compiler flags during the configure phase and they caused the compilation error:

 -DEIGEN_USE_MKL_ALL -DMKL_ILP64 

### Source code / logs

Configure phase:

```

drormeirovich@drormeirovich-xps-13-9360:~/projects/tensorflow$ sudo ./configure 
Please specify the location of python. [Default is /usr/bin/python]: 
Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]

Using python library path: /usr/local/lib/python2.7/dist-packages
Do you wish to build TensorFlow with MKL support? [y/N] y
MKL support will be enabled for TensorFlow
Do you wish to download MKL LIB from the web? [Y/n] n
Please specify the location where MKL is installed. [Default is /opt/intel/mklml]: /opt/intel/mkl
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: -O3 -DNDEBUG -fPIC -DEIGEN_USE_MKL_ALL -DMKL_ILP64 -fopenmp -m64 -v -I/opt/intel/mkl/include
Do you wish to use jemalloc as the malloc implementation? [Y/n] 
jemalloc enabled
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] 
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] 
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] 
No XLA support will be enabled for TensorFlow
Do you wish to build TensorFlow with VERBS support? [y/N] 
No VERBS support will be enabled for TensorFlow
Do you wish to build TensorFlow with OpenCL support? [y/N] 
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] 
No CUDA support will be enabled for TensorFlow
INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.
Configuration finished

```
Build command...

`sudo bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
`
Error output:

```
ERROR: /home/drormeirovich/projects/tensorflow/tensorflow/core/kernels/BUILD:998:1: C++ compilation of rule '//tensorflow/core/kernels:gather_functor' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 64 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
Using built-in specs.
COLLECT_GCC=/usr/bin/gcc
Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu 6.2.0-3ubuntu11~16.04' --with-bugurl=file:///usr/share/doc/gcc-6/README.Bugs --enable-languages=c,ada,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-6 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-6-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-6-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-6-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
Thread model: posix
gcc version 6.2.0 20160901 (Ubuntu 6.2.0-3ubuntu11~16.04) 
COLLECT_GCC_OPTIONS='-U' '_FORTIFY_SOURCE' '-fstack-protector' '-Wall' '-B' '/usr/bin' '-B' '/usr/bin' '-Wunused-but-set-parameter' '-Wno-free-nonheap-object' '-fno-omit-frame-pointer' '-g0' '-O2' '-D' '_FORTIFY_SOURCE=1' '-D' 'NDEBUG' '-ffunction-sections' '-fdata-sections' '-O3' '-D' 'NDEBUG' '-D' 'EIGEN_USE_MKL_ALL' '-D' 'MKL_ILP64' '-v' '-I' '/opt/intel/mkl/include' '-std=c++11' '-O3' '-D' 'NDEBUG' '-D' 'EIGEN_USE_MKL_ALL' '-D' 'MKL_ILP64' '-fopenmp' '-m64' '-v' '-I' '/opt/intel/mkl/include' '-MD' '-MF' 'bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/gather_functor/tensorflow/core/kernels/gather_functor.pic.d' '-frandom-seed=bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/gather_functor/tensorflow/core/kernels/gather_functor.pic.o' '-fPIC' '-D' 'EIGEN_MPL2_ONLY' '-iquote' '.' '-iquote' 'bazel-out/local-opt/genfiles' '-iquote' 'external/bazel_tools' '-iquote' 'bazel-out/local-opt/genfiles/external/bazel_tools' '-iquote' 'external/eigen_archive' '-iquote' 'bazel-out/local-opt/genfiles/external/eigen_archive' '-iquote' 'external/local_config_sycl' '-iquote' 'bazel-out/local-opt/genfiles/external/local_config_sycl' '-isystem' 'external/bazel_tools/tools/cpp/gcc3' '-isystem' 'external/eigen_archive' '-isystem' 'bazel-out/local-opt/genfiles/external/eigen_archive' '-D' 'EIGEN_AVOID_STL_ARRAY' '-I' 'external/gemmlowp' '-Wno-sign-compare' '-fno-exceptions' '-msse3' '-pthread' '-fno-canonical-system-headers' '-Wno-builtin-macro-redefined' '-D' '__DATE__=""redacted""' '-D' '__TIMESTAMP__=""redacted""' '-D' '__TIME__=""redacted""' '-c' '-o' 'bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/gather_functor/tensorflow/core/kernels/gather_functor.pic.o' '-mtune=generic' '-march=x86-64' '-pthread'
 /usr/lib/gcc/x86_64-linux-gnu/6/cc1plus -quiet -v -v -I /opt/intel/mkl/include -I /opt/intel/mkl/include -I external/gemmlowp -imultiarch x86_64-linux-gnu -MD bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/gather_functor/tensorflow/core/kernels/gather_functor.pic.d -MF bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/gather_functor/tensorflow/core/kernels/gather_functor.pic.d -MQ bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/gather_functor/tensorflow/core/kernels/gather_functor.pic.o -D_GNU_SOURCE -D_REENTRANT -U _FORTIFY_SOURCE -D _FORTIFY_SOURCE=1 -D NDEBUG -D NDEBUG -D EIGEN_USE_MKL_ALL -D MKL_ILP64 -D NDEBUG -D EIGEN_USE_MKL_ALL -D MKL_ILP64 -D EIGEN_MPL2_ONLY -D EIGEN_AVOID_STL_ARRAY -D __DATE__=""redacted"" -D __TIMESTAMP__=""redacted"" -D __TIME__=""redacted"" -iquote . -iquote bazel-out/local-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/local-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local-opt/genfiles/external/local_config_sycl -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/local-opt/genfiles/external/eigen_archive tensorflow/core/kernels/gather_functor.cc -quiet -dumpbase gather_functor.cc -m64 -msse3 -mtune=generic -march=x86-64 -auxbase-strip bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/gather_functor/tensorflow/core/kernels/gather_functor.pic.o -g0 -O2 -O3 -O3 -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -Wno-sign-compare -Wno-builtin-macro-redefined -std=c++11 -version -fstack-protector -fno-omit-frame-pointer -ffunction-sections -fdata-sections -fopenmp -frandom-seed=bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/gather_functor/tensorflow/core/kernels/gather_functor.pic.o -fPIC -fno-exceptions -fno-canonical-system-headers -Wformat-security -o /tmp/ccloY0qb.s
GNU C++11 (Ubuntu 6.2.0-3ubuntu11~16.04) version 6.2.0 20160901 (x86_64-linux-gnu)
	compiled by GNU C version 6.2.0 20160901, GMP version 6.1.0, MPFR version 3.1.4, MPC version 1.0.3, isl version 0.15
GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
ignoring nonexistent directory ""external/bazel_tools/tools/cpp/gcc3""
ignoring nonexistent directory ""bazel-out/local-opt/genfiles/external/eigen_archive""
ignoring duplicate directory ""/usr/include/x86_64-linux-gnu/c++/6""
ignoring nonexistent directory ""/usr/local/include/x86_64-linux-gnu""
ignoring nonexistent directory ""/usr/lib/gcc/x86_64-linux-gnu/6/../../../../x86_64-linux-gnu/include""
ignoring duplicate directory ""/opt/intel/mkl/include""
ignoring nonexistent directory ""bazel-out/local-opt/genfiles/external/bazel_tools""
ignoring duplicate directory ""external/eigen_archive""
  as it is a non-system directory that duplicates a system directory
ignoring nonexistent directory ""bazel-out/local-opt/genfiles/external/eigen_archive""
ignoring nonexistent directory ""bazel-out/local-opt/genfiles/external/local_config_sycl""
#include ""..."" search starts here:
 .
 bazel-out/local-opt/genfiles
 external/bazel_tools
 external/local_config_sycl
#include <...> search starts here:
 /opt/intel/mkl/include
 external/gemmlowp
 external/eigen_archive
 /usr/include/c++/6
 /usr/include/x86_64-linux-gnu/c++/6
 /usr/include/c++/6/backward
 /usr/lib/gcc/x86_64-linux-gnu/6/include
 /usr/local/include
 /usr/lib/gcc/x86_64-linux-gnu/6/include-fixed
 /usr/include/x86_64-linux-gnu
 /usr/include
End of search list.
GNU C++11 (Ubuntu 6.2.0-3ubuntu11~16.04) version 6.2.0 20160901 (x86_64-linux-gnu)
	compiled by GNU C version 6.2.0 20160901, GMP version 6.1.0, MPFR version 3.1.4, MPC version 1.0.3, isl version 0.15
GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
Compiler executable checksum: 23988a38771f71e4676d56931fe884f7
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:522:0,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/kernels/gather_functor.h:19,
                 from tensorflow/core/kernels/gather_functor.cc:52:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/products/GeneralMatrixMatrix_BLAS.h: In static member function 'static void Eigen::internal::general_matrix_matrix_product<Index, double, LhsStorageOrder, ConjugateLhs, double, RhsStorageOrder, ConjugateRhs, 0>::run(Index, Index, Index, const double*, Index, const double*, Index, double*, Index, double, Eigen::internal::level3_blocking<double, double>&, Eigen::internal::GemmParallelInfo<Index>*)':
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/products/GeneralMatrixMatrix_BLAS.h:106:1: error: cannot convert 'Eigen::BlasIndex* {aka long long int*}' to 'const int*' for argument '3' to 'int dgemm_(const char*, const char*, const int*, const int*, const int*, const double*, const double*, const int*, const double*, const int*, const double*, double*, const int*)'
 GEMM_SPECIALIZATION(double,   d,  double, d)
 ^

```"
10155,Estimator should be able to partially load checkpoints,"### Describe the problem
When training neural networks and experimenting with different architectures or simply adapting a model to a new number of classes, it is crucial to be able to reuse an existing trained model as far as possible. For example, if I want to use the inception-v4 architecture and train it on 700 instead of 1000 classes, I need to be able to load all layers but the logit ones.

Unfortunately, this is not possible (at least I wasn't able to find a way) with the Estimator API. Whenever the size of a variable in my model changes or I add or remove a variable, the Estimator cannot load an existing checkpoint any more. This is a major drawback making the Estimator basically unusable for developing a new architecture or adapting an existing one by iteratively adapting the model.

### Requested features
* It should be possible to tell the Estimator that it's ok if some variables aren't found in the checkpoint. Those should simply be initialized as if no checkpoint would be loaded.
* It should be possible to specify scopes that should not be loaded from the checkpoint or to specify a flag that says something like ""just don't load variables that have a different shape / that you can't load"".
* Be able to load an existing checkpoint from a different path than the Estimator's `model_dir` when there is no checkpoint in the `model_dir` yet. This is helpful to start training from a different checkpoint without manully having to copy those model's checkpoints into the new `model_dir`

### Inspiration
This request has been inspired by the parameters you can specify to the [train_image_classifier.py](https://github.com/tensorflow/models/blob/master/slim/train_image_classifier.py) script from the tensorflow-models/slim directory. There you have the parameters `--checkpoint_exclude_scopes`, `--ignore_missing_vars` and `--checkpoint_path`.

Of course, one could say it's possible to implement this manually. But I think these are basic functionalities for everyone doing a bit more deeplearning than only the tutorial. That's why I think this should be part of the otherwise easy to use Estimator API. "
10154,tensorflow-gpu windows10 ImportError: No module named '_pywrap_tensorflow_internal',"Hi, I have encountered the following issue when importing the gpu version of tensorflow in python3.5 on windows10:

C:\Users\Gwendoline>activate tensorflow-gpu

(tensorflow-gpu) C:\Users\Gwendoline>python
Python 3.5.3 |Continuum Analytics, Inc.| (default, May 15 2017, 10:43:23) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\Gwendoline\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Gwendoline\Anaconda3\envs\tensorflow-gpu\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 919, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Le module spécifié est introuvable.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Gwendoline\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Gwendoline\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Gwendoline\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Gwendoline\Anaconda3\envs\tensorflow-gpu\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Gwendoline\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Gwendoline\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\__init__.py"", line 51, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Gwendoline\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Gwendoline\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Gwendoline\Anaconda3\envs\tensorflow-gpu\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 919, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Le module spécifié est introuvable.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Gwendoline\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Gwendoline\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Gwendoline\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Gwendoline\Anaconda3\envs\tensorflow-gpu\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.


I have downloaded the 8.0 version of CUDA and cuDNN v5.1, and have put the cuDNN files cudnn64_5.dll, cudnn.h, and cudnn.lib respectively in the CUDA repositories bin\, include\ and lib\x64. The corresponding environment variable path is set as C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\bin. Can you help ? Many thanks"
10153,Android: No OpKernel was registered to support Op 'ExtractImagePatches',"Hi, I have read (#9763 =>) #9476 #5764 #8486 #6260 #5921 (#1269 This one is too old to be helpful).

I was trying to use YOLOv2 on Android TensorFlow. I followed the exact same procedure stated in the README that successfully got the tiny-yolo-voc model running on my phones, the only different in the code is the filename. But I got the following error message and the app die.

```
FATAL EXCEPTION: inference
Process: org.tensorflow.demo, PID: 31154
java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'ExtractImagePatches' with these attrs.  Registered devices: [CPU], Registered kernels: <no registered kernels>
                                                                     
[[Node: ExtractImagePatches = ExtractImagePatches[T=DT_FLOAT, ksizes=[1, 2, 2, 1], padding=""VALID"", rates=[1, 1, 1, 1], strides=[1, 2, 2, 1]](concat)]]
at org.tensorflow.Session.run(Native Method)
at org.tensorflow.Session.access$100(Session.java:48)
at org.tensorflow.Session$Runner.runHelper(Session.java:295)
at org.tensorflow.Session$Runner.run(Session.java:245)
at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:142)
at org.tensorflow.demo.TensorFlowYoloDetector.recognizeImage(TensorFlowYoloDetector.java:165)
at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:313)
at android.os.Handler.handleCallback(Handler.java:755)
at android.os.Handler.dispatchMessage(Handler.java:95)
at android.os.Looper.loop(Looper.java:156)
at android.os.HandlerThread.run(HandlerThread.java:61)
```

I also tried Yolo9000, it works (actually died because I didnot provide the matching label/name set, but it didnot yield the same error).

1. I tried optimize_for_inference (https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/amp/), but the new pb file still have ""ExtractImagePatches"" op, so it didn't work for me.
```
$ grep ""ExtractImagePatches"" *
Binary file graph-yolo-voc.pb matches
Binary file opt.pb matches
```

2. I generated ops_to_register.h (shown below) and put it in tensorflow/tensorflow/core/framework dir (#8486), and it didn't work for me.
```
#ifndef OPS_TO_REGISTER
#define OPS_TO_REGISTER
constexpr inline bool ShouldRegisterOp(const char op[]) {
  return false
     || (strcmp(op, ""BiasAdd"") == 0)
     || (strcmp(op, ""ConcatV2"") == 0)
     || (strcmp(op, ""Const"") == 0)
     || (strcmp(op, ""Conv2D"") == 0)
     || (strcmp(op, ""ExtractImagePatches"") == 0)
     || (strcmp(op, ""Identity"") == 0)
     || (strcmp(op, ""MaxPool"") == 0)
     || (strcmp(op, ""Maximum"") == 0)
     || (strcmp(op, ""Mul"") == 0)
     || (strcmp(op, ""NoOp"") == 0)
     || (strcmp(op, ""Pad"") == 0)
     || (strcmp(op, ""Placeholder"") == 0)
     || (strcmp(op, ""RealDiv"") == 0)
     || (strcmp(op, ""Sub"") == 0)
     || (strcmp(op, ""_Recv"") == 0)
     || (strcmp(op, ""_Send"") == 0)
  ;
}
#define SHOULD_REGISTER_OP(op) ShouldRegisterOp(op)


    namespace {
      constexpr const char* skip(const char* x) {
        return (*x) ? (*x == ' ' ? skip(x + 1) : x) : x;
      }

      constexpr bool isequal(const char* x, const char* y) {
        return (*skip(x) && *skip(y))
                   ? (*skip(x) == *skip(y) && isequal(skip(x) + 1, skip(y) + 1))
                   : (!*skip(x) && !*skip(y));
      }

      template<int N>
      struct find_in {
        static constexpr bool f(const char* x, const char* const y[N]) {
          return isequal(x, y[0]) || find_in<N - 1>::f(x, y + 1);
        }
      };

      template<>
      struct find_in<0> {
        static constexpr bool f(const char* x, const char* const y[]) {
          return false;
        }
      };
    }  // end namespace
    constexpr const char* kNecessaryOpKernelClasses[] = {
""BiasOp<CPUDevice, float>"",
""ConcatV2Op<CPUDevice, float>"",
""ConstantOp"",
""Conv2DOp<CPUDevice, float>"",
""ExtractImagePatchesOp<CPUDevice, float>"",
""IdentityOp"",
""MaxPoolingOp<CPUDevice, float>"",
""BinaryOp<CPUDevice, functor::maximum<float>>"",
""BinaryOp<CPUDevice, functor::mul<float>>"",
""NoOp"",
""PadOp<CPUDevice, float>"",
""PlaceholderOp"",
""BinaryOp<CPUDevice, functor::div<float>>"",
""BinaryOp<CPUDevice, functor::sub<float>>"",
""RecvOp"",
""SendOp"",
};
#define SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))

#define SHOULD_REGISTER_OP_GRADIENT false
#endif

```

3. I added `tensorflow/core/kernels/extract_image_patches_op.cc` to `tf_op_files.txt`(#5764), and run bazel like this `$ bazel build -c opt //tensorflow/examples/android:tensorflow_demo --copt=""-DSELECTIVE_REGISTRATION"" --define ANDROID_TYPES=__ANDROID_TYPES_FULL__` (#9476), still, didn't work for me.

I am using ubuntu 16.04, 
TensorFlow installed from (I think I did not install because I just `pip list` and tensorflow was not on that list. I just follow https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android , and every thing works for tiny-yolo-voc),
[bazel release 0.4.5],
my phone is running Android 7.0

Thanks,
CHL"
10152,Defragmentation support of BFCAllocator,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 208192074d7d19f6b51724bb06fd4c2a143649e7
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

From the documentation of the [`BFCAllocator`](https://github.com/tensorflow/tensorflow/blob/208192074d7d19f6b51724bb06fd4c2a143649e7/tensorflow/core/common_runtime/bfc_allocator.h#L44), it aims to bring defragmentation support, which might be important for GPUs with relatively small memory so that potentially larger graphs can be supported.  The defragmentation part seems to be missing from the current implementation. Is there any plan on this, or any thoughts can be shared?
"
10151,Support dictionaries of tensors in tf.contrib.data.Dataset,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary (nightly build)
- **TensorFlow version (use command below)**: v1.2.0-rc0-172-g9e25de3
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: `python3 test.py`

### Describe the problem
With the new `tf.contrib.data.Dataset`, it is possible to work with tuples and lists of tensors, but not with dictionaries. Using dictionary structures is supported for things like `tf.batch`, and it would make migrating to Dataset easier if it also supported it.

### Source code / logs
test.py
```python
import tensorflow as tf

dataset = tf.contrib.data.Dataset.range(3)

# Using tuples works
dataset_tup = dataset.map(lambda x: (x, x * 10))
# Using lists works
dataset_list = dataset.map(lambda x: [x, x * 10])
# Using dicts doesn't work
# dataset_dict = dataset.map(lambda x: {'a': x, 'b': x * 10})

# Select dataset_tup, dataset_list or dataset_struct to output the values
dataset_out = dataset_tup

iterator = dataset_tup.make_one_shot_iterator()
next_element = iterator.get_next()

sess = tf.Session()

for _ in range(3):
    print(sess.run(next_element))
```

When trying to use structs, I get this error
```
Traceback (most recent call last):
  File ""/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py"", line 460, in make_tensor_proto
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py"", line 460, in <listcomp>
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/util/compat.py"", line 65, in as_bytes
    (bytes_or_text,))
TypeError: Expected binary or unicode string, got {'a': <tf.Tensor 'arg0:0' shape=() dtype=int64>, 'b': <tf.Tensor 'mul:0' shape=() dtype=int64>}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/ede/test/test3.py"", line 10, in <module>
    dataset_dict = dataset.map(lambda x: {'a': x, 'b': x * 10})
  File ""/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py"", line 813, in map
    return MapDataset(self, map_func, num_threads, output_buffer_size)
  File ""/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py"", line 1436, in __init__
    self._map_func.add_to_graph(ops.get_default_graph())
  File ""/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/function.py"", line 619, in add_to_graph
    self._create_definition_if_needed()
  File ""/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/data/python/framework/function.py"", line 167, in _create_definition_if_needed
    outputs = self._func(*inputs)
  File ""/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py"", line 1427, in tf_map_func
    flattened_ret = [ops.convert_to_tensor(t) for t in nest.flatten(ret)]
  File ""/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py"", line 1427, in <listcomp>
    flattened_ret = [ops.convert_to_tensor(t) for t in nest.flatten(ret)]
  File ""/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 676, in convert_to_tensor
    as_ref=False)
  File ""/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 741, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 113, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 102, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py"", line 464, in make_tensor_proto
    ""supported type."" % (type(values), values))
TypeError: Failed to convert object of type <class 'dict'> to Tensor. Contents: {'a': <tf.Tensor 'arg0:0' shape=() dtype=int64>, 'b': <tf.Tensor 'mul:0' shape=() dtype=int64>}. Consider casting elements to a supported type.
```"
10150,tf.reshape modify the value of tensor,"![kaywxxy2r62q pv063 _ve](https://cloud.githubusercontent.com/assets/12975526/26388007/7e87051a-4084-11e7-941e-f9d38e37e3c8.png)
![nf8b f d1ae4_gi 9an](https://cloud.githubusercontent.com/assets/12975526/26388013/8154563a-4084-11e7-89b9-dfaf03761171.png)
when i reshape the samples, the value of it is modified, why?"
10149,tf.contrib.distributions: undocumented behaviour in Multinomial and Categorical,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes?
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra 10.12.3
- **TensorFlow installed from (source or binary)**: binary, via pip
- **TensorFlow version (use command below)**: 1.1.0 (v1.1.0-rc0-61-g1ec6ed5)
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```python
import tensorflow as tf
categorical = tf.contrib.distributions.Categorical(probs = [0.25, 0.5, 0.25])
multinomial = tf.contrib.distributions.Multinomial(total_count = 1., probs = [0.25, 0.5, 0.25])
mvn = tf.contrib.distributions.MultivariateNormalDiag(loc = [0., 0., 0.], scale_diag= [1., 1., 1.])

# expected values (points 1 and 2)

# The docs for Categorical say value should be float or double, but it expects an int
categorical.log_prob([0, 0, 1]) 
# <tf.Tensor 'Categorical_2/log_prob/Neg:0' shape=(3,) dtype=float32>
categorical.log_prob([0., 0., 1.])
# TypeError: Value passed to parameter 'labels' has DataType float32 not in list of allowed values: int32, int64

# The docs for Categorical say value should be float or double, which is how it behaves (though this is unlike categorical)
multinomial.log_prob([0, 0, 1]) 
# ValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(""Multinomial_2/log_prob/Log:0"", shape=(3,), dtype=float32)'
multinomial.log_prob([0., 0., 1.])
# <tf.Tensor 'Multinomial_1/log_prob/sub:0' shape=() dtype=float32>

# output shape (points 3 and 4)

# The docs for both say that the output should be:
# ""a Tensor of 'shape sample_shape(x) + self.batch_shape' with values of type self.dtype""
# though sample_shape doesn't seem to be relevant here, it's an argument to param_shapes() and sample()

# for Categorical (with int value), the result is a vector, matching the shape of value
categorical.log_prob([0, 0, 1]) 
# <tf.Tensor 'Categorical_2/log_prob/Neg:0' shape=(3,) dtype=float32>

# for Multinomial (with float value), the result is a scalar
multinomial.log_prob([0., 0., 1.])
# <tf.Tensor 'Multinomial_1/log_prob/sub:0' shape=() dtype=float32>

# for Multivariate Normal the result is a scalar
mvn.log_prob([0.1, 0.2, 0.3])
# <tf.Tensor 'MultivariateNormalDiag_2/log_prob/add:0' shape=() dtype=float32>
```

### Describe the problem
There are four related issues:

1. The expected type of `value` for the `log_prob()` method in `tf.contrib.distributions.Categorical` is inconsistent with the documentation.

2. The expected values for `tf.contrib.distributions.Categorical` and `tf.contrib.distributions.Multinomial` are inconsistent with one another, which is odd as the categorical distribution is a special case of the multinomial, with `total_count  = 1`

3. The output dimensions for `tf.contrib.distributions.Categorical` and `tf.contrib.distributions.Multinomial` are inconsistent with the documentation

4. The output dimensions for `tf.contrib.distributions.Categorical` are a vector, which doesn't really make sense for a multivariate distribution, and is inconsistent with `tf.contrib.distributions.Multinomial` and `tf.contrib.distributions.MultivariateNormal*`

Details are in the code snippet above
"
10148,TF tutorial getting started fix,"Hi, 

I am not personally having an issue, but I wanted to make you aware of some things in the tutorial that might throw off a new user.  

When the tutorial starts talking about the high level functions, they define the epochs here:

input_fn = tf.contrib.learn.io.numpy_input_fn({""x"":x}, y, batch_size=4, num_epochs=1000)

but then they redefine here:

estimator.fit(input_fn=input_fn, steps=1000)

This seems redundant, and per earlier issues:
https://github.com/tensorflow/tensorflow/issues/10042
https://github.com/tensorflow/tensorflow/issues/7677

There were people who said this made their system glitch.  If the system did accept both arguments, then I am not clear, nor have I found anywhere in the documentation, where it says which argument would take precedence if they are different numbers.

On another note, earlier in the tutorial, it where it calculates print(sess.run(linear_model, {x:[1,2,3,4]})), for some reason there is a floating decimal point way at the end of the output:

[ 0.          0.30000001  0.60000002  0.90000004]

This doesn't add up, unless there is just some kind of extra operation that is not being shown.  I know it is likely a lot of work to handle all the issues that you get, but fixing these things might reduce your workload over time, as new users will be less confused up front, not to mention keep users from getting discourages and changing to other platforms before they begin.  Thank you for your help.

"
10147,Strange Behavior During Grid Search,"I implemented a customized grid search wrapper for a sequence-to-sequence model. The implementation is extremely simple:

```
def single_round_model_eval(train_fun, decode_fun, eval_fun,
                            train_set, dev_set, metrics):
    # Train the model with a certain set of hyperparameters and evaluate on the
    # development set.

    # :param train_fun: Function to train the model.
    # :param decode_fun: Function to decode from the trained model.
    # :param eval_fun: Function to evaluate the decoding results.
    # :param train_set: Training dataset.
    # :param dev_set: Development dataset.
    # :param metrics: Name of the evaluation metrics to be tuned.

    # :return: The metrics being tuned.

    tf.reset_default_graph()
    train_fun(train_set, dev_set)

    tf.reset_default_graph()
    decode_sig = decode_fun(dev_set, verbose=False)

    M = eval_fun(dev_set, decode_sig, verbose=False)

    return M[metrics]
```

The algorithm basically calls the above function every time it moves to a new point in the grid (a new set of hyperparameters).

The complete implementation can be found here:
https://github.com/TellinaTool/awesome_nmt/blob/master/encoder_decoder/grid_search.py

The `single_round_model_eval` creates a training graph, trains the model; then creates a ""forward only"" decoding graph, and decode the predictions on the dev set; finally evaluates the newly decoded predictions (no graph operations is used in the evaluation step). 

Yet I encountered a strange behavior that my subsequent runs always achieve much worse numbers compared to the first run. I didn't find problems in hyperparameter settings, and think this may be caused by that some variables carries residue values from the previous run. I tried to use `tf.reset_default_graph()` to force reset but that also doesn't help. I'm also wondering if it is caused by missing resets of the optimizer I used (tf.train.AdamOptimizer).

Could anyone offer some help? Thanks!"
10146,CUDA_ERROR_INVALID_DEVICE,"Got the following error when trying to run tensorflow on GPU:

> E tensorflow/core/common_runtime/direct_session.cc:137] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE
> Traceback (most recent call last):
>   File ""main.py"", line 200, in <module>
>     model = PNN2(**pnn2_params)
>   File ""/homes/jwpan/Github/DL_MultiField_Categorical_Data/python/models.py"", line 767, in __init__
>     self.sess = tf.Session(config=config)
>   File ""/projects/ml/mlas/tensorflow/1.0.8/gpu/python2.7/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1176, in __init__
>     super(Session, self).__init__(target, graph, config=config)
>   File ""/projects/ml/mlas/tensorflow/1.0.8/gpu/python2.7/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 552, in __init__
>     self._session = tf_session.TF_NewDeprecatedSession(opts, status)
>   File ""/projects/ml/mlas/tensorflow/1.0.8/gpu/python2.7/lib/python2.7/contextlib.py"", line 24, in __exit__
>     self.gen.next()
>   File ""/projects/ml/mlas/tensorflow/1.0.8/gpu/python2.7/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
>     pywrap_tensorflow.TF_GetCode(status))
> tensorflow.python.framework.errors_impl.InternalError: Failed to create session.

Have tried to set CUDA_VISIBLE_DEVICES but still does not work.
Here is the output of nvidia-smi, just wonder why all GPUs are ""Off""?

> Tue May 23 21:28:08 2017
> +------------------------------------------------------+
> | NVIDIA-SMI 352.39     Driver Version: 352.39         |
> |-------------------------------+----------------------+----------------------+
> | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
> | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
> |===============================+======================+======================|
> |   0  Tesla K80           On   | 0000:06:00.0     Off |                    0 |
> | N/A   37C    P0    62W / 149W |  10983MiB / 11519MiB |      0%   E. Process |
> +-------------------------------+----------------------+----------------------+
> |   1  Tesla K80           On   | 0000:07:00.0     Off |                    0 |
> | N/A   57C    P0    73W / 149W |  10983MiB / 11519MiB |      0%   E. Process |
> +-------------------------------+----------------------+----------------------+
> |   2  Tesla K80           On   | 0000:0A:00.0     Off |                    0 |
> | N/A   38C    P0    60W / 149W |  10983MiB / 11519MiB |      0%   E. Process |
> +-------------------------------+----------------------+----------------------+
> |   3  Tesla K80           On   | 0000:0B:00.0     Off |                    0 |
> | N/A   54C    P0    73W / 149W |  10983MiB / 11519MiB |      0%   E. Process |
> +-------------------------------+----------------------+----------------------+
> |   4  Tesla K80           On   | 0000:0E:00.0     Off |                    0 |
> | N/A   37C    P0    61W / 149W |  10983MiB / 11519MiB |      0%   E. Process |
> +-------------------------------+----------------------+----------------------+
> |   5  Tesla K80           On   | 0000:0F:00.0     Off |                    0 |
> | N/A   59C    P0    75W / 149W |  10983MiB / 11519MiB |      0%   E. Process |
> +-------------------------------+----------------------+----------------------+
> |   6  Tesla K80           On   | 0000:12:00.0     Off |                    0 |
> | N/A   39C    P0    61W / 149W |  10983MiB / 11519MiB |      0%   E. Process |
> +-------------------------------+----------------------+----------------------+
> |   7  Tesla K80           On   | 0000:13:00.0     Off |                    0 |
> | N/A   62C    P0    74W / 149W |  10983MiB / 11519MiB |      0%   E. Process |
> +-------------------------------+----------------------+----------------------+
> 
> +-----------------------------------------------------------------------------+
> | Processes:                                                       GPU Memory |
> |  GPU       PID  Type  Process name                               Usage      |
> |=============================================================================|
> |    0     24673    C   /homes/bjaros/sw/python2.7-bigml/bin/python  10959MiB |
> |    1     16902    C   /homes/bjaros/sw/python2.7-bigml/bin/python  10959MiB |
> |    2     27468    C   /homes/bjaros/sw/python2.7-bigml/bin/python  10959MiB |
> |    3     11704    C   /homes/bjaros/sw/python2.7-bigml/bin/python  10959MiB |
> |    4     13475    C   /homes/bjaros/sw/python2.7-bigml/bin/python  10959MiB |
> |    5     16115    C   /homes/bjaros/sw/python2.7-bigml/bin/python  10959MiB |
> |    6      2095    C   /homes/bjaros/sw/python2.7-bigml/bin/python  10959MiB |
> |    7     23351    C   /homes/bjaros/sw/python2.7-bigml/bin/python  10959MiB |
> +-----------------------------------------------------------------------------+"
10145,local distributed tensorflow async btw graph multigpu example is extremely slow,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Not really, this primarily a copy and paste of the distributed tensorflow example
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.2 GPU
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: Titan X Pascal, 12G, 4 total
- **Exact command to reproduce**:


```bash
python async_btwgraph_launcher.py
```
async_btwgraph_launcher.py
```python
from async_btwgraph_dist_trainer import train
import os
from multiprocessing import Process
import time
from tensorflow.contrib.training import HParams
import tensorflow as tf
# Set up configurations to sweep
output_dir ='tfprojects/output_dir_debug'

cluster_spec ={""ps"": [""localhost:2227""
                      ],
    ""worker"": [
        ""localhost:2223"",
        ""localhost:2224"",
        ""localhost:2225"",
        ""localhost:2226""
        ]
    }

cluster = tf.train.ClusterSpec(cluster_spec)
def worker(device):
    params = HParams(cluster=cluster,
                     job_name = device[0],
                     task_index = device[1])

    if device[0]=='worker':
        # allow each worker to see only 1 of the 4 GPUS
        os.environ[""CUDA_VISIBLE_DEVICES""]=str(params.task_index)

    else:
        # hide all 4 GPUS from ps
        os.environ[""CUDA_VISIBLE_DEVICES""]=''


    train(output_dir, params)

if __name__ == '__main__':
    devices = [['ps',0],
               ['worker',0],
               ['worker',1],
               ['worker',2],
               ['worker',3]
               ]

    processes = []
    for d in devices:

        p = Process(target=worker, args=(d,))
        p.start()
        processes.append(p)

    for p in processes:
        p.join()
```
async_btwgraph_dist_trainer.py
```python
import os
import numpy as np
import tensorflow as tf
import time

tf.logging.set_verbosity(tf.logging.INFO)  # enables training error print out during training

def model_fn(features,labels,mode,params):


    outputs = layers.fully_connected(
                    inputs = features,
                    num_outputs = 4096)
    outputs = layers.fully_connected(
                    inputs = outputs,
                    num_outputs = 4096)
    outputs = layers.fully_connected(
                    inputs = outputs,
                    num_outputs = 256)

    loss = tf.losses.mean_squared_error(outputs, labels)


    train_op = tf.contrib.layers.optimize_loss(
              loss, None, optimizer='Adam',
                        learning_rate = .0001)


    predictions = {""predictions"":tf.identity(outputs,name = 'predictions')}
    return predictions, loss, train_op


def dumb_input_fn():

    x = tf.random_normal([128,256], dtype=tf.float32)
    y = tf.random_normal([128,256], dtype=tf.float32)

    return [x,y]

#
def train(output_dir, params={}):
    print('***JOBNAME**:',params.job_name)
    cluster = params.cluster
    job_name = params.job_name
    task_index = params.task_index
    gpu = task_index
    # Create and start a server for the local task.
    server = tf.train.Server(cluster,
                           job_name=job_name,
                           task_index=task_index)

    if job_name == ""ps"":
        server.join()
    elif job_name == ""worker"":

        # Assigns ops to the local worker by default.
        with tf.device(tf.train.replica_device_setter(
            worker_device=""/job:worker/replica:0/task:%d"" % task_index,
            cluster=cluster)):

            # Build model...
            x,y = dumb_input_fn()
            _, _, train_op = model_fn(x,y,None,params)

        global_step = tf.contrib.framework.get_or_create_global_step()
        # The StopAtStepHook handles stopping after running given steps.
        hooks=[tf.train.StopAtStepHook(last_step=100000)]

        step = 0
        start_time = time.time()

        with tf.train.MonitoredTrainingSession(master=server.target,
                                               is_chief=(task_index == 0),
                                               checkpoint_dir=output_dir,
                                               hooks=hooks) as mon_sess:
            while not mon_sess.should_stop():

                _ = mon_sess.run(train_op)

                if step % 10 == 0:
                    print(""Step:"", step,10/(time.time()-start_time),'steps/sec')
                    start_time = time.time()

                step+=1

```
### Describe the problem
I'm trying to use the distributed tensorflow [example](https://www.tensorflow.org/versions/r1.2/deploy/distributed) to do async between graph replication on a 4 Titan X machine, with 1 GPU per worker.  Without distributed TF and using a single GPU, the same code trains at ~150-200 steps/sec.  As shown at the end of the log below, this distributed trainer clocks at ~ 2 steps/sec.  The 4 GPUs are barely utilized,
![image](https://cloud.githubusercontent.com/assets/15891975/26372649/9d8f25cc-3fcc-11e7-87f0-0be4b1a80fb6.png)
with plenty of  CPU headroom,
![image](https://cloud.githubusercontent.com/assets/15891975/26372709/c4203ca8-3fcc-11e7-804f-5daa97bc5b70.png)

Also, if I simply remove the parameter server from this example, but keeping all 4 workers, they all grab GPU:0 maxing it out, and each worker process running at ~50steps/sec, and GPUs 1-3 are unused.
![image](https://cloud.githubusercontent.com/assets/15891975/26373341/28da56e0-3fcf-11e7-87a7-db64dfac13e4.png)
However, see that I'm setting os.environ[""CUDA_VISIBLE_DEVICES""] to enable only 1 unique GPU per worker.  

Is this expected behavior?
Thanks,
Luke 

### Source code / logs
```bash
python async_btwgraph_launcher.py
***JOBNAME**: ps
2017-05-23 15:06:16.761116: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.761187: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.761200: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.761212: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.761225: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
***JOBNAME**: worker
2017-05-23 15:06:16.763172: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.763247: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.763259: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.763269: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.763280: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
***JOBNAME**: worker
2017-05-23 15:06:16.765599: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.765671: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.765684: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.765693: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.765705: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
***JOBNAME**: worker
2017-05-23 15:06:16.767881: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.767951: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.767983: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.768005: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.768024: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
***JOBNAME**: worker
2017-05-23 15:06:16.771552: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.771617: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.771638: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.771649: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.771660: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 15:06:16.885876: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE
2017-05-23 15:06:16.885946: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: mlearn2
2017-05-23 15:06:16.885961: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: mlearn2
2017-05-23 15:06:16.886027: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 375.39.0
2017-05-23 15:06:16.886800: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:369] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  375.39  Tue Jan 31 20:47:00 PST 2017
GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4)
""""""
2017-05-23 15:06:16.886835: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 375.39.0
2017-05-23 15:06:16.886848: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 375.39.0
2017-05-23 15:06:16.898031: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2227}
2017-05-23 15:06:16.898063: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> localhost:2224, 2 -> localhost:2225, 3 -> localhost:2226}
2017-05-23 15:06:16.908193: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2227
2017-05-23 15:06:18.817380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] Found device 0 with properties:
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.911
pciBusID 0000:02:00.0
Total memory: 11.90GiB
Free memory: 11.60GiB
2017-05-23 15:06:18.817433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:927] DMA: 0
2017-05-23 15:06:18.817443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:937] 0:   Y
2017-05-23 15:06:18.817581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)
2017-05-23 15:06:18.859150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] Found device 0 with properties:
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.911
pciBusID 0000:03:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
2017-05-23 15:06:18.859216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:927] DMA: 0
2017-05-23 15:06:18.859227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:937] 0:   Y
2017-05-23 15:06:18.859274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0)
2017-05-23 15:06:18.900436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] Found device 0 with properties:
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.911
pciBusID 0000:81:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
2017-05-23 15:06:18.900506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:927] DMA: 0
2017-05-23 15:06:18.900520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:937] 0:   Y
2017-05-23 15:06:18.900562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:81:00.0)
2017-05-23 15:06:18.922840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] Found device 0 with properties:
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.911
pciBusID 0000:82:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
2017-05-23 15:06:18.922898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:927] DMA: 0
2017-05-23 15:06:18.922913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:937] 0:   Y
2017-05-23 15:06:18.922954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:82:00.0)
2017-05-23 15:06:18.947847: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2227}
2017-05-23 15:06:18.947913: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> localhost:2224, 2 -> localhost:2225, 3 -> localhost:2226}
2017-05-23 15:06:18.954688: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2223
2017-05-23 15:06:19.008071: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2227}
2017-05-23 15:06:19.008132: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> localhost:2224, 2 -> localhost:2225, 3 -> localhost:2226}
2017-05-23 15:06:19.016316: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2224
2017-05-23 15:06:19.052548: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2227}
2017-05-23 15:06:19.052589: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> localhost:2224, 2 -> localhost:2225, 3 -> localhost:2226}
2017-05-23 15:06:19.056154: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2227}
2017-05-23 15:06:19.056176: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> localhost:2224, 2 -> localhost:2225, 3 -> localhost:2226}
2017-05-23 15:06:19.060973: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2225
2017-05-23 15:06:19.063039: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2226
2017-05-23 15:06:19.444002: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 6bd586237b42120b with config:

2017-05-23 15:06:19.458159: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 924ffab74f941016 with config:

2017-05-23 15:06:19.478740: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session c85138445cc12f91 with config:

2017-05-23 15:06:19.481805: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session c77091e3456c2d32 with config:

Step: 0 5.582075516520828 steps/sec
Step: 10 2.1573368439379705 steps/sec
Step: 20 2.2082990011777794 steps/sec
Step: 30 2.1863694281593564 steps/sec
Step: 40 2.254566631295363 steps/sec
Step: 50 2.2088188362675036 steps/sec
Step: 60 2.163473831162752 steps/sec
2017-05-23 15:06:49.920556: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 617cf268cb5a24e3 with config:

2017-05-23 15:06:49.952686: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 9b89c26b8c702b9f with config:

2017-05-23 15:06:49.955667: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session b8d674f54212032f with config:

Step: 0 0.30616911528363916 steps/sec
Step: 0 0.30273764853875 steps/sec
Step: 0 0.3023278973613949 steps/sec
Step: 70 1.7908409267412564 steps/sec
Step: 10 1.4149145268366454 steps/sec
Step: 10 1.4475748080324984 steps/sec
Step: 10 1.3904163169606496 steps/sec
Step: 80 1.486386761414297 steps/sec
Step: 20 1.4357212501056003 steps/sec
Step: 20 1.4629177065889272 steps/sec
Step: 20 1.4312276702523932 steps/sec
Step: 90 1.4173087487398812 steps/sec
```"
10142,tf.nn.softmax overflow due to exp(x),"From reading the doc and code of ```tf.nn.softmax```, it looks like this function naively call a ```_softmax``` function which may have the overflow problem if the x in exp(x) is too big. (underflow can happen, too)

This is a typical problem of machine learning, and googling 'log sum exp' gives some information. TF even has a function ```reduce_logsumexp``` to calculate the value in a robust way. 

I think it makes sense that ```nn.softmax``` also use similar method of ```reduce_logsumexp```. I ran into a loss  = NaN today and suspect this is the cause. 

Thanks,
"
10141,slim.batch_norm as normalizer_fn in tf.contrib.layers.fully_connected destroys network,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Mac OS X 10.12.4
- **TensorFlow installed from (source or binary)**:
binary, conda
- **TensorFlow version (use command below)**:
1.1.0
- **Exact command to reproduce**:
`net = layers.fully_connected(input_dim, dim, biases_initializer=tf.constant_initializer(0.1), activation_fn=None, normalizer_fn=tf.contrib.slim.batch_norm)`

### Describe the problem
After training a network without normalizer_fn set (or set to =False) and after trying to restore the network with incorrect setting of normalizer_fn, in my case with tf.contrib.slim.batch_norm, TensorFlow creates a new network structure with batch norms and completely annihilates the previously trained model and its structure, leaving the previously trained structure unavailable for restoring. There should be some way to restore previously trained models even after setting incorrect parameters. 

### Source code / logs
Function for creating a network given an initial 
```
self.layers = [200, 100]
self.embedding_size = 50
    def dense_batchnorm_relu(self, dimensions, phase):
        input_dim = self.query
        for dim in dimensions:
            print input_dim
            net = layers.fully_connected(input_dim, dim, biases_initializer=tf.constant_initializer(0.1), activation_fn=None, normalizer_fn=tf.contrib.slim.batch_norm)
            self.layers.append(net)
            input_dim = net
        net = layers.fully_connected(input_dim, self.embedding_size, biases_initializer=tf.constant_initializer(0.1), activation_fn=None)
        self.layers.append(net)

Errors:
`

2017-05-23 21:09:50.047879: W tensorflow/core/framework/op_kernel.cc:1152] Not found: Key fully_connected_1/biases not found in checkpoint
2017-05-23 21:09:50.481725: W tensorflow/core/framework/op_kernel.cc:1152] Not found: Key fully_connected_1/weights not found in checkpoint

....
line 785, in restore
    loader = tf.train.Saver()
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1056, in __init__
    self.build()
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1086, in build
    restore_sequentially=self._restore_sequentially)
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 691, in build
    restore_sequentially, reshape)
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 407, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 247, in restore_op
    [spec.tensor.dtype])[0])
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 669, in restore_v2
    dtypes=dtypes, name=name)
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 768, in apply_op
    op_def=op_def)
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2336, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1228, in __init__
    self._traceback = _extract_stack()

NotFoundError (see above for traceback): Key fully_connected_1/biases not found in checkpoint
	 [[Node: save/RestoreV2_6 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_6/tensor_names, save/RestoreV2_6/shape_and_slices)]]


`
Etc."
10140,Cannot import tensorflow (python 3.5.2),"I downloaded the tensorflow pip, the cuda and the cudnn and installed all of them bu it still showing the same error.
`import tensorflow
Traceback (most recent call last):
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#0>"", line 1, in <module>
    import tensorflow
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 51, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.`"
10137,ImportError after upgrading pandas to newest version,"```python
from tensorflow.contrib.learn.python.learn.learn_io import data_feeder
```

Raise error:
```python
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-8-eae9a04c9717> in <module>()
----> 1 from tensorflow.contrib.learn.python.learn.learn_io import data_feeder

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/__init__.py in <module>()
     38 from tensorflow.contrib import labeled_tensor
     39 from tensorflow.contrib import layers
---> 40 from tensorflow.contrib import learn
     41 from tensorflow.contrib import legacy_seq2seq
     42 from tensorflow.contrib import linalg

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/learn/__init__.py in <module>()
     85 
     86 # pylint: disable=wildcard-import
---> 87 from tensorflow.contrib.learn.python.learn import *
     88 # pylint: enable=wildcard-import
     89 

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/learn/python/__init__.py in <module>()
     21 
     22 # pylint: disable=wildcard-import
---> 23 from tensorflow.contrib.learn.python.learn import *
     24 # pylint: enable=wildcard-import

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/__init__.py in <module>()
     23 from tensorflow.contrib.learn.python.learn import basic_session_run_hooks
     24 from tensorflow.contrib.learn.python.learn import datasets
---> 25 from tensorflow.contrib.learn.python.learn import estimators
     26 from tensorflow.contrib.learn.python.learn import graph_actions
     27 from tensorflow.contrib.learn.python.learn import learn_io as io

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/__init__.py in <module>()
    295 from tensorflow.contrib.learn.python.learn.estimators._sklearn import NotFittedError
    296 from tensorflow.contrib.learn.python.learn.estimators.constants import ProblemType
--> 297 from tensorflow.contrib.learn.python.learn.estimators.dnn import DNNClassifier
    298 from tensorflow.contrib.learn.python.learn.estimators.dnn import DNNEstimator
    299 from tensorflow.contrib.learn.python.learn.estimators.dnn import DNNRegressor

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py in <module>()
     27 from tensorflow.contrib.layers.python.layers import optimizers
     28 from tensorflow.contrib.learn.python.learn import metric_spec
---> 29 from tensorflow.contrib.learn.python.learn.estimators import dnn_linear_combined
     30 from tensorflow.contrib.learn.python.learn.estimators import estimator
     31 from tensorflow.contrib.learn.python.learn.estimators import head as head_lib

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py in <module>()
     29 from tensorflow.contrib.layers.python.layers import optimizers
     30 from tensorflow.contrib.learn.python.learn import metric_spec
---> 31 from tensorflow.contrib.learn.python.learn.estimators import estimator
     32 from tensorflow.contrib.learn.python.learn.estimators import head as head_lib
     33 from tensorflow.contrib.learn.python.learn.estimators import model_fn

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in <module>()
     47 from tensorflow.contrib.learn.python.learn.estimators import tensor_signature
     48 from tensorflow.contrib.learn.python.learn.estimators._sklearn import NotFittedError
---> 49 from tensorflow.contrib.learn.python.learn.learn_io import data_feeder
     50 from tensorflow.contrib.learn.python.learn.utils import export
     51 from tensorflow.contrib.learn.python.learn.utils import saved_model_export_utils

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/learn_io/__init__.py in <module>()
     19 from __future__ import print_function
     20 
---> 21 from tensorflow.contrib.learn.python.learn.learn_io.dask_io import extract_dask_data
     22 from tensorflow.contrib.learn.python.learn.learn_io.dask_io import extract_dask_labels
     23 from tensorflow.contrib.learn.python.learn.learn_io.dask_io import HAS_DASK

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/learn_io/dask_io.py in <module>()
     24 try:
     25   # pylint: disable=g-import-not-at-top
---> 26   import dask.dataframe as dd
     27   allowed_classes = (dd.Series, dd.DataFrame)
     28   HAS_DASK = True

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/dask/dataframe/__init__.py in <module>()
----> 1 from .core import (DataFrame, Series, Index, _Frame, map_partitions,
      2                    repartition)
      3 from .io import (from_array, from_bcolz, from_array, from_bcolz,
      4                  from_pandas, from_dask_array, from_castra, read_hdf,
      5                  from_imperative, from_delayed)

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/dask/dataframe/core.py in <module>()
     35 return_scalar = '__return_scalar__'
     36 
---> 37 pd.computation.expressions.set_use_numexpr(False)
     38 
     39 

AttributeError: module 'pandas' has no attribute 'computation'
```"
10135,pip default installs TF1.1 still. On purpose?,"On both my Windows 10 and Ubuntu 16.10 instances, trying to upgrade with pip still installs 1.1 even though 1.2.0rc0 is quite clearly available there. On ubuntu I managed to get it by forcing the version. Wondering if this is intentional?

Note: Will check again that this is the case on Windows still as it was yesterday. "
10134,C++ Tensor's Slice assignment works unexpectedly when using GPU,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
  Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
  Linux 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt20-1+deb8u2 (2016-01-02) x86_64 GNU/Linux
- **TensorFlow installed from (source or binary)**:
  binary
- **TensorFlow version (use command below)**:
  tensorflow-gpu (1.1.0rc2)
- **Bazel version (if compiling from source)**:
  None
- **CUDA/cuDNN version**:
  cuda 8.0
- **GPU model and memory**:
  GeForce GTX 1080 8110Mb
- **Exact command to reproduce**:
  See below.

### Describe the problem

`tensorflow::Tensor::Slice` returns another `tensorflow::Tensor` , which is the slice of the original `Tensor` in the first dimension,  but if you assign a slice to another when writing operation with `GPUDevice` , it seems to fail to work like expected,

In gist below I write a simple operation which set the output as the same value of input, by setting each slice of output in the first dimension to be the same of input. When registered only CPUKernel, this op works well, but when registered only GPUKernel, it gives some random values I had no idea with.

I use `g++ -std=c++11 -shared -o CopyByBatchOp.so CopyByBatchOp.cc -I $TF_INC -fPIC -lcudart -L $CUDA_HOME/lib64 -D GOOGLE_CUDA=1 -Wfatal-e
rrors -I $CUDA_HOME/include -D_GLIBCXX_USE_CXX11_ABI=0` to compile the operation, for both GPU version and CPU version.

After compiling, I use `test_op = tf.load_op_library('CopyByBatchOp.so')` in Python to load it. A simple test script is included in gist below.



FYI, I've been looking for a way to assign a slice of one tensor to another slice for a long time, Eigen tensors works well by `      output.slice(start, size).device(d) = input.slice(start, size);` with `d` as some `CPUDevice`, but somehow breakdown when I use a `GPUDevice`, could not figure out why since I don't know how to debug it. I guess maybe the two problems are relevant.

### Source code / logs

[gist](https://gist.github.com/Zardinality/616137e6edc309af57a3cbbb5032d848)"
10133,Segmentation fault: `help(tf.ConfigProto)` failed on python3,"tensorflow for python3 failed while ran the following code:

    $ python3
    Python 3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  2 2016, 17:53:06) 
    [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux
    >>> import tensorflow as tf
    >>> tf.__version__
    '1.1.0'
    >>> help(tf.ConfigProto)
    段错误 (核心已转储) In English: Segmentation fault (core dumped)

    $ python3
    Python 3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  2 2016, 17:53:06) 
    [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux
    Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
    >>> import tensorflow as tf
    >>> tf.__version__
    '1.2.0-rc0'
    >>> help(tf.ConfigProto)
    段错误 (核心已转储) In English: Segmentation fault (core dumped)

tensorflow for python2 worked fine.

    $ python2
    Python 2.7.12 (default, Nov 19 2016, 06:48:10) 
    [GCC 5.4.0 20160609] on linux2
    >>> import tensorflow as tf
    >>> tf.__version__
    '1.1.0-rc1'
    >>> help(tf.ConfigProto)

    >>> 

"
10132,TF ignore the computation graph and output the feeding value directly.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.1.0-rc0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
```
import tensorflow as tf

with tf.Graph().as_default():
  a = tf.placeholder(dtype=tf.float32, shape=[1])
  a = tf.multiply(a, 2)
  with tf.Session() as sess:
    output = sess.run(a, feed_dict={a: [1]})
    print(output)
```
```
[ 1.]
```

### Describe the problem
As the code above, I feed a value into the graph to do some computation. Notice that the input tensor's name is the same as the output tensor's name.
Maybe the naming is a little confusing, but I think it is used quite often. 
I think it is a bug of tensorflow.
"
10131,tf.contrib.distributions.Logistic   log_cdf method has wrong sign,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes?
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra 10.12.3
- **TensorFlow installed from (source or binary)**: binary, via pip
- **TensorFlow version (use command below)**: 1.1.0 (v1.1.0-rc0-61-g1ec6ed5)
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
```python
import tensorflow as tf
logistic = tf.contrib.distributions.Logistic(loc = 0., scale = 1.) 
logistic.log_cdf(0.).eval(session = tf.Session())
```
### Describe the problem
`tf.contrib.distributions.Logistic`'s `log_cdf()` method has the wrong sign. The above code returns `0.69314718`, but it should be `-0.69314718` (i.e. `log(0.5`).
Appears to be the same for all parameters and values.
The `cdf()` method is fine.
I've not seen this problem with `log_cdf()` in any of the other distributions, so it looks like an isolated bug."
10130,tf.contrib.data: No op named OneShotIterator in defined operations,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary (nightly build from May 23rd)
- **TensorFlow version (use command below)**: v1.2.0-rc0-172-g9e25de3
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```
python3 save.py
python3 load.py
```

### Describe the problem
I'm playing around with the new `tf.contrib.data` stuff because it looks awesome and can make a lot of my code a lot simpler. However, I'm trying to save and restore a graph that contains these new ops. Saving works fine, but loading doesn't work. If I inspect the saved file, it seems to be serializing everything correctly, I recognize the structure of all the data set ops I added in the code so that seems fine. In the case of the below example, I'm getting an error `No op named OneShotIterator in defined operations` when trying to load the graph again.

### Source code / logs
save.py
```python
import tensorflow as tf

dataset = tf.contrib.data.Dataset.range(10)
dataset = dataset.map(lambda x: x + 2)
batched_dataset = dataset.batch(4)

iterator = batched_dataset.make_one_shot_iterator()
next_element = iterator.get_next()

tf.add_to_collection('next', next_element)
tf.train.export_meta_graph(filename='graph', as_text=True)
```
load.py
```python
import tensorflow as tf

tf.train.import_meta_graph('graph')
next_element = tf.get_collection('next')[0]

sess = tf.Session()

print(sess.run(next_element))
```

load.py output
```
Traceback (most recent call last):
  File ""/home/ede/test/load.py"", line 3, in <module>
    tf.train.import_meta_graph('graph')
  File ""/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 1683, in import_meta_graph
    **kwargs)
  File ""/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py"", line 504, in import_scoped_meta_graph
    producer_op_list=producer_op_list)
  File ""/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 283, in import_graph_def
    raise ValueError('No op named %s in defined operations.' % node.op)
ValueError: No op named OneShotIterator in defined operations.
```"
10129,ZeroDivisionError: integer division or modulo by zero,"I got this error messsage when trying to train the images. FYI my images is more than 20.

My command

```
sudo python retrain.py --bottleneck_dir=tf_files/bottlenecks --how_many_training_steps=500 --model_dir=tf_files/inception --output_graph=tf_files/retrained_graph.pb --output_labels=tf_files/retrained_labels.txt --image_dir /Users/ZERO/Documents/Github/tensorflow-python/tf_files/images/..
```

Error message

```
Looking for images in 'bottlenecks'
No files found
Looking for images in 'images'
Looking for images in 'inception'
WARNING: Folder has less than 20 images, which may cause issues.
Looking for images in 'images'
Looking for images in 'inception'
WARNING: Folder has less than 20 images, which may cause issues.
2017-05-23 13:00:12.940135: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 13:00:12.940170: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 13:00:12.940177: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 13:00:12.940184: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 13:00:12.940190: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-05-23 13:00:21.462092: Step 0: Train accuracy = 100.0%
2017-05-23 13:00:21.462469: Step 0: Cross entropy = 0.427569
CRITICAL:tensorflow:Label inception has no images in the category validation.
Traceback (most recent call last):
  File ""retrain.py"", line 1107, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""retrain.py"", line 910, in main
    bottleneck_tensor))
  File ""retrain.py"", line 515, in get_random_cached_bottlenecks
    image_dir, category)
  File ""retrain.py"", line 227, in get_image_path
    mod_index = index % len(category_list)
ZeroDivisionError: integer division or modulo by zero
```

![screenshot 2017-05-23 13 39 48](https://cloud.githubusercontent.com/assets/5416242/26339821/52177d42-3fbd-11e7-88a2-17e6df781d52.png)

Please advice. Thank you."
10128,tensorflow 1.2 release will support python 3.6？,tensorflow 1.2 release will support python 3.6？
10125,Feature request: weight normalization,"Is it possible to incorporate Weight Normalization (https://arxiv.org/abs/1602.07868) into tensorflow itself?

https://github.com/openai/weightnorm/tree/master/tensorflow"
10124,[feature request] decode_raw for uint16 and other dtype,"### System information
- NO
- NOT RELATED
- NOT RELATED
- NOT RELATED
- NOT RELATED
- NOT RELATED
- NOT RELATED
- NOT RELATED


### Describe the problem
Current decode_raw only supports `tf.half, tf.float32, tf.float64, tf.int32, tf.uint8, tf.int16, tf.int8, tf.int64`. It would be helpful if more types are available.

"
10123,"why inputs transpose to inputs_transposed in fc_layer model? I think it not need to do this, and it just change sort of dim_1, dim_2, dim_3 ","Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
10121,Use file DLL of tensorflow on window,"Anyone have example to use file DLL of tensorflow on window, please help me! I working with VS 2015, thanks in advance."
10119,Memory leak in Java API due to missing TF_DeleteStatus,"This is a leak confirmed on [StackOverflow](https://stackoverflow.com/questions/44082297/memory-leak-using-tensorflow-for-java) by [ash](https://stackoverflow.com/users/6708503/ash).

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.  There's a short example in the above StackOverflow link and a [working JUNIT test written in a maven project on github](https://github.com/jpangburn/tensorflowmemorytest) that demonstrates the problem.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS Sierra using TF 1.1, and CentOS7 using both TF 1.1 and TF 1.2_rc0.
- **TensorFlow installed from (source or binary)**: binary from Maven
- **TensorFlow version (use command below)**: On Mac uses Maven `<version>1.1.0</version>`, on CentOS7 `<version>1.2.0-rc0</version>`

- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See above for the short example on the StackOverflow link or the full Java file at the github example project.

### Describe the problem
Running the example code given causes the process memory (outside the JDK, not heap space) to grow until the process exits.  Leaked was agreed to be in TensorFlow by ash at StackOverflow:

> I believe there is indeed a leak (in particular a missing TF_DeleteStatus corresponding to the [allocation in JNI code](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/java/src/main/native/session_jni.cc#L191)) 

### Source code / logs
The test file is [here](https://github.com/jpangburn/tensorflowmemorytest/blob/master/src/test/java/com/jessepangburn/tftest/tensorflowtest/AppTest.java) within the above reference github project.

Thank you for providing this amazing tool!
"
10118,Incorrect behavior from `tf.layers.batch_normalization()` when `training=0`,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8/5
- **GPU model and memory**: Nvidia Titan X 
- **Exact command to reproduce**: [gist](https://gist.github.com/zo7/87735a3f06a41e0f2b3c10f9950d07a3)

### Describe the problem

I've noticed that `tf.layers.batch_normalization` doesn't seem to give reasonable results when `training=0` (i.e. use distribution statistics instead of just the batch), especially if you apply BN *before* activations (e.g. ResNet-like architectures).

Using the Gist above, if you try to fit a model to noise with SGD (lr=0.01) using repeated applications of dense matrix multiplication -> batch normalization -> ReLU activations, you get this loss for the _same_ inputs over time: (blue: `training=1`, green: `training=0`)

![image](https://cloud.githubusercontent.com/assets/3229244/26330609/9f3fa0ca-3f01-11e7-8f73-81d67e64be97.png)

Using an Adam (lr=0.001) optimizer instead gets even weirder results:

![image](https://cloud.githubusercontent.com/assets/3229244/26330754/49c94eb0-3f02-11e7-9ff6-7d48ad471ea2.png)

However, if I use my own implementation of batch norm (included in gist) I get reasonable results, with the loss for each being similar to each other: (Adam has similar behavior)

![image](https://cloud.githubusercontent.com/assets/3229244/26330911/1922c81c-3f03-11e7-86bf-56f0e38e3ff3.png)

(Interestingly this doesn't seem to be as much of a problem if you have ReLU before BN, I haven't thought too deeply about why.)

Am I seeing things and just have some misunderstanding about what that function is doing, or is this actually a bug?"
10111,ImportError: No module named '_pywrap_tensorflow_internal',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 7 
- **TensorFlow installed from (source or binary)**:
Using pip3 
- **TensorFlow version (use command below)**:
1.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
8.0 
- **GPU model and memory**:
Quadro K620
- **Exact command to reproduce**:
import tensorflow as tf

I have installed everything following the instructions and opened a [ post on SO ](https://stackoverflow.com/questions/44080677/no-module-named-pywrap-tensorflow-internal) but haven't receieved many responses. I have looked at multiple other posts with the same keywords but was unable to solve the issue from there. I used the standard installation instructions using pip3 for windows. 

The following is the stacktrace when I run ""import tensorflow as tf"" in python command line. 

```
Microsoft Windows [Version 6.1.7601]
Copyright (c) 2009 Microsoft Corporation.  All rights reserved.

C:\Users\aagarwal>python
Python 3.5.3 (v3.5.3:1880cb95a742, Jan 16 2017, 16:02:32) [MSC v.1900 64 bit (AM
D64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\aagarwal\AppData\Local\Programs\Python\Python35\lib\site-packag
es\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_hel
per
    return importlib.import_module(mname)
  File ""C:\Users\aagarwal\AppData\Local\Programs\Python\Python35\lib\importlib\_
_init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 914, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\aagarwal\AppData\Local\Programs\Python\Python35\lib\site-packag
es\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\aagarwal\AppData\Local\Programs\Python\Python35\lib\site-packag
es\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\aagarwal\AppData\Local\Programs\Python\Python35\lib\site-packag
es\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_hel
per
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\aagarwal\AppData\Local\Programs\Python\Python35\lib\importlib\_
_init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\aagarwal\AppData\Local\Programs\Python\Python35\lib\site-packag
es\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\aagarwal\AppData\Local\Programs\Python\Python35\lib\site-packag
es\tensorflow\python\__init__.py"", line 51, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\aagarwal\AppData\Local\Programs\Python\Python35\lib\site-packag
es\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\aagarwal\AppData\Local\Programs\Python\Python35\lib\site-packag
es\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_hel
per
    return importlib.import_module(mname)
  File ""C:\Users\aagarwal\AppData\Local\Programs\Python\Python35\lib\importlib\_
_init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 914, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\aagarwal\AppData\Local\Programs\Python\Python35\lib\site-packag
es\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\aagarwal\AppData\Local\Programs\Python\Python35\lib\site-packag
es\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\aagarwal\AppData\Local\Programs\Python\Python35\lib\site-packag
es\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_hel
per
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\aagarwal\AppData\Local\Programs\Python\Python35\lib\importlib\_
_init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_probl
ems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>
```"
10110,Python bindings to CTC Beam Search do not allow a dictionary to be specified,"In the underlying C++ code, tested here:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/ctc/ctc_beam_search_test.cc#L103

a dictionary can be used.  However, the python bindings do not expose the ability to specify a dictionary scorer at all.  See here:

https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/python/ops/ctc_ops.py#L219"
10109,TensorFlow 1.2.0rc0 strange behavior with Benchmark scripts,"I'm running https://github.com/tensorflow/benchmarks/ scripts by @tfboyd and I'm observing strange behavior and crashes.

1. There's a LOT of messages like these (below).  These messages happen in both standalone and distributed runs.

```
2017-05-22 17:21:00.589871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla P40, pci bus id: 0000:85:00.0)
2017-05-22 17:21:00.600723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)
2017-05-22 17:21:00.600741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P40, pci bus id: 0000:05:00.0)
2017-05-22 17:21:00.600746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)
2017-05-22 17:21:00.600750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla P40, pci bus id: 0000:85:00.0)
2017-05-22 17:21:00.619525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)
2017-05-22 17:21:00.619543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P40, pci bus id: 0000:05:00.0)
2017-05-22 17:21:00.619548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)
2017-05-22 17:21:00.619552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla P40, pci bus id: 0000:85:00.0)
2017-05-22 17:21:00.629770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)
2017-05-22 17:21:00.629786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P40, pci bus id: 0000:05:00.0)
2017-05-22 17:21:00.629807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)
2017-05-22 17:21:00.629811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla P40, pci bus id: 0000:85:00.0)
2017-05-22 17:21:00.649854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)
2017-05-22 17:21:00.649871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P40, pci bus id: 0000:05:00.0)
2017-05-22 17:21:00.649892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)
```

2. ~~In distributed mode, TF 1.2.0 rc0 is not able to handle batch size 64 for Inception V3:~~ - This was figured out.

Slave worker crashes with:
```
2017-05-22 17:22:25.411569: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: OOM when allocating tensor with shape[64,32,149,149]
```

Chief worker crashes with:
```
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/threading.py"", line 810, in __bootstrap_inner
    self.run()
  File ""tf_cnn_benchmarks.py"", line 226, in run
    global_step_val, = self.sess.run([self.global_step_op])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 789, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 925, in _run
    raise RuntimeError('Attempted to use a closed Session.')
RuntimeError: Attempted to use a closed Session.
```

My command:
```
python -u tf_cnn_benchmarks.py --model inception3 --batch_size 64 --num_gpus 4 --worker_hosts {worker_hosts} --ps_hosts {ps_hosts} --task_index {task_index} --job_name {job_name} --local_parameter_device cpu
```

This worked perfectly fine on hash cae8ed1ca54a9fd4f9cc64d08cadebce31fd4607 (just a little bit past TF 1.1 release)."
10107,Tensorflow Library not loading in macOS (ImportError),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No. I've strictly followed the on-site tutorial
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
macOS Sierra
- **TensorFlow installed from (source or binary)**:
binary (https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.1.0-py2-none-any.whl)
- **TensorFlow version (use command below)**:
latest (same as that of on-site tutorial version)
- **Bazel version (if compiling from source)**:
Not used as it isn't given in tutorial
- **CUDA/cuDNN version**:
CUDA - 8.0
cuDNN - 6.0
- **GPU model and memory**:
GPU: NVIDIA GeForce GTX 775M 2048 MB
Memory: 8 GB 1600 MHz DDR3
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
You can obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
ImportError: Traceback (most recent call last):
  File ""/Users/dwdcw/miniconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Users/dwdcw/miniconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Users/dwdcw/miniconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: dlopen(/Users/dwdcw/miniconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: @rpath/libcudnn.5.dylib
  Referenced from: /Users/dwdcw/miniconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
  Reason: image not found


Failed to load the native TensorFlow runtime.


"
10106,Feature request: improve tensorboard embeddings search,"Currently, when making a search query using a custom variable, let's say a relevance score, then results display relevance scores. This is not always helpful. In my application, I have word embeddings that I would like to filter by a custom variable (relevance score), and I can't see what word is corresponding to each relevance score (see screenshot).

It would be better to have a separate field so that users can define how they would like to see the results of their search query.
![screen shot 2017-05-22 at 11 54 44](https://cloud.githubusercontent.com/assets/12895366/26317930/3acefc0e-3ee7-11e7-8f17-5b9866679e5f.png)

"
10105,Feature request: polarized gradient in tensorboard embeddings,"Currently, default tensorboard color gradient goes from light yellow to dark blue (see screenshot), which works great for highlighting positive values (yellow dots become nearly invisible).

But it doesn't work well for displaying, let's say, sentiment scales (going from -1 to +1). The ability to choose a gradient going from red to blue would be helpful for visualizing polarized scales.
![screen shot 2017-05-22 at 11 39 04](https://cloud.githubusercontent.com/assets/12895366/26317262/e66f4c88-3ee4-11e7-8cb8-7bfad7cd5a35.png)
"
10104,Tensorflow - No valid folders of images found at XXXXX,"I got this error message when trying to train the images using this command

```
sudo python retrain.py --bottleneck_dir=tf_files/bottlenecks --how_many_training_steps=500 --model_dir=tf_files/inception --output_graph=tf_files/retrained_graph.pb --output_labels=tf_files/retrained_labels.txt --image_dir=tf_files/data/
```

The python script is here, https://github.com/datomnurdin/tensorflow-python/blob/master/retrain.py.

Please advice. Thank you."
10103,"models/slim download_and_convert_data.py, unicodeDecodeError","Windows 10 x64 build 1703
Anaconda 4.3.1 with Python 3.6.1 x64
Geforce GTX1080
Tensorflow-gpu 1.1.0

![image](https://cloud.githubusercontent.com/assets/4515120/26315004/b640ffbe-3f4a-11e7-8474-2e399af66960.png)
"
10102,Tensorflow gpu vs cpu problem,"I am using Tenosrflow 1.0.1. I can run my program tensorflow installed on CPU, but when I run it on different computer tensorflow installed GPU, it says out of memory. What is the problem with the GPU Tensorflow. 

What is the reason I can run it on CPU but not on GPU? To run it on GUP I have to decrease the batch size and the image size of my data set. Why has the GPU tensorflow become very poor?

The error message for GPU Tensorflow is : 

`2017-05-04 14:24:03.511879: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:1152] Resource exhausted: OOM when allocating tensor with shape[1,6000,6000,64]
Traceback (most recent call last):
  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1039, in _do_call
    return fn(*args)
  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1021, in _run_fn
    status, run_metadata)
  File ""C:\Users\admin\Anaconda3\lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[1,6000,6000,64]
         [[Node: inference/Conv2D = Conv2D[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](sub, inference/conv1_1_w/read)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""FCN.py"", line 225, in <module>
    tf.app.run()
  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""FCN.py"", line 196, in main
    sess.run(train_op, feed_dict=feed_dict)
  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 778, in run
    run_metadata_ptr)
  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 982, in _run
    feed_dict_string, options, run_metadata)
  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1032, in _do_run
    target_list, options, run_metadata)
  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1052, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[1,6000,6000,64]
         [[Node: inference/Conv2D = Conv2D[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](sub, inference/conv1_1_w/read)]]

Caused by op 'inference/Conv2D', defined at:
  File ""FCN.py"", line 225, in <module>
    tf.app.run()
  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""FCN.py"", line 149, in main
    pred_annotation, logits = inference(image, keep_probability)
  File ""FCN.py"", line 84, in inference
    image_net = vgg_net(weights, processed_image)
  File ""FCN.py"", line 54, in vgg_net
    current = utils.conv2d_basic(current, kernels, bias)
  File ""C:\Users\admin\Documents\gray\TensorflowUtils.py"", line 89, in conv2d_basic
    conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=""SAME"")
  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 403, in conv2d
    data_format=data_format, name=name)
  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 768, in apply_op
    op_def=op_def)
  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 2336, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1228, in __init__
    self._traceback = _extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1,6000,6000,64]
         [[Node: inference/Conv2D = Conv2D[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](sub, inference/conv1_1_w/read)]]
`
"
10099,Model,how to change human detection and track into car detection in TF Detect?
10098,graph and model,how to change human detection and track into car detection in TF Detect?
10096,Cannot set Build Cost Model in Graph Options,"`init_op = tf.global_variables_initializer();
sess = tf.Session(tf.ConfigProto(graph_options=tf.GraphOptions(build_cost_model=1)));
sess.run(init_op);`
After I downloaded the updated repository I cannot get the _build_cost_model_ flag to work. I have attached the code snippet above.
The error I get is 

> Traceback (most recent call last):                                                                                                            File ""test_part.py"", line 55, in <module>                                                                                                     sess = tf.Session(tf.ConfigProto(graph_options=tf.GraphOptions(build_cost_model=1)))                                                      File ""/home/utagar/tensorflow_new_instrumented/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1292, in __init__                                                                                                                                             super(Session, self).__init__(target, graph, config=config)                                                                               File ""/home/utagar/tensorflow_new_instrumented/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 541, in __init__                                                                                                                                              raise TypeError('target must be a string, but got %s' % type(target))                                                                   TypeError: target must be a string, but got <class 'tensorflow.core.protobuf.config_pb2.ConfigProto'>                                       Exception AttributeError: ""'Session' object has no attribute '_session'"" in <bound method Session.__del__ of <tensorflow.python.client.session.Session object at 0x7fcef03fccd0>> ignored

Please suggest how to fix it."
10094,compile contrib/hvx failed.,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.2.0-rc0
- **Bazel version (if compiling from source)**:

- **CUDA/cuDNN version**:
not used 
- **GPU model and memory**:
not used
- **Exact command to reproduce**:
I follow the commands in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx 
to build tensorflow that running hvx.

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
 I have tryed ndk-r13b,r10d,r14d. All of them produce the errors as follows:

checking whether to enable maintainer-specific portions of Makefiles... yes
checking build system type... x86_64-unknown-linux-gnu
checking host system type... arm-unknown-linux-androideabi
checking target system type... arm-unknown-linux-androideabi
checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for arm-linux-androideabi-strip... no
checking for strip... strip
checking for a thread-safe mkdir -p... /bin/mkdir -p
checking for gawk... no
checking for mawk... mawk
checking whether make sets $(MAKE)... yes
checking whether make supports nested variables... yes
checking whether UID '1000' is supported by ustar format... yes
checking whether GID '1000' is supported by ustar format... yes
checking how to create a ustar tar archive... gnutar
checking for arm-linux-androideabi-gcc...  arm-linux-androideabi-gcc --sysroot ../Qualcomm/Hexagon_SDK/3.0/tools/android-ndk-r10d/platforms/android-21/arch-arm
checking whether the C compiler works... no
configure: error: in `/home/zhouzhan/tensorflow/tensorflow/contrib/makefile/downloads/protobuf':
configure: error: C compiler cannot create executables
See `config.log' for more details

Config.log is:
[config_log.txt](https://github.com/tensorflow/tensorflow/files/1018354/config_log.txt)


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
10092,tf.matmul unexpected exception,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
MacOS sierra and Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
on mac, installed using binary
on linux,installed using binary
**TensorFlow version (use command below)**:
v1.0.1
- **Bazel version (if compiling from source)**:

- **CUDA/cuDNN version**:
n/a
- **GPU model and memory**:
n/a
- **Exact command to reproduce**:

import tensorflow as tf
x = tf.placeholder(tf.float32, shape=(None, 1795,13))
u = tf.Variable(tf.truncated_normal(shape=(13,5), stddev=0.1))
y = tf.matmul(x,u)

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I expected y to go through, and be a tensor of shape=(?,1795,5)
instead, I got an exception:
ValueError: Shape must be rank 2 but is rank 3 for 'MatMul_1' (op: 'MatMul') with input shapes: [?,1795,13], [13,5].
"
10091,tf.assign does not change the shape of variables correctly,"Hi,

I have discovered what I believe to be a bug with tf.assign. This occurs when I have a matrix with a shape, e.g. (2, 3), and assign it a new, larger matrix, e.g. with the shape (2, 4). When you get the shape of your updated variable, its shape has not changed i.e. (2, 3) is returned. Furthermore, if you use tf.add on your new matrix and a matrix with the same shape (2, 4) as the updated matrix an exception is thrown:
`ValueError: Dimensions must be equal, but are 3 and 4 for 'Add_2' (op: 'Add') with input shapes: [2,3], [2,4].`

Similar exceptions are also thrown when you try and use tf.matmult, tf.subtract, etc. on your updated variable.

Below is an example of this issue.

<pre><code>import tensorflow as tf

import numpy as np

# Initialise some variables
sess = tf.Session()
x = tf.Variable(tf.ones([2, 3]))
y = tf.Variable(tf.ones([2, 3]))
z = tf.Variable(tf.ones([2, 4]))
sess.run(tf.variables_initializer([x, y, z]))

# Print information about the original matrix
print(x.get_shape())
print(x.eval(session=sess))
print(tf.add(x, y).eval(session=sess))
print()

# Enlarge the matrix by assigning it a new set of values
sess.run(tf.assign(x, np.ones([2, 4]), validate_shape=False))

# Print information about the new matrix
print(x.get_shape())
print(x.eval(session=sess))
print()

# Try add the updated matrix to a matrix with the same ORIGINAL shape
# I would expect this to fail because I have changed the shape
try:
  print(tf.add(x, y).eval(session=sess))
except:
  print(""Could not add x and y"")
print()

# Try add the updated matrix to a matrix with the same shape
# I would NOT expect this to fail because they both have the same shapes
try:
  print(tf.add(x, z).eval(session=sess))
except:
  print(""Could not add x and z"")
print()

# Try add the updated matrix to a matrix that had the same ORIGINAL
# shape as it, but has been updated so their actual shapes are the same
a = tf.Variable(tf.ones([2, 3]))
sess.run(tf.variables_initializer([a,]))
sess.run(tf.assign(a, np.ones([2, 4]), validate_shape=False))
print(tf.add(x, a).eval(session=sess))
</code></pre>

The output of this code is:

<code><pre>(2, 3)
[[ 1.  1.  1.]
 [ 1.  1.  1.]]
[[ 2.  2.  2.]
 [ 2.  2.  2.]]

(2, 3)
[[ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]]

Could not add x and y

Could not add x and z

[[ 2.  2.  2.  2.]
 [ 2.  2.  2.  2.]]
</code></pre>"
10090,How can I update my tensorflow 1.0 to 1.2 or 1.1 at the window10.,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I installed tensorflow at my window 10.
It is only 1.0 version and using python 3.5.
How can I update version 1.2 and can I use python 3.6?
I already installed anaconda newest version.
I installed it using anaconda prompt.



### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
10089,Scala API,"Hi,

I have open sourced a Scala API for TensorFlow that contains much more complete functionality than the Java API, [here](https://github.com/eaplatanios/tensorflow_scala). The README file in the repository contains information on what features are supported etc. I would really appreciate some feedback from interested parties in the community on the library.

One main limitation is that the library does not yet support while loops in the graph. This is due to being unable to implement gradient backpropagation using the current C API. @skye do you have any suggestions on how to proceed with respect to that? It would be really useful for implementing RNNs.

Other cool stuff (such as fetching arbitrarily structured data from sessions -- e.g., lists of tensors, or maps of tensors, etc.) are supported and are for the most part strongly typed.

Cheers,
Anthony"
10088,Surfacing REGISTER_OP to the public C API,"### Feature Request

I am working on the .NET bindings for C# and as part of this process, I am replicating the C API test suite in C# to ensure everything works as expected.

The REGISTER_OP capability is available to C developers, and I would like to have this capability surfaced so my binding (and likely other bindings) can roll out these tests as well.

This is what I would like to be able to do from C#:

https://github.com/tensorflow/tensorflow/blob/7d785f1e18af9d22d940f18aac6e8c9ffd268b22/tensorflow/c/c_api_test.cc#L1577
"
10087,Cannot get TensorBoard example working,"""""""A very simple MNIST classifer, modified to display data in TensorBoard

See extensive documentation for the original model at
http://tensorflow.org/tutorials/mnist/beginners/index.md

See documentaion on the TensorBoard specific pieces at
http://tensorflow.org/how_tos/summaries_and_tensorboard/index.md

""""""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import sys

import tensorflow as tf

# Import data
import input_data
mnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)

import tensorflow as tf
sess = tf.InteractiveSession()


# Create the model
x = tf.placeholder(""float"", [None, 784], name=""x-input"")
W = tf.Variable(tf.zeros([784,10]), name=""weights"")
w_hist = tf.summary.histogram(""weights"", W)
b = tf.Variable(tf.zeros([10], name=""bias""))
b_hist = tf.summary.histogram(""biases"", b)
with tf.name_scope(""Wx_b"") as scope:
  y = tf.nn.softmax(tf.matmul(x,W) + b)
#y_hist = tf.histogram_summary(""y"", y)
y_hist = tf.summary.histogram(""y"", y)

# Define loss and optimizer
y_ = tf.placeholder(""float"", [None,10], name=""y-input"")
with tf.name_scope(""xent"") as scope:
  cross_entropy = -tf.reduce_sum(y_*tf.log(y))
  ce_summ = tf.summary.scalar(""cross entropy"", cross_entropy) # use summary.scalar instead of tf.scalar_summary
with tf.name_scope(""train"") as scope:
  train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)

with tf.name_scope(""test"") as scope:
  correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
  accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))
  accuracy_summary = tf.summary.scalar(""accuracy"", accuracy) # use summary.scalar instead of tf.scalar_summary

merged = tf.summary.merge_all() # use tf.summary.merge_all instead merge_all_summaries
writer = tf.summary.FileWriter(""/tmp/mnist_logs"", sess.graph_def) # # use tf.summary.FileWriter instead SummaryWriter
tf.initialize_all_variables().run()

# Test trained model

for i in range(1000):
  if i % 10 == 0:  # Record summary data, and the accuracy
    feed = {x: mnist.test.images, y_: mnist.test.labels}
    result = sess.run([merged, accuracy], feed_dict=feed)
    summary_str = result[0]
    acc = result[1]
    writer.add_summary(summary_str, i)
    print(""Accuracy at step %s: %s"" % (i, acc))
  else:
    batch_xs, batch_ys = mnist.train.next_batch(100)
    feed = {x: batch_xs, y_: batch_ys}
    sess.run(train_step, feed_dict=feed)

print(accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))





while running the above example, it ran successfully , but while calling tensorborad it gives the below error. Please help.

Accuracy at step 980: 0.9151
Accuracy at step 990: 0.9156
0.9147
Abhisheks-MacBook-Air:POC abhi$ tensorboard --logdir=/tmp/mnist_logs
Traceback (most recent call last):
File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in 
from tensorflow.python.pywrap_tensorflow_internal import *
File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in 
_pywrap_tensorflow_internal = swig_import_helper()
File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py"", line 242, in load_module
return load_dynamic(name, filename, file)
File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py"", line 342, in load_dynamic
return _load(spec)
ImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: .8.0.dylib
Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
Reason: image not found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""/Library/Frameworks/Python.framework/Versions/3.6/bin/tensorboard"", line 7, in 
from tensorflow.tensorboard.tensorboard import main
File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/init.py"", line 24, in 
from tensorflow.python import *
File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/init.py"", line 51, in 
from tensorflow.python import pywrap_tensorflow
File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 52, in 
raise ImportError(msg)
ImportError: Traceback (most recent call last):
File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in 
from tensorflow.python.pywrap_tensorflow_internal import *
File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in 
_pywrap_tensorflow_internal = swig_import_helper()
File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py"", line 242, in load_module
 @abhis01
     
abhis01 commented 43 minutes ago
i am using python 2.7
 @abhis01
     
abhis01 commented 38 minutes ago
Hello, I am using these versions :--

Abhisheks-MacBook-Air:tensorflow abhi$ pip show tensorflow
Name: tensorflow
Version: 1.1.0
Summary: TensorFlow helps the tensors flow
Home-page: http://tensorflow.org/
Author: Google Inc.
Author-email: opensource@google.com
License: Apache 2.0
Location: /usr/local/lib/python2.7/site-packages
Requires: wheel, protobuf, numpy, mock, werkzeug, six
Abhisheks-MacBook-Air:tensorflow abhi$ pip show protobuf
Name: protobuf
Version: 3.3.0
Summary: Protocol Buffers
Home-page: https://developers.google.com/protocol-buffers/
Author: protobuf@googlegroups.com
Author-email: protobuf@googlegroups.com
License: 3-Clause BSD License
Location: /usr/local/lib/python2.7/site-packages
Requires: setuptools, six
Abhisheks-MacBook-Air:tensorflow abhi$ pip show six
Name: six
Version: 1.10.0
Summary: Python 2 and 3 compatibility utilities
Home-page: http://pypi.python.org/pypi/six/
Author: Benjamin Peterson
Author-email: benjamin@python.org
License: MIT
Location: /usr/local/lib/python2.7/site-packages
Requires:
Abhisheks-MacBook-Air:tensorflow abhi$ python
Python 2.7.13 (default, Apr 4 2017, 08:47:57)
[GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.38)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information."
10086,Type check failed in piecewise_constant.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:archlinux
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:v1.1.0-rc2-1140-g513b1e4 1.1.0-rc2 

```python
global_step = tf.get_variable('gs', trainable=False, dtype=tf.int32, shape=[])
global_step = tf.get_default_graph().get_tensor_by_name('gs:0')
boundaries = [100000, 110000]
values = [1.0, 0.5, 0.1]
learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)
```
```
    learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)
  File ""/xxx/.local/lib/python3.6/site-packages/tensorflow/python/training/learning_rate_decay.py"", line 144, in piecewise_constant
    b.dtype, x.dtype))
ValueError: Boundaries (<dtype: 'int32'>) must have the same dtype as x (<dtype: 'int32_ref'>).
```
I wonder if the check is over strict? Is there a reason to distinguish `int32` and `int32_ref` here?"
10085,pip install  --user  tensorflow==1.2.0rc0,"Error occurred:
Non-zero exit code(1)
![untitled](https://cloud.githubusercontent.com/assets/25774969/26286789/70e3d8be-3e97-11e7-8e60-6bcb27257d52.jpg)

"
10083,"Nullptr check failed, when using TensorArray in combination with while_loop and swap_memory","Environment: TensorFlow-gpu r1.1 build from source on Windows 10

I am using the python API of TensorFlow to train a variant of an LSTM.
For that purpose I use the `tf.while_loop` function to iterate over the time steps
With `swap_memory=True` and not limiting devices to cpu I get the following failure:

`...tensorflow/tensorflow/core/framework/tensor.cc:885] Check failed: nullptr != b.buf_ (nullptr vs. 00...)`

With `swap_memory=False` or limiting devices to cpu this does not happen.

The part of my code, that causes this failure (when commenting it out, it works) is in the body of the while loop:

    ...
    h_gathered = h_ta.gather(tf.range(time))
    h_gathered = tf.transpose(h_gathered, [1, 0, 2])
    syn_t = self.syntactic_weights_ta.read(time)[:, :time]
    syn_t = tf.expand_dims(syn_t, 1)
    syn_state_t = tf.squeeze(tf.tanh(tf.matmul(syn_t, h_gathered)), 1)
    ...

where `time` is zero based and incremented after each step, `h_ta` is a TensorArray

    h_ta = tf.TensorArray(
            dtype=dtype,
            size=max_seq_len,
            clear_after_read=False,
            element_shape=[batch_size, num_hidden],
            tensor_array_name=""fw_output"")
and `self.syntactic_weights_ta` is also a TensorArray

    self.syntactic_weights_ta = tf.TensorArray(
            dtype=dtype,
            size=max_seq_len,
            tensor_array_name=""fw_syntactic_weights"")
    self.syntactic_weights_ta = self.syntactic_weights_ta.unstack(syntactic_weights)

What I am trying to achieve in the code snippet is basically a weighted sum over the past outputs, stored in `h_ta`.
In the end I train the network with `tf.train.AdamOptimizer`.

The forward propagation seems to work, as inserting `tf.Print` commands in the sensitive part of the code works.
But I guess the backward propagation 

Unfortunately I could not find out which tensor's buffer points to nullptr.
"
10081,Nightly binaries are not being built successfully,"Seems like the nightly builds have been failing for a while. For example, Python 3 Mac GPU build history [here](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-mac/). Is this something being worked on since it's been a month or so. "
10080,compilation errors due to missing op classes when using selective registration (cmake windows build),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NO.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 7
- **TensorFlow installed from (source or binary)**:
From source
- **TensorFlow version (use command below)**:
commit 280374
- **Bazel version (if compiling from source)**:
n/a, using cmake build
- **CUDA/cuDNN version**:
n/a
- **GPU model and memory**:
n/a
- **Exact command to reproduce**:

cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DCMAKE_INCLUDE_CURRENT_DIR:BOOL=ON -DCMAKE_CXX_FLAGS=-DSELECTIVE_REGISTRATION -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX -Dtensorflow_BUILD_PYTHON_BINDINGS:BOOL=OFF -Dtensorflow_BUILD_CONTRIB_KERNELS:BOOL=OFF -Dtensorflow_BUILD_SHARED_LIB:BOOL=ON 

MSBuild /p:Configuration=Release tensorflow.vcxproj

### Describe the problem

I'm building with cmake on Windows, using selective registration.  I've put ops_to_register.h in my build dir and specified -DCMAKE_INCLUDE_CURRENT_DIR=ON in my cmake command so that it would be added to the include path.

Due (I'm assuming) to selective registration, the generated code omits many classes which would otherwise be generated in a full build.

When compiling non-generated code, e.g. tensorflow\cc\gradients\array_grad.cc, there are compile errors wherever it references the classes that were not generated due to selective registration, for example:

array_grad.cc(51): error C2653: 'Unstack': is not a class or namespace name [...omitted...\tensorflow\tensorflow\contrib\cmake\build\tf_cc.vcxproj]

There are many of these errors for various referenced classes, in various other source files such as:
tensorflow\cc\gradients\math_grad.cc
tensorflow\cc\gradients\nn_grad.cc
tensorflow\cc\framework\gradients.cc

When using selective registration, what's the right approach to avoid these errors?
"
10078,I get some error when I'm using tensorflow at the window anaconda,"> Please

 go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I installed it at the window.
When I'm test sample code and print it describes messages.
I repeated it.
At that time it succesfully well.
How can I solve this it?
Is it installed well?


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.



(C:\Users\KBS\Anaconda3) C:\Users\KBS>activate tensorflow

(tensorflow) C:\Users\KBS>python
Python 3.5.3 | packaged by conda-forge | (default, May 12 2017, 16:16:49) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> a=tf.constant('a')
>>> sess=tf.Session()
>>> print(sess.run(a))
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots
b'a'
>>> print(sess.run(a))
b'a'
>>>
"
10077,"Error happens when using tensorboard:No handlers could be found for logger ""werkzeug""","I install the tensorflow-gpu 1.1.0 by anaconda-pip, but every time i open http://0.0.0.0:6006 to start tensorboard the error happens.I have already installed  ""werkzeug"".

`No handlers could be found for logger ""werkzeug""`
"
10075,Missing includes in whl-file (cmake),"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: latest master
- **Bazel version (if compiling from source)**: CMAKE
- **Exact command to reproduce**: MSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj

### Describe the problem
- Using the cmake build command results in the successfull build of the file:
tensorflow\contrib\cmake\build\tf_python\dist\tensorflow_gpu-1.1.0-cp35-cp35m-win_amd64.whl
- In the whl-file, the header files (obtained via setup.py) are missing.
- The header files should be included in the whl file if I am not mistaken. I am not sure what the origin of the missing files is. 

### Source code / logs
The desired header files can be manually installed by calling 
```
python tensorflow\contrib\cmake\build\tf_python\setup.py install_headers
```
"
10074,fatal problem with saving variables,"I coded a simple feedforward neural network and it works very well.

I tried to save the computation time, and created:
**self.total_time** = tf.Variable(0, dtype = tf.float32, trainable = True, name = 'total_time')
in the fnn class.

and i tried to print the total training time per some training epoch.
**I made it to grow with time:**

**# Check & Print training time**
till_now = time.time() - start_time
self.total_time += till_now
print_time(self.total_time.eval())

**and the result look something like this :** 

Epoch :   0 | Evaluation :  115 | Learning Rate : 0.50
-------------------------------------------------------
Training Loss :         0.040919
Validation Loss :      0.0741969
Validation Accuracy :      97.77%
Total time cost : 0.38 seconds

Epoch :   1 | Evaluation :  116 | Learning Rate : 0.50
-------------------------------------------------------
Training Loss :        0.0417941
Validation Loss :       0.073841
Validation Accuracy :      97.73%
Total time cost : 0.71 seconds

Epoch :   2 | Evaluation :  117 | Learning Rate : 0.50
-------------------------------------------------------
Training Loss :        0.0334573
Validation Loss :      0.0745566
Validation Accuracy :      97.75%
Total time cost : 1.01 seconds

However, When i interrupt the training procedure and try to restore global variables and restart the training, I just lose the value of **variable total_time** and it initialized as 0 which is the value i first give to.
I also checked tf.global_variables() include self.total_time.

What is wrong?"
10073,fail to load tensorflow 1.1.0 inside docker container - libcuda.so.1 missing,"Hi,

I ran into this bug while trying to upgrade tensorflow version to 1.1.0
Using 'nvidia/cuda:8.0-cudnn5-devel-ubuntu16.04' docker image (I also tried other docker images as well).

Note that same code runs fine with tensorflow 1.0.0

Attached is python script that prints tensorflow version (see README.txt for instruction of how to build & run the docker image). 
[fail-to-load-tf-bug.zip](https://github.com/tensorflow/tensorflow/files/1016737/fail-to-load-tf-bug.zip)


Here is the stack trace for tensorflow 1.1.0:

```
Traceback (most recent call last):
  File ""/run.py"", line 3, in <module>
    import tensorflow as tf
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 51, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libcuda.so.1: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```


Here is the logs for tensorflow 1.0.0: (Here it works fine)
Please pay attention message 'Couldn't open CUDA library libcuda.so.1'
```
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:126] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: 522f0c0e9705
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: Permission denied: could not open driver version path for reading: /proc/driver/nvidia/version
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1065] LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1066] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
tensorflow version 1.0.0
tensorflow git version v1.0.0-rc2-15-g47bba63-dirty
```


Thanks :)
Erez

P.S:
I built and ran the docker image on 2 machines:
* MAC 
    - Docker version 17.03.1-ce, build c6d412e
    - no GPU
* Ubuntu 14.04.5 LTS 
   - Docker version 1.13.0, build 49bf474 
   - GPU info:

```
$nvidia-smi
Sun May 21 11:11:31 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX TIT...  Off  | 0000:05:00.0     Off |                  N/A |
| 22%   57C    P8    31W / 250W |      0MiB / 12206MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX TIT...  Off  | 0000:06:00.0     Off |                  N/A |
| 22%   54C    P8    21W / 250W |      0MiB / 12206MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX TIT...  Off  | 0000:09:00.0     Off |                  N/A |
| 22%   45C    P8    15W / 250W |      0MiB / 12206MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX TIT...  Off  | 0000:0A:00.0     Off |                  N/A |
| 22%   36C    P8    16W / 250W |      0MiB / 12206MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+  

```"
10072,can it stop printing not requested information?,"when i restore a ckpt, I found that there is an automated printing that:

INFO:tensorflow:Restoring parameters from checkpoint/fnn/params.ckpt-37000

can i stop this?"
10071,Cannot build jemalloc support using CMake on Linux (fails trying to include <windows.h>),"I am using the provided cmake build project files to compile tensorflow because I have a custom clang binary built with additional optimization passes of my own.

The build works fine without Jemalloc

    cmake -DCMAKE_BUILD_TYPE=Release ../tensorflow/contrib/cmake/  
    make -j4   # compiles O.K.

but when I add the jemalloc option it fails. 

    cmake -Dtensorflow_ENABLE_JEMALLOC_SUPPORT=ON -DCMAKE_BUILD_TYPE=Release ../tensorflow/contrib/cmake/  
    make 

    [  5%] Performing configure step for 'jemalloc'
    -- CMAKE_C_COMPILER_ID: Clang
    -- void* size is 8
    -- int size is 4
    -- long size is 8
    -- long long size is 8
    -- intmax_t size is 8
    -- CMAKE_SYSTEM_NAME: Linux
    -- whether pause instruction is compilable ... yes
    CMake Error at Utilities.cmake:755 (message):
      GetSystemPageSize failed compilation see
      cmake/jemalloc/src/jemalloc/GetPageSize/getpagesize.log
    Call Stack (most recent call first):
      CMakeLists.txt:464 (GetSystemPageSize)


Looking at cmake/jemalloc/src/jemalloc/GetPageSize/getpagesize.log there is

    Building C object CMakeFiles/cmTC_129ba.dir/getpagesize.c.o
    clang     -o CMakeFiles/cmTC_129ba.dir/getpagesize.c.o   -c  tensorflow-github/build-cmake/jemalloc/src/jemalloc/GetPageSize/getpagesize.c
    tensorflow-github/build-cmake/jemalloc/src/jemalloc/GetPageSize/getpagesize.c:1:10: fatal error: 'windows.h' file not found
    #include <windows.h>
                 ^~~~~~~~~~~
    1 error generated.

By looking at getpagesize.c it is clear it is a windows-only source file that should not have been compiled on Linux.

I went further investigating why jemalloc is trying to compile a windows source under Linux but I got nowhere after an hour or so. I lack understanding of the jemalloc build and I will continue to look into this but if you have someone on your side with a more prompt answer, that would save me time, thank you. 
"
10067,Tensorboard problem with pandas,"This is my first time to practice the tensorflow and tensorboard function.
My code below:
`import tensorflow as tf
a = tf.constant(10,name='a')
b = tf.constant(90,name='b')
y=tf.Variable(a+b*2, name = 'y')
model = tf.global_variables_initializer()
with tf.Session() as session:
    merged = tf.summary.merge_all()
    file_writer = tf.summary.FileWriter(""/temp/to/logs"",session.graph)
    session.run(model)
    print(session.run(y))`
Then I open the terminal and enter the following:
`tensorboard --logdir=/tmp/tensorflowlogs`
But tensorboar no work, and show the error as below:
c:\anaconda3\lib\site-packages\dask\dataframe\hashing.py:8: FutureWarning: The pandas.lib module is deprecated and will be removed in a future version. These are private functions and can be accessed from pandas._libs.lib instead
  from pandas.lib import is_bool_array
Traceback (most recent call last):
  File ""c:\anaconda3\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Anaconda3\Scripts\tensorboard.exe\__main__.py"", line 5, in <module>
  File ""c:\anaconda3\lib\site-packages\tensorflow\tensorboard\tensorboard.py"", line 33, in <module>
    from tensorflow.tensorboard.backend import application
  File ""c:\anaconda3\lib\site-packages\tensorflow\tensorboard\backend\application.py"", line 47, in <module>
    from tensorflow.tensorboard.plugins.projector import projector_plugin
  File ""c:\anaconda3\lib\site-packages\tensorflow\tensorboard\plugins\projector\projector_plugin.py"", line 28, in <module>
    from tensorflow.contrib.tensorboard.plugins.projector import PROJECTOR_FILENAME
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\__init__.py"", line 30, in <module>
    from tensorflow.contrib import factorization
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\factorization\__init__.py"", line 24, in <module>
    from tensorflow.contrib.factorization.python.ops.gmm import *
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\factorization\python\ops\gmm.py"", line 27, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import estimator
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\learn\__init__.py"", line 87, in <module>
    from tensorflow.contrib.learn.python.learn import *
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\learn\python\__init__.py"", line 23, in <module>
    from tensorflow.contrib.learn.python.learn import *
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\__init__.py"", line 25, in <module>
    from tensorflow.contrib.learn.python.learn import estimators
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\__init__.py"", line 297, in <module>
    from tensorflow.contrib.learn.python.learn.estimators.dnn import DNNClassifier
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\dnn.py"", line 29, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import dnn_linear_combined
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\dnn_linear_combined.py"", line 31, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import estimator
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 49, in <module>
    from tensorflow.contrib.learn.python.learn.learn_io import data_feeder
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\learn_io\__init__.py"", line 21, in <module>
    from tensorflow.contrib.learn.python.learn.learn_io.dask_io import extract_dask_data
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\learn_io\dask_io.py"", line 26, in <module>
    import dask.dataframe as dd
  File ""c:\anaconda3\lib\site-packages\dask\dataframe\__init__.py"", line 3, in <module>
    from .core import (DataFrame, Series, Index, _Frame, map_partitions,
  File ""c:\anaconda3\lib\site-packages\dask\dataframe\core.py"", line 40, in <module>
    pd.computation.expressions.set_use_numexpr(False)
AttributeError: module 'pandas' has no attribute 'computation'

C:\Users\Soars>tensorboard --logdir=/tmp/to/logs
c:\anaconda3\lib\site-packages\dask\dataframe\hashing.py:8: FutureWarning: The pandas.lib module is deprecated and will be removed in a future version. These are private functions and can be accessed from pandas._libs.lib instead
  from pandas.lib import is_bool_array
Traceback (most recent call last):
  File ""c:\anaconda3\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Anaconda3\Scripts\tensorboard.exe\__main__.py"", line 5, in <module>
  File ""c:\anaconda3\lib\site-packages\tensorflow\tensorboard\tensorboard.py"", line 33, in <module>
    from tensorflow.tensorboard.backend import application
  File ""c:\anaconda3\lib\site-packages\tensorflow\tensorboard\backend\application.py"", line 47, in <module>
    from tensorflow.tensorboard.plugins.projector import projector_plugin
  File ""c:\anaconda3\lib\site-packages\tensorflow\tensorboard\plugins\projector\projector_plugin.py"", line 28, in <module>
    from tensorflow.contrib.tensorboard.plugins.projector import PROJECTOR_FILENAME
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\__init__.py"", line 30, in <module>
    from tensorflow.contrib import factorization
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\factorization\__init__.py"", line 24, in <module>
    from tensorflow.contrib.factorization.python.ops.gmm import *
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\factorization\python\ops\gmm.py"", line 27, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import estimator
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\learn\__init__.py"", line 87, in <module>
    from tensorflow.contrib.learn.python.learn import *
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\learn\python\__init__.py"", line 23, in <module>
    from tensorflow.contrib.learn.python.learn import *
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\__init__.py"", line 25, in <module>
    from tensorflow.contrib.learn.python.learn import estimators
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\__init__.py"", line 297, in <module>
    from tensorflow.contrib.learn.python.learn.estimators.dnn import DNNClassifier
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\dnn.py"", line 29, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import dnn_linear_combined
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\dnn_linear_combined.py"", line 31, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import estimator
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 49, in <module>
    from tensorflow.contrib.learn.python.learn.learn_io import data_feeder
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\learn_io\__init__.py"", line 21, in <module>
    from tensorflow.contrib.learn.python.learn.learn_io.dask_io import extract_dask_data
  File ""c:\anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\learn_io\dask_io.py"", line 26, in <module>
    import dask.dataframe as dd
  File ""c:\anaconda3\lib\site-packages\dask\dataframe\__init__.py"", line 3, in <module>
    from .core import (DataFrame, Series, Index, _Frame, map_partitions,
  File ""c:\anaconda3\lib\site-packages\dask\dataframe\core.py"", line 40, in <module>
    pd.computation.expressions.set_use_numexpr(False)
AttributeError: module 'pandas' has no attribute 'computation'

What's wrong with me?

My environment:
OS: windows 10
IDE: Visual studio 2015
Package version:
1) tensorflow: .1.1.0
2) pandas: 0.20.1
3) matplotlib: 2.0.0
Please help me, thank you."
10066,cifar10 example running slower with Windows 10,"### System information 1
- **Have I written custom code**: No
- **OS Platform and Distribution**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 0.12.1
- **CUDA/cuDNN version**: 8.0.44 / 5.1
- **GPU model and memory**: GTX 1080 8gb
- **Exact command to reproduce**: 'python3 cifar10_train.py'

### System information 2
- **Have I written custom code**: No
- **OS Platform and Distribution**: Windows 10 Pro
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 0.12.1
- **CUDA/cuDNN version**: 8.0.44 / 5.1
- **GPU model and memory**: GTX 1080 8gb
- **Exact command to reproduce**: 'python cifar10_train.py'

### System information 3
- **Have I written custom code**: No
- **OS Platform and Distribution**: Windows 7 Pro
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 0.12.1
- **CUDA/cuDNN version**: 8.0.44 / 5.1
- **GPU model and memory**: Titan X maxwell 12gb
- **Exact command to reproduce**: 'python cifar10_train.py'

### Describe the problem
When running the cifar 10 CNN example code (https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10/), I get very different performance on several OS while having same installations of CUDA, CuDNN & TF, and the same code of course.
With _System 1_ (described above): running at ~2600 images/sec, Python CPU usage at ~50%
With _System 2_ (described above): running at ~600 images/sec, Python CPU usage at **100%** (!)
With _System 3_ (described above): running at ~2600 images/sec, Python CPU usage at ~50%

I'm experiencing the same problems with TF r1.0.1.
Seems like something in this code (pherhaps the queue threading?) doesn't go well with win10? CPU usage reaches 100% with win10, and significantly slower, that's very weird.

Thanks."
10062,C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed,"I got the following error when installing tensorflow v1.1.0 from source:

> ERROR: /home/software/tensorflow-1.1.0/tensorflow/stream_executor/BUILD:39:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 121 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.

My OS is centos7 with cuda 8.0 and cudnn 5."
10061,tensorflow is not a supported wheel on this p latform,"I have python 2.7.13 (Anaconda 4). When i try install **tensorflow** its raise error.
```
tensorflow_gpu-1.1.0-cp35-cp35m-win_amd64.whl is not a supported wheel on this p
latform.
```

I try use gpu version - same result."
10060,[XLA] [macOS] tfcompile via tf_library does not work on macOS,"I am trying to compile a model using XLA on macOS. I know the XLA is still in experimental stage, but I would be happy to help fix the problem. The problem can be reproduced from supplied tests, i.e: tensorflow/compiler/aot/tests:test_graph_tfadd.

### Environment
OS: macOS Sierra 10.12.4
Tensorflow source: v1.1.0-rc2
Bazel: bazel release 0.4.5-homebrew

### Describe the problem
The generated object file from the graph seems to be invalid or is not recognized by libtool:

`libtool: file: bazel-out/local-opt/genfiles/tensorflow/compiler/aot/tests/test_graph_tfadd.o is not an object file (not allowed in a library)`

`file bazel-out/local-opt/genfiles/tensorflow/compiler/aot/tests/test_graph_tfadd.o
bazel-out/local-opt/genfiles/tensorflow/compiler/aot/tests/test_graph_tfadd.o: ELF 64-bit LSB relocatable, x86-64, version 1 (SYSV), not stripped`

### How to reproduce the problem
The problem can be reproduced on a fresh checkout issuing the following commands:
`bazel build //tensorflow/compiler/aot/tests:test_graph_tfadd`

If you need more information, I am happy to provide it!"
10059,tf.summary not working in tf.cond functions...,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution**: Windows 10 as well as Linux Ubuntu 16.04.2
- **TensorFlow installed from (source or binary)**: installed with pip (in root of anaconda) like in the documentation (not installed from master)
- **TensorFlow version**: v1.1
- **CUDA/cuDNN version**: CUDA Version 8.0.44
- **GPU model and memory**: GeForce GTX TITAN X 12GB
- **Exact command to reproduce**: python test_summarizing.py

### Describe the problem
Hi everyone,
I was trying to write a function to do data augmentation on images. With a probability of 0.5, the function should do the augmentation and if not return the image unmodified. The basic idea of my usage you can extract from the code below.
`
image = tf.cond(tf.less(probability, 0.5), lambda: do_augmentation(image), lambda: image)
`
In the augmentation function, I want to see how the image changed so I added `tf.summary.image(...)` after every image-processing step. But when running the summary operation (after I merged all summaries with `tf.summary.merge_all()`) I get the following error:
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: The tensor returned for Merge/MergeSummary:0 was not valid.
```
I tried to debug the problem and saw that when I don't use the summaries (commenting them) in the augmentation function, the whole code works.

I couldn't find any help on StackOverflow regarding this problem. However, I saw only one other post (http://stackoverflow.com/questions/39275641/tensorflow-how-to-write-multistep-decay) which had this kind of error, so I took that code (to be sure that it was not a problem with `tf.summary.image()` but a problem in general with `tf.summary`) and played around to see where the problem is. Sadly, I couldn't figure it out...

In the attached zip file ([test_summarizing.zip](https://github.com/tensorflow/tensorflow/files/1015955/test_summarizing.zip)) there is the test_summarizing.py file, which contains 2 functions. 

1. `summary_not_working_simple()`:
Is a minimal example to replicate the error of my code and of the original problem I had.
(I used the scalar summary instead of the image summary because it doesn't make a difference...)

- If you comment both summaries in f1 and f2, the code always works. 
- If you comment one out of the 2 summary-calls (in either function f1 or f2), the code sometimes works and sometimes produces the traceback found below. 
To replicate, try commenting both calls then run the code. Then comment only one call and run the code (possibly multiple times). Then comment the other call and run the code again. You should see that with one commented call to `tf.summary.scalar`, the code sometimes produces the error and sometimes it simply works...
- If you don't comment anything (leaving both calls to the summary), the code never works and always produces the traceback shown below.

2. `summary_not_working_stack_overflow()`:
To replicate the error you must in the function `multi_step_decay` of class MultiStepDecay at lines 62-63 comment (resp. uncomment) the `with tf.control_dependencies` block and the error will appear.

Could someone please look into the problem of the `summary_not_working_simple()` and explain to me, why the summary is not working? I have found a workaround to using the `tf.cond()` but the code is very messy now :) Plus it would make sense to have the possibility of writing summaries from every point of the tensorflow code, right?
And if you could also explain why the issue in the second function `summary_not_working_stack_overflow()` occurs, I would appreciate it very much!

### Source code / logs
Traceback for summary_not_working_simple() example:
```
Traceback (most recent call last):
  File ""C:\Users\Andrei\Anaconda\lib\site-packages\tensorflow\python\client\session.py"", line 1039, in _do_call
    return fn(*args)
  File ""C:\Users\Andrei\Anaconda\lib\site-packages\tensorflow\python\client\session.py"", line 1021, in _run_fn
    status, run_metadata)
  File ""C:\Users\Andrei\Anaconda\lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""C:\Users\Andrei\Anaconda\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: The tensor returned for Merge/MergeSummary:0 was not valid.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/Andrei/PycharmProjects/testing/test_summarizing.py"", line 93, in <module>
    summary_not_working_simple()
  File ""C:/Users/Andrei/PycharmProjects/testing/test_summarizing.py"", line 30, in summary_not_working_simple
    _ = session.run(summary_merge_opt)
  File ""C:\Users\Andrei\Anaconda\lib\site-packages\tensorflow\python\client\session.py"", line 778, in run
    run_metadata_ptr)
  File ""C:\Users\Andrei\Anaconda\lib\site-packages\tensorflow\python\client\session.py"", line 982, in _run
    feed_dict_string, options, run_metadata)
  File ""C:\Users\Andrei\Anaconda\lib\site-packages\tensorflow\python\client\session.py"", line 1032, in _do_run
    target_list, options, run_metadata)
  File ""C:\Users\Andrei\Anaconda\lib\site-packages\tensorflow\python\client\session.py"", line 1052, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: The tensor returned for Merge/MergeSummary:0 was not valid.
```

Source code of my the original function:
```
def preprocess_for_train_summary_error(image, height, width, fast_mode=True, scope=None, central_fraction=0.875):
    with tf.name_scope(scope, 'train_image', [image, height, width]):
        if image.dtype != tf.float32:
            image = tf.image.convert_image_dtype(image, dtype=tf.float32)

        tf.summary.image(""original_image"", tf.expand_dims(image, axis=0), max_outputs=1)

        random_augment = tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32)

        def augmentation_pipeline(image_arg):
            random_translate = tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32)
            translated_image = tf.cond(tf.less(random_translate, 0.5),
                                       lambda: translate(image_arg),
                                       lambda: image_arg)
            tf.summary.image('translated_image', tf.expand_dims(translated_image, axis=0), max_outputs=1)

            random_rotate = tf.random_uniform([], minval=0, maxval=1)
            rotated_image = tf.cond(tf.less(random_rotate, 0.8),
                                    lambda: rotate(translated_image),
                                    lambda: translated_image)
            tf.summary.image('rotated_image', tf.expand_dims(rotated_image, axis=0), max_outputs=1)

            random_flip = tf.random_uniform([], minval=0, maxval=1)
            flipped_image = tf.cond(tf.less(random_flip, 0.5),
                                    lambda: flip(rotated_image),
                                    lambda: rotated_image)
            tf.summary.image('flipped_image', tf.expand_dims(flipped_image, axis=0), max_outputs=1)

            def f(x, ordering):
                return distort_color(x, color_ordering=ordering, fast_mode=fast_mode)

            random_distort_colors = tf.random_uniform([], minval=0, maxval=1)
            color_distorted_image = tf.cond(tf.less(random_distort_colors, 0.5),
                                            lambda: apply_with_random_selector(flipped_image, f, 4),
                                            lambda: flipped_image)
            tf.summary.image('color_distorted_image', tf.expand_dims(color_distorted_image, axis=0), max_outputs=1)

            return image_arg

        image = tf.cond(tf.less(random_augment, 0.5),
                        lambda: augmentation_pipeline(image),
                        lambda: image)

        image = tf.image.central_crop(image, central_fraction=central_fraction)
        tf.summary.image('central_cropped_image', tf.expand_dims(image, axis=0), max_outputs=1)
        image = tf.expand_dims(image, 0)
        image = tf.image.resize_bilinear(image, [height, width], align_corners=False)
        image = tf.squeeze(image, [0])
        tf.summary.image('resized_image', tf.expand_dims(image, axis=0), max_outputs=1)

        # Subtract off the mean and divide by the variance of the pixels.
        image = tf.subtract(image, 0.5, name=""sub_mean"")
        image = tf.multiply(image, 2.0, name=""div_var"")
        return image
```"
10058,Windows Installation permission denied. Why it required Admin?,"Hello, first time install Windows Tensorflow, and I am running Python3.5.2 and following example, I got ""Permission Denied"". I don't believe it requires admin, or does it?

```
d:\Experiments>conda create -n tensorflow
Fetching package metadata ...........
Solving package specifications:
Package plan for installation in environment C:\Users\mmansour\AppData\Local\conda\conda\envs\tensorflow:

Proceed ([y]/n)? Y

#
# To activate this environment, use:
# > activate tensorflow
#
# To deactivate this environment, use:
# > deactivate tensorflow
#
# * for power-users using bash, you must source
#


d:\Experiments>activate tensorflow

(tensorflow) d:\Experiments>pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.1.0-cp35-cp35m-win_amd64.whl
Collecting tensorflow-gpu==1.1.0 from https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.1.0-cp35-cp35m-win_amd64.whl
  Using cached https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.1.0-cp35-cp35m-win_amd64.whl
Collecting six>=1.10.0 (from tensorflow-gpu==1.1.0)
  Using cached six-1.10.0-py2.py3-none-any.whl
Collecting protobuf>=3.2.0 (from tensorflow-gpu==1.1.0)
Collecting wheel>=0.26 (from tensorflow-gpu==1.1.0)
  Using cached wheel-0.29.0-py2.py3-none-any.whl
Collecting werkzeug>=0.11.10 (from tensorflow-gpu==1.1.0)
  Using cached Werkzeug-0.12.2-py2.py3-none-any.whl
Collecting numpy>=1.11.0 (from tensorflow-gpu==1.1.0)
  Using cached numpy-1.12.1-cp35-none-win_amd64.whl
Collecting setuptools (from protobuf>=3.2.0->tensorflow-gpu==1.1.0)
  Using cached setuptools-35.0.2-py2.py3-none-any.whl
Collecting packaging>=16.8 (from setuptools->protobuf>=3.2.0->tensorflow-gpu==1.1.0)
  Using cached packaging-16.8-py2.py3-none-any.whl
Collecting appdirs>=1.4.0 (from setuptools->protobuf>=3.2.0->tensorflow-gpu==1.1.0)
  Using cached appdirs-1.4.3-py2.py3-none-any.whl
Collecting pyparsing (from packaging>=16.8->setuptools->protobuf>=3.2.0->tensorflow-gpu==1.1.0)
  Using cached pyparsing-2.2.0-py2.py3-none-any.whl
Installing collected packages: six, pyparsing, packaging, appdirs, setuptools, protobuf, wheel, werkzeug, numpy, tensorflow-gpu
Exception:
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\commands\install.py"", line 342, in run
    prefix=options.prefix_path,
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 784, in install
    **kwargs
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\req\req_install.py"", line 851, in install
    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\req\req_install.py"", line 1064, in move_wheel_files
    isolated=self.isolated,
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\wheel.py"", line 345, in move_wheel_files
    clobber(source, lib_dir, True)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\wheel.py"", line 323, in clobber
    shutil.copyfile(srcfile, destfile)
  File ""C:\ProgramData\Anaconda3\lib\shutil.py"", line 121, in copyfile
    with open(dst, 'wb') as fdst:
PermissionError: [Errno 13] Permission denied: 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\six-1.10.0.dist-info\\DESCRIPTION.rst'

(tensorflow) d:\Experiments>
```"
10054,Inconsistent functionality from tf.Graph with simple linear regression (not caused from random-seed-state),"I am getting very inconsistent behavior with my `tf.Graph` objects and I can't explain why it's happening.  I'm running this in a Jupyter notebook.  Could this affect it at all?  

My versions: 
```
tf.__version__
'1.0.1'
sys.version
'3.6.1 |Anaconda custom (x86_64)| (default, Mar 22 2017, 19:25:17) \n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]'
```
```
from sklearn.datasets import *
import multiprocessing
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

tf_max_threads = tf.ConfigProto(intra_op_parallelism_threads=multiprocessing.cpu_count())
def iris_data():
    # Iris dataset
    X = pd.DataFrame(load_iris().data,
                           index = [""iris_%d"" % i for i in range(load_iris().data.shape[0])],
                           columns = [x.split("" (cm)"")[0].replace("" "",""_"") for x in load_iris().feature_names])

    y = pd.Series(load_iris().target,
                           index = [""iris_%d"" % i for i in range(load_iris().data.shape[0])],
                           name = ""Species"")
    color_list = [{0:""red"",1:""green"",2:""blue""}[x] for x in y]
    cmap = {k:v for k,v in zip(X.index, color_list)}
    return (X, y, cmap)

# Data
X,y,c = iris_data()
x = X[""petal_width""].as_matrix()
y = X[""sepal_length""].as_matrix()
n = 50

# Containers
lasso_data = list()
A_data = list()
b_data = list()

# Graph
G_3_78 = tf.Graph()

# Iterations
n_iter = 1500

# Functions
def lasso_penalty(coef, alpha=0.9):
    lasso_param = tf.constant(alpha)
    heavyside_step = tf.truediv(1.0, tf.add(1.0, tf.exp(tf.multiply(-100.0, tf.subtract(coef, lasso_param)))))
    regularization_param = tf.multiply(heavyside_step, 99.0)
    return regularization_param
def l2(y, y_model):
    return tf.square(y - y_model)

# Build Graph
with G_3_78.as_default():
    # Placeholders
    pH_x_petal_width = tf.placeholder(tf.float32, shape=[None,1], name=""pH_x_petal_width"")
    pH_y_hat = tf.placeholder(tf.float32, shape=[None,1])
    
    # Model
    A = tf.Variable(tf.random_normal(mean=0, stddev=1, shape=[1,1]), name=""A"")
    b = tf.Variable(tf.random_normal(mean=0, stddev=1, shape=[1,1]), name=""b"")
    model = tf.add(tf.matmul(pH_x_petal_width, A), b)
    
    # Loss
    loss_lasso = tf.add(tf.reduce_mean(l2(pH_y_hat, model)), lasso_penalty(A, 0.9))
    
    with tf.Session(graph=G_3_78, config=tf_max_threads) as sess:
        sess.run(tf.global_variables_initializer())
        # Optimizer
        op = tf.train.GradientDescentOptimizer(0.001)
        train_step = op.minimize(loss_lasso)
        # Train linear model 
        for i in range(n_iter):
            idx_random = np.random.RandomState(i).choice(y.shape[0], size=n)
            tr_x = x[idx_random].reshape(-1,1)
            tr_y = y[idx_random].reshape(-1,1)
            sess.run(train_step, feed_dict={ pH_x_petal_width:tr_x, pH_y_hat:tr_y})
            # Iterations
            A_iter = sess.run(A)[0][0]
            b_iter = sess.run(b)[0][0]
            
            lasso_iter = sess.run(loss_lasso, feed_dict={ pH_x_petal_width:tr_x, pH_y_hat:tr_y})[0][0]
            lasso_data.append(lasso_iter)
            A_data.append(A_iter)
            b_data.append(b_iter)
            # Log
            if (i + 1) % 500 == 0:
                print(f""Step #{i + 1}\n\tA = {A_iter}"")
                print(f""\tb = {b_iter}"")
                print(f""\tLoss = {lasso_iter}"")
                print()        
# Plot path
with plt.style.context(""seaborn-white""):
    fig, ax = plt.subplots(nrows=3, figsize=(6,6))
    pd.Series(lasso_data,).plot(ax=ax[0], label=""loss(lasso)"", legend=True)
    pd.Series(A_data,).plot(ax=ax[1], color=""red"", label=""A"", legend=True)
    pd.Series(b_data,).plot(ax=ax[2], color=""black"", label=""b"", legend=True)
    fig.suptitle(""training-process"", fontsize=15, y=0.95)

    # Plot linear separation
with plt.style.context(""seaborn-white""):
    fig, ax = plt.subplots(figsize=(5,5))
    ax.scatter(x, y, c=X.index.map(lambda x:c[x]))
    x_lims = ax.get_xlim()
#     y_lims = ax.get_ylim()
    for i in range(n_iter):
        ax.plot(x, x*A_data[i] + b_data[i], alpha=i/(n_iter*100), color=""black"")
    ax.set_xlim(x_lims)
#     ax.set_ylim(y_lims)
```

I will run this code block and get the desired results: 
```
Step #500
	A = 0.8209055066108704
	b = 3.338909149169922
	Loss = 2.8257622718811035

Step #1000
	A = 0.812068521976471
	b = 4.303615570068359
	Loss = 0.5661464929580688

Step #1500
	A = 0.8036581873893738
	b = 4.6621904373168945
	Loss = 0.23930419981479645
```
![image](https://cloud.githubusercontent.com/assets/9061708/26270528/6378cea2-3cb0-11e7-9a7c-3ab10850bd5d.png)

and then I will run the exact same code block again: 
```
Step #500
	A = nan
	b = nan
	Loss = nan

Step #1000
	A = nan
	b = nan
	Loss = nan

Step #1500
	A = nan
	b = nan
	Loss = nan
```
![image](https://cloud.githubusercontent.com/assets/9061708/26270550/946e57ca-3cb0-11e7-877a-0674c45ddd66.png)


and then one more time: 
```
Step #500
	A = 2.6116130352020264
	b = 1.91126549243927
	Loss = 101.70783233642578

Step #1000
	A = 2.3814141750335693
	b = 2.4998815059661865
	Loss = 100.85863494873047

Step #1500
	A = 2.11511492729187
	b = 2.9327337741851807
	Loss = 99.995849609375
```
![image](https://cloud.githubusercontent.com/assets/9061708/26270564/a97558ee-3cb0-11e7-8bad-85cb2f5f4f1c.png)


"
10050,Function img_to_array() gives error for JPEG on Windows,"Hello,

I'm getting error when I use `img_to_array()` function for particular JPEG file. Here is stack trace:

    Traceback (most recent call last):
      File ""file.py"", line 41, in <module>
        tf.app.run()
      File ""C:\Users\...\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\platform\app.py"", line 48, in run
        _sys.exit(main(_sys.argv[:1] + flags_passthrough))
      File ""file.py"", line 38, in main
        print('label: ' + x.name + ', files: ' + str(load_label(x)))
      File ""file.py"", line 28, in load_label
        array = preprocessing.image.img_to_array(image)
      File ""C:\Users\...\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\contrib\keras\python\keras\preprocessing\image.py"", line 331, in img_to_array
        x = np.asarray(img, dtype=K.floatx())
      File ""C:\Users\...\AppData\Local\Programs\Python\Python35\lib\site-packages\numpy\core\numeric.py"", line 531, in asarray
        return array(a, dtype, copy=False, order=order)
    TypeError: float() argument must be a string or a number, not 'JpegImageFile'

Image was loaded with `load_img()` from same package. I found that if I try to convert image directly with NumPy and without `dtype` parameter like `np.asarray(image)` then it works.

I am using tensorflow-gpu 1.1.0 on Windows 8.1, cuda8, python3.5, pillow (for PIL)."
10045,Allow Disabling Password and Token Requirement in Jupyter Notebook,"Jupyter Notebook [allows disabling password / token requirements](http://jupyter-notebook.readthedocs.io/en/latest/security.html#alternatives-to-token-authentication) however the way `jupyter_notebook_config.py` is coded does not allow for this.
```
c.NotebookApp.password = passwd(os.environ['PASSWORD'])
```
should be be written as:
```
password = os.environ['PASSWORD']
if password:
   c.NotebookApp.password = passwd(password)
else:
   c.NotebookApp.password = ''
```

since `passwd('') == 'sha1:7d18ca389fad:8b3a353d37eb3caf006aa47a5cd6559511979278'`"
10044,Upgrading from tf 0.11 to tf 1.1 results in much slower GPU inference for same architecture.,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
0.11 and 1.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
8.0, 5.1.5
- **GPU model and memory**:
Titan X Pascal
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Last December, I created a finetuned a segmentation using the inception v1 using tensorflow 0.11.  I used the freeze_model.py script to freeze the model (Freeze A).  I have recently upgraded to tensorflow 1.1 and retrained a model using the (mostly) the same architecture.  I used the model_freeze.py to freeze the updated network (Freeze B).  I also created an optimized version of this model using the optimize for inference script (Freeze C)

I have run these frozen networks in the tensorflow 1.1 environment.  The Freeze A runs in 20ms, where as the Freeze B runs in 80ms and Freeze C runs in 60ms.

I have used the Timeline trace command to profile a single inference pass of a single image through the network. I have attached the profiling result files.  They have an extension of txt, because github doesnt accept json, but they should be loadable into chrome://tracing

[Freeze A.txt](https://github.com/tensorflow/tensorflow/files/1015360/Freeze.A.txt)
[Freeze B.txt](https://github.com/tensorflow/tensorflow/files/1015357/Freeze.B.txt)
[Freeze C.txt](https://github.com/tensorflow/tensorflow/files/1015359/Freeze.C.txt)

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
10043,Dynamic_attention_wrapper using rnn output or state to caculate next attention?,"In tensorflow 1.1,  https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/seq2seq/python/ops/dynamic_attention_wrapper.py#L535, dynamic_attention_wrapper use RNN output to calculate next attention 
but in previous seq2seq api, https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py#L692, attention_decoder use RNN state to calculate next attention. 
In paper, https://arxiv.org/pdf/1409.0473.pdf. page 3, under equation 6, it use RNN state. I hope some could tell me whether it will affect model in practice.
There is also a stackoverflow post without satisfied answer http://stackoverflow.com/questions/43248613/attention-decoder-implementation-in-tensorflow-r1-0
"
10042,Warning for \get_started TensorFlow Core tutorial - A custom model,"### System information
- **Have I used stock example script provided in TensorFlow from https://www.tensorflow.org/get_started/get_started

- **OS Platform and Distribution : Ubuntu 16.04.2 LTS (GNU/Linux 4.4.0-78-generic x86_64)
- **TensorFlow installed from binary
- **TensorFlow version: ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')
- **CUDA/cuDNN version**: Not using
- **GPU model and memory**: Not using
- **Exact command to reproduce**:

### Describe the problem
Kicks out an WARNING, ""WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32."" when I am running one of the the getting started scripts.

Script from https://www.tensorflow.org/get_started/get_started

```import numpy as np
import tensorflow as tf
# Declare list of features, we only have one real-valued feature
def model(features, labels, mode):
  # Build a linear model and predict values
  W = tf.get_variable(""W"", [1], dtype=tf.float64)
  b = tf.get_variable(""b"", [1], dtype=tf.float64)
  y = W*features['x'] + b
  # Loss sub-graph
  loss = tf.reduce_sum(tf.square(y - labels))
  # Training sub-graph
  global_step = tf.train.get_global_step()
  optimizer = tf.train.GradientDescentOptimizer(0.01)
  train = tf.group(optimizer.minimize(loss),
                   tf.assign_add(global_step, 1))
  # ModelFnOps connects subgraphs we built to the
  # appropriate functionality.
  return tf.contrib.learn.ModelFnOps(
      mode=mode, predictions=y,
      loss=loss,
      train_op=train)

estimator = tf.contrib.learn.Estimator(model_fn=model)
# define our data set
x = np.array([1., 2., 3., 4.])
y = np.array([0., -1., -2., -3.])
input_fn = tf.contrib.learn.io.numpy_input_fn({""x"": x}, y, 4, num_epochs=1000)

# train
estimator.fit(input_fn=input_fn, steps=1000)
# evaluate our model
print(estimator.evaluate(input_fn=input_fn, steps=10))
```

According to page it should do this:

```
When run, it produces

{'loss': 5.9819476e-11, 'global_step': 1000}

```

But what I get is an error and an output that never appears the same...
`WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp5JlIIa
2017-05-19 13:24:03.968664: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-19 13:24:03.968718: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-19 13:24:03.968731: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
{'loss': 2.003922e-11, 'global_step': 1000}
`

Next run:
`
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
{'loss': 4.1125374e-11, 'global_step': 1000}
`
Next run:
`
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
{'loss': 3.8334693e-11, 'global_step': 1000}
`
I am just learning machine learning and would like to continue on.  Not sure if this is a bug or a documentation error or what."
10040,Virtual memory Leak when using gpu,"Hello,

I have little problem with virtual memory in tensorflow when running on GPU:

When I run any network/graph, for example squeezenet:

```
args = ...
config = tf.ConfigProto()
config.gpu_options.allow_growth=True

with tf.Session(config=config) as session:
  model = SqueezeNet(args, session)
  raw_input()
```

I get proper allocation when I look on nvidia-smi:

```
+-----------------------------------------------------------------------------+
|  NVIDIA-SMI 367.48                 Driver Version: 367.48
|  ....
|   0  Quadro K620         Off  | 0000:11:00.0     Off |                  N/A |
| 34%   35C    P8     1W /  30W |     65MiB /  1999MiB |      0%      Default |
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|    0     12128    C   python                                          63MiB |
+-----------------------------------------------------------------------------+
```

However the top utility on the process looks very dangerous:
```
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
12128 xuser 20   0 64.696g 461928 162552 S   2.3  4.9   0:11.24 python

```

64 gigs in virtual memory is probably not normal, however when i run it on the CPU (just switching CUDA_VISIBLE_DEVICES to some big number):
```
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
12193 xuser 20   0 4468872 236988  74008 S   0.0  0.5   0:07.66 python

```
Then everything looks fine.

Do you know where can be the problem?

I am using tensorflow-gpu 1.1.0 on linux ubuntu16, cuda8, python2.7"
10036,Document functional differences between tf.stack and tf.parallel_stack,"What if `tf.stack` had the API:

```
tf.stack(
    values,
    axis=0,
    name='stack',
    algorithm='sequential'
)
```

Options for algorithm could be `sequential`, `parallel` to start. This could apply broadly to many ops and ops like `parallel_stack` could be deprecated. 

Any ideas for a better parameter name than `algorithm`?"
10033,ImportError: DLL load failed: The specified module could not be found.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: unknown, command fails (this is the problem)
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8.0/5.1
- **GPU model and memory**: GeForce GTX 970, 4 GB
- **Exact command to reproduce**: import tensorflow as tf

### Problem
This is something I've seen in a [previous issue report](https://github.com/tensorflow/tensorflow/issues/5949) added as a comment, but no one responded to it (except to say that you should be using cuDNN 5.1 instead of 6.0, which I am doing). I cannot even import tensorflow because it leads to the string of errors below. Someone in the linked issue suggested that this was caused by a missing MSVCP140.DLL, but this file is present in both my System32 and SysWOW64 folders (I have seen both mentioned). I have also installed the suggested Visual C++ Redistributable Update 3 with no luck.

### Log

Traceback (most recent call last):
  File ""C:\Users\Elijah\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Elijah\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Elijah\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Elijah\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Elijah\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Elijah\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Elijah\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Elijah\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 51, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Elijah\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Elijah\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Elijah\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Elijah\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Elijah\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Elijah\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Elijah\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'"
10029,about tf.train.ExponentialMovingAverage,"Hi,I met a little problem when use this API.
""/biased does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?""
And I saw someone written same problem in this issue https://github.com/tensorflow/tensorflow/issues/5827
but in comments of the issue,I did not find a efficient method to solve this problem.
By the way,a code ,such as 
```
def f(v):
    ema = tf.train.ExponentialMovingAverage(0.9)
    vema = ema.apply([v])
    return vema

with tf.variable_scope('s'):
    v1 = tf.get_variable('W', shape=[])
    v1 = v1 + 1
    f(v1)
with tf.variable_scope('s', reuse=True):
    v2 = tf.get_variable('W', shape=[])
    v2 = v2 + 2
    `f(v2)```
```
can normally run with tensorflow 0.11 but can't run with tensorflow 0.12 or higher version.
I hope there will be a valid method to solve this problem."
10026,Failed to load the native TensorFlow runtime : error while importing tensorflow,"**I built TensorFlow from source on my Ubuntu 17.04 32bit**

I got this message while importing tensorflow

palash@ash:~$ python
Python 3.6.0 |Anaconda 4.3.1 (32-bit)| (default, Dec 23 2016, 12:22:10) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
File ""/home/palash/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/palash/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/home/palash/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
  File ""/home/palash/anaconda3/lib/python3.6/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/palash/anaconda3/lib/python3.6/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /home/palash/anaconda3/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /home/palash/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow.so)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/palash/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/palash/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/palash/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/palash/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/home/palash/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
  File ""/home/palash/anaconda3/lib/python3.6/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/palash/anaconda3/lib/python3.6/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /home/palash/anaconda3/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /home/palash/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow.so)


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>> 

my bazel info:
palash@ash:~$ bazel version
Build label: 0.4.5- (@non-git)
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri May 19 01:31:26 2017 (1495157486)
Build timestamp: 1495157486
Build timestamp as int: 1495157486

my TensorFlow version : 1.0.1"
10024,Text summary: Error encountered when serializing __tensorboard_plugin_asset__tensorboard_text.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux CentOS 7
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: `v1.1.0-rc2-1185-ge4f5123' 1.2.0-rc0`
- **Bazel version (if compiling from source)**: `0.4.5`
- **CUDA/cuDNN version**: `8.0` / `5.1.10`
- **GPU model and memory**: GeForce GTX Titan X 12GB
- **Exact command to reproduce**:

```
import tensorflow as tf

logdir = ""/output_dir""
text = ""A text summary""
text_tensor = tf.constant(text)
text_summary = tf.summary.text(""Summary"", text_tensor)
merged = tf.summary.merge([text_summary])
writer = tf.summary.FileWriter(logdir, tf.get_default_graph())
s = tf.Session()
s.run(tf.global_variables_initializer())
summary = s.run(merged)
writer.add_summary(summary)
```

### Describe the problem
Tensorflow  gives the following error when I try to create a summary writer when the graph has a text summary:
```
WARNING:tensorflow:Error encountered when serializing __tensorboard_plugin_asset__tensorboard_text.
Type is unsupported, or the types of the items don't match field type in CollectionDef.
'TextSummaryPluginAsset' object has no attribute 'name'
```
On my larger project I was able to output some text to the summary page on tensorboard, but the Markdown I put in there wasn't being parsed at all. I don't know if these problems are related but I feel like they probably are.

### Source code / logs
Code in first section is an MWE
"
10023,Error while executing TF program with TF_CPP_MIN_VLOG_LEVEL=1,"I got this error when I exported TF_CPP_MIN_VLOG_LEVEL=1 and TF_CPP_MIN_LOG_LEVEL=1 to see the LOG files . I got this error.
> SetArgvFromEnv
> [libprotobuf ERROR external/protobuf/src/google/protobuf/io/zero_copy_stream_impl_lite.cc:173] Cannot allocate buffer larger than kint32max for StringOutputStream.
> 2017-05-19 10:13:31.716688: F tensorflow/compiler/tf2xla/dump_graph.cc:67] Non-OK-status: WriteTextProto(Env::Default(), path, graph_def) status: Failed precondition: Unable to convert proto to text.
> Aborted
"
10021,Class weighting in tf.losses.softmax_cross_entropy,"Feature request

In `tf.losses.softmax_cross_entropy` there's an optional field `weights`. I assumed this field was used for assigning a different weight to each class, but it actually is used to assign a weight to each sample in the batch. In my use case i have a `batch_size` of `128` and `30` classes, so I was passing a `[1, 30]` tensor and got this error:

```
InvalidArgumentError (see above for traceback): Incompatible shapes: [128] vs. [30]
	 [[Node: optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/Mul_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/Mul_grad/Shape, optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/Mul_grad/Shape_1)]]
```

I looked at the implementation, confirmed that the function expects `batch_size` as the dimension of thensor and realized that my expected behavior cannot be achieved easily as `tf.nn.softmax_cross_entropy_with_logits` doesn't have a weight parameter.

My current workaround solution is to re-implement this function calculating the loss for each class and then multiplying for the weight vector that I pass, but that's inefficient compared to the optimized implementation of `tf.nn.softmax_cross_entropy_with_logits`.

So my request is:
- provide an optimized `tf.nn.softmax_cross_entropy_with_logits` that also accepts `weights` for each class as a parameter
- use it inside `tf.losses.softmax_cross_entropy` so that one can pass weights as a scalar, a `[batch_size, 1]` tensor, a `[1, num_classes]` tensor or a `[batch_size, num_classes]` tensor (the same dimension of  `onehot_labels`)"
10020,undefined symbol: cuDevicePrimaryCtxRetain,"Ubuntu 14.04    / NVIDIA GTX 1060

I try nvidia 710 with Ubuntu 14.04,s ok!
I try nvidia 1060 with Ubuntu 14.04, Fault! -- undefined symbol: cuDevicePrimaryCtxRetain！

NVIDIA-Linux-x86_64-375.66.run
cuda_8.0.61_375.26_linux.run
CuDNN5.1
Anaconda3-4.3.1-Linux-x86_64.sh （Python3.6）
TensorflowGPU1.1.0

I have install this profile in one computer( not the same hardware) success.
But, today, another computer.
I get that:

import tensorflow as tf
Traceback (most recent call last):
File ""/home/student/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in 
from tensorflow.python.pywrap_tensorflow_internal import *
File ""/home/student/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in 
_pywrap_tensorflow_internal = swig_import_helper()
File ""/home/student/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
File ""/home/student/miniconda3/lib/python3.6/imp.py"", line 242, in load_module
return load_dynamic(name, filename, file)
File ""/home/student/miniconda3/lib/python3.6/imp.py"", line 342, in load_dynamic
return _load(spec)
ImportError: /home/student/miniconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: cuDevicePrimaryCtxRetain
"
10019,Java Api String tensors support,"from [Tensor.java](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/java/src/main/java/org/tensorflow/Tensor.java#L90):
**non-scalar DataType.STRING tensors are not supported yet**

Is there a plan for adding them for the Java interface?"
10017,"tensorboard fails with `Fatal error in launcher: Unable to create process using '""'`","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A, tensorboard command line issue
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64
- **TensorFlow installed from (source or binary)**: binary with Python 3.5, installed via pip
- **TensorFlow version (use command below)**: b'unknown' 1.1.0 (gpu version of TF)
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 8, cuDNN 5.1.10
- **GPU model and memory**: GTX 1080ti 11GB
- **Exact command to reproduce**: run `tensorboard`, get `Fatal error in launcher: Unable to create process using '""'`

### Describe the problem

Running `tensorboard` fails with `Fatal error in launcher: Unable to create process using '""'` no matter the command line arguments I pass."
10015,unable to install for new versin tensorflow 1.2,"Hi,
 already installed  tensorflow 1.1 in python3.5 and OS  win64. but now for upgrade to ""tensorflow-1.2.0rc0-cp35-cp35m-win_amd64.whl"" the python TAKE error "" unable to install tensorflow 1.2"""
10014,feature request: make placeholder_with_default work in batch prediction,"cc @qimingj

I ran into an issue with building a model that has missing values. During training, I can fill in missing values via the options to tf.decode_csv or tf.parse_example. I want to make my exported saved model also handle missing values. I know how to do this if the model reads from tf.example string or csv string, but this is impossible if the model takes each input tensor individually through placeholders. 

I looked at tf.placeholder_with_default(), but this function has useless behavior in batch prediction. As an example:

```
import tensorflow as tf

with tf.Session() as sess:
	default = tf.placeholder_with_default([1], shape=[None])
	regular = tf.placeholder(tf.int32, shape=[None])

	id_default = tf.identity(default)
	id_regular = tf.identity(regular)

	print('1) normal 1 example', sess.run([id_default, id_regular], feed_dict={default: [2], regular: [5]}))
	print('2) normal batch example', sess.run([id_default, id_regular], feed_dict={default: [2, 2], regular: [5, 5]}))
	print('3) missing 1 column in batch', sess.run([id_default, id_regular], feed_dict={regular: [5, 5]}))
	print('4) missing some data', sess.run([id_default, id_regular], feed_dict={default: [2, None], regular: [5, 5]}))	
```
The output is
```
('1) normal 1 example', [array([2], dtype=int32), array([5], dtype=int32)])
('2) normal batch example', [array([2, 2], dtype=int32), array([5, 5], dtype=int32)])
('3) missing 1 column in batch', [array([1], dtype=int32), array([5, 5], dtype=int32)])
Traceback (most recent call last):
  File ""placeholders.py"", line 13, in <module>
    print('4) missing some data', sess.run([id_default, id_regular], feed_dict={default: [2, None], regular: [5, 5]}))
  File ""/usr/local/google/home/brandondutra/miniconda2/envs/cml/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/usr/local/google/home/brandondutra/miniconda2/envs/cml/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 938, in _run
    np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)
  File ""/usr/local/google/home/brandondutra/miniconda2/envs/cml/lib/python2.7/site-packages/numpy/core/numeric.py"", line 531, in asarray
    return array(a, dtype, copy=False, order=order)
TypeError: long() argument must be a string or a number, not 'NoneType'
```

Note how line 3 is incorrect: I expected a result of [1,1], [5,5]

Also, placeholder_with_default does not allow some values to be missing in a batch as line 4 shows. 

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04.1
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.0.0-65-g4763edf-dirty', '1.0.1')
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: n/a
"
10013,GPU resources not released when session is closed,"This is a possible duplicate of #1727. Posting here as there is no way to re-open or comment on the previous bug. 
The comments suggests that the updating the driver would most probably fix the issue but its not the case. 

**Environment info**
Distributor ID:	Ubuntu
Description:	Ubuntu 16.04.2 LTS
Release:	16.04
Codename:	xenial

>`python3 -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
   `v1.1.0-rc0-61-g1ec6ed5 1.1.0`
 
>`nvidia-smi`
`NVIDIA-SMI 375.51                 Driver Version: 375.51`

**Before starting the service**
0     27825    C   /usr/bin/python3                                35MiB 

**After deleting the session**
Note that I am creating the session and loading a checkpoint, running the session and then closing it explicitly with `session.close()`. I am also resetting the graph by calling `tf.reset_default_graph()` after closing the session. 
0     27825    C   /usr/bin/python3                              1435MiB


    device_t='/gpu:0'
    graph = tf.Graph()
    soft_config = tf.ConfigProto(allow_soft_placement=True)
    soft_config.gpu_options.allow_growth = True . #tried both True and False and doesn't seem to help
    with graph.as_default(), graph.device(device_t), tf.Session(config=soft_config) as sess:
        batch_shape = (batch_size,) + img.shape
        img_placeholder = tf.placeholder(tf.float32, shape=batch_shape,
                                         name=""img_placeholder"")
        preds = transform.net(img_placeholder)
        saver = tf.train.Saver()
        saver.restore(sess, checkpoint_dir)

        content = np.zeros(batch_shape, dtype=np.float32)
        content[0] = img

        _preds = sess.run(preds, feed_dict={img_placeholder: content})
        result = crop_img(_preds[0].astype(data_in.dtype))
        sess.close()   #Explicitly closing the session and deleting does not help as well. 
        del sess
    tf.reset_default_graph()
    return result

Here are some of the relevant logs. 
```
2017-05-18 18:49:16.556719: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-18 18:49:16.556757: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-18 18:49:16.556775: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-05-18 18:49:16.584028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-05-18 18:49:16.584295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:03.0
Total memory: 3.94GiB
Free memory: 3.87GiB
2017-05-18 18:49:16.584326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-05-18 18:49:16.584344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-05-18 18:49:16.584368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)
2017-05-18 18:49:16.585190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)


```
"
10012,Session vs DeprecatedSession,"Hi,

I'm wondering as to the main differences between the new sessions in the C API and the deprecated sessions. I noticed that [the Python API is using the deprecated sessions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/session.py#L562) and since I am using the new sessions, I want to make sure I understand the differences correctly. I list here the differences I believe exist and I hope that someone could verify and/or correct my thoughts:
- `TF_ExtendGraph` is not necessary with the new sessions. The session will always be aware of changes in the underlying graph.
- `TF_Reset` still makes sense with the new sessions and has the same functionality. If that is indeed the case, maybe it should be moved outside the deprecated session API region of the C API header file.
- Most of the `TF_SessionOptions` still apply, but some of them do not make sense anymore (e.g., the `ConfigProto.graph_options.infer_shapes` option). Could someone please provide a list of which options have been deprecated?
- I do not really understand the whole dealing with session handles. I believe it's not necessary to deal with them with the new session API as the partial run handle is not provided as a feed/fetch value, but as a separate argument. Is that true? If so, should I completely ignore them when dealing with the sessions? That would mean that I never need to use the ops defined in `session_ops`. Is that correct?

Thank you,
Anthony"
10010,[Feature]: Bazel - Building TensorFlow with Polly-enabled-LLVM.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: ('v1.0.0-1783-g4c3bb1a', '1.0.0')
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: ```bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package```

#### TensorFlow build: 
Built from source. 
```
$ git rev-parse HEAD
a33022c1470ce1334766b0cad38d9e91c17a2e5d
```
### Description:
This is a feature request. 
As part of my Google Summer of Code project, I am trying to build TensorFlow with Polly-enabled LLVM. To do this, I have been trying to port Opt and Polly to bazel. I tried placing this dummy rule at the end of the [llvm.BUILD](https://github.com/tensorflow/tensorflow/blob/master/third_party/llvm/llvm.BUILD) file. 

```
cc_library(
    name = ""opt"",
    srcs = glob([
        ""tools/opt/*.c"",
        ""tools/opt/*.cpp"",
        ""tools/opt/*.h"",
    ]),     
    hdrs = glob([
        ""tools/opt/*.h"",
    ]),     
    copts = [""-Iexternal/llvm/tools/opt"",""-Iexternal/llvm/include""],
    deps = [
        "":intrinsics_gen"",
    ],
)
```

But this does not seem to work. All the other targets defined in the file are building:

```
tensorflow/bazel-bin/external/llvm/_objs# ls
aarch64_asm_printer   arm_asm_printer   bit_reader            execution_engine  mc                 objc_arc              powerpc_info   transform_utils   x86_utils
aarch64_code_gen      arm_code_gen      bit_writer            global_i_sel      mc_disassembler    object                profile_data   vectorize
aarch64_desc          arm_desc          code_gen              inst_combine      mc_parser          orc_jit               runtime_dyld   x86_asm_printer
aarch64_disassembler  arm_disassembler  core                  instrumentation   nvptx_asm_printer  powerpc_asm_printer   scalar         x86_code_gen
aarch64_info          arm_info          debug_info_code_view  ipo               nvptx_code_gen     powerpc_code_gen      selection_dag  x86_desc
aarch64_utils         asm_parser        debug_info_msf        ir_reader         nvptx_desc         powerpc_desc          support        x86_disassembler
analysis              asm_printer       demangle              linker            nvptx_info         powerpc_disassembler  target         x86_info

```

I know that I have to define opt as a dependency in my build configuration. I can't see why this doesn't do the same.

Thanks.
"
10009,Problems with `raw_rnn` with dynamic dimensions,"I am trying to implement the [pointing softmax](https://arxiv.org/abs/1603.08148) in TF 1.1 and I am using `raw_rnn` but I get an error. Since is hard to explain the whole thing, here is how to reproduce it:

```bash
$ git clone https://github.com/petrux/LiTeFlow.git
$ cd LiTeFlow
$ git checkout broken
$ ./bin/py3venv.sh
$ source .py3venv/bin/activate
$ python3 -m unittest liteflow.tests.test_layers._TestSmoke
```
the error that I have is:

```bash
petrux@orion:~/Projects/LiTeFlow$ python3 -m unittest liteflow.tests.test_layers._TestSmoke
./usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:212: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead
  arg_spec = inspect.getargspec(func)
/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/keras/python/keras/backend.py:3593: ResourceWarning: unclosed file <_io.TextIOWrapper name='/home/petrux/.keras/keras.json' mode='r' encoding='UTF-8'>
  _config = json.load(open(_config_path))
/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/labeled_tensor/python/ops/_typecheck.py:233: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead
  spec = inspect.getargspec(f)
elements_finished: Tensor(""PointingDecoder_1_1/rnn/GreaterEqual:0"", shape=(?,), dtype=bool)
next_cell_input: Tensor(""PointingDecoder_1_1/rnn/concat:0"", shape=(?, ?), dtype=float32)
next_cell_state: Tensor(""GRUCellZeroState/zeros:0"", shape=(?, 5), dtype=float32)
emit_output: Tensor(""zeros:0"", shape=(?, ?), dtype=float32)
next_loop_state: (<tf.Tensor 'PointingDecoder_1_1/rnn/LocationSoftmax_1/softmax/truediv:0' shape=(?, ?) dtype=float32>, <tf.Tensor 'PointingDecoder_1_1/rnn/LocationSoftmax_1/Sum:0' shape=(?, 4) dtype=float32>)

E
======================================================================
ERROR: test_smoke (liteflow.tests.test_layers._TestSmoke)
Build a pointer decoder and test that it works.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py"", line 1356, in zeros
    shape = tensor_shape.as_shape(shape)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py"", line 800, in as_shape
    return TensorShape(shape)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py"", line 436, in __init__
    self._dims = [as_dimension(d) for d in dims_iter]
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py"", line 436, in <listcomp>
    self._dims = [as_dimension(d) for d in dims_iter]
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py"", line 378, in as_dimension
    return Dimension(value)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py"", line 33, in __init__
    self._value = int(value)
TypeError: int() argument must be a string, a bytes-like object or a number, not 'Tensor'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/petrux/Projects/LiTeFlow/liteflow/tests/test_layers.py"", line 518, in test_smoke
    output = decoder()
  File ""/home/petrux/Projects/LiTeFlow/liteflow/layers.py"", line 616, in __call__
    return super(PointingDecoder, self).__call__()
  File ""/home/petrux/Projects/LiTeFlow/liteflow/layers.py"", line 147, in __call__
    result = self._call_helper(*args, **kwargs)
  File ""/home/petrux/Projects/LiTeFlow/liteflow/layers.py"", line 610, in _call_helper
    outputs_ta, _, _ = tf.nn.raw_rnn(self._decoder_cell, self._loop_fn)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py"", line 965, in raw_rnn
    for size_i, dtype_i in zip(flat_emit_size, flat_emit_dtypes)]
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py"", line 965, in <listcomp>
    for size_i, dtype_i in zip(flat_emit_size, flat_emit_dtypes)]
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py"", line 1359, in zeros
    shape = ops.convert_to_tensor(shape, dtype=dtypes.int32, name=""shape"")
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 639, in convert_to_tensor
    as_ref=False)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 704, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py"", line 905, in _autopacking_conversion_function
    return _autopacking_helper(v, inferred_dtype, name or ""packed"")
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py"", line 867, in _autopacking_helper
    constant_op.constant(elem, dtype=dtype, name=str(i)))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py"", line 102, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py"", line 360, in make_tensor_proto
    raise ValueError(""None values not supported."")
ValueError: None values not supported.

----------------------------------------------------------------------
Ran 2 tests in 0.430s

FAILED (errors=1)
```  
  
**EDIT** a working implementation is [here](https://github.com/petrux/LiTeFlow/tree/pointing-decoder/liteflow)."
10006,Tensorflow cifar 10 example memory leak,"### Describe the problem
I am new to Tensorflow. I tried to run the cifar10 examples from here: https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10

I didn't make any changes to the code, I just tried to run it on multiple GPUs. I am trying to use 6 GPUs and I am allocating 10 GB of RAM for my job but after a few minutes my jobs getting failed due to memory limit. Allocating more memory does not help, it just delays the error. I tried for up to 40GB of memory.

Here is my system info:

> == cat /etc/issue =============================================== Linux mmmdgx01 4.4.0-45-generic #66~14.04.1-Ubuntu SMP Wed Oct 19 15:05:38 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux DGX_OTA_VERSION=2.0.5 VERSION=""14.04.5 LTS, Trusty Tahr"" VERSION_ID=""14.04""
> 
> == are we in docker ============================================= No
> 
> == compiler ===================================================== c++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4 Copyright (C) 2013 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
> 
> == uname -a ===================================================== Linux mmmdgx01 4.4.0-45-generic #66~14.04.1-Ubuntu SMP Wed Oct 19 15:05:38 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
> 
> == check pips =================================================== numpy (1.11.1) protobuf (3.2.0) tensorflow (1.1.0rc1)
> 
> == check for virtualenv ========================================= False
> 
> == tensorflow import ============================================ tf.VERSION = 1.1.0-rc1 tf.GIT_VERSION = v1.1.0-rc1-272-gf77f19b tf.COMPILER_VERSION = v1.1.0-rc1-272-gf77f19b Sanity check: array(1, dtype=int32)
> 
> == env ========================================================== LD_LIBRARY_PATH /opt/sw/cuda/8.0/lib64/:/project/DGX/cuda/lib64/:/opt/sw/cuda/8.0/extras/CUPTI/lib64/:/project/DGX/lib DYLD_LIBRARY_PATH /project/DGX/torch/install/lib:/project/torch7new/install/lib:
> 
> == nvidia-smi =================================================== Fri May 12 15:46:50 2017 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 375.20 Driver Version: 375.20
> | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P100-SXM2... On | 0000:06:00.0 Off |
> 0 | | N/A 34C P0 42W / 300W | 0MiB / 16308MiB | 0%
> Default | +-------------------------------+----------------------+----------------------+ | 1 Tesla P100-SXM2... On | 0000:07:00.0 Off |
> 0 | | N/A 32C P0 32W / 300W | 0MiB / 16308MiB | 0%
> Default | +-------------------------------+----------------------+----------------------+ | 2 Tesla P100-SXM2... On | 0000:0A:00.0 Off |
> 0 | | N/A 34C P0 33W / 300W | 0MiB / 16308MiB | 0%
> Default | +-------------------------------+----------------------+----------------------+ | 3 Tesla P100-SXM2... On | 0000:0B:00.0 Off |
> 0 | | N/A 33C P0 32W / 300W | 0MiB / 16308MiB | 0%
> Default | +-------------------------------+----------------------+----------------------+ | 4 Tesla P100-SXM2... On | 0000:85:00.0 Off |
> 0 | | N/A 33C P0 30W / 300W | 0MiB / 16308MiB | 0%
> Default | +-------------------------------+----------------------+----------------------+ | 5 Tesla P100-SXM2... On | 0000:86:00.0 Off |
> 0 | | N/A 33C P0 33W / 300W | 0MiB / 16308MiB | 0%
> Default | +-------------------------------+----------------------+----------------------+ | 6 Tesla P100-SXM2... On | 0000:89:00.0 Off |
> 0 | | N/A 31C P0 32W / 300W | 0MiB / 16308MiB | 0%
> Default | +-------------------------------+----------------------+----------------------+ | 7 Tesla P100-SXM2... On | 0000:8A:00.0 Off |
> 0 | | N/A 35C P0 32W / 300W | 0MiB / 16308MiB | 0%
> Default | +-------------------------------+----------------------+----------------------+
> 
> +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name
> Usage | |=============================================================================| | No running processes found
> | +-----------------------------------------------------------------------------+
> 
> == cuda libs ===================================================
> 

### TensorFlow version

('v1.1.0-rc1-272-gf77f19b', '1.1.0-rc1')

### Source code / logs
Here is my job submission script:

> #! /bin/bash
> #SBATCH --account=AI
> #SBATCH --time=167:00:00
> #SBATCH --nodes=1
> #SBATCH --ntasks-per-node=20
> #SBATCH -J TFImgNet
> #SBATCH -e tf.err
> #SBATCH -o tf.log
> #SBATCH --mem=10960
> #SBATCH --gres=gpu:6
> cpath=$(pwd)
> cd ~
> source .bashrc
> cd $cpath
> which python
> python cifar10_multi_gpu_train.py --num_gpus 6

### Here is the error:

> 2017-05-12 15:14:07.162709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 2 3 4 5
> 2017-05-12 15:14:07.162718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y Y Y Y Y N
> 2017-05-12 15:14:07.162721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   Y Y Y Y N Y
> 2017-05-12 15:14:07.162724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 2:   Y Y Y Y N N
> 2017-05-12 15:14:07.162727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 3:   Y Y Y Y N N
> 2017-05-12 15:14:07.162729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 4:   Y N N N Y Y
> 2017-05-12 15:14:07.162732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 5:   N Y N N Y Y
> 2017-05-12 15:14:07.162743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:06:00.0)
> 2017-05-12 15:14:07.162747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:07:00.0)
> 2017-05-12 15:14:07.162751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0000:0a:00.0)
> 2017-05-12 15:14:07.162754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla P100-SXM2-16GB, pci bus id: 0000:0b:00.0)
> 2017-05-12 15:14:07.162756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:4) -> (device: 4, name: Tesla P100-SXM2-16GB, pci bus id: 0000:85:00.0)
> 2017-05-12 15:14:07.162759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:5) -> (device: 5, name: Tesla P100-SXM2-16GB, pci bus id: 0000:86:00.0)
> slurmstepd: error: Job 1313520 exceeded memory limit (11240536 > 11223040), being killed
> slurmstepd: error: Exceeded job memory limit
> slurmstepd: error: *** JOB 1313520 ON mmmdgx01 CANCELLED AT 2017-05-12T15:28:58 ***
"
10005,Py_func call slows down scipy,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04.4 and MacOS 10.12.4 are affected
- **TensorFlow installed from (source or binary)**: tried both, reproduces on both
- **TensorFlow version (use command below)**: 1.0.1 and 1.1.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 
- **GPU model and memory**: both on Tesla K80 and on CPU version
- **Exact command to reproduce**: https://nbviewer.jupyter.org/urls/dl.dropbox.com/s/z19wv70ogo15xg0/eigh%20speed%20bug.ipynb

Output of the enviromental script: https://pastebin.com/qdi8tJRV

### Describe the problem
Calling np.lianlg.svd via tf.py_fun forces scipy.linalg.eigh to become slower for some reason (4x slower on a server and 1.5x slower on my laptop). Here is the line that causes problems:
```
ret = tf.py_func(np.linalg.svd, [np.random.randn(2, 300)], [tf.float64, tf.float64, tf.float64])
%timeit sess.run(ret)
```
Before this line scipy.linalg.eigh used CPU for 2400% and worked in 13ms, after it it uses CPU for 600% and works in 43ms.

### Source code / logs
https://nbviewer.jupyter.org/urls/dl.dropbox.com/s/z19wv70ogo15xg0/eigh%20speed%20bug.ipynb
"
10004,"In reader.py, line 17 throws error on python 3.5. The decode phrase should be removed, and then it works","In reader.py, line 17 throws error on python 3.5. The decode phrase should be removed, and then it works"
10003,[feature] sparse_multiply,"Hi guys,

We'd like to ask if it's possible to add sparse_multiply and sparse_multiply_sparse as a sparse equivalent to existing tf.multiply but for SparseTensor arguments and both Tensor and SparseTensor result.

Thank you,

Joanna"
10002,[feature] sparse_reduce_max and sparse_reduce_max_sparse ,"Hi guys,

Currently we work on sparse tensors, and we are missing some sparse operations. Is there any way you could add sparse_reduce_max and sparse_reduce_max_sparse operations as functions equivalent to existing tf.sparse_reduce_sum and tf.sparse_reduce_sum_sparse but with max operation as reducing function?

Thank you,

Joanna"
10001,Segmentation fault on quit() tensorflow-ppc64,"### System information
- **Fedora 24 4.7.4-200.fc24.ppc64 #1 SMP Thu Sep 22 17:40:37 UTC 2016 ppc64 ppc64 ppc64 GNU/Linux**:
- **TensorFlow installed from source**
- **TensorFlow version:  1.1.0-rc1**
- **Bazel version: 0.4.5-2017-05-12**
- **Exact command to reproduce**:

1. `python`
2. `import tensorflow as tf`
3. `hello = tf.constant('Hello, TensorFlow!')`
4. `sess = tf.Session()`
5. `print(sess.run(hello))`
6. `quit()`

### Describe the problem
Even with the simple above example, when I import tensorflow in python I get a segmentation fault at the moment of terminating python; all commands before `quit()` are executed correctly

### Source code / logs
`Thread 1 ""python"" received signal SIGSEGV, Segmentation fault.
0x00003fffae6b8d78 in .std::_Function_handler<void (tensorflow::OpKernel*), tensorflow::DirectSession::GetOrCreateExecutors(tensorflow::thread::ThreadPool*, tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*)::{lambda(tensorflow::OpKernel*)#2}>::_M_invoke(std::_Any_data const&, tensorflow::OpKernel*&&) ()
   from /home/shady/dleoni/python_build/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so`

"
9999,Directories with spaces in their names don't compile due to missing quotes in shell scripts,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX Sierra, Macbook Pro 2015
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: v1.0.0-65-g4763edf-dirty 1.0.1
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

### Describe the problem

I was building tensorflow for iOS and as a part of this, tried running the `build_all_ios.sh` but it threw some errors. The error is mainly because there are parts in shell script that either attempt to create or move to a directory that is incorrectly referenced. In my case, it was because of spaces in directory names. This is true even for the 'compile_ios_protobuf.sh' script (and harder to detect because of the very verbose log). 

I was able to get-around this by modifying both of these shell scripts to include double quotes around 
lines that referenced directory paths. 

### Source code / logs

Few examples lines here:

Line 28 (build_all_ios.sh): 'cd ${SCRIPT_DIR}/../../../' (notice that this will ignore spaces in the directory path, if they exist

Line 35 (compile_ios_protobuf.sh): 'mkdir -p ${LIBDIR}' (this will similarly create a directory at an unexpected path and will not complete compilation)"
9998,How to build tensorflow for mips64el,"The recommended way to build TensorFlow from source is using the Bazel open-source build system.
Howerver, bazel depends `protobuf,grpc-java,osdetector` etc.  protocbuf can build  successfully for mips64el. But i  build grpc-java  failed. details see [#3012](https://github.com/grpc/grpc-java/issues/3012). i refer [#2022](https://github.com/grpc/grpc-java/issues/2202). but osdetector-gradle-plugin occur error.
Could anybody give me some help for building tensorflow for mips64el arch? "
9997,Find an error in mnist_softmax.py,"


------------------------

### System information
- **OS Platform and Distribution (Mac os)**:
- **TensorFlow installed from (binary)**:
- **TensorFlow version (master)**:




### Describe the problem
In mnist_softmax.py,there is a difference between github an the web of Tensorflow(https://www.tensorflow.org/versions/r0.12/tutorials/mnist/beginners/index.html),and the code on github has error in line 57,58.

### Source code / logs
I replaced `cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))` with `cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))`.And it works.
"
9996,Convolution_transpose layer now gives an error (Tensorflow 1.0.0). ,"I am implementing an architecture with conv and conv_transpose layers and this is what I am giving the convolution transpose layer: 
```
    ('convolution_transpose', dict(num_outputs=96, kernel_size=[41, 11],
                                     stride=[2, 1], padding=""SAME"", scope='dec_block_1'))
```

and this is what I get 


```
/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py in func_with_args(*args, **kwargs)
    175       current_args = current_scope[key_func].copy()
    176       current_args.update(kwargs)
--> 177     return func(*args, **current_args)
    178   _add_op(func)
    179   setattr(func_with_args, '_key_op', _key_op(func))

/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py in convolution2d_transpose(inputs, num_outputs, kernel_size, stride, padding, data_format, activation_fn, normalizer_fn, normalizer_params, weights_initializer, weights_regularizer, biases_initializer, biases_regularizer, reuse, variables_collections, outputs_collections, trainable, scope)
   1123         _scope=sc,
   1124         _reuse=reuse)
-> 1125     outputs = layer.apply(inputs)
   1126 
   1127     # Add variables to collections.

/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py in apply(self, inputs, **kwargs)
    301       Output tensor(s).
    302     """"""
--> 303     return self.__call__(inputs, **kwargs)
    304 
    305 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py in __call__(self, inputs, **kwargs)
    267           input_shapes = [x.get_shape() for x in input_list]
    268           if len(input_shapes) == 1:
--> 269             self.build(input_shapes[0])
    270           else:
    271             self.build(input_shapes)

/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/convolutional.py in build(self, input_shape)
   1048                                   regularizer=self.bias_regularizer,
   1049                                   trainable=True,
-> 1050                                   dtype=self.dtype)
   1051     else:
   1052       self.bias = None

/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)
    986       collections=collections, caching_device=caching_device,
    987       partitioner=partitioner, validate_shape=validate_shape,
--> 988       custom_getter=custom_getter)
    989 get_variable_or_local_docstring = (
    990     """"""%s

/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)
    888           collections=collections, caching_device=caching_device,
    889           partitioner=partitioner, validate_shape=validate_shape,
--> 890           custom_getter=custom_getter)
    891 
    892   def _get_partitioned_variable(self,

/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)
    339           reuse=reuse, trainable=trainable, collections=collections,
    340           caching_device=caching_device, partitioner=partitioner,
--> 341           validate_shape=validate_shape)
    342     else:
    343       return _true_getter(

/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py in variable_getter(getter, name, shape, dtype, initializer, regularizer, trainable, **kwargs)
    256           name, shape, initializer=initializer, regularizer=regularizer,
    257           dtype=dtype, trainable=trainable,
--> 258           variable_getter=functools.partial(getter, **kwargs))
    259 
    260     # Build (if necessary) and call the layer, inside a variable scope.

/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py in _add_variable(self, name, shape, dtype, initializer, regularizer, trainable, variable_getter)
    206                                initializer=initializer,
    207                                dtype=dtype,
--> 208                                trainable=trainable and self.trainable)
    209     # TODO(sguada) fix name = variable.op.name
    210     if variable in existing_variables:

/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py in layer_variable_getter(getter, *args, **kwargs)
   1308       getter = functools.partial(current_custom_getter, getter)
   1309     kwargs['rename'] = rename
-> 1310     return _model_variable_getter(getter, *args, **kwargs)
   1311   return layer_variable_getter
   1312 

/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py in _model_variable_getter(getter, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, rename, **_)
   1297       regularizer=regularizer, collections=collections, trainable=trainable,
   1298       caching_device=caching_device, partitioner=partitioner,
-> 1299       custom_getter=getter)
   1300 
   1301 

/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py in func_with_args(*args, **kwargs)
    175       current_args = current_scope[key_func].copy()
    176       current_args.update(kwargs)
--> 177     return func(*args, **current_args)
    178   _add_op(func)
    179   setattr(func_with_args, '_key_op', _key_op(func))

/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/variables.py in model_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device, partitioner, custom_getter)
    266                  trainable=trainable, collections=collections,
    267                  caching_device=caching_device, device=device,
--> 268                  partitioner=partitioner, custom_getter=custom_getter)
    269   return var
    270 

/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py in func_with_args(*args, **kwargs)
    175       current_args = current_scope[key_func].copy()
    176       current_args.update(kwargs)
--> 177     return func(*args, **current_args)
    178   _add_op(func)
    179   setattr(func_with_args, '_key_op', _key_op(func))

/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/variables.py in variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device, partitioner, custom_getter)
    223                   collections=collections,
    224                   caching_device=caching_device,
--> 225                   partitioner=partitioner)
    226 
    227 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)
    331           initializer=initializer, regularizer=regularizer, reuse=reuse,
    332           trainable=trainable, collections=collections,
--> 333           caching_device=caching_device, validate_shape=validate_shape)
    334 
    335     if custom_getter is not None:

/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)
    682         caching_device=caching_device,
    683         dtype=variable_dtype,
--> 684         validate_shape=validate_shape)
    685     self._vars[name] = v
    686     logging.vlog(1, ""Created variable %s with shape %s and init %s"", v.name,

/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope)
    224           name=name,
    225           dtype=dtype,
--> 226           expected_shape=expected_shape)
    227 
    228   def __str__(self):

/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py in _init_from_args(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape)
    301             with ops.name_scope(""Initializer""),  ops.device(None):
    302               self._initial_value = ops.convert_to_tensor(
--> 303                   initial_value(), name=""initial_value"", dtype=dtype)
    304               shape = (self._initial_value.get_shape()
    305                        if validate_shape else tensor_shape.unknown_shape())

/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py in <lambda>()
    671       else:
    672         init_val = lambda: initializer(
--> 673             shape.as_list(), dtype=dtype, partition_info=partition_info)
    674         variable_dtype = dtype.base_dtype
    675 

TypeError: __init__() got multiple values for argument 'dtype'

```

The same code worked on Tensorflow 0.12. "
9991,go install,"
我在安装的时候出现这个问题我的是mac环境，前面的命令都是成功的
manmans-MBP:~ manman$ go test github.com/tensorflow/tensorflow/tensorflow/go
can't load package: package github.com/tensorflow/tensorflow/tensorflow/go: cannot find package ""github.com/tensorflow/tensorflow/tensorflow/go"" in any of:
	/usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go (from $GOROOT)
	/Users/manman/Documents/go/document/src/github.com/tensorflow/tensorflow/tensorflow/go (from $GOPATH)
manmans-MBP:~ manman$ go test github.com/tensorflow/tensorflow/tensorflow/go
signal: killed
FAIL	github.com/tensorflow/tensorflow/tensorflow/go	0.003s
manmans-MBP:~ manman$ 

"
9988,[feature] Support Lanczos method in tf.image.resize_images,"Add support for a `Lanczos` (a truncated sinc) mode in `tf.image.resize_images`. Currently this mode is not offered.

[Pillow supports `LANCZOS`](http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters) (aka `ANTIALIAS`) as a resampling method and stipulates this method has the [highest quality for down sampling](http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters-comparison-table) ([default for `thumbnail` for example](http://pillow.readthedocs.io/en/3.4.x/reference/Image.html#PIL.Image.Image.thumbnail)).
![image](https://cloud.githubusercontent.com/assets/51059/26181858/91e9471e-3b41-11e7-8653-4fa7277e98a6.png)
"
9984,Missing .pb.h Files While Building from Source on Windows using CMake ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 Enterprise
- **TensorFlow installed from (source or binary)**:
Trying to install from source
- **TensorFlow version (use command below)**:
1.1
- **Bazel version (if compiling from source)**:
Using cmake to compile from source
- **CUDA/cuDNN version**:
Not using CUDA. Trying to build CPU only version from source
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
After following the instructions to build TF from source on Windows using cmake as given [here](https://github.com/tensorflow/tensorflow/tree/r0.12/tensorflow/contrib/cmake), at step 4, I am trying to build tf_tutorials_example_trainer.vcxproj using MS Visual Studio 2015 instead of MSBuild.

### Describe the problem
While trying to build tf_tutorials_example_trainer.vcxproj, I get a whole bunch of errors such as ""Cannot open include file <some/path/*.pb.h>"". After looking at the source code files, it looks like .pb.h files do not exist. However, the corresponding .proto files do exist. For example, I can see ""atr_value.proto"" under <TF source files/tensorflow/core/framework>, but not ""atr_value.pb.h"". I think for some reason protobuf is not generating the .pb.h files from .proto

Note: I have also installed the TF executable following instructions given [here](https://www.tensorflow.org/install/install_windows). It installed correctly. Hence, I have all dependencies installed on my system.

### Source code / logs
cmake log:
-- Building for: Visual Studio 14 2015
-- The C compiler identification is MSVC 19.0.24215.1
-- The CXX compiler identification is MSVC 19.0.24215.1
-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe
-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe
       -- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED
-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed
-- Found PythonInterp: C:/local/Anaconda3-4.1.1-Windows-x86_64/envs/tensorflow/python.exe (found version ""3.5.3"")
-- Found PythonLibs: C:/local/Anaconda3-4.1.1-Windows-x86_64/envs/tensorflow/libs/python35.lib (found version ""3.5.3"")
-- Found SWIG: C:/local/swigwin-3.0.10/swig.exe (found version ""3.0.10"")
-- Configuring done
-- Generating done
-- Build files have been written to: <TF source dir>/tensorflow/contrib/cmake/build
"
9979,Building with config MKL with current master (git version: v1.1.0-rc2-1163-gcbe5eb4) fails with a build rule error,"Building with config MKL with the current master fails with build rule error.

### System information: 
(contents from tf_env.txt)
== cat /etc/issue ===============================================
Linux desktop 4.4.0-75-generic #96-Ubuntu SMP Thu Apr 20 09:56:33 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.2 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

== uname -a =====================================================
Linux desktop 4.4.0-75-generic #96-Ubuntu SMP Thu Apr 20 09:56:33 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.12.1)
protobuf (3.3.0)
tensorflow (1.1.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.1.0-rc2
tf.GIT_VERSION = v1.1.0-rc2-1163-gcbe5eb4
tf.COMPILER_VERSION = v1.1.0-rc2-1163-gcbe5eb4
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================

== cuda libs  ===================================================

**Exact command to reproduce**:

bazel --output_user_root=/home/desktop/gtt/tfbuild_opt/ build --copt=""-DEIGEN_USE_VML"" --config=mkl -c opt //tensorflow/tools/pip_package:build_pip_package

### Describe the problem: Build with the above setup and following configure options fails with a build rule error.

$ ./configure
Please specify the location of python. [Default is /usr/bin/python]: 
Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]

Using python library path: /usr/local/lib/python2.7/dist-packages
Do you wish to build TensorFlow with MKL support? [y/N] y
MKL support will be enabled for TensorFlow
Do you wish to download MKL LIB from the web? [Y/n] 
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 
Do you wish to use jemalloc as the malloc implementation? [Y/n] 
jemalloc enabled
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] 
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] 
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] 
No XLA support will be enabled for TensorFlow
Do you wish to build TensorFlow with VERBS support? [y/N] 
No VERBS support will be enabled for TensorFlow
Do you wish to build TensorFlow with OpenCL support? [y/N] 
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] 
No CUDA support will be enabled for TensorFlow
Warning: ignoring http_proxy in environment.
...
.....................
INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.
Configuration finished

Build failed with following error:

`
ERROR: /home/desktop/gtt/tensorflow/core/BUILD:1544:1: undeclared inclusion(s) in rule '//tensorflow/core:core_cpu_base':
this rule is missing dependency declarations for the following files included by 'tensorflow/core/graph/mkl_tfconversion_pass.cc':
  '/home/desktop/gtt/tensorflow/core/common_runtime/function.h'
  '/home/desktop/gtt/tensorflow/core/common_runtime/device_mgr.h'
  '/home/desktop/gtt/tensorflow/core/common_runtime/optimization_registry.h'
  '/home/desktop/gtt/tensorflow/core/common_runtime/device_set.h'.
____Building complete.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
____Elapsed time: 615.488s, Critical Path: 227.18s
`
"
9978,graph_editor.copy_with_input_replacements crashes for some orderings of inputs,"Graph editor copy_with_input_replacements  visits nodes in order provided, and assumes that op referenced by ""op._original_op"" has already already been visited. When this assumption is false, it fails with KeyError inside transform.py

Reproducible case

```
import tensorflow as tf
import numpy as np
import tensorflow.contrib.graph_editor as ge

if __name__=='__main__':
  params = tf.Variable(1, dtype=np.float32, name=""params"")
  temp = tf.reduce_sum(params, name=""sum_temp"")
  cost1 = tf.square(temp, name=""cost1"")
  gradients1 = tf.gradients([cost1], [params])
  ops = tf.get_default_graph().get_operations()
  ops = list(sorted(ops, key=lambda op: op.name))
  copied_sgv, info = ge.copy_with_input_replacements(ge.sgv(ops), {})
```
It fails with following error

```
Traceback (most recent call last):
  File ""graph_editor_test.py"", line 13, in <module>
    copied_sgv, info = ge.copy_with_input_replacements(ge.sgv(ops), {})
  File ""/Users/yaroslav/anaconda/envs/memory/lib/python3.5/site-packages/tensorflow/contrib/graph_editor/transform.py"", line 620, in copy_with_input_replacements
    sgv, dst_graph, dst_scope, src_scope, reuse_dst_scope=reuse_dst_scope)
  File ""/Users/yaroslav/anaconda/envs/memory/lib/python3.5/site-packages/tensorflow/contrib/graph_editor/transform.py"", line 436, in __call__
    self._copy_ops(info)
  File ""/Users/yaroslav/anaconda/envs/memory/lib/python3.5/site-packages/tensorflow/contrib/graph_editor/transform.py"", line 450, in _copy_ops
    op_, op_outputs_ = self.transform_op_handler(info, op)
  File ""/Users/yaroslav/anaconda/envs/memory/lib/python3.5/site-packages/tensorflow/contrib/graph_editor/transform.py"", line 173, in copy_op_handler
    original_op = info.transform_original_op_handler(info, op._original_op)
  File ""/Users/yaroslav/anaconda/envs/memory/lib/python3.5/site-packages/tensorflow/contrib/graph_editor/transform.py"", line 125, in transform_op_if_inside_handler
    return info.transformed_ops[op]
KeyError: <tf.Operation 'sum_temp' type=Sum>
```

A work-around is to clear `_original_op` entries for all ops

```
def clear_original_ops(ops):
  for op in ops:
    op._original_op = None
```

@purpledog "
9976,Tensorboard 404 Errors,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win 10
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.1.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:5.1
- **GPU model and memory**:k40 12gb
- **Exact command to reproduce**: launch tensorboard


### Describe the problem
Tensorboard is 404'ing on a lot of resources. See the error messages below. I think I saw a similar issue somewhere so this may be a duplicate. You're probably aware but thought i'd file just in case.

### Source code / logs
```
WARNING:tensorflow:path ../external\weblas_weblas_js/file/weblas.map.json not found, sending 404
WARNING:tensorflow:path ../external\web_animations_js/web-animations-next-lite.min.js.map not found, sending 404
WARNING:tensorflow:path ../external\weblas_weblas_js/file/weblas.map.json not found, sending 404
WARNING:tensorflow:path ../external\web_animations_js/web-animations-next-lite.min.js.map not found, sending 404
WARNING:tensorflow:path ../external\data/plugin/text/runs not found, sending 404
WARNING:tensorflow:path ../external\data/plugin/text/runs not found, sending 404
WARNING:tensorflow:path ../external\data/plugin/text/runs not found, sending 404
WARNING:tensorflow:path ../external\data/plugin/text/runs not found, sending 404
```"
9974,tf.Estimator vs contrib. Very unclear differences and lots of deprecations.,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows/Ubuntu
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.1.0
- **Bazel version (if compiling from source)**: /
- **CUDA/cuDNN version**: 5
- **GPU model and memory**: K40
- **Exact command to reproduce**: N/A

### Describe the problem
Is it just me or is there quite a large disconnect between the two versions of Estimator. The tutorial in the docs guides you through the contrib version but I understand that it has also been moved to tf.Estimator? However in the docs it appears that almost all functionality, e.g. canned estimators or .fit() appears to be missing or altered and there is little documentation to explain this new API.

Have I misunderstood something here? I imagine that we are preferred to use tf.estimator because using the contrib version kicks up all sorts of warning about how it will be deprecated last year! Although the estimator tutorial still uses it and also .fit() which I can't clearly see the replacement for in the new API. Is there going to be any example code or tutorial for the new tf.Estimator API as I feel it desperately needs it. The contrib version was difficult enough to understand that I gave up but want to try again! Thanks

"
9972,#issue: broken or outdated link link for NLP tutorial,"The second link, in the highlights session of the webpage

[Vector Representations](https://www.tensorflow.org/tutorials/word2vec)

is broken or outdated.

This is the link that's not working (404 from github)

[https://www.github.com/tensorflow/tensorflow/blob/r1.1/tensorflow_models/tutorials/embedding/word2vec.py](https://www.github.com/tensorflow/tensorflow/blob/r1.1/tensorflow_models/tutorials/embedding/word2vec.py)


"
9971,Unable to build tensorflow for Java in Windows,"As many may have this problem, I am unable to build tensorflow for Java in Windows. I need to build tensorflow for Java with GPU support in Windows. I followed all the directions specified, and I get the following error message:

Thank you!

$ bazel build --config opt //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni
ERROR: C:/development/projects/tensorflow/tensorflow/java/BUILD:142:1: error loading package 'tensorflow/java/src/main/native': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl"", line 958
                _create_cuda_repository(repository_ctx)
        File ""C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl"", line 846, in _create_cuda_repository
                _get_cuda_config(repository_ctx)
        File ""C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl"", line 656, in _get_cuda_config
                _cudnn_install_basedir(repository_ctx)
        File ""C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl"", line 211, in _cudnn_install_basedir
                auto_configure_fail(""Cannot find cudnn install path....)
        File ""C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl"", line 128, in auto_configure_fail
                fail(""
%sAuto-Configuration Error:%s ...))

Auto-Configuration Error: Cannot find cudnn install path.
 and referenced by '//tensorflow/java:libtensorflow_jni.so'.
ERROR: Analysis of target '//tensorflow/java:libtensorflow_jni' failed; build aborted.
INFO: Elapsed time: 2.365s






Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10, msys2 64bit
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.1.0
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: Quadro K5200
- **Exact command to reproduce**: bazel build --config opt //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
9969,tensorflow-gpu crashes without libcuda.so,"I'm on a multi-machine cluster where not all the machines have gpus. Previously with 0.11 I could use one tensorflow-gpu installation (either from source or from the provided wheel) on all the machines. I've now upgraded to 1.1, and tensorflow crashes at import on the non-gpu machines:

    ImportError: libcuda.so.1: cannot open shared object file: No such file or directory

My current workaround is to have different tensorflows (cpu or gpu) in different conda environments and load the conda environment based on whether I need to use a gpu or not."
9968,ValueError: Refusing to perform an overparameterized separable convolution,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.1rc
- **Bazel version (if compiling from source)**: 
- **CUDA/cuDNN version**: No CUDA
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

I am using tensorflow function tf.nn.separable_conv2d. I want to understand why  channel_multiplier * in_channels > out_channels is not allowed. It was not clear anywhere from the documentation.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
9967,e libcupti,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
9966,gcc failed and fatal error: infiniband/verbs.h: No such file or directory,"When building TensorFlow from source

ERROR: /home/rock/tensorflow/tensorflow/core/distributed_runtime/rdma/BUILD:81:1: C++ compilation of rule '//tensorflow/core/distributed_runtime/rdma:rdma' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 118 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
In file included from tensorflow/core/distributed_runtime/rdma/rdma.cc:10:0:
./tensorflow/core/distributed_runtime/rdma/rdma.h:8:30: fatal error: infiniband/verbs.h: No such file or directory
compilation terminated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps."
9963,configure script overrides user's bazelrc,"The `./configure` script will write to a `.bazelrc` file in the cwd to save some of it's options (in particular, jemalloc). Unfortunately this has the effect of overriding `~/.bazelrc`. This is undocumented, and probably should not happen. For now, the solution is copy your own bazelrc into the tensorflow root before configuring - fortunately `./configure` will not overwrite it and only append to it."
9962,tf.contrib.seq2seq attention_wrapper.py memory_sequence_length can not set None,"tf version '1.1.0-rc2'
The problem is in attention_wrapper.py  77 
  memory_sequence_length = ops.convert_to_tensor(
      memory_sequence_length, name=""memory_sequence_length"")

Don't check if memory_sequence_length is None"
9961,[CMAKE] Unresolved external symbol rdft,"

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 (Visual studio 2015)
- **TensorFlow installed from (source or binary)**: via cmake and includes into c++ project as static lib.(tensorflor_static.lib file).

### Describe the problem
After successful building Release version I tried to add tensorflow_static.lib and dependencies into c++ project. Then linker threw an error:

> Severity	Code	Description	Project	File	Line	Suppression State
> Error	LNK2001	unresolved external symbol rdft	MyLib	C:\path\to\release\tensorflow_static.lib(spectrogram.obj)	1	
"
9960,Placeholder modification behavior,"I am using Tensorflow 1.0.1, on Mac OS Sierra 10.12.4, with CPU in a jupyter notebook (but the behavior I am describing appeared in a regular python script too).

The problem I am having is with placeholders. I have been using tensorflow for only 1 month so I was not really comfortable with using them and I made a mistake that took me a very long time to debug.
Here is the type of code I wrote:
```python
import numpy as np
import tensorflow as tf
x_ = tf.placeholder(tf.float64, [5])
x_ = tf.where(tf.is_nan(x_), tf.zeros_like(x_), x_)
a = tf.constant(1, shape=[5])
a = tf.cast(a, tf.float64)
c = a + x_
init_op = tf.group(
    tf.global_variables_initializer(),
    tf.local_variables_initializer())
sess = tf.Session()
sess.run(init_op)
x = np.arange(5, dtype=float)
x[3] = np.nan
sess.run(c, feed_dict={x_: x})
```
To this, the output was: `array([  1.,   2.,   3.,  nan,   5.])`, which is not what I expected since I had replaced all the `nan` with the `tf.where`. I understood only later that it was because tensorflow was feeding the placeholder in the latest position it appeared (maybe not exactly how it's implemented, but empirically, that is what happens).
Therefore a good (in the sense of what I intended to do) code is:
```python
import numpy as np
import tensorflow as tf
x_ = tf.placeholder(tf.float64, [5])
r = x_
r = tf.where(tf.is_nan(r), tf.zeros_like(r), r)
a = tf.constant(1, shape=[5])
a = tf.cast(a, tf.float64)
c = a + r
init_op = tf.group(
    tf.global_variables_initializer(),
    tf.local_variables_initializer())
sess = tf.Session()
sess.run(init_op)
x = np.arange(5, dtype=float)
x[3] = np.nan
sess.run(c, feed_dict={x_: x})
```
This indeed outputs `array([ 1.,  2.,  3.,  1.,  5.])`. However, to me it's a bit weird that you would need to throw in an extra variable to achieve this purpose and even weirder that I didn't receive any error when modifying my placeholder.
My suggestions are:
- Throw an error or a warning when reallocating a placeholder.
- State clearly that placeholders are not meant to be modified (I looked for it in the docs and didn't find it, maybe I didn't look well enough).
- Allow placeholders to be modified (might be offputting for someone who understands it well).
"
9959,Tensorboard Error Problem 'Can not convert a AdamOptimizer into a Tensor or Operation.',"I'm using tensorflow through anaconda3(64bit).
I have a problem when I try to make tensorboard.
cause I'm beginner, my ground is too low. I can't understand what is problem.
plz help me. 
------------------------
I'm not sure here is right or not to write this problem. 
plz tell me If I'm wrong.

### System information
Server Information:

You are using Jupyter notebook.

The version of the notebook server is 4.2.3 and is running on:
Python 3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]

Current Kernel Information:

Python 3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]
Type ""copyright"", ""credits"" or ""license"" for more information.

IPython 5.1.0 -- An enhanced Interactive Python.
?         -> Introduction and overview of IPython's features.
%quickref -> Quick reference.
help      -> Python's own help system.
object?   -> Details about 'object', use 'object??' for extra details.

When I command this line to check tf.version,
 
 tf.__version__
Out[63]:
'1.0.0-alpha'

### Describe the problem

TypeError: Can not convert a AdamOptimizer into a Tensor or Operation.
TypeError: Fetch argument <tensorflow.python.training.adam.AdamOptimizer object at 0x000001E08E7E1CF8> has invalid type <class 'tensorflow.python.training.adam.AdamOptimizer'>, must be a string or Tensor. (Can not convert a AdamOptimizer into a Tensor or Operation.)

what should I do? plz help me

### Source code / logs
w2_hist=tf.summary.histogram(""weight2"",W2)
cost_summ=tf.summary.scalar(""cost"",cost)
summary=tf.summary.merge_all()
#Create Summary writer
writer=tf.summary.FileWriter('C:\\Users\\jh902\\Documents\\.logs')  #I'm using window 10
writer.add_graph(sess.graph)
s,_= sess.run([summary, optimizer], feed_dict={X: x_data, Y: y_data})
writer.add_summary(s, global_step=2001)

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
C:\Users\jh902\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in __init__(self, fetches, contraction_fn)
    266         self._unique_fetches.append(ops.get_default_graph().as_graph_element(
--> 267             fetch, allow_tensor=True, allow_operation=True))
    268       except TypeError as e:

C:\Users\jh902\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in as_graph_element(self, obj, allow_tensor, allow_operation)
   2469     with self._lock:
-> 2470       return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
   2471 

C:\Users\jh902\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in _as_graph_element_locked(self, obj, allow_tensor, allow_operation)
   2558       raise TypeError(""Can not convert a %s into a %s.""
-> 2559                       % (type(obj).__name__, types_str))
   2560 

TypeError: Can not convert a AdamOptimizer into a Tensor or Operation.

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-20-b8394996caf6> in <module>()
----> 1 s,_= sess.run([summary, optimizer], feed_dict={X: x_data, Y: y_data})
      2 writer.add_summary(s, global_step=2001)

C:\Users\jh902\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    765     try:
    766       result = self._run(None, fetches, feed_dict, options_ptr,
--> 767                          run_metadata_ptr)
    768       if run_metadata:
    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

C:\Users\jh902\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
    950 
    951     # Create a fetch handler to take care of the structure of fetches.
--> 952     fetch_handler = _FetchHandler(self._graph, fetches, feed_dict_string)
    953 
    954     # Run request and get response.

C:\Users\jh902\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in __init__(self, graph, fetches, feeds)
    406     """"""
    407     with graph.as_default():
--> 408       self._fetch_mapper = _FetchMapper.for_fetch(fetches)
    409     self._fetches = []
    410     self._targets = []

C:\Users\jh902\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in for_fetch(fetch)
    228     elif isinstance(fetch, (list, tuple)):
    229       # NOTE(touts): This is also the code path for namedtuples.
--> 230       return _ListFetchMapper(fetch)
    231     elif isinstance(fetch, dict):
    232       return _DictFetchMapper(fetch)

C:\Users\jh902\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in __init__(self, fetches)
    335     """"""
    336     self._fetch_type = type(fetches)
--> 337     self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]
    338     self._unique_fetches, self._value_indices = _uniquify_fetches(self._mappers)
    339 

C:\Users\jh902\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in <listcomp>(.0)
    335     """"""
    336     self._fetch_type = type(fetches)
--> 337     self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]
    338     self._unique_fetches, self._value_indices = _uniquify_fetches(self._mappers)
    339 

C:\Users\jh902\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in for_fetch(fetch)
    236         if isinstance(fetch, tensor_type):
    237           fetches, contraction_fn = fetch_fn(fetch)
--> 238           return _ElementFetchMapper(fetches, contraction_fn)
    239     # Did not find anything.
    240     raise TypeError('Fetch argument %r has invalid type %r' %

C:\Users\jh902\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in __init__(self, fetches, contraction_fn)
    269         raise TypeError('Fetch argument %r has invalid type %r, '
    270                         'must be a string or Tensor. (%s)'
--> 271                         % (fetch, type(fetch), str(e)))
    272       except ValueError as e:
    273         raise ValueError('Fetch argument %r cannot be interpreted as a '

TypeError: Fetch argument <tensorflow.python.training.adam.AdamOptimizer object at 0x000001E08E7E1CF8> has invalid type <class 'tensorflow.python.training.adam.AdamOptimizer'>, must be a string or Tensor. (Can not convert a AdamOptimizer into a Tensor or Operation.)"
9958,Android demo app crashes when using quantized model obtained from graph_transform tool?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
Only edited `ClassifierActivity.java` to suit a custom model
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Android smartphone / host machine: Ubuntu 16.04 LTS
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.1
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: 8.0/5.1
- **GPU model and memory**: GTX 860M

### Describe the problem
I am currently following the guide in: http://nilhcem.com/android/custom-tensorflow-classifier

to train a custom classifier. However, I am using my own frozen graph that I obtained from my own training. What I noticed was when I used the quantized graph obtained through the `graph_transform` method, the app simply crashes without even running. In the guide, it is recommended to run this command on the inference graph:

```
bazel-bin/tensorflow/python/tools/optimize_for_inference \
  --input=/tf_files/retrained_graph.pb \
  --output=/tf_files/retrained_graph_optimized.pb \
  --input_names=Mul \
  --output_names=final_result
```

While I think it may be in conflict with the quantized graph transformations, I ran this command to test, and the app did crash. Here are the observations for all 4 permutations I tested:

1. quantization + optimize_for_inference = **app crash**
2. quantization only = **app crash**
3. optimize_for_inference on frozen graph = **app works**
4. frozen_graph only = **app works**

So my conclusion is the quantization operations within the quantized graph caused the app to fail. 

Here is the quantization command I ran:

```
/home/kwotsin/tensorflow-android/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=./frozen_model_mobilenet.pb \
--out_graph=./quantized_model_mobilenet.pb \
--inputs='Placeholder_only' \
--outputs='MobileNet/Predictions/Softmax' \
--transforms='
  add_default_attributes
  strip_unused_nodes(type=float, shape=""1,299,299,3"")
  remove_nodes(op=Identity, op=CheckNumerics)
  fold_constants(ignore_errors=true)
  fold_batch_norms
  fold_old_batch_norms
  quantize_weights
  quantize_nodes
  strip_unused_nodes
  sort_by_execution_order'
```

And the java file I edited to build my APK:

```
  private static final int INPUT_SIZE = 224;
  private static final int IMAGE_MEAN = 128;
  private static final float IMAGE_STD = 128;
  private static final String INPUT_NAME = ""Placeholder_only"";
  private static final String OUTPUT_NAME = ""MobileNet/Predictions/Softmax"";

  private static final String MODEL_FILE = ""file:///android_asset/quantized_model_mobilenet.pb"";
  private static final String LABEL_FILE =
      ""file:///android_asset/mobilenet_labels.txt"";
```

Other than these edit, every other file is the same. I then imported this project in Android Studio, and ran the `build apk` option located on the top toolbar of Android Studio.

So far, existing tutorials I've seen (e.g. those from Pete Warden) use the `quantize_graph` method to run on mobile devices. Is `graph_transform` compatible for quantizing models for mobile devices yet?"
9956,how to switch between cpu and gpu mode?,"Hi,
I have installed tensorflow-gpu.  I want to employ the cpu mode, whether i need to install tensorflow for cpu?
and if the two are all installed, how to switch between them?"
9955,MatchPath Implementation,"Hi,

I can't seem to find the implementation of the ""MatchPath"" function. I understand this depends on the environment being used, but is there a default implementation somewhere? I want to find out the details of how the path patterns are treated in TensorFlow so I can write a converter from a TF pattern to a regex and use it within another language that has no access to the TF file IO library.

Thanks,
Anthony"
9953,Tensorboard parsing graph.pbtxt failed after uploaded by by web UI,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS 10.12.4
- **TensorFlow installed from (source or binary)**: install binary with cpu version by pip based on Python 2.7.13
- **TensorFlow version (use command below)**: ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')
- **Bazel version (if compiling from source)**: none
- **CUDA/cuDNN version**: only use cpu version
- **GPU model and memory**: none
- **Exact command to reproduce**: just upload event file by web UI on 'GRAPH'

### Describe the problem
After uploaded the event file by web UI('GRAPH' table), it failed at the point of parsing graph.pbtxt with the error 'Cannot read property '' of undefined' as below. But it works fine for the cmd of 'tensorboard --logdir=xxx'.

Error info from chrome dev tool(console):
tf-tensorboard.html:9827 Uncaught TypeError: Cannot read property '' of undefined
    at addAttribute (tf-tensorboard.html:9827)
    at tf-tensorboard.html:9859
    at readHandler (tf-tensorboard.html:9729)
    at FileReader.file.onload (tf-tensorboard.html:9744)"
9952,Error loading saved_model.pb from C++,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes (minor changes), see below.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 17.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.1.0
- **Bazel version (if compiling from source)**:
0.4.5

### Describe the problem
I use the high level API to train an estimator (specifically, `tf.contrib.learn.DNNRegressor`) in Python. I then use `export_savedmodel` to save it to protobuf. When I try to load it from C++ I get:
`Data loss: Can't parse saved_model.pb as binary proto`.

### Source code / logs
- Python file
```
import numpy as np
import pandas as pd
import sys
import pickle
import tensorflow as tf
import random, os, shutil
from tensorflow.contrib.layers import create_feature_spec_for_parsing
from tensorflow.contrib.learn.python.learn.utils import input_fn_utils

tf.logging.set_verbosity(tf.logging.INFO)
train_filename = sys.argv[1]
test_filename = sys.argv[2]
saved_model_directory = sys.argv[3]

COLUMNS = [""X1"", ""Y1"", ""X2"", ""Y2"", ""SP""]
FEATURES = [""X1"", ""Y1"", ""X2"", ""Y2""]
TARGET = ""SP""
training_set = pd.read_csv(train_filename, skipinitialspace=True, names=COLUMNS)
test_set = pd.read_csv(test_filename, skipinitialspace=True, names=COLUMNS)

def input_fn(data_set):
    feature_cols = {k: tf.constant(data_set[k].values) for k in FEATURES}
    targets = tf.constant(data_set[TARGET].values)
    return feature_cols, targets

feature_cols = [tf.contrib.layers.real_valued_column(k) for k in FEATURES]
feature_spec = create_feature_spec_for_parsing(feature_cols)
serving_input_fn = input_fn_utils.build_parsing_serving_input_fn(feature_spec)

validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(
    input_fn=lambda: input_fn(test_set),
    eval_steps=1,
    every_n_steps=100)

estimator = tf.contrib.learn.DNNRegressor(
    feature_columns=feature_cols,
    hidden_units=[50, 25, 5],
    model_dir=saved_model_directory,
    config=tf.contrib.learn.RunConfig(save_checkpoints_secs=1))

estimator.fit(input_fn=lambda: input_fn(training_set), steps=100, monitors=[validation_monitor])

estimator.export_savedmodel(export_dir_base=saved_model_directory,
    serving_input_fn = serving_input_fn)

```

- CPP file
```
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/core/platform/env.h""

using namespace tensorflow;
using std::cout;
using std::vector;
using std::pair;

int main(int argc, char* argv[]) {
  // Initialize a tensorflow session
  Session* session;
  Status status = NewSession(SessionOptions(), &session);
  if (!status.ok()) {
    cout << status.ToString() << ""\n"";
    return 1;
  }

  // Read in the protobuf graph we exported
  // (The path seems to be relative to the cwd. Keep this in mind
  // when using `bazel run` since the cwd isn't where you call
  // `bazel run` but from inside a temp folder.)
  cout << ""READING MODEL.\n"";
  GraphDef graph_def;
  status = ReadBinaryProto(Env::Default(), ""saved_model.pb"", &graph_def);
  if (!status.ok()) {
    cout << status.ToString() << ""\n"";
    return 1;
  }
  cout << ""DONE reading model.\n"";
}
```"
9951,Would you please accomodate for building tensorflow with a custom clang (4.0.0) and libc++ instead of stdlibc++? ,"I have a custom clang with additional optimization passes but I cant get TS compiled with it. 

$ bazel build --cxxopt=-std=c++11 --cxxopt=-stdlib=libc++ tensorflow:libtensorflow.so
INFO: Found 1 target...
INFO: From Compiling external/protobuf/src/google/protobuf/compiler/js/embed.cc [for host]:
external/protobuf/src/google/protobuf/compiler/js/embed.cc:37:12: warning: unused variable 'output_file' [-Wunused-const-variable]
const char output_file[] = ""well_known_types_embed.cc"";
           ^
1 warning generated.
ERROR: /home/hbucher/.cache/bazel/_bazel_hbucher/ad427c7fddd5b68de5e1cfaa7cd8c8cc/external/com_googlesource_code_re2/BUILD:11:1: undeclared inclusion(s) in rule '@com_googlesource_code_re2//:re2':
this rule is missing dependency declarations for the following files included by 'external/com_googlesource_code_re2/re2/bitstate.cc':
  '/home/hbucher/install/include/c++/v1/stddef.h'
  '/home/hbucher/install/include/c++/v1/__config'"
9949,Convolution of zero length input gives junk gradients,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.1
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: python3 junk_gradients.py

### Describe the problem
When you pass a zero length input to tf.conv1d and then calculate gradients, you will get junk values. I think it's reading from uninitialized memory because it's nondeterministic and can be very small or very large positive or negative values or NaNs.

The expected behavior is that the gradients should all be 0 because the weights aren't making any contribution to the loss.

In the application I am writing the input has a variable length (including 0) but for this minimal example I've set it to a constant (tf.ones([0,2])).

If you force the convolution to always have an input with length > 0 then the bug goes away. I've included that in the reproduction code under the variable 'remove_bug'.

For me, with this reduced example, the gradients are always junk but vary widely. You might see 0s if it happens to read from zeroed out memory. Hopefully the bug will show up if you just run it a few times.

### Source code / logs
```python
import tensorflow as tf

remove_bug = False

vals = tf.ones([0,2])

if remove_bug: # hack it to not actually have zero length
    vals = tf.concat([tf.ones([2, 2]), vals], 0)

# At this point 'vals' will either have length 0 or 2 if the bug was removed

filter = tf.Variable(tf.ones([2, 2, 2]))

conv = tf.nn.conv1d(tf.expand_dims(vals, 0), filter, 2, 'SAME')[0]

# At this point 'conv' will either have length 0 or 1 if the bug was removed

if remove_bug:
    conv = conv[1:] # slice off hack, make 'conv' zero length again

# At this point 'conv' will have length 0 whether or not the bug was removed.

optimizer = tf.train.GradientDescentOptimizer(0.01)

grads = [g for g, _ in optimizer.compute_gradients(conv)]

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    gs = sess.run(grads)

print(gs)
```"
9946,StatSummarizer logs error messages for graph with a while loop,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see Python script below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: Built from source
- **TensorFlow version (use command below)**: From command below: ""b'unknown' 1.1.0-rc2"", commit: 69433c1f1adef96fde2074b05d3362e88d8587de
- **Bazel version (if compiling from source)**: Used CMake 3.6.3
- **CUDA/cuDNN version**: Built without GPU support
- **GPU model and memory**: Built without GPU support
- **Exact command to reproduce**:
```
  <create graph.pb with Python script below>
  <path to build output>/benchmark_model.exe --graph=""graph.pb"" --input_layer="""" --input_layer_shape="""" --input_layer_type=""""
```



You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
**Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.**

When running a graph containing a while loop with benchmark_model.exe, StatSummarizer reports numerous errors like this:

E c:\source\tensorflow\tensorflow\core\util\stat_summarizer.cc:146] Bad output slot '1' for 'loop_op/Switch'

It looks like StatSummarizer determines that something about the step stats produced for the Switch node in this case is invalid. This is reproducible with the simple graph in the Python script below, which does not seem like it should be an invalid graph.

### Source code / logs
**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.**

Python script:

```
import tensorflow as tf

initial_value = tf.constant(1)
loop_cond = lambda i: tf.less(i, 10)
loop_body = lambda i: tf.add(i, 1)
loop_op = tf.while_loop(loop_cond, loop_body, [initial_value], name=""loop_op"")
output = tf.identity(loop_op, name=""output"")

with tf.Session() as sess:
  tf.train.write_graph(sess.graph, ""."", ""graph.pb"", as_text=False)
```"
9944,Distributed tensorflow - possible memory leak on Linux,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.1.rc2 (v1.1.0-rc2-675-gd0042ed)
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: 8/5.0
- **GPU model and memory**: Geforce670 2GB (GPU not used in example)
- **Exact command to reproduce**:

`import tensorflow as tf`
`server = tf.train.Server.create_local_server()`
`session = tf.Session(server.target)`
`c = tf.constant(""Hello, distributed TensorFlow!"")`
`init = tf.global_variables_initializer()`
`session.run(init)`
`session.graph.finalize()`
`with session as sess:`
`  ` `while True:`
`    `  `    `  `out = sess.run(c)`

### Describe the problem
Running the above example, memory usage keeps slowly increasing. If the session is initialized as non-distributed with:

session = tf.Session()

The amount of memory used remains stable. This is tested on several machines both with binary and source code installations. Could someone else verify, if this is a problem with tensorflow.
"
9943,Tensorflow Layers conv3d_transpose,"I'm trying to use 3D transpose convolutions found [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L1376), but they are not available in the current build of Tensorflow when installing using pip. On another machine I was successfully able to build Tensorflow from source because I already had Bazel installed on my system, and then able to use the 3d transpose convolutions.

However, on a new machine without Bazel, I am unable to install Bazel, and therefore unable to compile Tensorflow from source. It appears that the Java project has moved. Below is the result of running `sudo apt-get install openjdk-8-jdk`, as per the instructions [here](https://bazel.build/versions/master/docs/install-ubuntu.html#install-compile-source.html).


`Setting up oracle-java9-installer (9b162-1~webupd8~0) ...
Using wget settings from /var/cache/oracle-jdk9-installer/wgetrc
Downloading Oracle Java 9...
--2017-05-16 14:23:32--  http://www.java.net/download/java/jdk9/archive/162/binaries/jdk-9-ea+162_linux-x64_bin.tar.gz
Resolving www.java.net (www.java.net)... 137.254.56.25
Connecting to www.java.net (www.java.net)|137.254.56.25|:80... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://home.java.net/download/java/jdk9/archive/162/binaries/jdk-9-ea+162_linux-x64_bin.tar.gz [following]
--2017-05-16 14:23:33--  https://home.java.net/download/java/jdk9/archive/162/binaries/jdk-9-ea+162_linux-x64_bin.tar.gz
Resolving home.java.net (home.java.net)... 156.151.59.19
Connecting to home.java.net (home.java.net)|156.151.59.19|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: http://www.oracle.com/splash/java.net/maintenance/index.html [following]
--2017-05-16 14:23:33--  http://www.oracle.com/splash/java.net/maintenance/index.html
Resolving www.oracle.com (www.oracle.com)... 23.209.61.60, 2600:1418:3:1a2::2d3e, 2600:1418:3:18b::2d3e
Connecting to www.oracle.com (www.oracle.com)|23.209.61.60|:80... connected.
HTTP request sent, awaiting response... 503 Service Unavailable
2017-05-16 14:23:33 ERROR 503: Service Unavailable.
download failed.
Oracle JDK 9 is NOT installed.
dpkg: error processing package oracle-java9-installer (--configure):
 subprocess installed post-installation script returned error exit status 1
Errors were encountered while processing:
 oracle-java9-installer
E: Sub-process /usr/bin/dpkg returned an error code (1)
`

Is there a way around this, or a timeframe when the new Tensorflow Layers will be included in the pip download?"
9940,Compiling Error with empty error message,"OS: centos 7.2 x64
TensorFlow installed from: source
TensorFlow version: r1.1 
Bazel: 0.45
CPU-only
```
 bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-msse4.2 --copt=-msse4.1 --copt=-msse3 --copt=-mfma -k //tensorflow/tools/pip_package:build_pip_package
```

![](https://cloud.githubusercontent.com/assets/18662395/26121841/2930b0ac-3aa8-11e7-8cb5-65cb3b0b847e.png)

and I check log file, It's an empty file
then I tried again 
exit with this: 
```
[1,801 / 2,762] Compiling tensorflow/core/graph/validate.cc

Server terminated abruptly (error code: 14, error message: '', log file: '/root/.cache/bazel/_bazel_root/bc2bd5c039dfc4a14bebd99e0322728d/server/jvm.out')
```"
9939,contrib.layers.avg_pool2d raises warnings when serializing metagraph,"
### System information

- **OS Platform and Distribution**: Ubuntu 14.04 LTS
- **TensorFlow installed from**: source
- **TensorFlow version**:  ('v1.1.0-rc2-1015-gf2047a3', '1.1.0-rc2')
- **Bazel version**: 0.4.5
- **CUDA/cuDNN version**: 8.0/5.1.5
- **GPU model and memory**: Maxwell Titan X (12 GiB)


### Overview of problem
Since updating to the most recent version (as of yesterday) of TensorFlow, I've started seeing the following ominous warning when serializing a wide resnet variant that I've been using for acoustic modeling:

    WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.
    Type is unsupported, or the types of the items don't match field type in CollectionDef.
    'dict' object has no attribute 'name'

However, a metagraphdef does export and I am able to successfully use it to recreate the trained model. After playing around with simpler architectures, it looks like the problem comes from the average pooling I do at the end, which involves a call to ``tf.contrib.layers.avg_pool2d``. For a trivial example that elicits this warning, please see the script at 

    https://gist.github.com/nryant/1f69cda71fd6a468fa5641855199f843
"
9937,reimplement core/util/cuda_kernel_helper.h?,"Hi,

I'm trying to implement a `GetCuda3DLaunchConfig` into `cuda_kernel_helper.h`, but while reading the code, I feel it's a bit confusing and there might be a better way to implement it.

Here is the pointer:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/cuda_kernel_helper.h

In line 55-57, there is something like `block_count = physical_thread_count / thread_per_block`. Why use `thread_per_block` instead of `virtual_thread_count`?  The number of blocks can be higher than physical maximum and cuda will automatically put these blocks into queue. On the other hand, limiting the number of blocks to physical limit would make the computation incomplete when the number of threads is large.
See: http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/automatic-scalability.png

The the kernel launch config computed is not optimal. I would suggest using the API provided by cuda >=6.5
See: https://devblogs.nvidia.com/parallelforall/cuda-pro-tip-occupancy-api-simplifies-launch-configuration/

Can anyone confirm what I said? If my suggestion make sense, I would like to reimplement these functions using cuda's occupancy api while writing my `GetCuda3DLaunchConfig`."
9935,Unable to freeze Keras layers in a Tensorflow workflow.,"I'm trying to freeze Keras layers in a Tensorflow workflow. It seems that the flag trainable does not work in tf.contrib.keras. This is how I define the graph : 

`sess = tf.Session()`

`K.set_session(sess)`
    
`labels = tf.placeholder(tf.float32, shape=(None, 1))`
`user_id_input = tf.placeholder(tf.float32, shape=(None, 1))`
`item_id_input = tf.placeholder(tf.float32, shape=(None, 1))`

    

`max_user_id = all_ratings['user_id'].max()`
`max_item_id = all_ratings['item_id'].max()`

`embedding_size = 30`
`user_embedding = Embedding(output_dim=embedding_size, input_dim=max_user_id+1,
                           input_length=1, name='user_embedding', trainable=all_trainable)(user_id_input)`
`item_embedding = Embedding(output_dim=embedding_size, input_dim=max_item_id+1,
                           input_length=1, name='item_embedding', trainable=all_trainable)(item_id_input)`



`user_vecs = Flatten()(user_embedding)`
`item_vecs = Flatten()(item_embedding)`


`input_vecs = concatenate([user_vecs, item_vecs])`

`x = Dense(30, activation='relu')(input_vecs)`
`x1 = Dropout(0.5)(x)`
`x2 = Dense(30, activation='relu')(x1)`
`y = Dense(1, activation='sigmoid')(x2)`

`loss = tf.reduce_mean(binary_crossentropy(labels, y))`

`train_step = tf.train.AdamOptimizer(0.004).minimize(loss)`

Then I just train the model : 

`with sess.as_default():`

`train_step.run(..)`

Everything is working fine when the trainable flag is set to `True`. Then when I set it to `False`, it does not freeze the layers.

I also tried to minimize only over the variable that I want to train by using `train_step_freeze = tf.train.AdamOptimizer(0.004).minimize(loss, var_list=[user_embedding]) `, and I get : 

`('Trying to optimize unsupported type ', <tf.Tensor 'Placeholder_33:0' shape=(?, 1) dtype=float32>)`

Is it possible to use Keras layers in Tensorflow and freeze them ? 



"
9934,No OpKernel was registered to support Op 'GatherNd' with these attrs,"env: macOS
tf version: master brunch

I built tensorflow on `macOS` using `build_all_ios.sh`. Then I tried to load `frozen.pb` data created by python program. Here is a code to load graph in `iOS`:

```
    Status status;
    Session *session;
    
    status = NewSession(SessionOptions(), &session);
    if (!status.ok()) {
        return NO;
    }
    
    GraphDef graph;
    NSString *modelPath = [[NSBundle bundleForClass:[self class]] pathForResource:@""frozen"" ofType:@""pb""];
    status = ReadBinaryProto(Env::Default(), modelPath.fileSystemRepresentation, &graph);
    if (!status.ok()) {
        return NO;
    }
    
    status = session->Create(graph);
    if (!status.ok()) {
        std::cout << status.ToString() << ""\n"";
        return NO;
    }
```

 First thing is with default build settings `libtensorflow-core.a` is extremely large - more then 400 MB that is not applicable for using in mobile devices. In any case while loading `graph` I got and error:

```
Invalid argument: No OpKernel was registered to support Op 'Less' with these attrs.  Registered devices: [CPU], Registered kernels:
     device='CPU'; T in [DT_FLOAT]
     
     [[Node: GRU-RNN/rnn/while/Less = Less[T=DT_INT32](GRU-RNN/rnn/while/Merge, GRU-RNN/rnn/while/Less/Enter)]]
```

I could not find how to solve it except manually change next files:
1) added next code to `cwise_op_less.cc`

```
#if defined(__ANDROID_TYPES_SLIM__)
// We only register the first type when we have multi-argument calls in the
// case where we're trying to reduce executable size, but it turns out that the
// int32 version of this op is needed, so explicitly include it.
REGISTER(BinaryOp, CPU, ""Less"", functor::less, int32);
#endif  // __ANDROID_TYPES_SLIM__
```
2) added next code to `cwise_op_add_2.cc`

```
#if defined(__ANDROID_TYPES_SLIM__)
    // We only register the first type when we have multi-argument calls in the
    // case where we're trying to reduce executable size, but it turns out that the
    // int32 version of this op is needed, so explicitly include it.
    REGISTER(BinaryOp, CPU, ""Add"", functor::add, int32);
#endif  // __ANDROID_TYPES_SLIM__
```

The second change was added because of the same error but for `Add` operation.

Then, I have got another error:
```
Invalid argument: No OpKernel was registered to support Op 'GatherNd' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

	 [[Node: GatherNd = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT](GRU-RNN/rnn/transpose, stack)]]
```
 I think this is because python script contains `tf.gather_nd` function. I found that `tf_op_files.txt` does not contain any `gather*` functions, so I tried to add them manually but I don't know that exactly should I add and where. Can you please provide some information about that and probably add some issues about `Less` and `Add` operations for `Int32` type?"
9932,Incorrect Timing Stats Reported by tfprof,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I have added profiling code as shown below to the cifar10 code ([https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10]).

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: ('v1.1.0-rc2-773-g7fa0cf3', '1.1.0-rc2')
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:8.0/6.0
- **GPU model and memory**:NVIDIA Quadra K1200 4GB

### Describe the problem

While doing profiling by tfprof I get the following stats

Timing and Memory

  conv1/weights (19.20KB/76.80KB, 8us/18us)
    conv1/weights/ExponentialMovingAverage (19.20KB/38.40KB, 6us/8us)
      conv1/weights/ExponentialMovingAverage/read (19.20KB/19.20KB, 2us/2us)
    conv1/weights/read (19.20KB/19.20KB, 2us/2us)
  conv2/BiasAdd (4.72MB/4.72MB, 225us/225us)
  conv2/Conv2D (4.72MB/4.72MB, 2.34ms/2.34ms)
  conv2/L2Loss (4B/4B, 21us/21us)
  conv2/biases (256B/1.02KB, 8us/58us)
    conv2/biases/ExponentialMovingAverage (256B/512B, 7us/49us)
      conv2/biases/ExponentialMovingAverage/read (256B/256B, 42us/42us)

Floating Point Operations

_TFProfRoot (0/5.23b flops)
  conv2/Conv2D (3.77b/3.77b flops)
  conv1/Conv2D (707.79m/707.79m flops)
  gradients/local3/MatMul_grad/MatMul (226.49m/226.49m flops)
  gradients/local3/MatMul_grad/MatMul_1 (226.49m/226.49m flops)
  local3/MatMul (226.49m/226.49m flops)
  gradients/local4/MatMul_grad/MatMul (18.87m/18.87m flops)
  gradients/local4/MatMul_grad/MatMul_1 (18.87m/18.87m flops)
  local4/MatMul (18.87m/18.87m flops)
  conv1/BiasAdd (4.72m/4.72m flops)
  conv2/BiasAdd (1.18m/1.18m flops)
  gradients/softmax_linear/MatMul_grad/MatMul (491.52k/491.52k flops)
  gradients/softmax_linear/MatMul_grad/MatMul_1 (491.52k/491.52k flops)
  softmax_linear/MatMul (491.52k/491.52k flops)

Computing Floating Point Performance for Conv2D operation gives surprising results: It comes out to be 3.77b/2.34ms = 1618 GFLOPS which is more than the manufacturer prescribed peak performance of 1052 GFLOPS. The timing stats seem to be wrong. This is impossible.

### Source code / logs
```
 run_metadata = tf.RunMetadata()
    with tf.train.MonitoredTrainingSession(
        checkpoint_dir=FLAGS.train_dir,
        hooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps),
               tf.train.NanTensorHook(loss),
               _LoggerHook()],
        config=tf.ConfigProto(
            log_device_placement=FLAGS.log_device_placement, 
            graph_options=tf.GraphOptions(build_cost_model=1))) as mon_sess:
      while not mon_sess.should_stop():
        #Disable Profiling 
        # mon_sess.run(train_op)

        #Enable Profiling 
        mon_sess.run(train_op, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), 
        run_metadata=run_metadata)
        analysis = tf.contrib.tfprof.model_analyzer.print_model_analysis(
        tf.get_default_graph(),
        run_meta=run_metadata,
        tfprof_options=tf.contrib.tfprof.model_analyzer.PRINT_ALL_TIMING_MEMORY)``
```"
9931,Go: SIGSEGV when using int32 instead of int64 and missing error in Resize functions,"## Problem

In Go, some operation causes a SIGSEGV when using an `int32` instead of an `int64` (and I have reasons to believe that the same will happen when using `float` instead of `double` and vice-versa).

The `Resize*` operations don't define the output shape correctly when the input is not a ""batch"": they just let the dimensions undefined instead of raising some errors.

The tests below are commented so I hope that's enough to let you understand what the problems are.

### Source code / logs

```go
package poc_test

import (
        ""fmt""
        //tf ""github.com/tensorflow/tensorflow/tensorflow/go""
        ""github.com/tensorflow/tensorflow/tensorflow/go/op""
        ""testing""
)

func TestResizeWithoutBatchIsNoSense(t *testing.T) {
        // Create root scope
        root := op.NewScope()

        // Define graph

        // 1: read image content
        imagePath := ""test.jpg""
        contents := op.ReadFile(root.SubScope(""ReadFile""), op.Const(root.SubScope(""filename""), imagePath))

        // 2: decode Jpeg
        value := op.DecodeJpeg(root.SubScope(""DecodeJpeg""), contents, op.DecodeJpegChannels(3))

        // I'd like to add noise to the image, so I'd like to define a nose tensor with the same shape of the image
        // Just to be sure that the image shape is fully defined, I resize it
        resize1 := op.ResizeNearestNeighbor(root.SubScope(""ResizeArea""), value, op.Const(root.SubScope(""size""), []int32{int32(80), int32(80)}))

        // If the size parameter is an int32, no error is raised but the operation is no sense
        // Because it returns ? instead of [80, 80, 3]
        // The reason is taht Resize* methods requires a batch of images: should raise an error?
        fmt.Println(""Shape with int32: "", resize1.Shape().String())
        if dims64, err := resize1.Shape().ToSlice(); err != nil {                                                                                                                                                                                                              
                fmt.Println(dims64)                                                                                                                                                                                                                                            
        } else {                                                                                                                                                                                                                                                               
                t.Error(""Error: "", err.Error())                                                                                                                                                                                                                                
        }                                                                                                                                                                                                                                                                      
                                                                                                                                                                                                                                                                               
        // I expect a fully defined shape                                                                                                                                                                                                                                      
        if !resize1.Shape().IsFullySpecified() {                                                                                                                                                                                                                               
                t.Error(""Not defined shape"")                                                                                                                                                                                                                                   
        }                                                                                                                                                                                                                                                                      
                                                                                                                                                                                                                                                                               
        // create the batch and see how things changes                                                                                                                                                                                                                         
        batch := op.ExpandDims(root.SubScope(""expand""), value, op.Const(root.SubScope(""axis""), []int32{0}))                                                                                                                                                                    
        resize1 = op.ResizeNearestNeighbor(root.SubScope(""ResizeArea""), batch, op.Const(root.SubScope(""size""), []int32{int32(80), int32(80)}))                                                                                                                                 
        fmt.Println(""Shape with int32 and input as a batch: "", resize1.Shape().String())                                                                                                                                                                                       
        if dims64, err := resize1.Shape().ToSlice(); err == nil {                                                                                                                                                                                                              
                fmt.Println(dims64)                                                                                                                                                                                                                                            
        } else {                                                                                                                                                                                                                                                               
                fmt.Println(""Error: "", err.Error())                                                                                                                                                                                                                            
        }                                                                                                                                                                                                                                                                      
        // Now the things have sense and the shape is defined and equals to [ 1, 80, 80, 3]                                                                                                                                                                                    
}                                                                                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                               
func TestResizeWithIn64ShapeSigSegvs(t *testing.T) {                                                                                                                                                                                                                           
        defer func() {                                                                                                                                                                                                                                                         
                if r := recover(); r != nil {                                                                                                                                                                                                                                  
                        t.Error(""Panic!"")                                                                                                                                                                                                                                      
                }                                                                                                                                                                                                                                                              
        }()
        // Create root scope
        root := op.NewScope()

        // Define graph

        // 1: read image content
        imagePath := ""test.jpg""
        contents := op.ReadFile(root.SubScope(""ReadFile""), op.Const(root.SubScope(""filename""), imagePath))

        // 2: decode Jpeg
        value := op.DecodeJpeg(root.SubScope(""DecodeJpeg""), contents, op.DecodeJpegChannels(3))

        // However, changing int32 with int64 breaks everyting (no matter if I use `batch` or `value`)
        resize2 := op.ResizeArea(root.SubScope(""ResizeArea2""), value, op.Const(root.SubScope(""size2""), []int64{int64(80), int64(80)}))
        // This operation causes a SIGSEGV
        fmt.Println(""Shape value: "", resize2.Shape())
        fmt.Println(""Shape with int64: "", resize2.Shape().String())

        // In short, chaning int32 with int64 causes SIGSEGV. It looks like kernels are not registered to handle both types

        // This can bring the code to be a mess to debug, because If I'd like to, for example, add noise to an image
        // I have to generate a set of values with the same shape of the input images.
        // Using the one with the defined shape (the batch) I'd like to use the output of Shape().ToSlice()
        // But I can't.
}

func TestGenerateNoiseWithInt32Shape(t *testing.T) {
        defer func() {
                if r := recover(); r != nil {
                        t.Error(""Panic!"")
                }
        }()
        // Create root scope
        root := op.NewScope()

        // Define graph

        // 1: read image content
        imagePath := ""test.jpg""
        contents := op.ReadFile(root.SubScope(""ReadFile""), op.Const(root.SubScope(""filename""), imagePath))
        // 2: decode Jpeg
        value := op.DecodeJpeg(root.SubScope(""DecodeJpeg""), contents, op.DecodeJpegChannels(3))
        batch := op.ExpandDims(root.SubScope(""expand""), value, op.Const(root.SubScope(""axis""), []int32{0}))
        resize1 := op.ResizeNearestNeighbor(root.SubScope(""ResizeArea""), batch, op.Const(root.SubScope(""size""), []int32{int32(80), int32(80)}))
        fmt.Println(""Shape with int32 and input as a batch: "", resize1.Shape().String())
        if dims64, err := resize1.Shape().ToSlice(); err != nil {
                fmt.Println(dims64)
        } else {
                fmt.Println(""Error: "", err.Error())
        }

        dims64, _ := resize1.Shape().ToSlice()
        noise := op.ParameterizedTruncatedNormal(root.SubScope(""ParameterizedTruncatedNormal""),
                op.Const(root.SubScope(""shape""), dims64),
                op.Const(root.SubScope(""means""), 0.),
                op.Const(root.SubScope(""stddev""), 1.),
                op.Const(root.SubScope(""minvals""), 0.),
                op.Const(root.SubScope(""maxvals""), 1.))
        fmt.Println(noise)

        // ^ This operation causes SIGSEGV
        // I have to convert dims64 to a slice of int32 and then the operation works

}

func TestGenerateNoiseWithInt64Shape(t *testing.T) {
        // Create root scope
        root := op.NewScope()

        // Define graph

        // 1: read image content
        imagePath := ""test.jpg""
        contents := op.ReadFile(root.SubScope(""ReadFile""), op.Const(root.SubScope(""filename""), imagePath))
        // 2: decode Jpeg
        value := op.DecodeJpeg(root.SubScope(""DecodeJpeg""), contents, op.DecodeJpegChannels(3))
        batch := op.ExpandDims(root.SubScope(""expand""), value, op.Const(root.SubScope(""axis""), []int32{0}))
        resize1 := op.ResizeNearestNeighbor(root.SubScope(""ResizeArea""), batch, op.Const(root.SubScope(""size""), []int32{int32(80), int32(80)}))
        fmt.Println(""Shape with int32 and input as a batch: "", resize1.Shape().String())
        if dims64, err := resize1.Shape().ToSlice(); err != nil {
                fmt.Println(dims64)
        } else {
                fmt.Println(""Error: "", err.Error())
        }

        dims64, _ := resize1.Shape().ToSlice()

        var dims []int32 = make([]int32, len(dims64))
        for i, dim := range dims64 {
                dims[i] = int32(dim)
        }

        noise := op.ParameterizedTruncatedNormal(root.SubScope(""ParameterizedTruncatedNormal""),
                op.Const(root.SubScope(""shape""), dims64),
                op.Const(root.SubScope(""means""), 0.),
                op.Const(root.SubScope(""stddev""), 1.),
                op.Const(root.SubScope(""minvals""), 0.),
                op.Const(root.SubScope(""maxvals""), 1.))
        fmt.Println(noise.Shape().String())
}

```

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Archlinux
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.1.0-rc2
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: cuda 8, cudnn 5.1
- **GPU model and memory**:  GeForce GTX 1080
- **Exact command to reproduce**: `go test`
"
9930,AttributeError: type object 'NewBase' has no attribute 'is_abstract',"

------------------------

### System information
- 
-OS Platform and Distribution  =Ubuntu 14.04
- **TensorFlow installed = 
 i mtrying to install tensorflow using following link

https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.1.0-cp27-none-linux_x86_64.whl





### Describe the problem
when i am trying to import tensorflow its not working what to do you can check following logs

### Source code / logs
root@ubuntu:/home/vivek# python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 104, in <module>
    from tensorflow.python.platform import test
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/test.py"", line 38, in <module>
    from tensorflow.python.framework import test_util as _test_util
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py"", line 45, in <module>
    from tensorflow.python.platform import googletest
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/googletest.py"", line 33, in <module>
    from tensorflow.python.platform import benchmark  # pylint: disable=unused-import
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/benchmark.py"", line 119, in <module>
    class Benchmark(six.with_metaclass(_BenchmarkRegistrar, object)):
  File ""/usr/lib/python2.7/dist-packages/six.py"", line 617, in with_metaclass
    return meta(""NewBase"", bases, {})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/benchmark.py"", line 114, in __new__
    if not newclass.is_abstract():
AttributeError: type object 'NewBase' has no attribute 'is_abstract'
root@ubuntu:/home/vivek# 
"
9929,Compiling tensorflow 1.1 under Centos 7,"I'm trying to compile tensorflow with cuda support under linux Centos 7 distribution.

I followed the instructions provides at [github: gentaiscool/tensorflow.md ](https://gist.github.com/gentaiscool/a628fab5cd98953af7f46b69463394b3) with no success.

In this page they ask for hacking the file tensorflow/third_party/gpus/crosstool/CROSSTOOL (adding the line ""cxx_builtin_include_directory : ""/usr/local/cuda/targets/x86_64-linux/include"").

In the current version of Tensorflow (1.1) there is no such a file; similar files, such as: CROSSTOOL.tpl, CROSSTOOL_nvcc.tpl, CROSSTOOL_clang.tpl, are found. I tried the proposed hacking on each of these files and didn't get successful compilation.

Since I'm interested is tensorflow deployment, I'm compiling it using the instructions found in [gitgub cjweeks/tensorflow-cmake](https://github.com/cjweeks/tensorflow-cmake) (building a library to be linked with a C++ based program). My build command is:

`bazel build -c opt --config=cuda tensorflow:libtensorflow_all.so`

The error I've got is:

`ERROR: /root/tensorflow/tensorflow/core/kernels/BUILD:3004:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:depth_space_ops_gpu':
this rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/spacetodepth_op_gpu.cu.cc':
  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/limits.h'
  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/syslimits.h'
  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stddef.h'
  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdarg.h'
  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdint.h'
  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/x86intrin.h'
  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/ia32intrin.h'
  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/mmintrin.h'
  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/xmmintrin.h'
  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/mm_malloc.h'
  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/emmintrin.h'
  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/immintrin.h'
  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/fxsrintrin.h'
  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/adxintrin.h'
  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/float.h'
  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdbool.h'.`

System information:

- Linux Centos 7.2
- Kernel: 3.10.0-327.36.1.el7.x86_64
- Compiler: gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC)
- Tensorflow: 1.1, installed from source.
- nvcc: Cuda compilation tools, release 8.0, V8.0.44
- CUDA: 8.0
- cudaa: 5.1.5
- bazel: 0.4.5

Any help please?

Thanks in advance,

Ron."
9928,I'm curious about whether these two graphs are same network,"![cifar](https://cloud.githubusercontent.com/assets/10510469/26090870/2fd017a4-3a3a-11e7-9303-13b912a8b0c1.png)
As we know, the upper graph is the provided example instance of CIFAR10. But when I copied these inference() code to my code, it likes as bellow. So my question is does these two graphs are equal? If yes, why they are looks like so different?
![cifar-my](https://cloud.githubusercontent.com/assets/10510469/26090871/2fe2502c-3a3a-11e7-8608-c5aa91324871.png)

"
9927,Some small problems with the RNN and seq2seq implementations,"1. In the class `AttentionCellWrapper` , https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L1116  , maybe the `lstm_output` should be replaced by some other token, since the wrapper is not specified for LSTM.

2.  In the class `Decoder`, https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/decoder.py#L90 , I think the `step()` method should not return an output as an instance of `BasicDecoderOutput`. From an object-oriented-programming view, the `BasicDecoder` is an inheritance of `Decoder`, the basic class should not have access to something designed for the inherited class, the same problem exists in the `dynamic_decode` method."
9926,Issues with RoCE support,"When rendezvous_mgr->RecvLocalAsync fails, grpc responds with the Status, while grpc+verbs does not. Should we consider this situation ? @junshi15 "
9925,graph_editor copy_with_input_replacements doesn't update colocation constraints,"It seems if you try to use graph_editor to make copy of a model to place on another device, the new graph will still refer to old version inside colocation constraints. 

This causes errors like below when trying to run resulting graph.
`tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot colocate nodes 'gradients/Max_1_1_grad/mul' and 'gradients/AddN_13: Cannot merge devices with incompatible ids: '/GPU:0' and '/GPU:1'
`

More natural might be to update colocation constraints to point to newly created copies of ops.

Test case
```
  import tensorflow.contrib.graph_editor as ge
  tf.reset_default_graph()
  with tf.device('/cpu:0'):
    a = tf.ones((), name='a')
    with tf.get_default_graph().colocate_with(a):
      b = tf.add(a, 1, name='b')
  g = tf.get_default_graph()
  ops = g.get_operations()
  copied_sgv, info = ge.copy_with_input_replacements(ge.sgv(ops),
                                                       {})
  print(tf.get_default_graph().as_graph_def())
```

You will see that newly created `b_1` op will refer to old op `a`

```
node {
  name: ""b/y_1""
  op: ""Const""
  device: ""/device:CPU:0""
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@a""
      }
    }
  }
```

@purpledog "
9920,Freezing graphs with custom ops,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 and MacOS Sierra
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master (quite recent)
- **Bazel version (if compiling from source)**: 0.4.5
- **Exact command to reproduce**: freeze_graph

### Describe the problem
I've asked my question on StackOverflow here:
http://stackoverflow.com/questions/43880729/using-new-op-while-importing-graph-in-tensorflow
I'm trying to freeze a model which contains a custom op. But `freeze_graph` gives the following error: 
```
Traceback (most recent call last):
  File ""<local path>/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 202, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""<local path>/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""<local path>/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 134, in main
    FLAGS.variable_names_blacklist)
  File ""<local path>/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 99, in freeze_graph
    _ = importer.import_graph_def(input_graph_def, name="""")
  File ""<local path>/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/importer.py"", line 260, in import_graph_def
    raise ValueError('No op named %s in defined operations.' % node.op)
ValueError: No op named RoiPool in defined operations.
```
I was suggested on StackOverflow to build `freeze_graph` with my custom op as a dependency. I did that, but `freeze_graph` still gives the same error. 

It was also suggested for me to open a **feature request** to make an easier-to-use interface for using freeze_graph with custom ops.

### Source code / logs
Here is the freeze_graph command I'm using: 
`bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=<local path>/data/graph_vgg.pb --input_checkpoint=<local path>/data/VGGnet_fast_rcnn_iter_70000.ckpt --output_node_names=""cls_prob,bbox_pred"" --output_graph=<local path>/graph_frozen.pb`"
9918,preserving specific checkpoints,"Savers automatically clean up checkpoints and that's lovely.  But there are special points during training that I want to be sure to save (e.g. transitioning from a pre-training phrase to full training), which I can't ensure with the current options (unless I just keep everything, by making ```max_to_keep``` & ```keep_checkpoint_every_n_hours``` huge).

Two possible approaches:

1)  ```saver.save(..., preserve=True)```
Never clean up this checkpoint.

2)  ```max_to_keep``` is defined **per** ```save_path```
i.e. Whenever I change the ```save_path``` argument to ```save()``` in the middle of the session, don't clean up the checkpoints with the previous ```save_path```.

This is related to #8658 (with a little book-keeping on the client, you could do #8658 yourself)."
9917,Feature request: tf.nn.depthwise_conv2d_transpose,"### System information
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.1.0-0-g1ec6ed5', '1.1.0')
- **Bazel version (if compiling from source)**: 0.4.5- (@non-git)
- **CUDA/cuDNN version**: 7.5/5.1
- **GPU model and memory**: NVIDIA M40, 12GB

### Describe the problem
I would like to apply a `tf.nn.conv2d_transpose` operation to each channel of a feature image independently. There is no `tf.nn.depthwise_conv2d_transpose` operation.

I tried using `tf.nn.depthwise_conv2d_native_backprop_input` however, when I try to optimize a function that involves one of these operations, it results in an error because there is no gradient operation defined:

```
LookupError: No gradient defined for operation 'DepthwiseConv2dNativeBackpropInput_1' (op type: DepthwiseConv2dNativeBackpropInput)
```

This is related to https://github.com/tensorflow/tensorflow/issues/7934

It is possible to achieve this functionality using `conv2d_transpose` by constructing a large filter with many coefficients set to zero. However, it is relatively inefficient, especially for a large number of channels."
9916,tf.self_adjoint_eig doesn't behave the same way with float32 and float64,"Here is an example to reproduce the problem

```
l = tf.constant([[10., -4., -4., -2.],
                  [-4., 10., -2., -4.],
                  [-4., -2., 6., 0.],
                  [-2., -4., 0., 6.]], dtype=tf.float64)
e, v = tf.self_adjoint_eig(tf.expand_dims(l, 0))
```

When I set `dtype=tf.float64` the output is

```
// Eigen values
[[ -2.31986627e-15   5.52786405e+00   1.20000000e+01   1.44721360e+01]]

// Eigen vectors
[[-0.5        -0.16245985  0.5        -0.68819096]
 [-0.5         0.16245985  0.5         0.68819096]
 [-0.5        -0.68819096 -0.5         0.16245985]
 [-0.5         0.68819096 -0.5        -0.16245985]]
```

When `dtype=tf.float32` the output is

```
// Eigen values
[[ -1.02379988e-06   5.52786446e+00   1.20000019e+01   1.44721375e+01]]

// Eigen vectors
[[ 0.49999985  0.16245979  0.49999985 -0.68819106]
 [ 0.5        -0.16246006  0.50000018  0.68819082]
 [ 0.5         0.68819106 -0.49999988  0.16246004]
 [ 0.49999991 -0.68819088 -0.50000012 -0.16245979]]
```
In this case the sign of the second eigen vector changed.

The numpy equivalent of this code always give the same result (in float or float32) and is similar to the result I got with the tf.float64 version

```
L = np.array([[10., -4., -4., -2.],
                  [-4., 10., -2., -4.],
                  [-4., -2., 6., 0.],
                  [-2., -4., 0., 6.]])
D, V =np.linalg.eigh(L)
```


```
// numpy eigen values
[  1.11716192e-15   5.52786405e+00   1.20000000e+01   1.44721360e+01]

// numpy eigen vectors
[[ 0.5        -0.16245985  0.5        -0.68819096]
 [ 0.5         0.16245985  0.5         0.68819096]
 [ 0.5        -0.68819096 -0.5         0.16245985]
 [ 0.5         0.68819096 -0.5        -0.16245985]]
```

"
9915,problem importing tensorflow with tensorflow-gpu pip package and Nvidia PRIME,"
### System information
- OS Platform and Distribution: Linux Ubuntu 16.10
- TensorFlow installed from: binary
- TensorFlow version: tensorflow-gpu-1.1.0
- **CUDA/cuDNN version**:
- GPU model and memory: GeForce 940MX 982MiB
- Exact command to reproduce: import tensorflow


== cat /etc/issue ===============================================
Linux Lyn 4.8.0-51-generic #54-Ubuntu SMP Tue Apr 25 16:32:21 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.10 (Yakkety Yak)""
VERSION_ID=""16.10""
VERSION_CODENAME=yakkety

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 6.2.0-5ubuntu12) 6.2.0 20161005
Copyright (C) 2016 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux Lyn 4.8.0-51-generic #54-Ubuntu SMP Tue Apr 25 16:32:21 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.12.1)
numpydoc (0.6.0)
protobuf (3.3.0)
tensorflow-gpu (1.1.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
2017-05-15 16:22:31.009080: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-15 16:22:31.009102: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-15 16:22:31.009124: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-05-15 16:22:31.009131: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-15 16:22:31.009139: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-05-15 16:22:31.119107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-05-15 16:22:31.119494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: GeForce 940MX
major: 5 minor: 0 memoryClockRate (GHz) 0.993
pciBusID 0000:01:00.0
Total memory: 982.12MiB
Free memory: 675.25MiB
2017-05-15 16:22:31.119518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-05-15 16:22:31.119526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-05-15 16:22:31.119542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0)
tf.VERSION = 1.1.0
tf.GIT_VERSION = v1.1.0-rc0-61-g1ec6ed5
tf.COMPILER_VERSION = v1.1.0-rc0-61-g1ec6ed5
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Mon May 15 16:21:29 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce 940MX       Off  | 0000:01:00.0     Off |                  N/A |
| N/A   43C    P0    N/A /  N/A |    262MiB /   982MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1017    G   /usr/lib/xorg/Xorg                             168MiB |
|    0      1860    G   /usr/bin/compiz                                 41MiB |
|    0      2324    G   ...el-token=2DD3BDBDD08C58317A0131100BC13BC1    52MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================

------------------
tf.GIT_VERSION
v1.1.0-rc0-61-g1ec6ed5

tf.VERSION
1.1.0

### Describe the problem
I have a laptop with a dedicated Nvidia GPU. I use it only for prototyping my tensorflow code.
But dedicated GPUs drain a lot of energy and reduce the laptop's battery life.
So when I'm outside on battery (eg: in the library at university) I always set Nvidia PRIME to use the integrated card only (type nvidia-settings in a console to reach this setting).
With the previous versions of tensorflow-gpu (installedi via pip3 on Ubuntu) everything worked well.
Now with the current release I can no longer use tensorflow-gpu while I have the Nvidia card disabled with PRIME.
Now, to be able to work with tensorflow AND have enough battery to conclude my day, I have to install the pip package ""tensorflow"" (and not ""tensorflow-gpu""). but that turns useless if, for some reason, need to test my code with GPU acceleration, turning back on the dedicated graphic card via Nvidia PRIME.
If I really want GPU acceleration I have to re-enable the dedicated card in PRIME, uninstall tensorflow and reinstall tensorflow-gpu. every time. that's a mess!
To me, there are two way to resolve this bug:
1) make tensorflow-gpu again able to handle situation with all the Nvidia GPU are temporarily disabled.
2) unify the tensorflow and tensorflow-gpu packages, including a smart logic inside them that enable the software to handle theese ibrid situations, which I think will be very common in the laptops in the near future

### Source code / logs
import tensorflow
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
/home/federico/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     40     sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_GLOBAL)
---> 41   from tensorflow.python.pywrap_tensorflow_internal import *
     42   from tensorflow.python.pywrap_tensorflow_internal import __version__

/home/federico/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

/home/federico/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

/usr/lib/python3.5/imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

/usr/lib/python3.5/imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: libnvidia-fatbinaryloader.so.375.39: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-a649b509054f> in <module>()
----> 1 import tensorflow

/home/federico/.local/lib/python3.5/site-packages/tensorflow/__init__.py in <module>()
     22 
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26 

/home/federico/.local/lib/python3.5/site-packages/tensorflow/python/__init__.py in <module>()
     49 import numpy as np
     50 
---> 51 from tensorflow.python import pywrap_tensorflow
     52 
     53 # Protocol buffers

/home/federico/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     50 for some common reasons and solutions.  Include the entire stack trace
     51 above this error message when asking for help."""""" % traceback.format_exc()
---> 52   raise ImportError(msg)
     53 
     54 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""/home/federico/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/federico/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/federico/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libnvidia-fatbinaryloader.so.375.39: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

"
9914,"Optimizing valid TensorFlow graph yields ""graph_def is invalid"" ValueError","`tensorflow.python.tools.optimize_for_inference_lib.optimize_for_inference` is producing an invalid graph definition. So far as I can tell this is not user error; optimizing a valid graph definition should produce a valid graph definition, so this appears to be a bug.

The following code demonstrates the problem. You will need the input graph definition [model.txt.gz](https://github.com/tensorflow/tensorflow/files/1001369/model.txt.gz). Running the code loads the graph definition, verifies it is valid (by importing it and printing the number of nodes) then calls `optimize_for_inference`. We then attempt to verify the resulting graph definition but get the error

    ValueError: graph_def is invalid at node u'valid/valid_fed/model/rnn/rnn/while/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/concat/axis': More inputs specified ('valid/valid_fed/model/rnn/rnn/while/Switch:1') than the op expects..

The error originates from

    tensorflow/python/framework/importer.py, line 362, in import_graph_def

The attached model definition has been manually altered to reduce the size of the (frozen) parameters but the same error occurs with the unmodified original. Attempts to reproduce this problem with a simpler graph failed. Simpler graphs can be optimized successfully. I don't know what it is about this graph that causes the failure.

    import gzip
    
    import tensorflow as tf
    from google.protobuf import text_format
    from tensorflow.python.tools import optimize_for_inference_lib
    
    
    def verify(graph_def):
        with tf.Graph().as_default():
            tf.import_graph_def(graph_def, name="""")
            print(len(tf.get_default_graph().as_graph_def().node))
    
    
    def read_graph_def(path):
        graph_def = tf.GraphDef()
    
        with gzip.open(path, ""rb"") as input_file:
            text_format.Merge(input_file.read(), graph_def)
    
        return graph_def
    
    
    def optimize(input_graph_def):
        output_graph_def = optimize_for_inference_lib.optimize_for_inference(
            input_graph_def, [""valid/valid_fed/model/input/x""],
            [""valid/valid_fed/model/output/y""], tf.int32.as_datatype_enum)
        return output_graph_def
    
    
    def main():
        input_graph_def = read_graph_def(""model.txt.gz"")
        verify(input_graph_def)
        output_graph_def = optimize(input_graph_def)
        verify(output_graph_def)
    
    
    if __name__ == ""__main__"":
        main()

Environment details:

- Linux Ubuntu 14.04
- TensorFlow installed from source
- TensorFlow version: ('v1.1.0-0-g1ec6ed5', '1.1.0')
- Bazel version: 0.4.5
- No GPU
"
9912,Bazel Build fails with missing input file error @grpc,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No. Stock script is used.
- **OS Platform and Distribution**: CentOS Linux release 7.2.1511. 
- **TensorFlow installed from** : Source.
- **Bazel version**: 0.4.5
- **CUDA/cuDNN version**: 7.5
- **Exact command to reproduce**:
./bazel build --config=opt --config=cuda --spawn_strategy=standalone --genrule_strategy=standalone //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
I am installing Tensorflow 1.1 on a system where I do not have root priviliges. Upon running Bazel command (mentioned above), I get the following error - 

**missing input file '@grpc//:include/grpc++/impl/codegen/string_ref.h'.**

I need to include the path of gprc to the Bazel. However, I do not know how to find the location of these files and how to refer them to Bazel. I have tried to find these files in the filesystem and bazel downloads, but couldnt find them.

#### Another insteresting thing that is happening is - 
Everytime I run the bazel command again, it gives me different error each time ( However, they all are related to ""@grpc//: "". Some of the other errors I got were - 

ERROR: missing input file '@grpc//:include/grpc++/impl/service_type.h'.
ERROR: missing input file '@grpc//:LICENSE'.
ERROR: missing input file '@grpc//:include/grpc++/server_builder.h'.
etc.

### Error Log
`
ERROR: missing input file '@grpc//:include/grpc++/impl/codegen/string_ref.h'.
ERROR: /home_dir/xxxxx/nn/installation/tensorflow/tensorflow/core/BUILD:1308:1: C++ compilation of rule '//tensorflow/core:lib_hash_crc32c_accelerate_internal' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 43 argument(s) skipped): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 15.
ERROR: /home_dir/xxxxx/.cache/bazel/_bazel_xxxxx/feba725fe17da6f4082aa7bd94139644/external/grpc/BUILD.bazel:1664:1: @grpc//:grpc++_unsecure: missing input file '@grpc//:include/grpc++/impl/codegen/string_ref.h'.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home_dir/xxxxx/.cache/bazel/_bazel_xxxxx/feba725fe17da6f4082aa7bd94139644/external/grpc/BUILD.bazel:1664:1 1 input file(s) do not exist.
INFO: Elapsed time: 13.184s, Critical Path: 2.25s
`"
9911,Estimator predict() fails after fit() or evaluate(),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, I did write an `input_fn` based code based on the [Abalone](https://www.tensorflow.org/extend/estimators) example. Specifically, I replaced the `x` and `y` parameters with custom `input_fn` implementations loading images and applying `map_fn` to them before batching. This resulted in a batch of unknown tensor shape (`None`).
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.1.0-1-g10ec24a
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: 8.0, 5.1
- **GPU model and memory**: GeForce 1080 Ti
- **Exact command to reproduce**:  running the code

### Describe the problem
 When calling `predict()` after calling `fit()` or `evaluate()` on an `Estimator` instance (an unlikely scenario, but shown in the example), the call crashes with 

```
Traceback (most recent call last):
  File ""/home/mmayer/dev/everybag/orientation_network/orientation.py"", line 169, in <module>
    main()
  File ""/home/mmayer/dev/everybag/orientation_network/orientation.py"", line 35, in main
    as_iterable=False)
  File ""/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 281, in new_func
    return func(*args, **kwargs)
  File ""/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 565, in predict
    as_iterable=as_iterable)
  File ""/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 857, in _infer_model
    infer_ops = self._get_predict_ops(features)
  File ""/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1187, in _get_predict_ops
    self._labels_info)
  File ""/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/tensor_signature.py"", line 164, in create_placeholders_from_signatures
    return signatures.get_placeholder()
  File ""/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/tensor_signature.py"", line 89, in get_placeholder
    shape=[None] + list(self.shape[1:]))
  File ""/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 478, in __iter__
    raise ValueError(""Cannot iterate over a shape with unknown rank."")
ValueError: Cannot iterate over a shape with unknown rank.
```

if `self._labels_info` of the `Estimator` has unknown shape (e.g. as an effect of using `map_fn` on images to obtain both feature and target from them. This is due to

```python
  def get_placeholder(self):
    if self.is_sparse:
      return array_ops.sparse_placeholder(dtype=self.dtype)
    return array_ops.placeholder(dtype=self.dtype,
                                 shape=[None] + list(self.shape[1:]))
```

in `tensor_signature.py`, since `self.shape[1:]` is `None`.

The call comes through

```python
  def _get_predict_ops(self, features):
    labels = tensor_signature.create_placeholders_from_signatures(
        self._labels_info)
    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.INFER)
```

which uses the labels to construct the inference graph, when it probably shouldn't.

The `predict()` operation succeeds if it is called without prior calls to `fit()` or `evaluate()` as in this case the method

```python
def create_placeholders_from_signatures(signatures):
  if signatures is None:
    return None
  if not isinstance(signatures, dict):
    return signatures.get_placeholder()
  return {
      key: signatures[key].get_placeholder()
      for key in signatures}
```

early exits.


### Source code / logs

Example code that triggers the problem; requires images in `img` directory. The problem can be resolved by explicitly setting the `indices` Tensor's shape in `input_fn` using a `tf.reshape` to `[-1]`, but given that it works for training and evaluation and that inference probably shouldn't use the targets at all (especially not from previous runs), I consider this a bug.

```python
import os
import time
from typing import Dict, Optional, Any
import tensorflow as tf
import tensorflow.contrib.learn as tfl
import tensorflow.contrib.slim as slim
from tensorflow.contrib.slim.python.slim.nets import inception_v3 as inception


def main():
    model_params = {'learning_rate': 0.001,
                    'batch_size': 16,
                    'num_epochs': None}

    nn = tfl.Estimator(model_fn=model_fn, params=model_params)
    nn.fit(input_fn=lambda: input_fn('img', params=model_params), steps=10)
    ev = nn.evaluate(input_fn=lambda: input_fn('img', params=model_params), steps=1)
    predictions = nn.predict(input_fn=lambda: input_fn('img', params=model_params), as_iterable=False)


def model_fn(features, targets, mode, params):
    training = (mode == tfl.ModeKeys.TRAIN)
    num_classes = 5

    image = features['image']
    image.set_shape((None, 299, 299, 3))
    targets.set_shape((None, num_classes))

    with tf.contrib.slim.arg_scope(inception.inception_v3_arg_scope()):
        _, end_points = inception.inception_v3(image, is_training=training, num_classes=num_classes)

        logits = end_points['Logits']
        predictions = end_points['Predictions']

    predictions_dict = {'predictions': predictions}

    loss = tf.losses.softmax_cross_entropy(targets, logits)
    eval_metric_ops = {
        'rmse': tf.metrics.root_mean_squared_error(targets, predictions)
    }

    train_op = tf.contrib.layers.optimize_loss(
        loss=loss,
        global_step=tf.contrib.framework.get_global_step(),
        learning_rate=params['learning_rate'],
        optimizer='Adam')

    return tfl.ModelFnOps(
        mode=mode,
        predictions=predictions_dict,
        loss=loss,
        train_op=train_op,
        eval_metric_ops=eval_metric_ops)


def input_fn(dataset_dir, params):
    pattern = os.path.join(dataset_dir, '*.jpg')
    filename_queue = tf.train.string_input_producer(
        tf.train.match_filenames_once(pattern),
        num_epochs=params['num_epochs'])

    image_reader = tf.WholeFileReader()
    _, image_file = image_reader.read_up_to(filename_queue, 10)

    image_batch = tf.train.shuffle_batch([image_file], params['batch_size'], 1000, 100,
                                         num_threads=2,
                                         enqueue_many=True,
                                         allow_smaller_final_batch=True)
    images, indices = tf.map_fn(image_fn, image_batch,
                                dtype=(tf.float32, tf.int32),
                                infer_shape=False)

    images = tf.reshape(images, [-1, 299, 299, 3])

    features = {'image': images}
    targets = tf.one_hot(indices=indices, depth=5, dtype=tf.float32)
    return features, targets


def image_fn(image_file):
    image = tf.image.decode_jpeg(image_file, channels=3)

    image = tf.expand_dims(image, 0)
    image = tf.image.resize_bilinear(image, [299, 299])
    image = tf.squeeze(image, [0])

    k = 2
    image = tf.image.rot90(image, k=k, name='rotate')
    image = tf.cast(image, tf.float32)
    image = tf.subtract(tf.divide(image, 128.), 1.)

    return image, k


if __name__ == '__main__':
    main()
```

Full output:

```
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpv1o9e6jk
2017-05-15 11:55:54.834870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-05-15 11:55:54.835292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.6325
pciBusID 0000:01:00.0
Total memory: 10.91GiB
Free memory: 9.24GiB
2017-05-15 11:55:54.835305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-05-15 11:55:54.835309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-05-15 11:55:54.835315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0)
2017-05-15 11:56:03.923926: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1546 get requests, put_count=1200 evicted_count=1000 eviction_rate=0.833333 and unsatisfied allocation rate=0.935317
2017-05-15 11:56:03.923951: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
2017-05-15 11:57:49.153141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0)
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
WARNING:tensorflow:From untitled.py:18: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with as_iterable is deprecated and will be removed after 2016-12-01.
Instructions for updating:
Estimator is decoupled from Scikit Learn interface by moving into
separate class SKCompat. Arguments x, y and batch_size are only
available in the SKCompat class, Estimator will only accept input_fn.
Example conversion:
  est = Estimator(...) -> est = SKCompat(Estimator(...))
Traceback (most recent call last):
  File ""untitled.py"", line 97, in <module>
  File ""untitled.py"", line 18, in main
    predictions = nn.predict(input_fn=lambda: input_fn('img', params=model_params), as_iterable=False)
  File ""/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 281, in new_func
    return func(*args, **kwargs)
  File ""/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 565, in predict
    as_iterable=as_iterable)
  File ""/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 857, in _infer_model
    infer_ops = self._get_predict_ops(features)
  File ""/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1187, in _get_predict_ops
    self._labels_info)
  File ""/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/tensor_signature.py"", line 164, in create_placeholders_from_signatures
    return signatures.get_placeholder()
  File ""/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/tensor_signature.py"", line 89, in get_placeholder
    shape=[None] + list(self.shape[1:]))
  File ""/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 478, in __iter__
    raise ValueError(""Cannot iterate over a shape with unknown rank."")
ValueError: Cannot iterate over a shape with unknown rank.
```"
9910,Issues with spark-tensorflow-connector,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.1.0 and 1.0.0
- **Bazel version (if compiling from source)**:NA
- **CUDA/cuDNN version**:None
- **GPU model and memory**:None
- **Exact command to reproduce**:

### Describe the problem
When using **spark-tensorflow-connector**, I run the given example and get the the TFRecord test-output.tfr. Then I want to use it in tensorflow, but some errors occur.
https://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-connector

### Source code / logs

scala code：
```scala
import org.apache.commons.io.FileUtils
import org.apache.spark.sql.{ DataFrame, Row }
import org.apache.spark.sql.catalyst.expressions.GenericRow
import org.apache.spark.sql.types._

val path = ""test-output.tfr""
val testRows: Array[Row] = Array(
new GenericRow(Array[Any](11, 1, 23L, 10.0F, 14.0, ""r1"")),
new GenericRow(Array[Any](21, 2, 24L, 12.0F, 15.0, ""r2"")))
val schema = StructType(List(StructField(""id"", IntegerType), 
                             StructField(""IntegerTypelabel"", IntegerType), 
                             StructField(""LongTypelabel"", LongType), 
                             StructField(""FloatTypelabel"", FloatType), 
                             StructField(""DoubleTypelabel"", DoubleType), 
                             StructField(""name"", StringType)))
                             
val rdd = spark.sparkContext.parallelize(testRows)

//Save DataFrame as TFRecords
val df: DataFrame = spark.createDataFrame(rdd, schema)
df.write.format(""tfrecords"").save(path)

//Read TFRecords into DataFrame.
//The DataFrame schema is inferred from the TFRecords if no custom schema is provided.
val importedDf1: DataFrame = spark.read.format(""tfrecords"").load(path)
importedDf1.show()

//Read TFRecords into DataFrame using custom schema
val importedDf2: DataFrame = spark.read.format(""tfrecords"").schema(schema).load(path)
importedDf2.show()
```

python code:
```python
import tensorflow as tf
print(tf.__version__)
fileNameQue = tf.train.string_input_producer([""/home/wj/test-output.tfr""])
reader = tf.TFRecordReader()
key, value = reader.read(fileNameQue)
features = tf.parse_single_example(value, features={
        'id': tf.FixedLenFeature([], tf.int64),
        'IntegerTypelabel': tf.FixedLenFeature([], tf.int64),
        'LongTypelabel': tf.FixedLenFeature([], tf.int64),
        'FloatTypelabel': tf.FixedLenFeature([], tf.float32),
        'DoubleTypelabel': tf.FixedLenFeature([], tf.float32),
        'name': tf.FixedLenFeature([], tf.string),
    })
# with tf.Session() as sess:
sess = tf.InteractiveSession()

coord=tf.train.Coordinator()
threads= tf.train.start_queue_runners(coord=coord)
#for i in range(2):
# a,b,c,d,e,f = sess.run([id,IntegerTypelabel,LongTypelabel,FloatTypelabel,DoubleTypelabel,name])
a = sess.run(features)
coord.request_stop()
coord.join(threads)
```
logs:
```
---------------------------------------------------------------------------
FailedPreconditionError                   Traceback (most recent call last)
<ipython-input-13-889826d0ffbd> in <module>()
      7 #for i in range(2):
      8 # a,b,c,d,e,f = sess.run([id,IntegerTypelabel,LongTypelabel,FloatTypelabel,DoubleTypelabel,name])
----> 9 a = sess.run(features)
     10 
     11 

/home/wj/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)
    765     try:
    766       result = self._run(None, fetches, feed_dict, options_ptr,
--> 767                          run_metadata_ptr)
    768       if run_metadata:
    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/home/wj/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)
    963     if final_fetches or final_targets:
    964       results = self._do_run(handle, final_targets, final_fetches,
--> 965                              feed_dict_string, options, run_metadata)
    966     else:
    967       results = []

/home/wj/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1013     if handle is None:
   1014       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
-> 1015                            target_list, options, run_metadata)
   1016     else:
   1017       return self._do_call(_prun_fn, self._session, handle, feed_dict,

/home/wj/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)
   1033         except KeyError:
   1034           pass
-> 1035       raise type(e)(node_def, op, message)
   1036 
   1037   def _extend_graph(self):

FailedPreconditionError: /home/wj/test-output.tfr
	 [[Node: ReaderReadV2 = ReaderReadV2[_device=""/job:localhost/replica:0/task:0/cpu:0""](TFRecordReaderV2, input_producer)]]

Caused by op u'ReaderReadV2', defined at:
  File ""/home/wj/anaconda2/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/home/wj/anaconda2/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py"", line 3, in <module>
    app.launch_new_instance()
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py"", line 653, in launch_instance
    app.start()
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py"", line 474, in start
    ioloop.IOLoop.instance().start()
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py"", line 162, in start
    super(ZMQIOLoop, self).start()
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py"", line 887, in start
    handler_func(fd_obj, events)
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 276, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 228, in dispatch_shell
    handler(stream, idents, msg)
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 390, in execute_request
    user_expressions, allow_stdin)
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py"", line 501, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2717, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2821, in run_ast_nodes
    if self.run_code(code, result):
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-6-7fd6f1c21551>"", line 1, in <module>
    key, value = reader.read(fileNameQue)
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py"", line 272, in read
    return gen_io_ops._reader_read_v2(self._reader_ref, queue_ref, name=name)
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 410, in _reader_read_v2
    queue_handle=queue_handle, name=name)
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/wj/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

FailedPreconditionError (see above for traceback): /home/wj/test-output.tfr
	 [[Node: ReaderReadV2 = ReaderReadV2[_device=""/job:localhost/replica:0/task:0/cpu:0""](TFRecordReaderV2, input_producer)]]
```"
9906,tf.import_graph_def() restricts the order of nodes in graph proto.,"-----------------------
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
commit 3bee923c9
- **Bazel version (if compiling from source)**:
0.4.5
- **CUDA/cuDNN version**:
cuda 8.0/cudnn 5.1.5
- **GPU model and memory**:
Tesla P40 
- **Exact command to reproduce**:

### Describe the problem
I used auto parallel optimizer in grappler for cifar10 example, but grappler changes the order of nodes in graph proto. It causes the failure of import_graph_def, because ops.set_shapes_for_outputs(op), which is line 407 of tensorflow/tensorflow/python/framework/importer.py is failed. This is because all of the input nodes must be defined before to define a node. importer restricts the order of nodes for graph definition proto, but I think importer shoud be flexible to the order of nodes. 

### Source code / logs
  ```
with tf.Graph().as_default() as graph:
    #global_step = tf.contrib.framework.get_or_create_global_step()
    global_step = tf.get_variable(
        'global_step', [], dtype=tf.int32,
        initializer=tf.constant_initializer(0), trainable=False)

    # Get images and labels for CIFAR-10.
    images, labels = cifar10.distorted_inputs()

    # Build a Graph that computes the logits predictions from the
    # inference model.
    logits = cifar10.inference(images)

    # Calculate loss.
    loss = cifar10.loss(logits, labels)

    # Build a Graph that trains the model with one batch of examples and
    # updates the model parameters.
    train_op = cifar10.train(loss, global_step)

    init_op = tf.global_variables_initializer()

    queue_runners = []
    for qr in ops.get_collection(ops.GraphKeys.QUEUE_RUNNERS):
      queue_runners.append(qr.to_proto())

    mg = meta_graph.create_meta_graph_def(graph=graph)

  #Auto-parallel
  rewriter_config = rewriter_config_pb2.RewriterConfig()
  rewriter_config.optimizers.append('autoparallel')
  rewriter_config.auto_parallel.num_replicas = FLAGS.num_gpus

  graph_def = tf_optimizer.OptimizeGraph(rewriter_config, mg)

  with tf.Graph().as_default() as g:
    tf.import_graph_def(graph_def=graph_def, name='')
```

This is the log
```
 File ""/cmsdata/ssd0/soojeong/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""tensorflow_rdag/examples/distributed/cifar10/cifar10_in_graph_auto_parallel.py"", line 157, in main
    train()
  File ""tensorflow_rdag/examples/distributed/cifar10/cifar10_in_graph_auto_parallel.py"", line 105, in train
    tf.import_graph_def(graph_def=graph_def, name='')
  File ""/cmsdata/ssd0/soojeong/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 404, in import_graph_def
    ops.set_shapes_for_outputs(op)
  File ""/cmsdata/ssd0/soojeong/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1723, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/cmsdata/ssd0/soojeong/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1673, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""/cmsdata/ssd0/soojeong/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.py"", line 610, in call_cpp_shape_fn
    debug_python_shape_fn, require_shape_fn)
  File ""/cmsdata/ssd0/soojeong/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.py"", line 653, in _call_cpp_shape_fn_impl
    v = tensor_util.constant_value(op.inputs[idx])
  File ""/cmsdata/ssd0/soojeong/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 710, in constant_value
    ret = _ConstantValue(tensor)
  File ""/cmsdata/ssd0/soojeong/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 676, in _ConstantValue
    fill_value = constant_value(tensor.op.inputs[1])
  File ""/cmsdata/ssd0/soojeong/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1443, in __getitem__
    return self._op._inputs[i]
IndexError: list index out of range

```"
9905,Wrong hyperlink in web page tutorial,"There is a wrong html link in text in the tutorial ""Vector Representations of Words"" at address https://www.tensorflow.org/tutorials/word2vec

The text is in the first paragraph after the bullet points in the Highlights section at the top of the page.

The link for:

**tensorflow_models/tutorials/embedding/word2vec.py** 
https://www.github.com/tensorflow/tensorflow/blob/r1.1/tensorflow_models/tutorials/embedding/word2vec.py

is wrong, it should point to the following address:

https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec.py

I hope that this is helpful."
9904,BeamSearchDecoder cell state never changed ,"tf version '1.1.0-rc2'
It looks BeamSearchDecoder never used next_cell_state, but always using the inital cell state.
In beam_search_decoder.py 423
        next_cell_state = nest.map_structure(
            self._maybe_split_batch_beams,
            next_cell_state, self._cell.state_size)
But next_cell_state is never used later, since just pass   state to _beam_search_step function where state.cell_state is never changed 
    beam_search_output, beam_search_state = _beam_search_step(
          time=time,
          logits=cell_outputs,
          beam_state=state,
          batch_size=batch_size,
          beam_width=beam_width,
          end_token=end_token,
          length_penalty_weight=length_penalty_weight)"
9903,Mac nightlies blocked by failing test in contrib,"Last Mac nightly is May 1st

Looking at the last failure it seems it was caused by contrib
//bazel_pip/tensorflow/contrib/batching:batch_ops_test                   FAILED in 1 out of 2 in 14.5s

Since /contrib/ packages don't always have maintainers, should the ""Success"" criteria perhaps be relaxed to still release the pip package if some /contrib/ thing fail? @martinwicke 

https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=mac-slave/

"
9902,Session#run method's feed_dict argument implicitly converts types,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes?
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.12.3
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.1.0
- **Bazel version (if compiling from source)**: NA
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: (see below)

### Describe the problem
Hi TensorFlow humans! Thanks so much for making TensorFlow!

Right now, if you feed a floating point array into a integral placeholder type, it will be converted implicitly. To my knowledge, most python operations will not implicitly convert.

The implicit conversion potentially creates convenience, but of course it also creates the opportunity for a hard-to-see bug. In my case, I lost about 1 day to find this bug and experienced great sadness. That probably says more about me than it does about TF.

Still, my feeling is that it is a more sensible default to require the user to do the conversion explicitly. Alternatively, perhaps it would be logical to log a warning to the user.

Note that, to my knowledge, TF operations like `tf.equal` require both tensors to have the same type. So users might have a belief that that TF and `Session#run` require them to be fairly explicit about types.

If it would be likely to be accepted, I am happy to write a patch for TF that warns the user when they feed a tensor of the wrong type.

Thanks for reading this issue!

### Source code / logs

```
import numpy as np
import tensorflow as tf

session = tf.Session()
convert_implicitly = tf.placeholder(tf.int64, [None])
float_input = np.random.uniform(size = 10)
result = session.run(
    convert_implicitly,
    feed_dict = { convert_implicitly: float_input }
)
print(result)
```

**Output**

```
>> python test.py
2017-05-14 18:20:22.524862: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-14 18:20:22.524890: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-14 18:20:22.524906: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine andcould speed up CPU computations.
[0 0 0 0 0 0 0 0 0 0]
```
"
9901,tf.gradients runtime scales suboptimally with size of the graph,"`tf.gradients` can be inefficient on large graphs and it runtime increases with size of the graph, even when the amount of work it needs to do is constant. This inefficiency is apparent when trying to differentiate small parts of large graph many times.

Discovered when trying to scale to 8 GPUs using data parallelism using 8 identical copies of model -- time spent inside gradients grows for each new replica even though replicas are identical and independent. We are calling `tf.gradients` many times (calling tf.gradients on parts of model in order to do memory saving gradients [trick](https://arxiv.org/abs/1604.06174)), our largest models spend >2 hours inside `tf.gradients`.

I've profiled the runs and saw that most of the time is spent inside

`_MarkReachedOps(from_ops, reached_ops)` inside `gradients_impl.py`

It's called as follows

```
  reached_ops = [False] * (graph._last_id + 1)
  for op in to_ops:
    reached_ops[op._id] = True
```
You can see that it's using Python list initialized with the size of the entire graph so this initialization step would grow with size of the graph.

![screenshot 2017-05-14 15 35 58](https://cloud.githubusercontent.com/assets/23068/26038340/102545da-38bb-11e7-873b-b57bc628ae7c.png)


Profile of the `_MarkReachedOps` when calling when calling tf gradients 560 times, with each gradient call adding 35 nodes on average, and total size of the graph being 200k nodes

```
Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   101                                           @profile
   102                                           def _MarkReachedOps(from_ops, reached_ops):
   103                                             """"""Mark all ops reached from ""from_ops"".
   104                                           
   105                                             Args:
   106                                               from_ops: list of Operations.
   107                                               reached_ops: list of booleans, indexed by operation id.
   108                                             """"""
   109       568         1967      3.5      0.0    queue = collections.deque()
   110       568         4835      8.5      0.0    queue.extend(from_ops)
   111  39912648     14661885      0.4      8.4    while queue:
   112  39912080     17079278      0.4      9.8      op = queue.popleft()
   113  39912080     41203709      1.0     23.7      if not reached_ops[op._id]:
   114  28997056     21267924      0.7     12.2        reached_ops[op._id] = True
   115  58483932     42196549      0.7     24.2        for output in op.outputs:
   116  29486876     37595214      1.3     21.6          queue.extend(output.consumers())
```

Possible solutions could be a more efficient implementation of `_PendingCount`, or a different algorithm for `tf.gradients` which is more efficient for large graphs"
9900,DNNClassifier and GridSearchCV example,"Hi , 

I am trying to run an gridsearch example in [https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/contrib/learn/python/learn/tests/test_grid_search.py](url)
If example inside testIrisDNN intend to be working ?

I am getting 

> TypeError: __init__() got an unexpected keyword argument 'params'"
9898,Support for float16 in tf.layers,"It seems like `tf.layers.*` do not support `tf.float16`, because they rely on the default value handed to the `_Layer` superclass [here](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/python/layers/base.py#L64). It would be great if tf.float16 could be supported there."
9896,Little issues in documentation,"What is the proper forum for contributing / discussing documentation errors?

There are dead links on this page: https://www.tensorflow.org/tutorials/deep_cnn

and in the ""Manual Device Placement"" section of this page https://www.tensorflow.org/tutorials/using_gpu
a, b and the result of the matmul should all be on the cpu, but the output in the docs shows that the matmul is done on the gpu.

```
Device mapping:
/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: Tesla K40c, pci bus
id: 0000:05:00.0
b: /job:localhost/replica:0/task:0/cpu:0
a: /job:localhost/replica:0/task:0/cpu:0
MatMul: /job:localhost/replica:0/task:0/gpu:0
[[ 22.  28.]
 [ 49.  64.]]

```

I'm working through a bunch of the tutorials today and if I find any issues I can let you know via the appropriate forum if you want. It's hard to keep up with the documentation!"
9895,"XLA ""Aborted (core dumped)""","OS: Ubuntu/Linux (16.04)
TensorFlow: Compiled from source
TensorFlow Version: r1.1
Bazel Version: 0.4.5
CUDA/CuDNN Versions: 8.0/5.1
GPU Model/Memory: TitanX/12Gb

After turning on XLA JIT compiling, TF fails with a core dump.

```
2017-05-14 14:50:38.673877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:06:00.0
Total memory: 11.90GiB
Free memory: 11.75GiB
2017-05-14 14:50:38.673951: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x3cb6960
2017-05-14 14:50:38.900484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 1 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:05:00.0
Total memory: 11.90GiB
Free memory: 11.67GiB
2017-05-14 14:50:38.901416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 
2017-05-14 14:50:38.901427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y Y 
2017-05-14 14:50:38.901430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   Y Y 
2017-05-14 14:50:38.901437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:06:00.0)
2017-05-14 14:50:38.901441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: TITAN X (Pascal), pci bus id: 0000:05:00.0)
2017-05-14 14:50:38.979726: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 2 visible devices
2017-05-14 14:50:38.979748: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 12 visible devices
2017-05-14 14:50:38.981294: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x4a7a7b0 executing computations on platform Host. Devices:
2017-05-14 14:50:38.981305: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>
2017-05-14 14:50:38.981438: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 2 visible devices
2017-05-14 14:50:38.981446: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 12 visible devices
2017-05-14 14:50:38.982608: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x4a42c80 executing computations on platform CUDA. Devices:
2017-05-14 14:50:38.982619: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): TITAN X (Pascal), Compute Capability 6.1
2017-05-14 14:50:38.982623: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (1): TITAN X (Pascal), Compute Capability 6.1
2017-05-14 14:51:53.443289: F tensorflow/compiler/xla/service/algebraic_simplifier.cc:768] Check failed: user->operand(reshape_or_broadcast_operand_index) == reshape_or_broadcast (0x7f30bc938e70 vs. 0x7f30bcb3e340)
Aborted (core dumped)
```
"
9894,Computing gradient when using tf.split generates console log,"### System information
- **Have I written custom code**: No
- **OS Platform and Distribution**: Arch Linux
- **TensorFlow installed from**: source
- **TensorFlow version**: 1.1.0
- **CUDA/cuDNN version**: 8, 6
- **GPU model and memory**: Titan X, 12 GB

### Describe the problem
When computing gradient and using tf.split with size_splits (not num_splits), tensorflow generates console log

### Source code / logs

```

import tensorflow as tf

SIZE = 1

x = tf.random_normal([1, 1])
w = tf.get_variable(""w"", [1, SIZE*2])
g = tf.matmul(x, w)
s1, s2 = tf.split(g, [SIZE, SIZE], 1) # CONSOLE MESSAGE
# s1, s2 = tf.split(g, 2, 1) # NO CONSOLE MESSAGE
y = s1 * s2

loss = tf.reduce_mean(tf.square(x - y))
train_op = tf.train.AdamOptimizer().minimize(loss)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(train_op)

```
Console message looks like:
[<tf.Tensor 'gradients/split_grad/concat:0' shape=(1, 2) dtype=float32>, None, None]"
9893,tf.where bug,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.1.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:cuda 8.0 cuDNN 5.0
- **GPU model and memory**: 1060 6GB
- **Exact command to reproduce**:

```
import tensorflow as tf
import numpy as np

if __name__ == '__main__':
    bool_lists = np.array([[False, True, True, False],
                           [False, False, True, True]])
    k = tf.Variable(tf.zeros(shape=[2, 4], dtype=tf.int32))
    where_val = []
    bool_ops = []
    k_opes = []
    where_ops = []
    for j in range(bool_lists.shape[0]):
        for i in range(bool_lists.shape[1]):
            bool_i = tf.constant(bool_lists[j, i], dtype=tf.bool)
            bool_ops.append(bool_i)
            where_val.append(k[j, i].assign(tf.where(bool_lists[j, i], i, k[j, i])))
            tf_i = tf.constant(i, dtype=tf.int32)
            where_ops.append(tf.where(bool_i, tf_i, k[j, i]))
            k_opes.append(k[j, i])
    with tf.control_dependencies(where_val):
        k = tf.identity(k)
```

The results of above code is:
>[[0 1 2 3]
 [0 1 2 3]]
[False, True, True, False, False, False, True, True]
[0, 1, 2, 3, 0, 1, 2, 3]
[0, 1, 2, 3, 0, 1, 2, 3]

I think the right result of k's value should be:
>[[0 1 2 0]
 [0 0 2 3]]

I just update my tensorflow for 1.0 to 1.1.0. I remember  version 1.0 is right.

The following is results from tf 1.0:
>[[0 1 2 0]
 [0 0 2 3]]
[False, True, True, False, False, False, True, True]
[0, 1, 2, 0, 0, 0, 2, 3]
[0, 1, 2, 0, 0, 0, 2, 3]"
9892,tf.while_loop runs very slow,"I'm writing a simple RNN implementation using tf.while_loop but it runs incredibly slow. Any insights would be incredibly helpful.

OS: Ubuntu/Linux (16.04)
TensorFlow: Compiled from source
TensorFlow Version: r1.1
Bazel Version: 0.4.5
CUDA/CuDNN Versions: 8.0/5.1
GPU Model/Memory: TitanX/12Gb

Here's the implementation:
```python
def forward(self, inputs):
    inputs_shape = tf.shape(inputs)
    timesteps = inputs_shape[0]
    batch_size = inputs_shape[1]
    # Compute the forward pass for every time step.
    output = tf.reshape(inputs, [-1, self._inputs_dim])
    output = tf.add(tf.matmul(output, self._weights), self._bias)
    output = tf.reshape(output, [timesteps, batch_size, self._units])
    # Create a tensor array to hold the output of our rnn layer.
    temp = tf.TensorArray(dtype=self._dtype, size=timesteps)

    def step(c, t, h, s, i, o):
      h = self._activation(tf.add(tf.matmul(h, s), i[c]))
      o = o.write(c, h)
      return [c + 1, t, h, s, i, o]

    def step_condition(c, t, h, s, i, o):
      return tf.less(c, t)

    # Create a counter to track the number of timesteps.
    count = tf.constant(0)
    # Define the initial hidden state.
    hidden = tf.zeros([batch_size, self._units], dtype=self._dtype)

    _, _, _, _, _, output = tf.while_loop(
      step_condition,
      step,
      [count, timesteps, hidden, self._state, output, temp]
    )

    return output.stack()
```

It takes ~5 minutes to complete one epoch of the mnist dataset."
9891,tf.random_crop  assert erroneously,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.1.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:8.0
- **GPU model and memory**:TitanX Geforce 16G 
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I use `tf.random_crop` for image augmentation, the original image was ""232x232x3"" rgb image. when I try to do random_crop it to ""224x224x3"",  the assert occurs  as following, seems very strange:
""InvalidArgumentError (see above for traceback): assertion failed: [Need value.shape >= size, got ] [232 232 3] [224 224 3]""  

### Source code / logs
```python
    def img_augmentation(self, image_tensor):
        # resize image
        resize_image = tf.image.resize_images(
             image_tensor, [self.IMAGE_HEIGHT, self.IMAGE_WIDTH], method=0, align_corners=False)
        padded_image = tf.image.pad_to_bounding_box(
            resize_image, 4, 4, self.IMAGE_HEIGHT+8, self.IMAGE_WIDTH+8)
        print ""padded_image shape:"", padded_image.get_shape()
        # random crop image
        distorted_image = tf.random_crop(
            padded_image,  [self.IMAGE_HEIGHT, self.IMAGE_WIDTH, self.NUM_CHANNELS])
```

logs:
```
Caused by op u'random_crop/Assert/Assert', defined at:
  File ""model.py"", line 315, in <module>
    m.start_train()
  File ""model.py"", line 236, in start_train
    ins = ImageLabelInputStreams(self.graph,self.config)
  File ""/home/guoqingpei/Project/EXPERIMENT/model/inputPipeline.py"", line 62, in __init__
    train_image = self.img_augmentation(train_image)
  File ""/home/guoqingpei/Project/EXPERIMENT/model/inputPipeline.py"", line 132, in img_augmentation
    padded_image,  [self.IMAGE_HEIGHT, self.IMAGE_WIDTH, self.NUM_CHANNELS])
  File ""/home/guoqingpei/guoqp/local/lib/python2.7/site-packages/tensorflow/python/ops/random_ops.py"", line 303, in random_crop
    [""Need value.shape >= size, got "", shape, size])
  File ""/home/guoqingpei/guoqp/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 121, in Assert
    condition, data, summarize, name=""Assert"")
  File ""/home/guoqingpei/guoqp/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_logging_ops.py"", line 39, in _assert
    summarize=summarize, name=name)
  File ""/home/guoqingpei/guoqp/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 768, in apply_op
    op_def=op_def)
  File ""/home/guoqingpei/guoqp/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2336, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/guoqingpei/guoqp/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1228, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): assertion failed: [Need value.shape >= size, got ] [232 232 3] [224 224 3]
	 [[Node: random_crop/Assert/Assert = Assert[T=[DT_STRING, DT_INT32, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/cpu:0""](random_crop/All/_11, random_crop/Assert/Assert/data_0, random_crop/Shape/_13, random_crop/size/_15)]]
	 [[Node: random_crop/Assert/Assert/_18 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_42_random_crop/Assert/Assert"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
```"
9890,terminate called after throwing an instance of 'std::bad_alloc' what():  std::bad_alloc  Aborted (core dumped),"Hi everyone, I was learning the tutorials of mnist based on tensorflow.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_deep.py

I ran the py file and got  this problem.

please give me a hand.

Mocayo, thanks."
9881,how can creat the module clstm((convolution Long short-term memory)) in tensorflow,"I wish to implement clstm's network architecture in the tensorflow, to facilitate extension and modifications to the network. already create in module?"
9880,Building a Convolutional Neural Network,"### Describe the problem
I just completed the implementation of A Guide to TF Layers: Building a Convolutional Neural Network for the MNIST data set. The training model successfully ran and gave accuracy of 97.3%.

However, the tutorial does not mention how to use this new trained model to supply own images and see the predictions. Does anyone know how to use the output of the training model to make predictions? I see in the tmp/mnist_convnet_model$ folder, there are some output files like  .pbtxt , meta files and index files. But I can't find instructions to use them for making predictions on my own images.
"
9878,Does tensorflow support forward multiple times then backward once like average_loss in Caffe ?,"Hi, everyone. In my training tasks, the batch size is expected to be  large My training task is a little special that costs a lot of GPU memory. Now only set batch size = 6 almost run out of entire GPU memory 11GB GTX 1080Ti. However, I need to enlarge my batch size. In Caffe Platform, there is an **average_loss**  setting in solver.prototxt to perform forward multiple times and compute the average loss, then backward once. I scaned the documentation of tensorflow but there is no operation can implement such function. Does anyone have any ideas or how to use the existing op to implement this process?  Thanks

Following is my system info:
GTX1080Ti
CUDA version: 8.0
cuDNN:6.0
Tensorflow: r1.0.1
Ubuntu 16.04"
9873,Can't not find the tutorial code for Convolutional Neural network,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
The tutorial here: https://www.tensorflow.org/tutorials/deep_cnn, the code can't find in github

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
9872,www.tensorflow.org/versions/ incorrectly lists 0.12 as the most recent stable branch,"https://www.tensorflow.org/versions/ says that 0.12 is most recent stable branch, which is causing new users to install the older version --
https://github.com/tensorflow/tensorflow/issues/9590#issuecomment-301211285"
9871,errors in ipython sessions cause core dump or segfault,"------------------------

### System information

Python 2.7.13 :: Anaconda custom (64-bit)
ipython :: 5.3.0
linux :: 3.16.0-4-amd64


https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh gives:

== cat /etc/issue ===============================================
Linux leto26 3.16.0-4-amd64 #1 SMP Debian 3.16.43-2 (2017-04-30) x86_64 GNU/Linux
VERSION_ID=""8""
VERSION=""8 (jessie)""

== are we in docker =============================================
No

== compiler =====================================================
c++ (Debian 4.9.2-10) 4.9.2
Copyright (C) 2014 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux leto26 3.16.0-4-amd64 #1 SMP Debian 3.16.43-2 (2017-04-30) x86_64 GNU/Linux

== check pips ===================================================
msgpack-numpy (0.3.9)
numpy (1.12.1)
numpydoc (0.6.0)
protobuf (3.2.0)
tensorflow-gpu (1.0.1)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.0.1
tf.GIT_VERSION = v1.0.0-65-g4763edf-dirty
tf.COMPILER_VERSION = v1.0.0-65-g4763edf-dirty
Sanity check: array([1], dtype=int32)
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally

== env ==========================================================
LD_LIBRARY_PATH /Tmp/lisa/os_v5/cudnn_v5.1:/Tmp/lisa/os_v5/nccl_8/lib:/Tmp/lisa/os_v5/lib:/Tmp/lisa/os_v5/lib64:/usr/local/lib:/usr/lib64/atlas:/Tmp/lisa/os_v5/lib32:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri May 12 19:38:18 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1050    Off  | 0000:05:00.0      On |                  N/A |
| 61%   67C    P0    36W /  75W |    145MiB /  1998MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  TITAN X (Pascal)    Off  | 0000:06:00.0     Off |                  N/A |
| 54%   84C    P2    68W / 250W |  11530MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  TITAN X (Pascal)    Off  | 0000:09:00.0     Off |                  N/A |
| 45%   78C    P2    85W / 250W |  11820MiB / 12189MiB |     35%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1518    G   /usr/bin/X                                     141MiB |
|    1     20850    C   python                                       11527MiB |
|    2     18363    C   python                                       11817MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7
/usr/local/cuda-7.5/doc/man/man7/libcudart.7
/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18
/usr/local/cuda-7.5/lib64/libcudart_static.a
/usr/local/cuda-7.5/lib/libcudart.so.7.5.18
/usr/local/cuda-7.5/lib/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
/usr/local/cuda-8.0/lib64/libcudart_static.a

== cat /etc/issue ===============================================
Linux leto26 3.16.0-4-amd64 #1 SMP Debian 3.16.43-2 (2017-04-30) x86_64 GNU/Linux
VERSION_ID=""8""
VERSION=""8 (jessie)""

== are we in docker =============================================
No

== compiler =====================================================
c++ (Debian 4.9.2-10) 4.9.2
Copyright (C) 2014 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux leto26 3.16.0-4-amd64 #1 SMP Debian 3.16.43-2 (2017-04-30) x86_64 GNU/Linux

== check pips ===================================================
msgpack-numpy (0.3.9)
numpy (1.12.1)
numpydoc (0.6.0)
protobuf (3.2.0)
tensorflow-gpu (1.0.1)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.0.1
tf.GIT_VERSION = v1.0.0-65-g4763edf-dirty
tf.COMPILER_VERSION = v1.0.0-65-g4763edf-dirty
Sanity check: array([1], dtype=int32)
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally

== env ==========================================================
LD_LIBRARY_PATH /Tmp/lisa/os_v5/cudnn_v5.1:/Tmp/lisa/os_v5/nccl_8/lib:/Tmp/lisa/os_v5/lib:/Tmp/lisa/os_v5/lib64:/usr/local/lib:/usr/lib64/atlas:/Tmp/lisa/os_v5/lib32:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri May 12 19:38:27 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1050    Off  | 0000:05:00.0      On |                  N/A |
| 61%   67C    P0    36W /  75W |    145MiB /  1998MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  TITAN X (Pascal)    Off  | 0000:06:00.0     Off |                  N/A |
| 54%   84C    P2    68W / 250W |  11530MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  TITAN X (Pascal)    Off  | 0000:09:00.0     Off |                  N/A |
| 46%   78C    P2   155W / 250W |  11820MiB / 12189MiB |     38%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1518    G   /usr/bin/X                                     141MiB |
|    1     20850    C   python                                       11527MiB |
|    2     18363    C   python                                       11817MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7
/usr/local/cuda-7.5/doc/man/man7/libcudart.7
/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18
/usr/local/cuda-7.5/lib64/libcudart_static.a
/usr/local/cuda-7.5/lib/libcudart.so.7.5.18
/usr/local/cuda-7.5/lib/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
/usr/local/cuda-8.0/lib64/libcudart_static.a

== cat /etc/issue ===============================================
Linux leto06 3.16.0-4-amd64 #1 SMP Debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 GNU/Linux
VERSION_ID=""8""
VERSION=""8 (jessie)""

== are we in docker =============================================
No

== compiler =====================================================
c++ (Debian 4.9.2-10) 4.9.2
Copyright (C) 2014 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux leto06 3.16.0-4-amd64 #1 SMP Debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 GNU/Linux

== check pips ===================================================
msgpack-numpy (0.3.9)
numpy (1.12.1)
numpydoc (0.6.0)
protobuf (3.2.0)
tensorflow-gpu (1.0.1)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.0.1
tf.GIT_VERSION = v1.0.0-65-g4763edf-dirty
tf.COMPILER_VERSION = v1.0.0-65-g4763edf-dirty
Sanity check: array([1], dtype=int32)
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally

== env ==========================================================
LD_LIBRARY_PATH /Tmp/lisa/os_v5/nccl_8/lib:/Tmp/lisa/os_v5/cudnn_v5.1:/Tmp/lisa/os_v5/nccl_8/lib:/Tmp/lisa/os_v5/lib:/Tmp/lisa/os_v5/lib64:/usr/local/lib:/usr/lib64/atlas:/Tmp/lisa/os_v5/lib32:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri May 12 19:40:02 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 367.44                 Driver Version: 367.44                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX TIT...  Off  | 0000:02:00.0     Off |                  N/A |
| 22%   46C    P8    18W / 250W |      1MiB / 12206MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-7.5/lib/libcudart.so.7.5.18
/usr/local/cuda-7.5/lib/libcudart_static.a
/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18
/usr/local/cuda-7.5/lib64/libcudart_static.a
/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7
/usr/local/cuda-7.5/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7

== cat /etc/issue ===============================================
Linux leto06 3.16.0-4-amd64 #1 SMP Debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 GNU/Linux
VERSION_ID=""8""
VERSION=""8 (jessie)""

== are we in docker =============================================
No

== compiler =====================================================
c++ (Debian 4.9.2-10) 4.9.2
Copyright (C) 2014 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux leto06 3.16.0-4-amd64 #1 SMP Debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 GNU/Linux

== check pips ===================================================
msgpack-numpy (0.3.9)
numpy (1.12.1)
numpydoc (0.6.0)
protobuf (3.2.0)
tensorflow-gpu (1.0.1)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.0.1
tf.GIT_VERSION = v1.0.0-65-g4763edf-dirty
tf.COMPILER_VERSION = v1.0.0-65-g4763edf-dirty
Sanity check: array([1], dtype=int32)
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally

== env ==========================================================
LD_LIBRARY_PATH /Tmp/lisa/os_v5/nccl_8/lib:/Tmp/lisa/os_v5/cudnn_v5.1:/Tmp/lisa/os_v5/nccl_8/lib:/Tmp/lisa/os_v5/lib:/Tmp/lisa/os_v5/lib64:/usr/local/lib:/usr/lib64/atlas:/Tmp/lisa/os_v5/lib32:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri May 12 19:40:54 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 367.44                 Driver Version: 367.44                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX TIT...  Off  | 0000:02:00.0     Off |                  N/A |
| 22%   47C    P8    18W / 250W |      1MiB / 12206MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-7.5/lib/libcudart.so.7.5.18
/usr/local/cuda-7.5/lib/libcudart_static.a
/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18
/usr/local/cuda-7.5/lib64/libcudart_static.a
/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7
/usr/local/cuda-7.5/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7




python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"" gives
('v1.0.0-65-g4763edf-dirty', '1.0.1')



### Describe the problem
When I run tensorflow code (e.g. keras's mnist_cnn.py) in ipython, errors and Ctrl+C often crashes the interactive ipython session with a core dump or segfault, e.g....


### Source code / logs

In [1]: run mnist_cnn.py 
Using TensorFlow backend.
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
x_train shape: (60000, 1, 28, 28)
60000 train samples
10000 test samples
/u/kruegerd/python_modules/keras/keras/backend/tensorflow_backend.py:2252: UserWarning: Expected no kwargs, you passed 1
kwargs passed to function are ignored with Tensorflow backend
  warnings.warn('\n'.join(msg))
Train on 60000 samples, validate on 10000 samples
Epoch 1/12
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:02:00.0
Total memory: 11.92GiB
Free memory: 11.81GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)
23680/60000 [==========>...................] - ETA: 10s - loss: 0.5787 - acc: 0.8213^C---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
/u/kruegerd/python_modules/keras/examples/mnist_cnn.py in <module>()
     65           epochs=epochs,
     66           verbose=1,
---> 67           validation_data=(x_test, y_test))
     68 score = model.evaluate(x_test, y_test, verbose=0)
     69 print('Test loss:', score[0])

/u/kruegerd/python_modules/keras/keras/models.pyc in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)
    863                               class_weight=class_weight,
    864                               sample_weight=sample_weight,
--> 865                               initial_epoch=initial_epoch)
    866 
    867     def evaluate(self, x, y, batch_size=32, verbose=1,

/u/kruegerd/python_modules/keras/keras/engine/training.pyc in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)
   1499                               val_f=val_f, val_ins=val_ins, shuffle=shuffle,
   1500                               callback_metrics=callback_metrics,
-> 1501                               initial_epoch=initial_epoch)
   1502 
   1503     def evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None):

/u/kruegerd/python_modules/keras/keras/engine/training.pyc in _fit_loop(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)
   1153                 batch_logs['size'] = len(batch_ids)
   1154                 callbacks.on_batch_begin(batch_index, batch_logs)
-> 1155                 outs = f(ins_batch)
   1156                 if not isinstance(outs, list):
   1157                     outs = [outs]

/u/kruegerd/python_modules/keras/keras/backend/tensorflow_backend.pyc in __call__(self, inputs)
   2229         session = get_session()
   2230         updated = session.run(self.outputs + [self.updates_op],
-> 2231                               feed_dict=feed_dict)
   2232         return updated[:len(self.outputs)]
   2233 

/u/kruegerd/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)
    765     try:
    766       result = self._run(None, fetches, feed_dict, options_ptr,
--> 767                          run_metadata_ptr)
    768       if run_metadata:
    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/u/kruegerd/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)
    963     if final_fetches or final_targets:
    964       results = self._do_run(handle, final_targets, final_fetches,
--> 965                              feed_dict_string, options, run_metadata)
    966     else:
    967       results = []

/u/kruegerd/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1013     if handle is None:
   1014       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
-> 1015                            target_list, options, run_metadata)
   1016     else:
   1017       return self._do_call(_prun_fn, self._session, handle, feed_dict,

/u/kruegerd/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)
   1020   def _do_call(self, fn, *args):
   1021     try:
-> 1022       return fn(*args)
   1023     except errors.OpError as e:
   1024       message = compat.as_text(e.message)

/u/kruegerd/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1002         return tf_session.TF_Run(session, options,
   1003                                  feed_dict, fetch_list, target_list,
-> 1004                                  status, run_metadata)
   1005 
   1006     def _prun_fn(session, handle, feed_dict, fetch_list):

KeyboardInterrupt: 

Fatal Python error: GC object already tracked
Aborted (core dumped)

"
9870,Incorrect inference in convnet with batch size >65535,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.1.0-rc0-61-g1ec6ed5 1.1.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8.0.44
- **GPU model and memory**: GTX 1080, 16GB
- **Exact command to reproduce**: See source code below


### Describe the problem

When inference is performed on a convolutional NN with batch sizes larger than 65,535, the outputs on cases after 65,535 are incorrect.  The convolution operation appears to be important because I have not been able to reproduce this problem with a fully connected NN.

In the source code below, I am training a convnet with one convolutional layer to distinguish between sine waves of two periods.  If I perform inference on a batch size of 100,000, I find that the output for case number 65,535 is correct, but the output for case number 65,536 is zero.

### Source code / logs

```
import tensorflow as tf
import numpy as np

n_features = 80
def gen_data(size):
  '''Generate sine waves of one of two periods.'''
  labels = np.random.randint(0, 2, size)
  periods = np.zeros(size)
  periods[labels == 0] = 1
  periods[labels == 1] = 10

  x = np.arange(0, n_features)
  data = np.zeros((size, n_features))
  for i in range(size):
    data[i] = np.sin(x / periods[i])

    data = np.expand_dims(data, axis=2)
    return (data, labels)

X = tf.placeholder(tf.float32, [None, n_features, 1])
Y = tf.placeholder(tf.int32, [None])

W_conv = tf.get_variable('W_conv', [8, 1, 16])
preact = tf.nn.conv1d(X, W_conv, stride=1, padding='SAME')
pooled = tf.nn.pool(preact, window_shape=[2], pooling_type='MAX', padding='SAME', strides=[2])
conv_output = tf.contrib.layers.flatten(tf.nn.relu(pooled))

W_out = tf.get_variable('W_out', [int(n_features/2) * 16, 1])
logits = tf.squeeze(tf.matmul(conv_output, W_out))

xentropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(
    labels=tf.cast(Y, tf.float32), logits=logits))
train_step = tf.train.AdamOptimizer(1e-2).minimize(xentropy)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

# Train a little while with small batches
for i in range(1000):
  data, labels = gen_data(size=128)
  sess.run(train_step, feed_dict={X: data, Y: labels})

# Now try performing inference on a much larger batch
large_batch = np.zeros((100000, n_features, 1))
large_batch[2**16 - 1] = data[0]
logits_ = sess.run(logits, feed_dict={X: large_batch})

# This should be some non-zero number
print('Logit for input number 65,535:', logits_[2**16-1])

# Now do the same thing, but putting the input in spot 65,536
large_batch = np.zeros((100000, n_features, 1))
large_batch[2**16] = data[0]
logits_ = sess.run(logits, feed_dict={X: large_batch})

# This is now zero on my system
print('Logit for input number 65,536:', logits_[2**16])
```"
9868,Chrome timeline for Keras?,"I've been looking at the chrome timeline to profile my Keras models, but have been unable to find any documentation on how I could use it with my Keras models.
I am running keras with a tf backend, and my sequential models are all built in keras. For instance, this is how my model is being built:


    'model.fit_generator( \
				generator= data_gen(args, 1), \
			 	steps_per_epoch=tr_steps, \
			 	epochs=args.epochs, \
			 	validation_data=data_gen(args, 2), \
			 	validation_steps=val_steps, \
			 	verbose=1, \
			 	callbacks=[checkpointer])`


 


I am at a loss for how I would try to generate a timeline trace for this model, and wanted to know if there is any related documentation for how I can do this?"
9867,distributed runtime leaking memory on windows,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: b'unknown' 1.1.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: follow https://www.tensorflow.org/deploy/distributed

### Describe the problem
The memory footprint for parameter server keep growing.
The cause is a memory leak in a windows specific path in grpc which is fixed here:
https://github.com/grpc/grpc/commit/fa242cba900ece728d2910e7396d02ebab4ddb2c
I filed the issue issue so others don't need to spend the time debugging it and as reason to update the grpc version.
"
9866,Issue with tensor flow (undefined symbol: cuDevicePrimaryCtxRetain),"When I try to import tensorflow in python I get this issue:
Note: It was working fine yesterday.  It is really strange that today it no longer works.
Also note: This is my first time to ever post a question online, as I prefer to find answers to my own problems.  The issue here is that there is no documentation on this problem online and I am hitting a wall on ideas for once for how to resolve it.  I am using TF without gpu support, but am hopeful the capable team on this git can at least tell me if the problem on my end or not.  I realize it is likely something on my end, but a response to verify that would be helpful, even if the problem is perceived as too simple to bother with.

>>> import tensorflow as tf
Traceback (most recent call last):
  File ""/home/student/miniconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/student/miniconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/student/miniconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/student/miniconda3/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/student/miniconda3/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /home/student/miniconda3/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: cuDevicePrimaryCtxRetain

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/student/miniconda3/lib/python3.5/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/student/miniconda3/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 51, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/student/miniconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/student/miniconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/student/miniconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/student/miniconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/student/miniconda3/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/student/miniconda3/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /home/student/miniconda3/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: cuDevicePrimaryCtxRetain
"
9865,Issue with TensorFlow,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
9863,Bug in census_widendeep.py downloading of test data,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: nightly builds
- **TensorFlow version (use command below)**: 1.1.0-rc2

### Describe the problem
There seems to be an error in the downloading of the test data in the distributed training example `census_widendeep.py`. The test data is loaded from the file before the file is actually downloaded (lines 152-153):

```python
test_file = open(test_file_path)
urllib.urlretrieve(test_data_url, test_file_path)
```
"
9862,output of tf.nn.convolution is not consistent for bigger 3d nets,"Code :
```python
import tensorflow as tf
def Conv3D(x, filter_shape, stride = 1, padding = 'VALID', dilation_rate = (1,1,1)):
      W = tf.get_variable(name = 'weights', shape = filter_shape, initializer = tf.contrib.layers.xavier_initializer())
      b = tf.get_variable(name = 'bias', shape = filter_shape[4], initializer = tf.constant_initializer(0.1))
      conv_out = tf.nn.convolution(x, W, b, padding = padding, strides = [1,1,1], dilation_rate = dilation_rate)
     ret_val = tf.nn.bias_add(conv_out,b)
     return ret_val

def MaxPool3D(x, dilation_rate = (1,1,1)):
      ret_val = tf.nn.pool(x, window_shape = [2,2,2], pooling_type = 'MAX', padding = 'VALID', dilation_rate = dilation_rate, strides = [1,1,1])
      return ret_val


layer = {}
dilation = 1
inputs = tf.placeholder(tf.float32, (None, 80, 80, 80, 1))
layer[1] = Conv3D(inputs, [3,3,3,1,128], dilation_rate = (dilation, dilation, dilation))
layer[2] = Conv3D(layer[1], [3,3,3,128,64], dilation_rate = (dilation, dilation, dilation))
layer[3] = MaxPool3D(layer[2], dilation_rate = (dilation, dilation, dilation))
dilation = 2*dilation
layer[4] = Conv3D(layer[3], [3,3,3,64,128], dilation_rate = (dilation, dilation, dilation))
layer[5] = MaxPool3D(layer[4], dilation_rate = (dilation, dilation, dilation))
dilation = 2*dilation
layer[6] = Conv3D(layer[5], [3,3,3,128, 256], dilation_rate = (dilation, dilation, dilation))
layer[7] = MaxPool3D(layer[6], dilation_rate = (dilation, dilation, dilation))
dilation = 2*dilation
layer[8] = Conv3D(layer[7], [3,3,3,256,64], dilation_rate = (dilation, dilation, dilation))

inputs_np = np.random.randn(1,80,80,80,1)
sess = tf.Session()
logits1 = sess.run(layer[8], feed_dict = {inputs : inputs_np})
logits1_duplicate = sess.run(layer[8], feed_dict = {inputs : inputs_np})

sum_diff = np.sum(logits1 - logits1_duplicate)
print(' sum diff : ', sum_diff)

```
When I run this code, I get a non zero sum_diff. Why is the network's output not consistent with multiple runs ? I will appreciate some help"
9861,Exporting and loading models with crossed_columns gives errors,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes, see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.1.0

### Describe the problem
I trained a `LinearClassifier` that includes a `crossed_column`. When I export it and then load and run it again I get an error message: ""ValueError: No op named SparseFeatureCross in defined operations"".

### Source code / logs
To train and export the model I used the following python script:
```
import tensorflow as tf
from tensorflow.contrib.learn.python.learn.utils import input_fn_utils

def input_fn():
    features = {'a': tf.constant([[1],[2]]),
                'b': tf.constant([[3],[4]]) }
    labels = tf.constant([0, 1])
    return features, labels

feature_a = tf.contrib.layers.sparse_column_with_integerized_feature(""a"", bucket_size=10)
feature_b = tf.contrib.layers.sparse_column_with_integerized_feature(""b"", bucket_size=10)
feature_c = tf.contrib.layers.crossed_column([feature_a, feature_b], hash_bucket_size=100)
feature_columns = [feature_a, feature_b, feature_c]
model = tf.contrib.learn.LinearClassifier(feature_columns=feature_columns)
model.fit(input_fn=input_fn, steps=10)

feature_spec = tf.contrib.layers.create_feature_spec_for_parsing(feature_columns)
serving_input_fn = input_fn_utils.build_parsing_serving_input_fn(feature_spec)
model.export_savedmodel('simple-cross/export', serving_input_fn)
```
To load and run the model I used the following python script:
```
import tensorflow as tf

def _int_feature(value):
  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

with tf.Session() as session:
    model = tf.saved_model.loader.load(session, ['serve'], ""simple-cross/export/1494601566/"")
    probs = tf.get_default_graph().get_tensor_by_name('linear/binary_logistic_head/predictions/probabilities:0')

    feature_dict = {'a': _int_feature(value=0),
                    'b': _int_feature(value=5)}
    example = tf.train.Example(features=tf.train.Features(feature=feature_dict)).SerializeToString()
    feed_dict = { 'input_example_tensor:0' : [example] }
    print(session.run(probs, feed_dict=feed_dict))
```
(BTW: is this the best way to import/run a saved model? It feels like plugging in constants like `linear/binary_logistic_head/predictions/probabilities:0` isn't the way to go.)

This results in the following error:
```
ValueError: No op named SparseFeatureCross in defined operations.
```
### Notes
When I add the import
```
from tensorflow.contrib.learn.python.learn.utils import input_fn_utils
```
to the load/run script, it magically works.

Unfortunately, I like to run the model from Java as well, and in Java there is no analogous workaround AFAIK (input_fn_utils doesn't exist there).
"
9859,Where is tf.layers.flatten?,"I hope this question is considered suitable for asking here on GitHub rather than StackOverflow as it relates to a possibly missing feature of the TensorFlow API. I apologize beforehand if I am wasting your time.

This concerns TensorFlow 1.1.0 and `tf.layers`

The tutorial on `tf.layers` uses manual reshaping to get from 4-rank to 2-rank tensors (search for the word flatten):

https://www.tensorflow.org/tutorials/layers

This is not very elegant and I would prefer to use a `flatten()` function.

The documentation for `tf.layers.dense()` says something about flattening the input, but it apparently does something else, as discussed in other threads https://github.com/tensorflow/tensorflow/issues/8175 and https://github.com/tensorflow/tensorflow/pull/9043

There does not seem to be any `tf.layers.flatten()` function, see e.g. the API docs:

https://www.tensorflow.org/api_docs/python/tf/layers

Although there is one for `tf.contrib.layers.flatten()` as shown here:

https://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten

I wonder if `flatten()` has been omitted for some reason when moving `layers` to TF core (why?) or perhaps it has been moved somewhere else, but I have searched and I cannot find it anywhere?

Furthermore, I would like to ask if I can expect that `tf.layers` is going to be the standard builder API going forward? Or will you focus on Keras instead? I have previously used PrettyTensor. There is also tf.slim and other builder APIs. I don't want to change builder API every 6 months, so I'd like to know what the TF developers are betting on this time?"
9858,Function decode_raw ignoring parameter little_endian.,"`little_endian` parameter in `decode_raw` takes no effect. I had a look at the C++ source code (`tensorflow/tensorflow/core/kernels/decode_raw_op.cc`), you are just using `reinterpret_cast` without any awareness of this parameter."
9857,Is there a way to use tensorflow to load a few consecutive frames from a queue of many video frame folders?,"### System information
- **OS Platform and Distribution **: Linux fedora fc25.x86_64
- **TensorFlow installed from (source or binary)**: from pip
- **TensorFlow version (use command below)**: v1.0.0-rc2-15-g47bba63-dirty
- **CUDA/cuDNN version**: cuda 8.0/cudnn 5
- **GPU model and memory**: TITAN X (Pascal)
### Describe the problem
I have a file which lists video names. Frames of each video are saved in a separate folder. Now I want to use tensorflow to get a queue from list of video names and then load a subset of consecutive frames from a video folder every time. Is there a way to do that? 
I tried the following way but it does not work.

### Source code / logs
input_queue is got using tf.train.slice_input_producer.
```
def glob_frame_list(video_path):
     frame_list = gfile.Glob(os.path.join(video_path[0]))
     return frame_list
def read_video_sequence_from_disk(input_queue, seq_len, input_size):    
    frame_list = tf.py_func(glob_frame_list, [input_queue], tf.string) # TensorShape(None)
    frames = []
    start_idx = tf.random_uniform([], minval=0, maxval=tf.shape(frame_list)[0]-seq_len, dtype=tf.int32) # TensorShape([])
    # for i in tf.range(start_idx, start_idx+seq_len): # has a problem of 'tensor is not iterable'
    def body(i):
        img_contents = tf.read_file(frame_list[i]) 
        img = tf.image.decode_jpeg(img_contents, channels=3)        
        frames.append(tf.expand_dims(img, axis=0))
        return i+1    
    i = start_idx
    while_condition = lambda i: tf.less(i, start_idx+seq_len)
    new_i = tf.while_loop(while_condition, body, [i])
    frames = tf.concat(frames, axis=0)
    return frames
```
   
When I run 
`threads = tf.train.start_queue_runners(coord=coord, sess=sess)
`there is the following error.
""
InternalError (see above for traceback): Failed to run py callback pyfunc_0: see error log.
         [[Node: create_inputs/PyFunc = PyFunc[Tin=[DT_STRING], Tout=[DT_STRING], token=""pyfunc_0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](create_inputs/PyFunc/input_0)]]
""
Can anyone help me on this?
Thanks a lot."
9856,Bazel Clean hangs ,"Hi ,

I am trying to build TensorFlow example for Android using Bazel but it hanged. So I tried to find the reason
and after a while tried to clean the previous build with ""bazel clean"" and it hangs again the same way as it happened while building TensorFlow examples:

$ bazel clean
.................................................................................................................................................................................{hangs here}

It doesn't move beyond that and we get no other info. Is there any other way to debug this or log the ""bazel clean"" output which we can later use to debug the issue?

We are using:
tensorflow: 1.1.0
Python: 2.7.6

We tried to find bazel version using ""bazel version"" now this is also hanging:
$ bazel version
. {hangs here}

Looks like its an issue with bazel. may be the installation is not correct. Can someone help?
"
9855,Unexpected error at contrib.seq2seq's BeamSearchDecoder,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**:  v1.1.0-rc2-773-g7fa0cf39f
- **Bazel version (if compiling from source)**:  0.4.5
- **CUDA/cuDNN version**:  None
- **GPU model and memory**: CPU

### Describe the problem

I encountered an unexpected error at tf.contrib.seq2seq's BeamSearchDecoder, *when the beam width is smaller than number of vocabs*.
This is a part of `_beam_search_step` operation in `beam_search_decoder.py`:

```
  scores = _get_scores(
      log_probs=total_probs,
      sequence_lengths=new_prediction_lengths,
      length_penalty_weight=length_penalty_weight)

  time = ops.convert_to_tensor(time, name=""time"")
  # During the first time step we only consider the initial beam
  scores_flat = control_flow_ops.cond(
      time > 0,
      lambda: array_ops.reshape(scores, [batch_size, -1]),
      lambda: scores[:, 0])

  # Pick the next beams according to the specified successors function
  next_beam_scores, word_indices = nn_ops.top_k(scores_flat, k=beam_width)
  next_beam_scores.set_shape([static_batch_size, beam_width])
  word_indices.set_shape([static_batch_size, beam_width])
```

Since the shape of `scores` is `[batch_size, beam_width, vocab_size]`,  the shape of `scores_flat` is`[batch_size, vocab_size]` at time step 0. However, if `k < shape[-1]` in `nn.top_k` operation, it just throws `InvalidArgumentError: input must have at least k columns`. Thus the code just throws an error and dies.
I think code should be modified to handle cases where `vocab_size ** n < beam_width`, or at least throw an appropriate error message when the input is `vocab_size < beam_width`.

"
9854,TensorBoard --logdir=c:\foo support,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no.  I am working with the full example to get the tensorboard to work.  I changed the file directory for the logs into something on my system:

  `train_writer = tf.summary.FileWriter('D:/logs_dt' + '/train', sess.graph)`
  `test_writer = tf.summary.FileWriter('D:/logs_dt' + '/test')
`

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Window 10
- **TensorFlow installed from (source or binary)**:
binary with GPU
- **TensorFlow version (use command below)**:
1.1.0
- **Bazel version (if compiling from source)**:
n/a
- **CUDA/cuDNN version**:
CUDA 8.0
cuDNN v5 for CUDA 8.0 (27 May 2016)
- **GPU model and memory**:
NVIDIA GTX 1070 4GB
- **Exact command to reproduce**:
from windows cmd: 
`tensorboard --logdir='D:\logs_dt'`

You can collect some of this information using our environment capture script:
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/995339/tf_env.txt)

python
1.1.0

### Describe the problem
I'm not seeing anything on the tensorboard at all.  I can see where the log director(ies) is/are created by the tutorial script, and the subdirectories /test and /train are there with the event data present.  I point the tensorboard to the populated log directory with the following command,  but it cannot see the event files.  Nothing is present in tensorboard, and I'm redirected back to the tutorials.
`tensorboard --logdir='D:\logs_dt'`

### Source code / logs
[tensor_board_hello.zip](https://github.com/tensorflow/tensorflow/files/995362/tensor_board_hello.zip)
[logs_dt.zip](https://github.com/tensorflow/tensorflow/files/995357/logs_dt.zip)
"
9853,Multiprocessing for input pipeline ,"I have asked this [question](http://stackoverflow.com/questions/43889941/enqueuing-a-tf-randomshufflequeue-from-multiple-processes-using-multiprocessing) on StackOverFlow but I also feel that it can also be seen as a feature request. 

------------------------
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
   : YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
    : Linux version 3.10.0-229.11.1.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC) ) 
- **TensorFlow installed from (source or binary)**:
    :  Installed from sources
- **TensorFlow version (use command below)**:
    :  v1.0.0-63-g5b4bb03-dirty' 1.0.1
- **Bazel version (if compiling from source)**:
    :  0.4.4
- **CUDA/cuDNN version**:
    :  CUDA 8.0 
    :  CuDNN 5.1
- **GPU model and memory**:
    : NVidia Titan X (Maxwell)
- **Exact command to reproduce**:
    : It is a feature request or clarification


### Describe the problem

**Basic Objective**
I have to train the OverFeat architecture for patch classification from scratch.

**Problem scope**
Input to the graph

**Problem description**
During the training of the OverFeat model, for each image 5 random crops and their mirrored versions (a total of 10 augmented images) are created and they are fed to the model.

I previously used `feed_dict` to provide input to the model. However, it led to a very very slow  training and turned out to be impractical when handling 12 million input images (1.2 million of imagenet X 10).
Hence, I want to use native Tensorflow queues to provide input to the graph.

I have used `multiprocessing` module of Python to create sharded TFRecord files of Imagenet dataset. 
Now, I would like to use `multiprocessing` to enqueue augmented images to a `tf.RandomShuffleQueue` from which the graph takes it input. 
I could have used `multithreading` but it is still slow. Seeing the speedup in creation of TFRecord files through `multiprocessing` as opposed to `multithreading`, I am convinced that using `multiprocessing` would increase the speed of preprocessing.

The problem is that, it is not clear to me that how do I use `multiprocessing` in TensorFlow. One code snippet is [here](https://github.com/WeiTang114/tf-image-classification/blob/ff790fcb1ab6062979688647bb078c2765ad0e7a/input.py)  , but it ends up using a `feed_dict` at the end of the pipeline which leads to one extra copy operation. 

So, I want to launch like 20 processes using `multiprocessing`, each of which will process a range of shards and enqueue the augmented images and corresponding labels to a `tf.RandomShuffleQueue`
It would be very helpful if there is a feature for using multiprocessing with TensorFlow for input pipeline and data augmentation. A good guideline in that direction would be very helpful.


### Source code / logs
The source code which is used for creating the TFRecord files is [here](https://gitlab.inria.fr/uujjwal/overfeat-tensorflow/blob/master/buildtfrecords.py)

I also wrote a basic code to try out multiprocessing (to enqueue a queue from multiple processes) but it does not work.
```
import os
import tensorflow as tf
from pathos.multiprocessing import Pool as mp

def f(g,i):
    #g = tf.Graph()                                                                                                  
    with g.as_default():
        q = tf.FIFOQueue(capacity=100, dtypes=[tf.string], shapes=[[]],
                         shared_name=""shared_q"", name=""q"")
        a = tf.constant(""Hello"")
        b =	q.enqueue(a)
        c = q.size()
    with tf.Session(graph=g) as sess:
        for n in range(i):
            sess.run(b)
            print(sess.run(c))
    return

if __name__ == ""__main__"":
    os.environ[""CUDA_VISIBLE_DEVICES""]=""""
    p = mp(5)
    g = tf.Graph()
    with g.as_default():
        q = tf.FIFOQueue(capacity=100, dtypes=[tf.string], shapes=[[]],
                         shared_name=""shared_q"", name=""q"")
        qsz = q.size()
	p.starmap(f,[(g,1),(g,2)])
        with tf.Session(graph=g) as sess:
            for	i in range(10):
                print(sess.run(qsz))
    	os.unsetenv(""CUDA_VISIBLE_DEVICES"")
```

The output is 

```
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: nefgpu11
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: nefgpu11
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 367.48.0
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.48  Sat Sep  3 18:21:08 PDT 2016
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) 
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.48.0
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 367.48.0
W tensorflow/compiler/xla/service/platform_util.cc:61] platform CUDA present but no visible devices found
I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices
I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:
I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>
1
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: nefgpu11
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: nefgpu11
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 367.48.0
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.48  Sat Sep  3 18:21:08 PDT 2016
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) 
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.48.0
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 367.48.0
W tensorflow/compiler/xla/service/platform_util.cc:61] platform CUDA present but no visible devices found
I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices
I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:
I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>
1
2
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: nefgpu11
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: nefgpu11
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 367.48.0
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.48  Sat Sep  3 18:21:08 PDT 2016
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) 
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.48.0
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 367.48.0
W tensorflow/compiler/xla/service/platform_util.cc:61] platform CUDA present but no visible devices found
I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices
I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:
I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>
0
0
0
0
0
0
0
0
0
0
```

As can be seen in the subprocesses, the queue is being enqueued, but the size as shown in the main module is continuously zero.
"
9851,Issue with tf.nn.max_pool,"I am a little confused about the implementation of tf.nn.max_pool() function. Below it's a toy example.

```
import tensorflow as tf
import numpy as np

def weight_variable(shape, name):
	initial = tf.truncated_normal(shape, stddev = 0.1)
	return tf.Variable(initial, name = name)


xe_ = tf.placeholder(tf.float32, shape = (None, 4, 20, 1))
W_conv1 = weight_variable([4, 10, 1, 10], 'W_conv1')

layer0 = tf.nn.conv2d(xe_, W_conv1, strides = [1,1,1,1], padding='VALID')
layer1 = tf.nn.relu(layer0)
layer2 = tf.nn.max_pool(layer1, ksize = (1,1,5,1), strides = (1,1,5,1), padding='SAME')



sess = tf.Session()
sess.run(tf.global_variables_initializer())

np.random.seed(0)
sample  = np.random.rand(1,4,20,1)
mat1 = layer1.eval(feed_dict = {xe_: sample}, session = sess)
mat2 = layer2.eval(feed_dict = {xe_: sample}, session = sess)

np.max(mat1[0,0,0:5,:],axis=0)-mat2[0,0,0,:]
```
Here I just want to perform a convolution operation with ten `4*10` filters on a single `4*20` image with only one channel. After the convolution ,there is a ReLU activation layer, followed with a max pooling layer. For the max pooling layer, the window size is just `1*5` and the stride comes with the same size. `sample` is just a random image with desired size. `mat1` and `mat2` are the output after `sample` going through these designed layers, `conv2d+ReLU` and `conv2d+ReLU+max_pool` respectively. 

Am I supposed to get an all-zero array from the last line of the code `np.max(mat1[0,0,0:5,:],axis=0)-mat2[0,0,0,:]`. Please point out if I understand `tf.nn.max_pool()` in a wrong way. Great thanks.


### Environment info
- Ubuntu  14.04.5 LTS
- Python 2.7.6
- cuda 8, V8.0.44
- cudnn 5.1.3
- TensorFlow 1.0.1
- GeForce GTX 1080
"
9849,Building failure on KNL,"Build with
Current master branch source code from github

> ~/tensorflow$ ./configure 
> Please specify the location of python. [Default is /usr/bin/python]: 
> Found possible Python library paths:
>   /usr/local/lib/python2.7/dist-packages
>   /usr/lib/python2.7/dist-packages
> Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]
> Using python library path: /usr/local/lib/python2.7/dist-packages
> Do you wish to build TensorFlow with MKL support? [y/N] y
> MKL support will be enabled for TensorFlow
> Do you wish to download MKL LIB from the web? [Y/n] 
> Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 
> Do you wish to use jemalloc as the malloc implementation? [Y/n] 
> jemalloc enabled
> Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] 
> No Google Cloud Platform support will be enabled for TensorFlow
> Do you wish to build TensorFlow with Hadoop File System support? [y/N] 
> No Hadoop File System support will be enabled for TensorFlow
> Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] 
> No XLA support will be enabled for TensorFlow
> Do you wish to build TensorFlow with VERBS support? [y/N] 
> No VERBS support will be enabled for TensorFlow
> Do you wish to build TensorFlow with OpenCL support? [y/N] 
> No OpenCL support will be enabled for TensorFlow
> Do you wish to build TensorFlow with CUDA support? [y/N] 
> No CUDA support will be enabled for TensorFlow
> Warning: ignoring http_proxy in environment.
> INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.
> Configuration finished

Command
`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures`

The error only part of build log
[Build Log.txt](https://github.com/tensorflow/tensorflow/files/994830/Build.Log.txt)

The Env collected by tf_env_collect.sh
[Env.txt](https://github.com/tensorflow/tensorflow/files/994893/Env.txt)


Bazel
> $ bazel version
> Warning: ignoring http_proxy in environment.
> Build label: 0.4.5
> Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
> Build time: Thu Mar 16 12:19:38 2017 (1489666778)
> Build timestamp: 1489666778
> Build timestamp as int: 1489666778

Xeon Phi Platform(KNL)
No GPU

One thing to notice is that with my experiments,
The changes in
[Fix TensorFlow compilation errors with KNL optimization flags](https://github.com/tensorflow/tensorflow/commit/51331365b60dd042ebc0849ea4e1ab6a396b60fd)
fixed the building issue. Although I don't know whether it is functioning correctly.

Then the code is rolled back in
[FIxed merge issues ](https://github.com/tensorflow/tensorflow/commit/bbdd4a7f6508c32042dcff10025fd39aeba72cdc)

Could you please look into this.
Thank you."
9846,Increase compatibility with pathlib,"Pathlib (https://docs.python.org/3/library/pathlib.html) is a nice standard-library for dealing with paths, but it seems that TensorFlow currently does not always accept them. E.g. when creating a FileWriter like:

    logdir = pathlib.Path('/my/path')
    log = tf.summary.FileWriter(logdir, sess.graph)

I get:

    TypeError: Expected binary or unicode string, got PosixPath('/my/path')

While it's easy to work around this, it would be nice if this could be supported out of the box.

EDIT: this was using TF 1.1"
9842,Dynamic Library Compilation Error,"Hi,

When I compile the dynamic library without the optimization flag, using the command `bazel build //tensorflow:libtensorflow.so`, everything works well (i.e., I am able to compile it and load it in the Java API). However, if I compile with some optimization flags, using the command `bazel build --config=opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse3 --copt=-msse4.1 --copt=-msse4.2 //tensorflow:libtensorflow.so`, the library compiles fine (i.e., no errors), but when I try to load it with the Java API, I get the following error:

```txt
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGILL (0x4) at pc=0x000000012b7670a8, pid=62398, tid=7171
#
# JRE version: Java(TM) SE Runtime Environment (8.0_77-b03) (build 1.8.0_77-b03)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.77-b03 mixed mode bsd-amd64 compressed oops)
# Problematic frame:
# C  [libtensorflow.so+0x1bb80a8]  _ZN10tensorflow66protobuf_tensorflow_2fcore_2fframework_2fresource_5fhandle_2eproto11TableStruct16InitDefaultsImplEv+0x38
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
#
# An error report file with more information is saved as:
# /xxx/tensorflow_scala/hs_err_pid62398.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
```

The relevant part of the error log stack trace is:

```txt
Stack: [0x000070000606e000,0x000070000616e000],  sp=0x0000700006166ad0,  free space=994k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [libtensorflow.so+0x1bb80a8]  _ZN10tensorflow66protobuf_tensorflow_2fcore_2fframework_2fresource_5fhandle_2eproto11TableStruct16InitDefaultsImplEv+0x38
C  [libtensorflow.so+0x1d50941]  _ZN6google8protobuf8internal16FunctionClosure03RunEv+0x11
C  [libtensorflow.so+0x1d512eb]  _ZN6google8protobuf18GoogleOnceInitImplEPlPNS0_7ClosureE+0x3b
C  [libtensorflow.so+0x1bb8124]  _ZN10tensorflow66protobuf_tensorflow_2fcore_2fframework_2fresource_5fhandle_2eproto12InitDefaultsEv+0x44
C  [libtensorflow.so+0x1bc9614]  _ZN10tensorflow55protobuf_tensorflow_2fcore_2fframework_2ftensor_2eproto11TableStruct16InitDefaultsImplEv+0x24
C  [libtensorflow.so+0x1d50941]  _ZN6google8protobuf8internal16FunctionClosure03RunEv+0x11
C  [libtensorflow.so+0x1d512eb]  _ZN6google8protobuf18GoogleOnceInitImplEPlPNS0_7ClosureE+0x3b
C  [libtensorflow.so+0x1bc9724]  _ZN10tensorflow55protobuf_tensorflow_2fcore_2fframework_2ftensor_2eproto12InitDefaultsEv+0x44
C  [libtensorflow.so+0x1b79447]  _ZN10tensorflow61protobuf_tensorflow_2fcore_2fframework_2fattr_5fvalue_2eproto11TableStruct16InitDefaultsImplEv+0x27
C  [libtensorflow.so+0x1d50941]  _ZN6google8protobuf8internal16FunctionClosure03RunEv+0x11
C  [libtensorflow.so+0x1d512eb]  _ZN6google8protobuf18GoogleOnceInitImplEPlPNS0_7ClosureE+0x3b
C  [libtensorflow.so+0x1b795e4]  _ZN10tensorflow61protobuf_tensorflow_2fcore_2fframework_2fattr_5fvalue_2eproto12InitDefaultsEv+0x44
C  [libtensorflow.so+0x1ba0354]  _ZN10tensorflow61protobuf_tensorflow_2fcore_2fframework_2fkernel_5fdef_2eproto11TableStruct16InitDefaultsImplEv+0x24
C  [libtensorflow.so+0x1d50941]  _ZN6google8protobuf8internal16FunctionClosure03RunEv+0x11
C  [libtensorflow.so+0x1d512eb]  _ZN6google8protobuf18GoogleOnceInitImplEPlPNS0_7ClosureE+0x3b
C  [libtensorflow.so+0x1ba0454]  _ZN10tensorflow61protobuf_tensorflow_2fcore_2fframework_2fkernel_5fdef_2eproto12InitDefaultsEv+0x44
C  [libtensorflow.so+0x1ba16af]  _ZN10tensorflow9KernelDefC2Ev+0x5f
C  [libtensorflow.so+0x19793c8]  _ZN10tensorflow16KernelDefBuilderC2EPKc+0x28
C  [libtensorflow.so+0x10e58e]  _GLOBAL__sub_I_batchtospace_op.cc+0x1e
C  0x0000000112b39a1b
C  0x0000000112b39c1e
C  0x0000000112b354aa
C  0x0000000112b35441
C  0x0000000112b34524
C  0x0000000112b345b9
C  0x0000000112b297cd
C  0x0000000112b313ec
C  [libdyld.dylib+0x2832]  dlopen+0x3b
V  [libjvm.dylib+0x4820f6]
V  [libjvm.dylib+0x3456a8]
C  [libjava.dylib+0x28b0]  Java_java_lang_ClassLoader_00024NativeLibrary_load+0x77
```

Why is this happening and how could it be fixed?

@alextp I think it may have to do with resources / resource-based variables given the error message, and I think you might be knowledgable with respect to that topic. I'm not sure if that's the issue though.

Thank you!

P.S. Let me know if the complete error log file would help and I'll post it here. :)
"
9841,Unable to use two gmms in one classifier in tf gpu 1.1,"I've tried to use tf.contrib.factorization.python.ops' gmm model

I've watched the gmm_test.py as the doc about this model is not clear to use

My code is as below

```
class DTW_GMM:
    def __init__(self, cluster_num=3):
        self.genuine_graph = tf.Graph()
        self.forgery_graph = tf.Graph()
        with self.genuine_graph.as_default():
            self.genuine_gmm = gmm.GMM(cluster_num)
        with self.forgery_graph.as_default():
            self.forgery_gmm = gmm.GMM(cluster_num)

    def compare(self, reference, target):
        channel_dtw = []
        for channel_index in range(len(reference)):
            dis, _, _, _ = dtw(reference[channel_index], target[channel_index], dist=my_custom_norm)
            channel_dtw.append(dis)
        return channel_dtw

    def input_fn(self, data):
        def fn():
            return data, None
        return fn

    def train_genuine(self, data, steps=10):
        with self.genuine_graph.as_default():
            self.genuine_gmm.fit(input_fn=self.input_fn(data), steps=steps)

    def train_forgery(self, data, steps=10):
        with self.forgery_graph.as_default():
            self.forgery_gmm.fit(input_fn=self.input_fn(data), steps=steps)

    def infer(self, data):
        genuine_score = self.genuine_gmm.score(input_fn=self.input_fn(data), steps=1)
        forgery_score = self.forgery_gmm.score(input_fn=self.input_fn(data), steps=1)
        return genuine_score >= forgery_score

batch_size = 64
loop = 100


def train():
    model = DTW_GMM()
    data = Data()

    sess = tf.Session()
    with sess.as_default():
        sess.run(tf.global_variables_initializer())
        for step in range(loop):
            print('step: {}'.format(step))
            genuine_data = []
            forgery_data = []
            for i in range(batch_size):
                reference, target = data.get_genuine_pair()
                genuine_data.append(model.compare(reference, target))
                reference, target = data.get_fake_pair()
                forgery_data.append(model.compare(reference, target))
            genuine_data = tf.constant(genuine_data, dtype=np.float32)
            forgery_data = tf.constant(forgery_data, dtype=np.float32)
            model.train_genuine(genuine_data)
            model.train_forgery(forgery_data)
```

Even I try to define graph for each of the two gmm, it still connot work with the error log:

```
  File ""DTW_GMM.py"", line 39, in train_genuine
    self.genuine_gmm.fit(input_fn=self.input_fn(data), steps=steps)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 281, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 430, in fit
    loss = self._train_model(input_fn=input_fn, hooks=hooks)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 927, in _train_model
    model_fn_ops = self._get_train_ops(features, labels)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1132, in _get_train_ops
    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1103, in _call_model_fn
    model_fn_results = self._model_fn(features, labels, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/factorization/python/ops/gmm.py"", line 137, in _model_fn
    self._params)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/factorization/python/ops/gmm_ops.py"", line 498, in gmm
    covariance_type, random_seed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/factorization/python/ops/gmm_ops.py"", line 147, in __init__
    self._create_variables(data, initial_means)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/factorization/python/ops/gmm_ops.py"", line 173, in _create_variables
    _init_clusters_random(data, self._num_classes, self._random_seed),
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/factorization/python/ops/gmm_ops.py"", line 81, in _init_clusters_random
    [check_ops.assert_less_equal(num_clusters, num_data)]):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3595, in control_dependencies
    return get_default_graph().control_dependencies(control_inputs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3324, in control_dependencies
    c = self.as_graph_element(c)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2414, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2498, in _as_graph_element_locked
    raise ValueError(""Operation %s is not an element of this graph."" % obj)
ValueError: Operation name: ""assert_less_equal/Assert/Assert""
op: ""Assert""
input: ""assert_less_equal/All""
input: ""assert_less_equal/Assert/Assert/data_0""
input: ""assert_less_equal/Assert/Assert/data_1""
input: ""assert_less_equal/Assert/Assert/data_2""
input: ""assert_less_equal/x""
input: ""assert_less_equal/Assert/Assert/data_4""
input: ""assert_less_equal/Assert/Assert/data_5""
input: ""strided_slice_1""
attr {
  key: ""T""
  value {
    list {
      type: DT_STRING
      type: DT_STRING
      type: DT_STRING
      type: DT_INT32
      type: DT_STRING
      type: DT_STRING
      type: DT_INT32
    }
  }
}
attr {
  key: ""summarize""
  value {
    i: 3
  }
}
 is not an element of this graph.
```

The input is a constant tensor with shape of [batch_size, 4]"
9838,Importing tensorflow fails on travis-xenial because of glibc version,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:travis-ubuntu-16.04
- **TensorFlow installed from (source or binary)**:pip (to get whatever is the latest version there)
- **GPU model and memory**:travis default build target

You can collect some of this information using our environment capture script

### Describe the problem
Since the last few days, our utility which imports tensorflow has been failing automated builds on travis-ci. We are using a ubuntu-xenial target (16.04). This issue was NOT present a few days ago, but suddenly appeared causing travis builds to fail. I am assuming it is an issue with dependencies or version checks.

### Source code / logs

Here is the travis build history:
https://travis-ci.org/autonomio/core-module

Here is the .travis.yml:
https://github.com/autonomio/core-module/blob/master/.travis.yml

Here is the error we are getting:
`Using TensorFlow backend.
Traceback (most recent call last):
  File ""./test_script.py"", line 1, in <module>
    from autonomio.commands import data, train, test
  File ""/home/travis/build/m-anish/autonomio/autonomio/__init__.py"", line 3, in <module>
    from train_new import kuubio
  File ""/home/travis/build/m-anish/autonomio/autonomio/train_new.py"", line 7, in <module>
    from transform_data import transform_data
  File ""/home/travis/build/m-anish/autonomio/autonomio/transform_data.py"", line 6, in <module>
    from y_transform import y_transform
  File ""/home/travis/build/m-anish/autonomio/autonomio/y_transform.py"", line 2, in <module>
    from keras.utils import np_utils
  File ""/home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/keras/__init__.py"", line 3, in <module>
    from . import activations
  File ""/home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/keras/activations.py"", line 4, in <module>
    from . import backend as K
  File ""/home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/keras/backend/__init__.py"", line 73, in <module>
    from .tensorflow_backend import *
  File ""/home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py"", line 1, in <module>
    import tensorflow as tf
  File ""/home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 51, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.17' not found (required by /home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
`

Here is the full log:
https://s3.amazonaws.com/archive.travis-ci.org/jobs/231031263/log.txt?X-Amz-Expires=30&X-Amz-Date=20170511T053131Z&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJRYRXRSVGNKPKO5A/20170511/us-east-1/s3/aws4_request&X-Amz-SignedHeaders=host&X-Amz-Signature=de398e3a80f8114605f1a334832b1dc024321d3c46dc8453384b53620c38ec37


"
9837,Optimizers in the C++ API,"There is currently no Optimizer in the [C++ API](https://www.tensorflow.org/api_docs/cc/) compared as the ones that we can find in the [Python API](https://www.tensorflow.org/api_docs/python/).

It means, when using only the C++ API that we have to manually collect the gradients and apply them.

Don't you think it should be a convenient add? Retrieving the gradients and applying them is kind of hard and error prone. If yes, I could work on it and create an Optimizer + GradientDescentOptimizer in the C++ API.

I maybe missed something."
9836,Unable to run model in iOS: dtype() == expected_dtype (9 vs. 4),"### System information
- **iOS emulator**
- **TensorFlow version v1.1.0rc2**
- compiler flag: -O3, `-D__ANDROID_TYPES_SLIM__` replaced by `-D__ANDROID_TYPES_FULL__` inside `tensorflow/contrib/makefile/Makefile` in order to fix a issue of missing kernel
- modification made: `tensorflow/core/kernels/cwise_op_floor_mod.cc` added to `tensorflow/contrib/makefile/tf_op_files.txt b/tensorflow/contrib/makefile/tf_op_files.txt` in order to fix another issue of missing kernel

### Describe the problem

I am trying to run the [deeplab image segmentation](https://github.com/DrSleep/tensorflow-deeplab-resnet) on iOS.  I have freezed the model, which can then be run on a python shell.  But when I put it on to iOS, it crashes.  Please see the log from xcode below.  I am sure my build of tensorflow is working because I can run another model.

### Source code / logs

The log comes from running a quantized model.  running a non-quantized version lead to the same problem.
```
2017-05-11 18:57:16.053474: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-11 18:57:16.053595: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-11 18:57:16.053717: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-05-11 18:57:16.053830: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-11 18:57:16.053889: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-05-11 18:57:16.054 tf_ios_makefile_example[10355:45103643] Graph created.
[libprotobuf INFO google/protobuf/io/coded_stream.cc:610] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 45491214
2017-05-11 18:57:16.094 tf_ios_makefile_example[10355:45103643] Creating session.
2017-05-11 18:57:17.110386: F tensorflow/core/framework/tensor.cc:487] Check failed: dtype() == expected_dtype (9 vs. 4)
```

Thanks in advance.
"
9835,JobDef omit key 0 when convert to string,"I installed the package from the official binary through pip, I want save a custom ClusterDef and found the zero key is omitted from the JobDef, is this an expected behavior?
```
kingder@WEICHAO-NEW:/mnt/d$ lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 14.04.5 LTS
Release:        14.04
Codename:       trusty

kingder@WEICHAO-NEW:/mnt/d$ python
Python 2.7.6 (default, Oct 26 2016, 20:30:19)
[GCC 4.8.4] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> jobdef = tf.train.JobDef()
>>> jobdef.name='worker'
>>> jobdef.tasks[0]=""localhost:10000""
>>> jobdef.tasks[1]=""localhost:10001""
>>> jobdef
name: ""worker""
tasks {
  value: ""localhost:10000""
}
tasks {
  key: 1
  value: ""localhost:10001""
}

>>> print(tf.GIT_VERSION, tf.VERSION)
('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')
>>>
```"
9834,how to install tensorflow 0.12 or 0.9 version on windows now?,"I have tried several ways to install tf based on docker, but the installation code can only install the latest tf version.......  "
9833,New seq2seq interface(basic_decoder) does not support sampled softmax?,"basic_decoder init function has param output_layer which must be a type of layer like Dense.
But for using sampled softmax we need some code like below, as w_t and v is needed by tf.nn.sampled_softmax_loss 
      with tf.variable_scope('output_projection'):
          self.w_t = melt.variable.get_weights_truncated('w',   
                                               [vocab_size, num_units],   
                                               stddev=FLAGS.weight_stddev)   
          #weights  
          self.w = tf.transpose(self.w_t)    
          #biases  
          self.v = melt.variable.get_weights_truncated('v',     
                                              [vocab_size], 
                                              stddev=FLAGS.weight_stddev)   
for old seq2seq interface, we can just pass output_function like 
      def output_fn(output):  
          return tf.nn.xw_plus_b(output, self.w, self.v )  

How can we do sampled softmax with seq2seq new internface ?"
9832,BeamSearchDecoder not working,"I intalled TensorFlow from 2017-05-10 nightly binaries. The TensorFlow version is v1.1.0-rc2-773-g7fa0cf3 1.1.0-rc2. I have CUDA 8.0 and a Tesla K20c with 4.5G memory. My OS is Linux Ubuntu 14.04.

I tried to run the following code to test the latest BeamSearchDecoder:
```
lstm = rnn.OutputProjectionWrapper(
rnn.LayerNormBasicLSTMCell(n_hidden, dropout_keep_prob=keep_prob), n_classes)
infer_decoder = BeamSearchDecoder(lstm, 
embedding=lambda tokens:tf.nn.embedding_lookup(embedding_matrix, tokens),
start_tokens=start_tokens, end_token=EOS, initial_state=encoder_state, beam_width=5)
decoder_outputs_infer, decoder_state_infer, decoder_seq_infer = dynamic_decode(infer_decoder)
```

But I got :
```
Traceback (most recent call last):
  File ""/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/framework/tensor_util.py"", line 458, in make_tensor_proto
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/framework/tensor_util.py"", line 458, in <listcomp>
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/util/compat.py"", line 65, in as_bytes
    (bytes_or_text,))
TypeError: Expected binary or unicode string, got None

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/anxf/test_beam.py"", line 127, in <module>
    decoder_outputs_infer, decoder_state_infer, decoder_seq_infer = dynamic_decode(infer_decoder)
  File ""/home/anxf/.local/lib/python3.4/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py"", line 286, in dynamic_decode
    swap_memory=swap_memory)
  File ""/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2705, in while_loop
    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2534, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2484, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/home/anxf/.local/lib/python3.4/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py"", line 234, in body
    decoder_finished) = decoder.step(time, inputs, state)
  File ""/home/anxf/.local/lib/python3.4/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py"", line 437, in step
    length_penalty_weight=length_penalty_weight)
  File ""/home/anxf/.local/lib/python3.4/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py"", line 516, in _beam_search_step
    final_shape=[static_batch_size, beam_width])
  File ""/home/anxf/.local/lib/python3.4/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py"", line 638, in _tensor_gather_helper
    output = array_ops.reshape(output, final_shape)
  File ""/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 2548, in reshape
    name=name)
  File ""/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py"", line 493, in apply_op
    raise err
  File ""/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py"", line 490, in apply_op
    preferred_dtype=default_dtype)
  File ""/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 714, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/framework/constant_op.py"", line 113, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/framework/constant_op.py"", line 102, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/framework/tensor_util.py"", line 462, in make_tensor_proto
    ""supported type."" % (type(values), values))
TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [None, 5]. Consider casting elements to a supported type.

Process finished with exit code 1
```

I wonder how I can fix this problem? Or does this mean BeamSearchDecoder is still testing and does not work right now?"
9831,gpu_memory_fraction for tf.contrib.lean Estimators stopped working in tensorflow 1.1.0,"I was updating to Tensorflow 1.1.0 to use the tf.estimator API and I found, that I couldn't get it to use less than the full GPU RAM for one process.
I was trying to find out where the issue is and found, that also the tf.contrib.lean.Estimators had the same problem. 

Here a code snippet to reproduce the issue:
```
import tensorflow as tf
import numpy as np
from tensorflow.contrib.learn import DNNClassifier
import os

tf.logging.set_verbosity(tf.logging.INFO)

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)

feature_columns = [tf.contrib.layers.real_valued_column(""x"", dimension=mnist.train.images.shape[1])]
model = DNNClassifier(hidden_units=[10,10],
                      feature_columns=feature_columns,
                      n_classes=10,
                      config=tf.contrib.learn.RunConfig(gpu_memory_fraction=0.1))

input_fn_train = tf.contrib.learn.io.numpy_input_fn(
                                x={""x"":mnist.train.images.reshape([-1,28,28,1]).astype(np.float32)[:1000]},
                                y=np.argmax(mnist.train.labels[:1000],1).astype(np.int32),
                                batch_size=50,
                                num_epochs=50,
                                )


model.fit(input_fn=input_fn_train)

```

With tensorflow 1.0.1 it works as expected and only allocated 1/10 of the GPU RAM, but in tensorflow 1.1.0 it ignores this parameter. 

I also check with the code from the MNIST for ML beginners code  and used the GPUOptions when initializing the session:
```
gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.1)
sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))
```

It works as expected in both verions. 
Somehow this GPU options is not set correctly when using the tf.contib.learn.Estimator framework OR The gpu_memory is allocated before this call.

I asked this before on the tensorflow discuss group and I was told that is is a bug."
9829,Kernel Restarting The kernel appears to have died. It will restart automatically (Jupyter-Tensorflow),"I am facing a huge problem where the jupyter kernel keeps dying. This is my first experience of kernel ending on Jupyter. I'm using Tensorflow to fit a CNN model (the total input datasize is only 9.8MB)

I did not have this issue in my previous run on the same code (however I did have errors where it said ""dst tensor is not initialized"". That was when I made the dataset really small to attempt to fit it. Could someone please kindly help ?

This is the code:

import tensorflow as tf
import numpy as np

IMG_PX_SIZE = 50
HM_SLICES = 20

n_classes = 2

x = tf.placeholder('float')
y = tf.placeholder('float')

keep_rate = 0.8
keep_prob = tf.placeholder(tf.float32)

def conv3d(x, W):
    return tf.nn.conv3d(x, W, strides=[1,1,1,1,1], padding='SAME')

def maxpool3d(x):
    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')

def convolutional_neural_network(x):
    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,3,1,32])),
               'W_conv2':tf.Variable(tf.random_normal([3,3,3,32,64])),
               'W_fc':tf.Variable(tf.random_normal([62720  ,1024])),
               'out':tf.Variable(tf.random_normal([1024, n_classes]))}

    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),
               'b_conv2':tf.Variable(tf.random_normal([64])),
               'b_fc':tf.Variable(tf.random_normal([1024])),
               'out':tf.Variable(tf.random_normal([n_classes]))}

    x = tf.reshape(x, shape=[-1, IMG_PX_SIZE, IMG_PX_SIZE, HM_SLICES, 1])

    conv1 = tf.nn.relu(conv3d(x, weights['W_conv1']) + biases['b_conv1'])
    conv1 = maxpool3d(conv1)
    
    conv2 = tf.nn.relu(conv3d(conv1, weights['W_conv2']) + biases['b_conv2'])
    conv2 = maxpool3d(conv2)

    fc = tf.reshape(conv2,[-1, 62720  ])
    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])
    fc = tf.nn.dropout(fc, keep_rate)

    output = tf.matmul(fc, weights['out']) + biases['out']

    return output

 def train_neural_network(x):
    
    much_data = np.load('muchdata_sampled-50-50-20.npy')
    train_data = much_data[:100]
    validation_data = much_data[-100:]
    
    prediction = convolutional_neural_network(x)
    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y) )
    optimizer = tf.train.AdamOptimizer().minimize(cost)
    
    hm_epochs = 3
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())

        for epoch in range(hm_epochs):
            epoch_loss = 0
            for data in train_data:
                X = data[0]
                Y = data[1]
                _, c = sess.run([optimizer, cost], feed_dict={x: X, y: Y})
                epoch_loss += c

            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)

        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))

        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))
        print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))

 train_neural_network(x)



(My System information)

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
* Yes

OS Platform and Distribution:
* OS = Windows 10

TensorFlow installed from (source or binary):
*Installed from Source

TensorFlow version (use command below):
*When I ran => python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
*I get only this result => b'unknown' 1.0.0

CUDA/cuDNN version:
*cuda_8.0.61_win10
*cuDNN v5.1 (Jan 20, 2017), for CUDA 8.0

GPU model and memory:
*GeForce GTX 1050 graphics card
*RAM 32GB"
9827,"tf.abs() isn't documented to handle complex, but it does appear to work as tf.complex_abs() used to.","Issue #7405 was a bug filed that tf.complex_abs() was removed in 1.0.  At the bottom it says that tf.abs() now does that work, but the docs for tf.abs() only mention float.  I confirmed that tf.abs() does in fact do as the comment on the issue describes (https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/python/ops/math_ops.py#L225)

Please incorporate the info from https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops/complex_number_functions#complex_abs 

into
https://www.tensorflow.org/api_docs/python/tf/abs

... specifically the parts about it computing sqrt(a^2 + b^2) for complex numbers.
"
9826,C API Exception,"Hi,

The C API method ""TF_GraphGetTensorNumDims"" throws an exception with message ""Node X was not found in the graph"" even for valid nodes that are in the graph. This tends to happen with the outputs of particular ops (e.g., reshape and matmul). I think it may have to do with shape inference after looking at the C API implementation but I'm not sure what's wrong. Is there something I need to do to enable shape inference when compiling the shared library? I thought that was enabled by default. Or is there something else that's broken?

Thank you,
Anthony"
9824,possible bug - LSTMCell and GRUCell have different variable reuse behavior,"[tf_env.txt](https://github.com/tensorflow/tensorflow/files/999124/tf_env.txt)
### System information

Capture script output is attached.

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes -- see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  OS X 10.12.4
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.1.0
- **Bazel version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**:  NA
- **Exact command to reproduce**:

```
class ToyModel(object):
	def __init__(self):
		x = tf.get_variable(""x"", shape=[10,32,3], initializer=tf.random_uniform_initializer(), trainable=False)
		cell = tf.contrib.rnn.LSTMCell(3)
		self.y, _ = tf.nn.dynamic_rnn(cell, inputs=x, dtype=tf.float32)

graph_context = tf.Graph()
with graph_context.as_default():
	with tf.name_scope(""Train""):
		with tf.variable_scope(""Model"", reuse=None):
			m1 = ToyModel()
	with tf.name_scope(""Valid""):
		with tf.variable_scope(""Model"", reuse=True):
			m2 = ToyModel()

	tf_init = tf.global_variables_initializer()

	# sv = tf.train.Supervisor(logdir=save_dir)
	# with sv.managed_session() as session:
	session = tf.Session(graph=graph_context)
	with session as sess:
		sess.run(tf_init)
		y1 = m1.y.eval()

		y2 = m2.y.eval()

		print(y1 == y2)

Traceback (most recent call last):
  File ""src/minimal_tf.py"", line 30, in <module>
    m2 = ToyModel()
  File ""src/minimal_tf.py"", line 21, in __init__
    self.y, _ = tf.nn.dynamic_rnn(cell, inputs=x, dtype=tf.float32)
  File ""/Users/delkind/Desktop/whd/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 553, in dynamic_rnn
    dtype=dtype)
  File ""/Users/delkind/Desktop/whd/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 720, in _dynamic_rnn_loop
    swap_memory=swap_memory)
  File ""/Users/delkind/Desktop/whd/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2623, in while_loop
    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""/Users/delkind/Desktop/whd/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2456, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/Users/delkind/Desktop/whd/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2406, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/Users/delkind/Desktop/whd/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 705, in _time_step
    (output, new_state) = call_cell()
  File ""/Users/delkind/Desktop/whd/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 691, in <lambda>
    call_cell = lambda: cell(input_t, state)
  File ""/Users/delkind/Desktop/whd/venv/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py"", line 398, in __call__
    reuse=self._reuse) as unit_scope:
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/contextlib.py"", line 59, in __enter__
    return next(self.gen)
  File ""/Users/delkind/Desktop/whd/venv/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py"", line 93, in _checked_scope
    ""the argument reuse=True."" % (scope_name, type(cell).__name__))
ValueError: Attempt to have a second RNNCell use the weights of a variable scope that already has weights: 'Model/rnn/lstm_cell'; and the cell was not constructed as LSTMCell(..., reuse=True).  To share the weights of an RNNCell, simply reuse it in your second calculation, or create a new one with the argument reuse=True.
```
### Describe the problem

When cell is an LSTMCell instance, this code raises the the ValueError above, which I believe to be a bug. My expectation is that invoking the second model in a scope with reuse=True will mean that m2 uses the already-existing variables in m1. The error message indicates that this is not happening, apparently because the LSTMCell is not aware of the reuse flag set in m2's scope.

By contrast, if you swap the LSTMCell for GRUCell, no errors are raised, and the code completes as expected. Indeed, the outputs of y1 and y2 are equal when the graph is evaluated.

Likewise, if you use the LSTMCell but skip creating m2 at all, y1 can be evaluated without any errors."
9823,Tensorflow consumes much more memory than expected,"My model has four CPU variables:
[500M, 3] tf.int32
[500M] tf.float32
[500M] tf.float32 (FTRL accumulate slot)
[500M] tf.float32 (FTRL linear slot)
expected memory consumption should be (500M * 6) * 4 = 12G, however tensorflow used 20G memory.

When I increased 500M to 1B, total memory usage is 40G, seems tensorflow do allocate much more memory than needed, any idea? By the way I am not using any tcmalloc stuff.
I also used timeline show_memory to print allocated tensor size, everything is consistent with my calculating."
9821,tf.constant_initializer does not accept Tensor as input,"I'd like to load multiple copies of VGG on each of 4 different GPUs.. rather than copy the weights down to each GPU every time, I'd like a local copy of the weights. Unfortunately using constant_initializer and a numpy array explodes the graph def (as all the n copies of the weights are stored). 

Instead, I convert the weights to a tensor once, and want to use this tensor to initialize the variable. this keeps the graphdef small and allows me to be super flexible. Unfortunately it looks like tf.constant_initializer doesn't accept Tensors as values."
9820,Unable to build TensorFlow Java native libraries for arm64-v8a Android devices,"### Problem

I'm trying to build TensorFlow Java and the relevant native libraries for an Android device (I'm targeting a Pixel).

### What I've tried

Following the instructions on building TensorFlow Java from source [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/README.md) and on Android TensorFlow support [here](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/android/README.md), (relative to my checked out TensorFlow repo at `~/Android/tensorflow-master`) I updated my `WORKSPACE` to include the following

```
android_sdk_repository(
    name = ""androidsdk"",
    api_level = 25,
    build_tools_version = ""25.0.3"",
    path = ""~/Android/Sdk/"",
)

android_ndk_repository(
    name=""androidndk"",
    path=""~/Android/android-ndk-r12b/"",
    api_level=24)
```

I ran `./configure` and supplied the following options

```
Please specify the location of python. [Default is /usr/bin/python]: 
Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
  /usr/local/buildtools/current/sitecustomize
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]

Using python library path: /usr/local/lib/python2.7/dist-packages
Do you wish to build TensorFlow with MKL support? [y/N] n
No MKL support will be enabled for TensorFlow
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: -std=c++11 -march=armv8-a
Do you wish to use jemalloc as the malloc implementation? [Y/n] n
jemalloc disabled
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] n
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] n
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] n
No XLA JIT support will be enabled for TensorFlow
Do you wish to build TensorFlow with VERBS support? [y/N] n
No VERBS support will be enabled for TensorFlow
Do you wish to build TensorFlow with OpenCL support? [y/N] n
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] n
No CUDA support will be enabled for TensorFlow
```

and then I attempted to build the relevant libraries with the following command

```
bazel build -c opt --config opt \
 //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni \
 --crosstool_top=//external:android/crosstool \
 --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
 --cpu=arm64-v8a
```

### What happens

I get a bunch of warnings (stdout from the above command: [log.txt](https://github.com/tensorflow/tensorflow/files/991284/log.txt)) and the following linking error

`ERROR: /usr/local/google/home/tsamson/Android/tensorflow-master/tensorflow/java/BUILD:142:1: Linking of rule '//tensorflow/java:libtensorflow_jni.so' failed: link_dynamic_library.sh failed: error executing command external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64/bin/aarch64-linux-android-gcc ... (remaining 38 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.`

(I get the same errors building for `armeabi-v7a`. I am able to build the above targets for my Ubuntu machine by doing the above (instead using `-march=native` and the default `--crosstool_top`, `--host_crosstool_top`, and `--cpu` flags), but I want to compile for Android ☺.)

### Environment

Here's the output of `tools/tf_env_collect.sh` (which had to be run from a directory other than `tensorflow-master` to keep from getting a tensorflow import error):

```
== cat /etc/issue ===============================================
VERSION=""14.04.5 LTS, Trusty Tahr""
VERSION_ID=""14.04""

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4
Copyright (C) 2013 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================

== check pips ===================================================
numpy (1.12.1)
protobuf (3.3.0)
tensorflow (1.1.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.1.0
tf.GIT_VERSION = v1.1.0-rc0-61-g1ec6ed5
tf.COMPILER_VERSION = v1.1.0-rc0-61-g1ec6ed5
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed May 10 13:42:21 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro K1200        On   | 0000:01:00.0      On |                  N/A |
| 39%   45C    P8     1W /  35W |    553MiB /  4016MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      3426    G   /usr/lib/xorg/Xorg                             331MiB |
|    0      4959    G   cinnamon                                        61MiB |
|    0      8578    G   /proc/self/exe                                 158MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================

```
"
9819,Importing tensorflow fails.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Home
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: I can't import, but the wheel file is ""tensorflow_gpu-1.1.0-cp35-cp35m-win_amd64.whl (48.5MB)""
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: cuda_8.0.61_win10.exe / cudnn-8.0-windows10-x64-v5.1.zip
- **GPU model and memory**: NVIDIA GeForce GTX 1070
- **Exact command to reproduce**: import tensorflow

### Describe the problem
Importing tensorflow fails.

### Source code / logs
Installing tensorflow-gpu:
```
λ pip3 install tensorflow-gpu --upgrade
Collecting tensorflow-gpu
  Downloading tensorflow_gpu-1.1.0-cp35-cp35m-win_amd64.whl (48.5MB)
    100% |################################| 48.6MB 16.9MB/s
Requirement already up-to-date: wheel>=0.26 in c:\users\arthu\appdata\local\programs\python\python35\lib\site-packages (from tensorflow-gpu)
Requirement already up-to-date: protobuf>=3.2.0 in c:\users\arthu\appdata\local\programs\python\python35\lib\site-packages (from tensorflow-gpu)
Requirement already up-to-date: werkzeug>=0.11.10 in c:\users\arthu\appdata\local\programs\python\python35\lib\site-packages (from tensorflow-gpu)
Requirement already up-to-date: numpy>=1.11.0 in c:\users\arthu\appdata\local\programs\python\python35\lib\site-packages (from tensorflow-gpu)
Requirement already up-to-date: six>=1.10.0 in c:\users\arthu\appdata\local\programs\python\python35\lib\site-packages (from tensorflow-gpu)
Requirement already up-to-date: setuptools in c:\users\arthu\appdata\local\programs\python\python35\lib\site-packages (from protobuf>=3.2.0->tensorflow-gpu)
Requirement already up-to-date: packaging>=16.8 in c:\users\arthu\appdata\local\programs\python\python35\lib\site-packages (from setuptools->protobuf>=3.2.0->tensorflow-gpu)
Requirement already up-to-date: appdirs>=1.4.0 in c:\users\arthu\appdata\local\programs\python\python35\lib\site-packages (from setuptools->protobuf>=3.2.0->tensorflow-gpu)
Requirement already up-to-date: pyparsing in c:\users\arthu\appdata\local\programs\python\python35\lib\site-packages (from packaging>=16.8->setuptools->protobuf>=3.2.0->tensorflow-gpu)
Installing collected packages: tensorflow-gpu
Successfully installed tensorflow-gpu-1.1.0
```

Importing tensorflow:
```
λ python
Python 3.5.3 (v3.5.3:1880cb95a742, Jan 16 2017, 16:02:32) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\arthu\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\arthu\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 914, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\arthu\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\arthu\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\arthu\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\arthu\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\arthu\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\arthu\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 51, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\arthu\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\arthu\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\arthu\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 914, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\arthu\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\arthu\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\arthu\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\arthu\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

"
9818,fatal error: 'infiniband/verbs.h' file not found,"When building Tensorflow from Source

```
ERROR: /Users/dendisuhubdy/dev/dlframeworks/tensorflow/tensorflow/contrib/verbs/BUILD:104:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed: cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 135 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
In file included from tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:18:
In file included from ./tensorflow/contrib/verbs/rdma_rendezvous_mgr.h:21:
In file included from ./tensorflow/contrib/verbs/rdma_mgr.h:24:
./tensorflow/contrib/verbs/rdma.h:21:10: fatal error: 'infiniband/verbs.h' file not found
#include <infiniband/verbs.h>
         ^
1 error generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 4656.328s, Critical Path: 2662.37s
```"
9816,Variable validate_shape not honored,"A Variable with validate_shape=True does not seem to be honored and throws a error.  I am using two Variables to store the contents of a SparseTensor and the assign fails because the sizes do not match on one, but not the other Variable.  

Platform:  Ubuntu 14.04
Code:  Both build from source and binary release
Version: 1.1.0

2017-05-10 14:36:12.973654: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1] rhs shape= [10]
	 [[Node: Assign = Assign[T=DT_INT32, _class=[""loc:@Variable_1""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable_1, SparseAdd:1)]]
2017-05-10 14:36:12.973661: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1] rhs shape= [10]
	 [[Node: Assign = Assign[T=DT_INT32, _class=[""loc:@Variable_1""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable_1, SparseAdd:1)]]
2017-05-10 14:36:12.973657: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1] rhs shape= [10]
	 [[Node: Assign = Assign[T=DT_INT32, _class=[""loc:@Variable_1""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable_1, SparseAdd:1)]]
done
Traceback (most recent call last):
  File ""parse.py"", line 52, in <module>
    print(sess.run([st2, asop2]))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 778, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 982, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1032, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1052, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [1] rhs shape= [10]
	 [[Node: Assign = Assign[T=DT_INT32, _class=[""loc:@Variable_1""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable_1, SparseAdd:1)]]

Caused by op u'Assign', defined at:
  File ""parse.py"", line 39, in <module>
    asop2 = tf.assign(var_feature_count_cnt, st2.values)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py"", line 270, in assign
    validate_shape=validate_shape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py"", line 47, in assign
    use_locking=use_locking, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 768, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2336, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1228, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [1] rhs shape= [10]
	 [[Node: Assign = Assign[T=DT_INT32, _class=[""loc:@Variable_1""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable_1, SparseAdd:1)]]



Code:
import tensorflow as tf


# parse logic
# start with example feature list
# parse into distinct tokens
# collect counts
file_queue = tf.FIFOQueue(100, [tf.string])

# the feature count that we maintain between batches
# must be kept in a variable, but variables dont support sparse tensors
# so we have to keep the source index and value arrays
var_feature_count_idx = tf.Variable(tf.zeros([1,1],dtype=tf.int64), validate_shape=False)
var_feature_count_cnt = tf.Variable(tf.zeros([1],dtype=tf.int32), validate_shape=False)

#read a chunk of features
reader = tf.TextLineReader()
_, line = reader.read_up_to(file_queue, 1000)

#parse into tokens
tokens = tf.string_split(line,delimiter='\t')
vals = tf.string_to_hash_bucket_fast(tokens.values, 1024) # hash
y, idx, count = tf.unique_with_counts(vals)  # get distinct
y2 = tf.expand_dims(y,1)

# now create a sparse array with the hashbucket of as the index
fcount = tf.SparseTensor(indices=y2,values=count,dense_shape=[1024]) # running count of features

# update the global count
st = tf.SparseTensor(indices=var_feature_count_idx, values=tf.identity(var_feature_count_cnt), dense_shape=[1024])
st2 = tf.sparse_add(st, fcount)
#asop1 = tf.assign(var_feature_count_idx, st2.indices)
asop2 = tf.assign(var_feature_count_cnt, st2.values)

with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  sess.run(file_queue.enqueue_many((['features.txt'],)))
  sess.run(file_queue.close())

  try:
    while True:
      print(sess.run([st2, asop2]))
  except tf.errors.OutOfRangeError:
    print 'load finished!'
  finally:
    print 'done'


"
9814,failed to load the native TensorFlow runtime,"Hi! 

I'm worked with Tensorflow on a CPU without. No I'm trying to run it on a device with following specs:
CPU: Intel Xeon(E5-2670)  and win7 64bit and NVIDIA GeForce GTX 980 Ti.
I've installed python3.5 and Tensorflow for GPU just as described in TF homepage.   when I run a test program here what I get when  I try to import Tensorflow : 


```
(C:\Users\Engine>python
Python 3.5.3 (v3.5.3:1880cb95a742, Jan 16 2017, 16:02:32) [MSC v.1900 64 bit (AM
D64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

>>> import tensorflow as tf
Traceback (most recent call last):](url)
  File ""C:\Users\Engine\AppData\Local\Programs\Python\Python35\lib\site-pack
ages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_h
elper
    return importlib.import_module(mname)
  File ""C:\Users\Engine\AppData\Local\Programs\Python\Python35\lib\importlib
\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 914, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Engine\AppData\Local\Programs\Python\Python35\lib\site-pack
ages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Engine\AppData\Local\Programs\Python\Python35\lib\site-pack
ages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Engine\AppData\Local\Programs\Python\Python35\lib\site-pack
ages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_h
elper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Engine\AppData\Local\Programs\Python\Python35\lib\importlib
\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Engine\AppData\Local\Programs\Python\Python35\lib\site-pack
ages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Engine\AppData\Local\Programs\Python\Python35\lib\site-pack
ages\tensorflow\python\__init__.py"", line 51, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Engine\AppData\Local\Programs\Python\Python35\lib\site-pack
ages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Engine\AppData\Local\Programs\Python\Python35\lib\site-pack
ages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_h
elper
    return importlib.import_module(mname)
  File ""C:\Users\Engine\AppData\Local\Programs\Python\Python35\lib\importlib
\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 914, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Engine\AppData\Local\Programs\Python\Python35\lib\site-pack
ages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Engine\AppData\Local\Programs\Python\Python35\lib\site-pack
ages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Engine\AppData\Local\Programs\Python\Python35\lib\site-pack
ages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_h
elper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Engine\AppData\Local\Programs\Python\Python35\lib\importlib
\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_probl
ems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>
```"
9813,Failed to load the native TensorFlow runtime.,
9811,Tensorflow Session Memory Leak,"### System information
- **Have I written custom code :                                     YES
- **OS Platform and Distribution :                                   Mac OS
- **TensorFlow installed from (source or binary)**:      From Source
- **TensorFlow version (use command below)**:         tensorflow (1.1.0)
- **Bazel version (if compiling from source)**:             bazel-0.4.5
- **CUDA/cuDNN version**:                                            not used
- **GPU model and memory**:                                        not used
- **Exact command to reproduce**:                               used in an app


### Describe the problem

I am using the tensorlfow C++ API and I have linked the tensor flow framework to perform a prediction using an inference.pb file.

The inference works but I have a Memory leak which (according to instruments) is linked to the session->Close().

My function is:

```
- (double) MLPredictionObjC: (float[24]) inputFeatures {
    double results[]={0.0};

    NSString *path = [[NSBundle mainBundle] pathForResource:@""inference84"" ofType:@""pb""];

    if ([self loadGraphFromPath:path] && [self createSession]) {
        double resultsP = [self predictML:inputFeatures];
        session->Close();
        return resultsP;
    }
    return *results;
}
```

// the other functions associated with loading and creating the session are:

```
- (BOOL)loadGraphFromPath:(NSString *)path  
{
    ReadBinaryProto(tensorflow::Env::Default(), path.fileSystemRepresentation, &graph);
    return YES;
}

- (BOOL)createSession
{
        tensorflow::SessionOptions options;
        tensorflow::NewSession(options, &session);
        session->Create(graph);
        return YES;
}
```

I am not sure how to resolve this? should I be using std::unique_ptr session?


"
9810,how to install the previous tf version on windows (based python2.7)? like 0.9 or 0.1....,
9809,Bug of CPU detection?,"### System information
Linux 4.9.0-kali4-686-pae #1 SMP Debian 4.9.25-1kali1 (2017-05-04) i686 GNU/Linux
TensorFlow installed from source code
TensorFlow version : 1.0.1
Bazel version : 0.4.2
CUDA/cuDNN version: None
GPU model and memory: None

> 
> == cat /etc/issue ===============================================
> Linux 4.9.0-kali4-686-pae #1 SMP Debian 4.9.25-1kali1 (2017-05-04) i686 GNU/Linux
> VERSION=""2017.1""
> VERSION_ID=""2017.1""
> 
> == are we in docker =============================================
> No
> 
> == compiler =====================================================
> c++ (Debian 6.3.0-16) 6.3.0 20170425
> Copyright (C) 2016 Free Software Foundation, Inc.
> This is free software; see the source for copying conditions.  There is NO
> warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
> 
> 
> == uname -a =====================================================
> Linux 4.9.0-kali4-686-pae #1 SMP Debian 4.9.25-1kali1 (2017-05-04) i686 GNU/Linux
> 
> == check pips ===================================================
> numpy (1.12.1)
> 
> == check for virtualenv =========================================
> False
> 
> == tensorflow import ============================================
> Traceback (most recent call last):
>   File ""<string>"", line 1, in <module>
>   File ""tensorflow/__init__.py"", line 24, in <module>
>     from tensorflow.python import *
>   File ""tensorflow/python/__init__.py"", line 72, in <module>
>     raise ImportError(msg)
> ImportError: Traceback (most recent call last):
>   File ""tensorflow/python/__init__.py"", line 61, in <module>
>     from tensorflow.python import pywrap_tensorflow
> ImportError: cannot import name pywrap_tensorflow
> 
> 
> Failed to load the native TensorFlow runtime.
> 
> See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error
> 
> for some common reasons and solutions.  Include the entire stack trace
> above this error message when asking for help.
> 
> == env ==========================================================
> LD_LIBRARY_PATH is unset
> DYLD_LIBRARY_PATH is unset
> 
> == nvidia-smi ===================================================
> 
> == cuda libs  ===================================================

### Describe the problem
I compiled it from source,  and got this fatal error when try to import tensorflow in python3.5.3:


```
Python 3.5.3 (default, Jan 19 2017, 14:11:04)
[GCC 6.3.0 20170118] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
F tensorflow/core/platform/cpu_feature_guard.cc:35] The TensorFlow library was compiled to use SSE instructions, but these aren't available on your machine.
Aborted
```

And here is the information of my CPU:

```
.......
processor    : 3
vendor_id    : GenuineIntel
cpu family    : 6
model        : 37
model name    : Intel(R) Core(TM) i3 CPU       M 370  @ 2.40GHz
stepping    : 5
microcode    : 0x4
cpu MHz        : 933.000
cache size    : 3072 KB
physical id    : 0
siblings    : 4
core id        : 2
cpu cores    : 2
apicid        : 5
initial apicid    : 5
fdiv_bug    : no
f00f_bug    : no
coma_bug    : no
fpu        : yes
fpu_exception    : yes
cpuid level    : 11
wp        : yes
flags        : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe nx rdtscp lm constant_tsc arch_perfmon pebs bts xtopology nonstop_tsc aperfmperf eagerfpu pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm arat
bugs        :
bogomips    : 4787.91
clflush size    : 64
cache_alignment    : 64
address sizes    : 36 bits physical, 48 bits virtual
......

```

that's all."
9808,multiple kernel_constraint error: keyword argument repeated,"When I did a conv2d operation like this:
`    model.add(Convolution2D(k_size_0, 3, 3, init='he_normal', border_mode='valid', 
                        input_shape=(image_width, image_height, 1), 
                        kernel_constraint=max_norm(3., axis=0), 
                        kernel_constraint=nonneg(),
                         activity_regularizer = keras.regularizers.l1(0.0000008)))`

I put **two** constraints for `kernel_constraint` and there was an error:
`SyntaxError: keyword argument repeated`

So can I pass a **list** with multiple items to the kernel_constraint argument instead of only one?
Thank you"
9806,(pandas) read_csv(compression='gzip') fails while reading compressed file with tf.gfile.GFile in Python 2,"Originally I opened an issue on pandas, apparently it maybe some bug in tensorflow side and they asked to be verified here. The original issue is pandas-dev/pandas#16241

I'm pasting bellow the same steps I used to replicate the behavior in the original issue. The last comment from pandas was about: ""At a glance, it looks like gfile.GFile doesn't follow python's IO interface for seek"" (please see more details on the original issue linked above)

#### Code Sample, a copy-pastable example if possible

Sample (1)
```python
import tensorflow as tf
import pandas as pd
with tf.gfile.GFile('test.csv') as f: pd.read_csv(f)
```

Sample (2)
```python
import tensorflow as tf
import pandas as pd
with tf.gfile.GFile('test.csv.gz') as f: pd.read_csv(f, compression='gzip')
```

#### Problem description

I'm converting some code to run on Google Cloud, and in the process I'm changing the way my datasets are read. I started using tf.gfile.GFile implementation from Tensorflow, as it is portable and can read both local files and files from storage buckets.

Also in the process I'm changing my code to work with Python 2 instead of Python 3.

Not sure if it is a bug in Pandas or Tensorflow code, but this issue seems similar to #14222, so I'm opening an issue here first.

To reproduce, create two files: test.csv and test.csv.gz locally. Run both samples in python 2. The sample (1) works fine, but sample (2) crashes with an error: ""AttributeError: 'NoneType' object has no attribute 'Tell'""

Strangely, both samples work fine in python 3. I'm using the same library versions in python 2 and 3: pandas 0.19.2 and tensorflow 1.0.

#### Expected Output

Both samples should work in python 2.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 2.7.13.final.0
python-bits: 64
OS: Darwin
OS-release: 16.5.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.19.2
nose: None
pip: 9.0.1
setuptools: 32.1.0
Cython: None
numpy: 1.12.0
scipy: 0.19.0
statsmodels: None
xarray: None
IPython: 5.3.0
sphinx: None
patsy: None
dateutil: 2.5.2
pytz: 2016.10
blosc: None
bottleneck: None
tables: None
numexpr: None
matplotlib: 2.0.0
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
httplib2: 0.10.3
apiclient: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
boto: None
pandas_datareader: None
</details>

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: see sample code above
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macos
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.0.0
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: see above

"
9804,OpenCL support,When will TensorFlow be supporting OpenCL?
9801,"ValueError: Cannot feed value of shape (1, 40, 3) for Tensor u'cls1_fc_pose_xyz_target:0', which has shape '(?, ?)","I have installed keras using conda with tensorflow backend. I am experimenting with the keras version of posenet architecture. I have introduced a few LSTM layers into the original architecture and now my input should be of 5 dim(n_batch,n_frame,row,col,channel) and my model compiles correctly but when I call the fit function, it throws following **error:**

    Train on 129 samples, validate on 129 samples

    Epoch 1/800

    Traceback (most recent call last):

    File ""train.py"", line 72, in <module>

    callbacks=[checkpointer])

    File ""/u/some_user/conda-envs/my_root2/lib/python2.7/site-packages/keras/engine/training.py"", line 1485, in fit

    initial_epoch=initial_epoch)

    File ""/u/some_user/conda-envs/my_root2/lib/python2.7/site-packages/keras/engine/training.py"", line 1140, in _fit_loop

    outs = f(ins_batch)

    File ""/u/some_user/conda-envs/my_root2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py"", line 2073, in __call__

    feed_dict=feed_dict)

    File ""/u/some_user/conda-envs/my_root2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 767, in run

    run_metadata_ptr)

    File ""/u/some_user/conda-envs/my_root2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 944, in _run

    % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))

    ValueError: Cannot feed value of shape (1, 40, 3) for Tensor u'cls1_fc_pose_xyz_target:0', which has shape '(?, ?)'
`

and here the piece of code that I am using in **training**:

    X_train=np.squeeze(np.array(dataset_train.images,dtype=float))

    X_test=np.squeeze(np.array(dataset_test.images,dtype=float))

    y_train=np.squeeze(np.array(dataset_train.poses,dtype=float))

    print(""X_train shape:""+str(X_train.shape))#X_train shape:(129, 40, 224, 224, 3)

    print(""y_train shape:""+str(y_train.shape))#y_train shape:(129, 40, 7)

    y_train_x = y_train[:,:,0:3]

    y_train_q = y_train[:,:,3:7]

    y_test = np.squeeze(np.array(dataset_test.poses))

    print(""X_test shape:""+str(X_test.shape))#X_test shape:(129, 40, 224, 224, 3)

    print(""y_test shape:""+str(y_test.shape))#y_test shape:(129, 40, 7)

    y_test_x = y_test[:,:,0:3]

    y_test_q = y_test[:,:,3:7]

    #Setup checkpointing
    checkpointer = ModelCheckpoint(filepath=""checkpoint_weights.h5"", verbose=1, save_best_only=True, save_weights_only=True)

    model.fit(X_train, [y_train_x, y_train_q, y_train_x, y_train_q, y_train_x, y_train_q],
      batch_size=batch_size,
      nb_epoch=800,
      verbose=1,
      validation_data=(X_test, [y_test_x, y_test_q, y_test_x, y_test_q, y_test_x, y_test_q]),
      callbacks=[checkpointer])
and here is the **input** of my model:

    input = Input(shape=(helper.frames_per_sequence,224, 224, 3))

and here is the **lines causing error**:

    cls1_fc1_flat = Flatten()(cls1_reduction_pose)

    cls1_fc1_pose = Dense(1024,activation='relu',name='cls1_fc1_pose')(cls1_fc1_flat)

    cls1_fc_pose_xyz = Dense(3,name='cls1_fc_pose_xyz')(cls1_fc1_pose)

    cls1_fc_pose_wpqr = Dense(4,name='cls1_fc_pose_wpqr')(cls1_fc1_pose)"
9800,"TensorBoard relies on a version of ""tsify"" which includes an incompatible TypeScript version","The version of ""tsify"" in the package.json is:

""tsify"": ""^0.14.8""

but this package includes a 1.x version of typescript. Therefore, compiling certain constructs e.g. ""number | null"" fails.

This can be (seemingly) resolved by changing the version of tsify to a more recent version not relying on its own TypeScript package.

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from**: Source (master)
- **TensorFlow version**: last commit to tensorboard @ 9dd8e7aec9d3a8ddc458af01c2e51541ad876fb8
- **Exact command to reproduce**: ""gulp""
"
9799,"TensorBoard ""gulp"" assigns to const","The TensorBoard build is all kinds of broken, but the latest introduction to the tree includes an assignment to a const within the gulpfile.

In particular:

tensorflow/tensorboard/gulp_tasks/compiler.js:51 reassigns to the variable ""entries"" which is marked with const.

The same occurs on the next line for the variable ""deps"".

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from**: Source (master)
- **TensorFlow version**: last commit to tensorboard @ 9dd8e7aec9d3a8ddc458af01c2e51541ad876fb8
- **Exact command to reproduce**: ""gulp""
"
9798,What will dynamic_rnn reuse when set reuse of variable_scope as true,"I'm sorry but I havn't found a clear answer in StackOverflow

I wonder what will dynamic_rnn reuse when set reuse of variable_scope as true, only the gate parameters will be share or including the output & status?

The example is that my code wanna process two sequence with different length by one bidirectional LSTM network, and then compare them for further use. Will I set reuse as True when process the second sequence at one step? Do I need to clear the state in this situation?
"
9797,Bijector caching breaks when used with TransformedDistribution,"Currently, the caching that [Bijector](https://github.com/tensorflow/tensorflow/blob/7fa0cf39f854d5fdaaa19ad6425dfed02f5fea64/tensorflow/python/ops/distributions/bijector_impl.py#L114) objects do to avoid unnecessary calculations does not work when the bijector is used in a [TransformedDistribution](https://github.com/tensorflow/tensorflow/blob/7fa0cf39f854d5fdaaa19ad6425dfed02f5fea64/tensorflow/python/ops/distributions/transformed_distribution.py#L124) object. I believe the culprit is the reshaping that the distribution object does [here](https://github.com/tensorflow/tensorflow/blob/7fa0cf39f854d5fdaaa19ad6425dfed02f5fea64/tensorflow/python/ops/distributions/distribution.py#L640); when we call Bijector.inverse on the output, the Bijector object cannot tell that this is merely a reshaped version of what it calculated previously.

My use case is to sample from a TransformedDistribution and then later calculate the log probability of that sample.

This issue is particularly a problem when using a bijector whose inverse is numerically delicate (in my case, I'm chaining together softplus bijectors and my own custom affine bijector).

I'm willing to work on a fix for this problem, but I'm not sure what the best way to do it is (adding caching to the TransformedDistribution code might work, but that seems like code duplication).

### System information
- **OS Platform and Distribution**: Ubuntu 16.04
- **TensorFlow installed from**: Source
- **TensorFlow version**: v1.1.0-rc2-773-g7fa0cf3 (commit 7fa0cf39f854d5fdaaa19ad6425dfed02f5fea64)
- **Bazel version**: 0.4.5"
9796,doing inference using batch normalization with only one example,"I build my model using batch normalization with `tf.layers.batch_normalization` . But when i am doing inference using saved model, I have to set the parameter `training=True`  in `tf.layers.batch_normalization` to make it work. Does it should be `training=False`?  Moreover, I have to feed a batch of test examples to make it work. It fails when fed with only one example. 
So what is the right way to use `tf.layers.batch_normalization`?
"
9795,Custom configuration of tf.estimator.Estimator - unfavorable change on tf1.1,"This is a feature request, following a change in behavior from TF1.0 to TF1.1.

------------------------

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 8 (jessie) 64-bit
- **TensorFlow installed from (source or binary)**: installed using pip
- **TensorFlow version (use command below)**: v1.1.0-rc0-61-g1ec6ed5
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 
    my_estimator = tf.estimator.Estimator(\
        model_fn = my_model_fn, 
        model_dir = my_model_dir,
        config=tf.contrib.learn.RunConfig(
            save_checkpoints_steps=20,
            save_checkpoints_secs=None,
            save_summary_steps=40,
        )
    )

### Describe the problem
on tf version 1.0 we could easily configure the checkpoint dump (and more) of tf.contrib.learn.Estimator, as stated above: by sending as an input to the estimator tf.contrib.learn.RunConfig with the desired configuration.
on tf version 1.1 this became more complex: the above (and more) configurations in RunConfig are static, and changing them requires either changing the original tf code, or maybe creating a class inheriting from tf.estimator.RunConfig to serve as the config for the estimator.

### Source code / logs
in tf.estimator.RunConfig (v1.1), e.g.:
  @property
  def save_checkpoints_secs(self):
    return 600

in tf.contrib.learn.RunConfig (v1.0, yet supported on tf1.1):
  @property
  def save_checkpoints_secs(self):
    return self._save_checkpoints_secs
"
9794,how to get image shape after decode in C++ ,"### System information
- **OS Platform and Distribution**: Debian
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.0.1
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: cuda-8.0, cudnn5.1.5
- **GPU model and memory**: 12GB

I follow the tutorial of [inception label_image](https://www.tensorflow.org/tutorials/image_recognition),  
[source codes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc) , 
[README.md](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image) , I can compile and run the demo c++ code successfully.

I want to adapt this demo to my own project, the input images to my own Network is height fixed, while width varies accordingly, for example, the original image is size of 64x100, and I want to resize it to 32x50, as I said 32 is the new_height, and I want to know original image size after reading from the file, how can I get width=100 and height=64? then I can get new_width = new_height/height x width=32/64x100=50

the following is a small piece of the image_recognition tutorial C++ codes, resize is hard coded to a pre-define size, I try `float_caster.shape()`, `tensor()`, `float_caster.dimension(0)`, etc, all failed(`float_caster`, `file_reader` are all not `Tensor`, I don't know why Google design like this, really slow down the development, and I find no documentation about this), is there any easy way to get the image size? or cast the `tensorflow::Ouput` type to `Tensor`?

one possible way is first use opencv to load the image, and resize it, then copy the elements to tensor like this [example](https://gist.github.com/kyrs/9adf86366e9e4f04addb) **pixel by pixel**, but the performance is the main problem and it seems hard to compile tensorflow along with opencv.  Any one knows some methods using tensorflow's API?

Thanks in advance!

```

// Given an image file name, read in the data, try to decode it as an image,
// resize it to the requested size, and then scale the values as desired.
Status ReadTensorFromImageFile(string file_name, const int input_height,
                               const int input_width, const float input_mean,
                               const float input_std,
                               std::vector<Tensor>* out_tensors) {
  auto root = tensorflow::Scope::NewRootScope();
  using namespace ::tensorflow::ops;  // NOLINT(build/namespaces)

  string input_name = ""file_reader"";
  string output_name = ""normalized"";
  auto file_reader =
      tensorflow::ops::ReadFile(root.WithOpName(input_name), file_name);
  // Now try to figure out what kind of file it is and decode it.
  const int wanted_channels = 3;
  tensorflow::Output image_reader;
  if (tensorflow::StringPiece(file_name).ends_with("".png"")) {
    image_reader = DecodePng(root.WithOpName(""png_reader""), file_reader,
                             DecodePng::Channels(wanted_channels));
  } else if (tensorflow::StringPiece(file_name).ends_with("".gif"")) {
    image_reader = DecodeGif(root.WithOpName(""gif_reader""), file_reader);
  } else {
    // Assume if it's neither a PNG nor a GIF then it must be a JPEG.
    image_reader = DecodeJpeg(root.WithOpName(""jpeg_reader""), file_reader,
                              DecodeJpeg::Channels(wanted_channels));
  }
  // Now cast the image data to float so we can do normal math on it.
  auto float_caster =
      Cast(root.WithOpName(""float_caster""), image_reader, tensorflow::DT_FLOAT);
  // The convention for image ops in TensorFlow is that all images are expected
  // to be in batches, so that they're four-dimensional arrays with indices of
  // [batch, height, width, channel]. Because we only have a single image, we
  // have to add a batch dimension of 1 to the start with ExpandDims().
  auto dims_expander = ExpandDims(root, float_caster, 0);
  // Bilinearly resize the image to fit the required dimensions.
  auto resized = ResizeBilinear(
      root, dims_expander,
      Const(root.WithOpName(""size""), {input_height, input_width}));
  // Subtract the mean and divide by the scale.
  Div(root.WithOpName(output_name), Sub(root, resized, {input_mean}),
      {input_std});

  // This runs the GraphDef network definition that we've just constructed, and
  // returns the results in the output tensor.
  tensorflow::GraphDef graph;
  TF_RETURN_IF_ERROR(root.ToGraphDef(&graph));

  std::unique_ptr<tensorflow::Session> session(
      tensorflow::NewSession(tensorflow::SessionOptions()));
  TF_RETURN_IF_ERROR(session->Create(graph));
  TF_RETURN_IF_ERROR(session->Run({}, {output_name}, {}, out_tensors));
  return Status::OK();
}
```"
9793,is model ready when Fine-tuning in distributed case,"When we finetune a model on a different task, only a part of vars in the model are restored from the pretrained task and others are left as initial values.
As many docs recommends([page1](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim) [page2](https://github.com/tensorflow/models/blob/master/inception/inception/inception_train.py)), when training with a local graph, we restore the pretrained model **after** running the global init op(call restoring in ""init_fn"" if MonitoredSession or supervisor is included).
But in the distributed case, does global init op make ""model_ready"" returns true before the restoring-model called? other non-chief nodes will use the ""not ready"" values. 

"
9790,ResourceExhaustedError,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.1.0
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: 8/6
- **GPU model and memory**: NVIDIA GTX 1080 TI, 11 GB
- **Exact command to reproduce**:

```python
# Disable linter warnings to maintain consistency with tutorial.
# pylint: disable=invalid-name

import argparse
import sys
from collections import namedtuple

import numpy as np
import tensorflow as tf
import tensorflow.contrib.slim as slim

from dataset import DataSet

FLAGS = None
HEIGHT = 276
WIDTH = 72
DEPTH = 3
INPUT_DIMENSION = HEIGHT * WIDTH * DEPTH
NUMBER_CLASSES = 2

import numpy as np
import tensorflow as tf

class DataSet(object):

    def __init__(self,
                 images,
                 labels,
                 dtype=tf.float32,
                 reshape=False):
        dtype = tf.as_dtype(dtype).base_dtype
        if dtype not in (tf.uint8, tf.float32):
            raise TypeError(
                'Invalid image dtype %r, expected uint8 or float32' % dtype)

        assert images.shape[0] == labels.shape[0], (
            'images.shape: %s labels.shape: %s' % (images.shape, labels.shape))
        self._num_examples = images.shape[0]

        # Convert shape from [num examples, rows, columns, depth]
        # to [num examples, rows*columns] (assuming depth == 1)
        if reshape:
            assert images.shape[3] == 1
            images = images.reshape(images.shape[0],
                                    images.shape[1] * images.shape[2])

        self._images = images
        self._labels = labels
        self._epochs_completed = 0
        self._index_in_epoch = 0

    @property
    def images(self):
        return self._images

    @property
    def labels(self):
        return self._labels

    @property
    def num_examples(self):
        return self._num_examples

    @property
    def epochs_completed(self):
        return self._epochs_completed

    def next_batch(self, batch_size, shuffle=True):
        """"""Return the next `batch_size` examples from this data set.""""""
        start = self._index_in_epoch
        # Shuffle for the first epoch
        if self._epochs_completed == 0 and start == 0 and shuffle:
            perm0 = np.arange(self._num_examples)
            np.random.shuffle(perm0)
            self._images = self.images[perm0]
            self._labels = self.labels[perm0]
        # Go to the next epoch
        if start + batch_size > self._num_examples:
            # Finished epoch
            self._epochs_completed += 1
            # Get the rest examples in this epoch
            rest_num_examples = self._num_examples - start
            images_rest_part = self._images[start:self._num_examples]
            labels_rest_part = self._labels[start:self._num_examples]
            # Shuffle the data
            if shuffle:
                perm = np.arange(self._num_examples)
                np.random.shuffle(perm)
                self._images = self.images[perm]
                self._labels = self.labels[perm]
            # Start next epoch
            start = 0
            self._index_in_epoch = batch_size - rest_num_examples
            end = self._index_in_epoch
            images_new_part = self._images[start:end]
            labels_new_part = self._labels[start:end]
            return np.concatenate((images_rest_part, images_new_part), axis=0),\
                np.concatenate((labels_rest_part, labels_new_part), axis=0)
        else:
            self._index_in_epoch += batch_size
            end = self._index_in_epoch
            return self._images[start:end], self._labels[start:end]


def deepnn(x):
    x_image = tf.reshape(x, [-1, DEPTH, HEIGHT, WIDTH])
    is_training = tf.placeholder(tf.bool)

    with slim.arg_scope([slim.conv2d, slim.max_pool2d],
                        data_format='NCHW', padding='SAME'):
        with slim.arg_scope([slim.conv2d, slim.fully_connected],
                            weights_initializer=tf.truncated_normal_initializer(stddev=0.1),
                            biases_initializer=tf.constant_initializer(0.1)):
            with slim.arg_scope([slim.dropout],
                                is_training=is_training):
                net = slim.conv2d(x_image, 64, [5, 5], scope='conv1')
                net = slim.max_pool2d(net, kernel_size=3, stride=2, scope='pool1')
                net = slim.conv2d(x_image, 64, [5, 5], scope='conv2')
                net = slim.max_pool2d(net, kernel_size=3, stride=2, scope='pool2')
                net = slim.flatten(net, scope=""Flatten"")
                net = slim.fully_connected(net, 384, scope='fc_1',
                                           weights_regularizer=slim.l2_regularizer(0.000005))
                net = slim.fully_connected(net, 192, scope='fc_2',
                                           weights_regularizer=slim.l2_regularizer(0.000005))
                net = slim.fully_connected(net, 2, activation_fn=None, scope='fc_out')

    return net, is_training


def import_images_and_labels():
    file_path = ""/path/to/file/samples.npz""
    data = np.load(file_path)

    print(data['one_hot_labels'].shape)
    print(data['images'].shape)

    images = data['images'].astype(np.float32)
    labels = data['one_hot_labels'].astype(np.float32)

    number_training_samples = 17950
    train_data = DataSet(images=images[:number_training_samples, :],
                         labels=labels[:number_training_samples, :])
    validation_data = DataSet(images=images[16000:17950, :], labels=labels[16000:17950, :])
    test_data = DataSet(images=images[17950:-1, :], labels=labels[17950: -1, :])

    DataSets = namedtuple('DataSets', ['train', 'validation', 'test'])

    return DataSets(train=train_data,
                    validation=validation_data,
                    test=test_data)


def main(_):
    dataset = import_images_and_labels()

    # Create the model
    x = tf.placeholder(tf.float32, [None, INPUT_DIMENSION])

    # Define loss and optimizer
    y_ = tf.placeholder(tf.float32, [None, NUMBER_CLASSES])

    # Build the graph for the deep net
    y_conv, is_training = deepnn(x)

    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_,
                                                                           logits=y_conv))
    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

    with tf.Session(config=tf.ConfigProto(inter_op_parallelism_threads=1)) as sess:
        sess.run(tf.global_variables_initializer())

        for i in range(20000):
            batch = dataset.train.next_batch(50)

            if i % 100 == 0:
                train_accuracy = accuracy.eval(feed_dict={x: batch[0],
                                                          y_: batch[1],
                                                          is_training: False})
                print('step %d, training accuracy %g' % (i, train_accuracy))

            train_step.run(feed_dict={x: batch[0],
                                      y_: batch[1],
                                      is_training: True})

        print('test accuracy %g' % accuracy.eval(feed_dict={x: dataset.test.images,
                                                            y_: dataset.test.labels,
                                                            is_training: False}))


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_dir', type=str,
                        default='/tmp/tensorflow/mnist/input_data',
                        help='Directory for storing input data')
    FLAGS, unparsed = parser.parse_known_args()
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)

```

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

```

== cat /etc/issue ===============================================
Linux pcdekon0082 4.8.0-51-generic #54~16.04.1-Ubuntu SMP Wed Apr 26 16:00:28 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.2 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux pcdekon0082 4.8.0-51-generic #54~16.04.1-Ubuntu SMP Wed Apr 26 16:00:28 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.11.0)
protobuf (3.2.0)
tensorflow (1.1.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named tensorflow

== env ==========================================================
LD_LIBRARY_PATH /usr/local/lib:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/lib/nvidia-375
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Tue May  9 13:52:52 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Graphics Device     Off  | 0000:01:00.0      On |                  N/A |
| 26%   46C    P8    18W / 250W |    378MiB / 11171MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1514    G   /usr/lib/xorg/Xorg                             190MiB |
|    0      1904    G   kwin_x11                                        41MiB |
|    0      1910    G   /usr/bin/krunner                                 2MiB |
|    0      1915    G   /usr/bin/plasmashell                            94MiB |
|    0      2047    G   ...s-passed-by-fd --v8-snapshot-passed-by-fd    46MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7

```

### Describe the problem
When loading the test images in order to determine the accuracy, an ResourceExhaustedError is thrown.
This network model works fine with a private theano based framework.

### Source code / logs
```
...
...
step 19800, training accuracy 1
step 19900, training accuracy 1
2017-05-09 13:13:15.970193: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (GPU_0_bfc)
ran out of memory trying to allocate 9.21GiB.  Current allocation summary follows.
2017-05-09 13:13:15.970227: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256):   Total Chu
nks: 1, Chunks in use: 0 256B allocated for chunks. 4B client-requested for chunks. 0B in use in bin. 0B
client-requested in use in bin.
2017-05-09 13:13:15.970239: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512):   Total Chu
nks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl
ient-requested in use in bin.
2017-05-09 13:13:15.970247: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024):  Total Chu
nks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl
ient-requested in use in bin.
2017-05-09 13:13:15.970257: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048):  Total Chu
nks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl
ient-requested in use in bin.
2017-05-09 13:13:15.970266: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096):  Total Chu
nks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl
ient-requested in use in bin.
2017-05-09 13:13:15.970275: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192):  Total Chu
nks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl
ient-requested in use in bin.
2017-05-09 13:13:15.970285: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384):         T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970295: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768):         T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970304: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536):         T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970313: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072):        T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970325: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144):        T
otal Chunks: 1, Chunks in use: 0 288.0KiB allocated for chunks. 18.8KiB client-requested for chunks. 0B i
n use in bin. 0B client-requested in use in bin.
2017-05-09 13:13:15.970334: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288):        T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970342: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576):       T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970350: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152):       T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970357: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304):       T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970365: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608):       T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970374: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216):      T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970383: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432):      T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970392: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864):      T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970401: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728):     T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970413: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456):     T
otal Chunks: 1, Chunks in use: 0 7.61GiB allocated for chunks. 242.58MiB client-requested for chunks. 0B
in use in bin. 0B client-requested in use in bin.
2017-05-09 13:13:15.970423: I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 9.21GiB was 25
6.00MiB, Chunk State:
2017-05-09 13:13:15.970436: I tensorflow/core/common_runtime/bfc_allocator.cc:666]   Size: 7.61GiB | Requ
ested Size: 242.58MiB | in_use: 0, prev:   Size: 465.75MiB | Requested Size: 465.75MiB | in_use: 1
2017-05-09 13:13:15.970444: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600000
 of size 1280
2017-05-09 13:13:15.970450: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600500
 of size 256
2017-05-09 13:13:15.970456: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600600
 of size 256
2017-05-09 13:13:15.970463: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600700
 of size 256
2017-05-09 13:13:15.970470: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600800
 of size 256
2017-05-09 13:13:15.970476: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600900
 of size 256
2017-05-09 13:13:15.970483: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600a00
 of size 256
2017-05-09 13:13:15.970490: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600b00
 of size 256
2017-05-09 13:13:15.970496: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600c00
 of size 256
2017-05-09 13:13:15.970504: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600d00
 of size 1536
2017-05-09 13:13:15.970510: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601300
 of size 256
2017-05-09 13:13:15.970517: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601400
 of size 256
2017-05-09 13:13:15.970524: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601500
2017-05-09 13:13:15.970531: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601800
 of size 256
2017-05-09 13:13:15.970538: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601900
 of size 256
2017-05-09 13:13:15.970544: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601a00
 of size 256
2017-05-09 13:13:15.970551: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601b00
 of size 256
2017-05-09 13:13:15.970557: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601c00
 of size 256
2017-05-09 13:13:15.970565: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601d00
 of size 19200
2017-05-09 13:13:15.970571: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d606800
 of size 256
2017-05-09 13:13:15.970578: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d606900
 of size 488374272
2017-05-09 13:13:15.970585: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a7c6900
 of size 1536
2017-05-09 13:13:15.970592: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a7c6f00
 of size 294912
2017-05-09 13:13:15.970599: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a80ef00
 of size 768
2017-05-09 13:13:15.970606: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a80f200
 of size 1536
2017-05-09 13:13:15.970612: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a80f800
 of size 256
2017-05-09 13:13:15.970619: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a80f900
 of size 19200
2017-05-09 13:13:15.970625: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a814400
 of size 256
2017-05-09 13:13:15.970632: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a814500
 of size 1536
2017-05-09 13:13:15.970639: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a814b00
 of size 17664
2017-05-09 13:13:15.970646: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a819000
 of size 256
2017-05-09 13:13:15.970653: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a819100
 of size 294912
2017-05-09 13:13:15.970659: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a861100
 of size 488079360
2017-05-09 13:13:15.970666: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x102479d9100
 of size 1536
2017-05-09 13:13:15.970673: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21700
 of size 768
2017-05-09 13:13:15.970680: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21a00
 of size 256
2017-05-09 13:13:15.970686: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21b00
 of size 256
2017-05-09 13:13:15.970686: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21b00
 of size 256
2017-05-09 13:13:15.970693: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21c00
 of size 256
2017-05-09 13:13:15.970700: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21d00
 of size 256
2017-05-09 13:13:15.970706: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21e00
 of size 256
2017-05-09 13:13:15.970713: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a22000
 of size 256
2017-05-09 13:13:15.970720: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a22100
 of size 256
2017-05-09 13:13:15.970726: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a22200
 of size 256
2017-05-09 13:13:15.970733: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a22300
 of size 19200
2017-05-09 13:13:15.970740: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a26e00
 of size 19200
2017-05-09 13:13:15.970747: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a2b900
 of size 256
2017-05-09 13:13:15.970753: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a2ba00
 of size 256
2017-05-09 13:13:15.970760: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a2bb00
 of size 488374272
2017-05-09 13:13:15.970767: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10264bebb00
 of size 488374272
2017-05-09 13:13:15.970773: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281dabb00
 of size 1536
2017-05-09 13:13:15.970780: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281dac100
 of size 1536
2017-05-09 13:13:15.970787: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281dac700
 of size 294912
2017-05-09 13:13:15.970793: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281df4700
 of size 294912
2017-05-09 13:13:15.970800: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3c700
 of size 768
2017-05-09 13:13:15.970807: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3ca00
 of size 768
2017-05-09 13:13:15.970814: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3cd00
 of size 1536
2017-05-09 13:13:15.970820: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3d300
 of size 1536
2017-05-09 13:13:15.970827: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3d900
 of size 256
2017-05-09 13:13:15.970834: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3da00
 of size 256
2017-05-09 13:13:15.970840: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3db00
 of size 19200
2017-05-09 13:13:15.970847: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e42600
 of size 488374272
2017-05-09 13:13:15.970855: I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x102479d9700
of size 294912
2017-05-09 13:13:15.970861: I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x10247a21f00
of size 256
2017-05-09 13:13:15.970868: I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x1029f002600
of size 8176048640
2017-05-09 13:13:15.970875: I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use
 Chunks by size:
2017-05-09 13:13:15.970884: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 31 Chunks of size 256
totalling 7.8KiB
2017-05-09 13:13:15.970892: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 5 Chunks of size 768 t
otalling 3.8KiB
2017-05-09 13:13:15.970899: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1280
totalling 1.2KiB
2017-05-09 13:13:15.970908: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 9 Chunks of size 1536
totalling 13.5KiB
2017-05-09 13:13:15.970916: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 17664
 totalling 17.2KiB
2017-05-09 13:13:15.970924: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 5 Chunks of size 19200
 totalling 93.8KiB
2017-05-09 13:13:15.970931: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 4 Chunks of size 29491
2 totalling 1.12MiB
2017-05-09 13:13:15.970940: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 48807
9360 totalling 465.47MiB
2017-05-09 13:13:15.970947: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 4 Chunks of size 48837
4272 totalling 1.82GiB
2017-05-09 13:13:15.970955: I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use ch
unks: 2.27GiB
2017-05-09 13:13:15.970966: I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats:
Limit:                 10619240448
InUse:                  2442896640
MaxInUse:               2995339008
NumAllocs:                  803473
MaxAllocSize:            488374272
2017-05-09 13:13:15.970984: W tensorflow/core/common_runtime/bfc_allocator.cc:277] **********************
**____________________________________________________________________________
2017-05-09 13:13:15.971003: W tensorflow/core/framework/op_kernel.cc:1152] Resource exhausted: OOM when a
llocating tensor with shape[1945,64,276,72]
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1039, in _do_ca
ll
    return fn(*args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1021, in _run_f
n
    status, run_metadata)
  File ""/usr/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 466, in
raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[194
5,64,276,72]
         [[Node: conv2/convolution = Conv2D[T=DT_FLOAT, data_format=""NCHW"", padding=""SAME"", strides=[1, 1
, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](Reshape, conv2/weights/r
ead)]]
         [[Node: Mean_1/_9 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/
cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge
_20_Mean_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s
lim.py"", line 145, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s
lim.py"", line 136, in main
    is_training: False}))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 569, in eval
    return _eval_using_default_session(self, feed_dict, self.graph, session)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3741, in _eval_u
sing_default_session
    return session.run(tensors, feed_dict)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 778, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 982, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1032, in _do_ru
n
    target_list, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1052, in _do_ca
ll
raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[194
5,64,276,72]
         [[Node: conv2/convolution = Conv2D[T=DT_FLOAT, data_format=""NCHW"", padding=""SAME"", strides=[1, 1
, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](Reshape, conv2/weights/r
ead)]]
         [[Node: Mean_1/_9 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/
cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge
_20_Mean_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op 'conv2/convolution', defined at:
  File ""/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s
lim.py"", line 145, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s
lim.py"", line 110, in main
    y_conv, is_training = deepnn(x)
  File ""/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s
lim.py"", line 56, in deepnn
    net = slim.conv2d(x_image, 64, [5, 5], scope='conv2')
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", lin
e 181, in func_with_args
    return func(*args, **current_args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 9
18, in convolution
    outputs = layer.apply(inputs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py"", line 320, in apply
    return self.__call__(inputs, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py"", line 290, in __call__
    outputs = self.call(inputs, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/convolutional.py"", line 156, in c
all
    data_format=utils.convert_data_format(self.data_format, self.rank + 2))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py"", line 661, in convolution
    op=op)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py"", line 331, in with_space_
to_batch
    return op(input, num_spatial_dims, padding)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py"", line 653, in op
    name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py"", line 129, in _non_atrous
_convolution
    name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 403, in conv2d
    data_format=data_format, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 768,
in apply_op
    op_def=op_def)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2336, in create_
op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1228, in __init_
_
    self._traceback = _extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1945,64,276,72]
         [[Node: conv2/convolution = Conv2D[T=DT_FLOAT, data_format=""NCHW"", padding=""SAME"", strides=[1, 1
, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](Reshape, conv2/weights/r
ead)]]
         [[Node: Mean_1/_9 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/
cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge
_20_Mean_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
```
"
9789,Tensorflow Support Problem,"In my support tags there is ""py27     winamd64"", but I have installed python 3.5.2 in my Windows 10, now please tell me what should I do to upgrade my support tags to py35 or py36. "
9788,tensorflow for Nvidia TX1,"### System information
- OS Platform and Distribution: Linux Ubuntu 16.4
- Bazel version (if compiling from source): 0.4.4
- CUDA/cuDNN version: cuda-8.0

### Describe the problem

I want to install tensorflow 1.0.0 in Nvidia TX1. I am following [this](http://www.yuthon.com/2017/03/10/TensorFlow-r1-0-on-TX1/) so as to install version 1.0.0. But while installing bazel-0.4.4... I am getting this error 

### Logs

	INFO: You can skip this first step by providing a path to the bazel binary as second argument:
	INFO:    ./compile.sh compile /path/to/bazel
	🍃  Building Bazel from scratch.......
	🍃  Building Bazel with Bazel.
	.WARNING: /tmp/bazel_OpcCR2sk/out/external/bazel_tools/WORKSPACE:1: Workspace name in /tmp/bazel_OpcCR2sk/out/external/bazel_tools/WORKSPACE (@io_bazel) does not match the name given in the repository's definition (@bazel_tools); this will cause a build error in future versions.
	ERROR: No toolchain found for cpu 'unknown'. Valid cpus are: [
	  arm,
	  armeabi-v7a,
	  x64_windows_msvc,
	  s390x,
	].
	INFO: Elapsed time: 6.533s

	ERROR: Could not build Bazel
	cp: cannot stat 'output/bazel': No such file or directory

Any suggestion on this, why this is happing, really helpful.

Thanks,


"
9786,Not a JPEG issue,">>bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir /Training_image
Looking for images in 'non-human'
Looking for images in 'human'
Creating bottleneck at /tmp/bottleneck/non-human/Data__negatives_jpeg_cr_night_512x384_cr_night_512x384_rCR_m26_a10_d2005-04-07_t22-38_wN.jpg.txt
2017-05-09 01:56:48.890091: W tensorflow/core/framework/op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().
Not a JPEG file: starts with 0x89 0x50
Traceback (most recent call last):
  File ""/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 1105, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 844, in main
    bottleneck_tensor)
  File ""bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 469, in cache_bottlenecks
    jpeg_data_tensor, bottleneck_tensor)
  File ""bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 417, in get_or_create_bottleneck
    bottleneck_tensor)
  File ""bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 376, in create_bottleneck_file
    raise RuntimeError('Error during processing file %s' % image_path)
RuntimeError: Error during processing file /Training_images/non-human/Data__negatives_jpeg_cr_night_512x384

How to fix this?"
9783,About slim.datasets,"Hello,
I am using tensorflow r1.1.
I try to following the example [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/data/dataset_data_provider.py)

```
pascal_voc_data_provider = DatasetDataProvider(
      slim.datasets.pascal_voc.get_split('train'),
      shuffle=False)
  images, labels = pascal_voc_data_provider.get(['images', 'labels'])
```
But I get the following error

```
AttributeError: module 'tensorflow.contrib.slim' has no attribute 'datasets'
```
Is it a bug? or how can I solve this problem?"
9781,Changing default type from tf.int32 to tf.int64,"I am trying to allocate very large variables inside single-box, but the default out_type of some operators are tf.int32, it's too small as most modern data center machines have more than 100G memory.

Here is the operators I found:
`tf.shape(input, name=None, out_type=tf.int32)`
`tf.size(input, name=None, out_type=tf.int32)`
`tf.shape_n(input, out_type=None, name=None), out_type defaults to tf.int32`

For example if I have one [500000000000] tensor, tf.shape fails with the following error message inside Windows:

> OverflowError: Python int too large to convert to C long

Linux has the same issue and chooses overflow instead of raising exceptions."
9779,memory leak when implement rnn attention decoder,"### System information

== cat /etc/issue ===============================================
Linux quad 4.4.0-72-generic #93-Ubuntu SMP Fri Mar 31 14:07:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04 LTS (Xenial Xerus)""
VERSION_ID=""16.04""

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 4.9.4-2ubuntu1~16.04) 4.9.4
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux quad 4.4.0-72-generic #93-Ubuntu SMP Fri Mar 31 14:07:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.12.1)
protobuf (3.3.0)
tensorflow-gpu (1.1.0)

== check for virtualenv =========================================
True

== tensorflow import ============================================
tf.VERSION = 1.1.0
tf.GIT_VERSION = v1.1.0-rc0-61-g1ec6ed5
tf.COMPILER_VERSION = v1.1.0-rc0-61-g1ec6ed5
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Tue May  9 12:16:54 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX TIT...  Off  | 0000:05:00.0     Off |                  N/A |
| 22%   47C    P0    76W / 250W |      0MiB / 12205MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX TIT...  Off  | 0000:06:00.0     Off |                  N/A |
| 22%   60C    P2   129W / 250W |  11713MiB / 12207MiB |     80%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX TIT...  Off  | 0000:09:00.0     Off |                  N/A |
| 22%   49C    P0    83W / 250W |      0MiB / 12207MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX TIT...  Off  | 0000:0A:00.0     Off |                  N/A |
| 24%   63C    P2   117W / 250W |  11713MiB / 12207MiB |     70%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    1     21558    C   python3                                      11709MiB |
|    3     21346    C   python3                                      11709MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-7.5/doc/man/man7/libcudart.7
/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7
/usr/local/cuda-7.5/lib64/libcudart_static.a
/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18
/usr/local/cuda-7.5/lib/libcudart_static.a
/usr/local/cuda-7.5/lib/libcudart.so.7.5.18
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.27


### Describe the problem
When we try to implement a complex network which contain a rnn attention decoder, It will consume all the memory after several days. I extract the decoder in a test file, the memory still grow in a slower speed. also found that if change softmax to sigmoid, memory doesn't leak.

### Source code / logs
test code:
```
# __author__ = ""liusiye""
# -*- coding: utf-8 -*-
import tensorflow as tf
from tensorflow.contrib.rnn import GRUCell, MultiRNNCell
from os import getpid
import psutil
import gc
import tensorflow as tf
import numpy as np


process = psutil.Process(getpid())
B, T, H = 20, 60, 256
layer_num = 4

def apply_attention(encoding, rnn_output):
    ''' encoding: [t, b, h1]
        rnn_output: [b, h2]
    '''
    T, B, H1 = encoding.get_shape().as_list()
    _, H2 = rnn_output.get_shape().as_list()
    with tf.variable_scope('attention'):
        w_encoder = tf.get_variable(
            name='W_encoder',
            shape=[H1, H1],
            initializer=tf.random_uniform_initializer(-0.01, 0.01))
        w_decoder = tf.get_variable(
            name='W_decoder',
            shape=[H2, H1],
            initializer=tf.random_uniform_initializer(-0.01, 0.01))
        w_attention = tf.get_variable(
            name='W_attention',
            shape=[H1, 1],
            initializer=tf.random_uniform_initializer(-0.01, 0.01))
    r_decoder = tf.matmul(rnn_output, w_decoder)  # [b, h1]

    r_encoder = tf.matmul(tf.reshape(encoding, [-1, H1]), w_encoder)
    r_encoder = tf.reshape(r_encoder, [T, B, H1])
    # [t, b, h] -> [t * b, h] -> [t, b, h]

    r_attention = tf.tanh(r_encoder + r_decoder)  # [t, b, h1]
    attention = tf.matmul(tf.reshape(r_attention, [-1, H1]), w_attention)
    attention = tf.nn.softmax(tf.reshape(attention, [T, B]), dim=0)
    #attention = tf.nn.sigmoid(tf.reshape(attention, [T, B]))
    encoding = tf.transpose(encoding, perm=[2, 0, 1])  # [t, b, h1]->[h1, t, b]

    context = tf.reduce_sum(encoding * attention, axis=1)  # [h1, b]
    return tf.transpose(context)  # [b, h1]

def rnn_attention_decoder_test():
    encoding = tf.get_variable(name='encoding', shape=[T, B, H], dtype=tf.float32)
    rnn_outputs = []  # t * [b, h]
    scope = tf.get_variable_scope()

    zero_input = tf.constant(0, shape=[B, H], dtype=tf.float32)
    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.GRUCell(H) for i in range(layer_num)], state_is_tuple=True)
    state = cell.zero_state(B, tf.float32)
    with tf.variable_scope(scope) as outer_scope:
        for t in range(T):#T):
            attention_input = zero_input
            rnn_input = apply_attention(encoding, attention_input)
            rnn_output, state = cell(rnn_input, state)
            rnn_outputs.append(rnn_output)
            outer_scope.reuse_variables()
    return tf.stack(rnn_outputs, axis=1), state  # t * [b, h] -> [b, t, h]

with tf.Session() as sess, tf.variable_scope('model'):
    with tf.variable_scope('model', reuse=None):
        tensor_lists_test = rnn_attention_decoder_test()
    init_op = tf.group(
        tf.global_variables_initializer(),
        tf.local_variables_initializer()
    )
    sess.run(init_op)
    sess.graph.finalize()
    for step in range(100000):
        after = process.memory_percent()
        if step > 0:
            print(""MEMORY CHANGE %.7f -> %.7f"" % (before, after))
        before = process.memory_percent()
        sess.run(tensor_lists_test)
        gc.collect()
```

log:
```
2017-05-09 09:27:55.435870: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-09 09:27:55.435903: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-09 09:27:55.435909: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-05-09 09:27:55.435913: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-09 09:27:55.435916: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-05-09 09:27:55.732204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.2405
pciBusID 0000:09:00.0
Total memory: 11.92GiB
Free memory: 11.81GiB
2017-05-09 09:27:55.732232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-05-09 09:27:55.732238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-05-09 09:27:55.732247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0)
MEMORY CHANGE 1.1297021 -> 1.3666840
MEMORY CHANGE 1.3666840 -> 1.3666840
MEMORY CHANGE 1.3666840 -> 1.3697929
MEMORY CHANGE 1.3697929 -> 1.3697929
MEMORY CHANGE 1.3697929 -> 1.3729200
MEMORY CHANGE 1.3729200 -> 1.3733208
MEMORY CHANGE 1.3733208 -> 1.3733208
MEMORY CHANGE 1.3733208 -> 1.3737215
MEMORY CHANGE 1.3737215 -> 1.3768487
MEMORY CHANGE 1.3768487 -> 1.3799758
MEMORY CHANGE 1.3799758 -> 1.3803644
MEMORY CHANGE 1.3803644 -> 1.3834976
MEMORY CHANGE 1.3834976 -> 1.3834976
MEMORY CHANGE 1.3834976 -> 1.3838862
MEMORY CHANGE 1.3838862 -> 1.3838862
MEMORY CHANGE 1.3838862 -> 1.3842870
MEMORY CHANGE 1.3842870 -> 1.3846878
MEMORY CHANGE 1.3846878 -> 1.3850885
MEMORY CHANGE 1.3850885 -> 1.3850885
MEMORY CHANGE 1.3850885 -> 1.3850885
MEMORY CHANGE 1.3850885 -> 1.3850885
MEMORY CHANGE 1.3850885 -> 1.3850885
MEMORY CHANGE 1.3850885 -> 1.3854771
MEMORY CHANGE 1.3854771 -> 1.3854771
MEMORY CHANGE 1.3854771 -> 1.3854771
MEMORY CHANGE 1.3854771 -> 1.3858779
MEMORY CHANGE 1.3858779 -> 1.3858779
MEMORY CHANGE 1.3858779 -> 1.3858779
MEMORY CHANGE 1.3858779 -> 1.3858779
MEMORY CHANGE 1.3858779 -> 1.3858779
MEMORY CHANGE 1.3858779 -> 1.3858779
MEMORY CHANGE 1.3858779 -> 1.3862665
MEMORY CHANGE 1.3862665 -> 1.3866673
MEMORY CHANGE 1.3866673 -> 1.3866673
MEMORY CHANGE 1.3866673 -> 1.3870680
MEMORY CHANGE 1.3870680 -> 1.3870680
MEMORY CHANGE 1.3870680 -> 1.3870680
MEMORY CHANGE 1.3870680 -> 1.3870680
MEMORY CHANGE 1.3870680 -> 1.3870680
MEMORY CHANGE 1.3870680 -> 1.3870680
MEMORY CHANGE 1.3870680 -> 1.3870680
MEMORY CHANGE 1.3870680 -> 1.3874688
MEMORY CHANGE 1.3874688 -> 1.3874688
MEMORY CHANGE 1.3874688 -> 1.3878695
MEMORY CHANGE 1.3878695 -> 1.3878695
MEMORY CHANGE 1.3878695 -> 1.3882581
MEMORY CHANGE 1.3882581 -> 1.3882581
MEMORY CHANGE 1.3882581 -> 1.3886589
MEMORY CHANGE 1.3886589 -> 1.3886589
MEMORY CHANGE 1.3886589 -> 1.3890597
MEMORY CHANGE 1.3890597 -> 1.3894604
MEMORY CHANGE 1.3894604 -> 1.3894604
MEMORY CHANGE 1.3894604 -> 1.3898612
MEMORY CHANGE 1.3898612 -> 1.3898612
MEMORY CHANGE 1.3898612 -> 1.3902619
MEMORY CHANGE 1.3902619 -> 1.3906627
MEMORY CHANGE 1.3906627 -> 1.3906627
MEMORY CHANGE 1.3906627 -> 1.3906627
MEMORY CHANGE 1.3906627 -> 1.3906627
MEMORY CHANGE 1.3906627 -> 1.3906627
MEMORY CHANGE 1.3906627 -> 1.3906627
MEMORY CHANGE 1.3906627 -> 1.3906627
MEMORY CHANGE 1.3906627 -> 1.3906627
MEMORY CHANGE 1.3906627 -> 1.3937898
MEMORY CHANGE 1.3937898 -> 1.3937898
MEMORY CHANGE 1.3937898 -> 1.3937898
MEMORY CHANGE 1.3937898 -> 1.3937898
MEMORY CHANGE 1.3937898 -> 1.3937898
MEMORY CHANGE 1.3937898 -> 1.3937898
MEMORY CHANGE 1.3937898 -> 1.3937898
MEMORY CHANGE 1.3937898 -> 1.3937898
MEMORY CHANGE 1.3937898 -> 1.3941906
MEMORY CHANGE 1.3941906 -> 1.3945913
MEMORY CHANGE 1.3945913 -> 1.3945913
MEMORY CHANGE 1.3945913 -> 1.3945913
MEMORY CHANGE 1.3945913 -> 1.3945913
MEMORY CHANGE 1.3945913 -> 1.3945913
MEMORY CHANGE 1.3945913 -> 1.3945913
MEMORY CHANGE 1.3945913 -> 1.3945913
MEMORY CHANGE 1.3945913 -> 1.3945913
MEMORY CHANGE 1.3945913 -> 1.3945913
MEMORY CHANGE 1.3945913 -> 1.3945913
MEMORY CHANGE 1.3945913 -> 1.3945913
MEMORY CHANGE 1.3945913 -> 1.3945913
MEMORY CHANGE 1.3945913 -> 1.3945913
MEMORY CHANGE 1.3945913 -> 1.3945913
MEMORY CHANGE 1.3945913 -> 1.3945913
MEMORY CHANGE 1.3945913 -> 1.3945913
MEMORY CHANGE 1.3945913 -> 1.3945913
MEMORY CHANGE 1.3945913 -> 1.3945913
MEMORY CHANGE 1.3945913 -> 1.3945913
MEMORY CHANGE 1.3945913 -> 1.3945913
MEMORY CHANGE 1.3945913 -> 1.3949921
MEMORY CHANGE 1.3949921 -> 1.3949921
MEMORY CHANGE 1.3949921 -> 1.3949921
MEMORY CHANGE 1.3949921 -> 1.3949921
MEMORY CHANGE 1.3949921 -> 1.3949921
MEMORY CHANGE 1.3949921 -> 1.3949921
MEMORY CHANGE 1.3949921 -> 1.3949921
MEMORY CHANGE 1.3949921 -> 1.3949921
MEMORY CHANGE 1.3949921 -> 1.3949921
MEMORY CHANGE 1.3949921 -> 1.3949921
MEMORY CHANGE 1.3949921 -> 1.3949921
MEMORY CHANGE 1.3949921 -> 1.3949921
MEMORY CHANGE 1.3949921 -> 1.3949921
MEMORY CHANGE 1.3949921 -> 1.3949921
MEMORY CHANGE 1.3949921 -> 1.3949921
MEMORY CHANGE 1.3949921 -> 1.3949921
MEMORY CHANGE 1.3949921 -> 1.3949921
MEMORY CHANGE 1.3949921 -> 1.3949921
MEMORY CHANGE 1.3949921 -> 1.3953929
MEMORY CHANGE 1.3953929 -> 1.3953929
MEMORY CHANGE 1.3953929 -> 1.3953929
MEMORY CHANGE 1.3953929 -> 1.3953929
MEMORY CHANGE 1.3953929 -> 1.3953929
MEMORY CHANGE 1.3953929 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3957936
MEMORY CHANGE 1.3957936 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3961944
MEMORY CHANGE 1.3961944 -> 1.3965951
MEMORY CHANGE 1.3965951 -> 1.3965951
MEMORY CHANGE 1.3965951 -> 1.3965951
MEMORY CHANGE 1.3965951 -> 1.3969959
MEMORY CHANGE 1.3969959 -> 1.3969959
MEMORY CHANGE 1.3969959 -> 1.3969959
MEMORY CHANGE 1.3969959 -> 1.3969959
MEMORY CHANGE 1.3969959 -> 1.3969959
MEMORY CHANGE 1.3969959 -> 1.3969959
MEMORY CHANGE 1.3969959 -> 1.3969959
MEMORY CHANGE 1.3969959 -> 1.3969959
MEMORY CHANGE 1.3969959 -> 1.3973967
MEMORY CHANGE 1.3973967 -> 1.3973967
MEMORY CHANGE 1.3973967 -> 1.3973967
MEMORY CHANGE 1.3973967 -> 1.3973967
MEMORY CHANGE 1.3973967 -> 1.3977974
MEMORY CHANGE 1.3977974 -> 1.3977974
```"
9778,ImportError: No module named 'tensorflow.tools',"
### Describe the problem
Run the demo ""transform_graph_test.py"" in Windows, get the error 
""ImportError: No module named 'tensorflow.tools'""

Did I miss some installations before running the demo?
The code ""from tensorflow.tools.graph_transforms import TransformGraph"" cannot work?"
9777,tf.nn.embedding_lookup  poor performance,"In my programe, I use  f.nn.embedding_lookup as follow:

embedding = tf.get_variable(""embedding"", [200000, 128], tf.float32, initializer=tf.        random_normal_initializer(stddev=0.1), trainable=True,partitioner=tf.fixed_size_partitioner(10))

word_embedding = tf.nn.embedding_lookup(embedding, query_tensor)
while(1):
    sess.run(word_embedding )


when I start 30 worker，each worker qps is 3000.
but when I test tf.nn.embedding_lookup_sparse, each worker qps is 110000

how to solve this problem,any suggestion welcome"
9775,"Feature Request: ""training"" argument for contrib.rnn.DropoutWrapper like the one in tf.layers.dropout","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.0.4
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.1.0


### Feature Request: ""training"" argument for contrib.rnn.DropoutWrapper for applying dropout depending on train/inference phase.

In [`tf.layers.dropout`](https://www.tensorflow.org/api_docs/python/tf/layers/dropout), the `training` parameter is a handy setting that lets you apply dropout depending on whether the model is training or doing inference. It's very convenient to be able to pass a boolean to the model placeholder and have it automatically do the right thing when it comes to dropout.

Unfortunately, [`tf.contrib.rnn.DropoutWrapper`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper) does not have this same parameter, and I think it would greatly benefit from it. This is a feature request for it.

I tried implementing it myself with `tf.cond` and either returning the dropped-out outputs/states or the untouched ones, but I couldn't figure out how to share the variables between them in the cond.
"
9774,Cifar-10 link inside Tensorflow webpage report 404,"Not sure if this is the right place for an 404 error...

The link to CIFAR-10 source code on Tensorflow webpage reports 404 since yesterday afternoon (08/May/2017), kindly help to fix it.

This is the page found contains error:
[https://www.tensorflow.org/tutorials/deep_cnn](url)

This is the link found to reports 404:
[https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow_models/tutorials/image/cifar10/](url)

Thanks"
9771,"building error, tensorflow master, with bazel 0.4.5, Cuda 8.0, Cudnn 6, Nvidia p100 pci, Ubuntu 16.04","- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:   ('v1.1.0-rc2-221-g48d9915', '1.1.0-rc2')
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: 8.0/6
- **GPU model and memory**: P100 PCI - 16 GB
- **Exact command to reproduce**:  
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package



python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

./tensorflow/core/util/tensor_format.h(58): warning: missing return statement at end of non-void function ""tensorflow::GetTensorFeatureDimIndex""                                                                                    

./tensorflow/core/util/tensor_format.h(71): warning: missing return statement at end of non-void function ""tensorflow::GetTensorSpatialDimIndex""                                                                                    

ERROR: /root/tensorflow/tensorflow/contrib/verbs/BUILD:135:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 151 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
tensorflow/contrib/verbs/rdma.cc: In member function 'virtual void tensorflow::RdmaTensorBuffer::SendNextItem()':
tensorflow/contrib/verbs/rdma.cc:785:11: error: 'struct tensorflow::WorkerSession' has no member named 'rendezvous_mgr'
         ->rendezvous_mgr->RecvLocalAsync(step_id, parsed, cb);
           ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 252.968s, Critical Path: 205.52s

Thanks"
9770,Sample Github TensorFlow Issue Renamed,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
9767,MKL only supported on 32-bit machine?,"according to [mkl BUILD file](https://github.com/tensorflow/tensorflow/blob/master/third_party/mkl/BUILD#L20) and [mkl util file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/mkl_util.h), the current mkl code is only for 32-bit machine... And I didn't find the declaration of the function`dnnLayoutSerializationBufferSize_F32/64` in header files or `.so` files..."
9766,Tensorflow worked and now suddenly giving errors ?,"Hello,

I was able to run tensorflow (both CPU and GPU) without issues yesterday. Today, after restarting my laptop, I get the below error as I tried running Tensorflow. 

I have previously checked that my path variables were set properly and only then Tensorflow worked, but I'm not sure how it suddenly isn't working.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
* Yes

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
* OS = Windows 10

TensorFlow installed from (source or binary):
*Installed from Source 

TensorFlow version (use command below):
*When I ran =>  python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
*I get only this result =>  b'unknown' 1.0.0

Bazel version (if compiling from source):
*I'm unsure about this as I don't remember installing this

CUDA/cuDNN version:
*cuda_8.0.61_win10
*cuDNN v5.1 (Jan 20, 2017), for CUDA 8.0

GPU model and memory:
*GeForce GTX 1050 graphics card
*RAM 32GB

Exact command to reproduce:
*import tensorflow as tf

Error code I got:
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     17         try:
---> 18             return importlib.import_module(mname)
     19         except ImportError:

C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\importlib\__init__.py in import_module(name, package)
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127 

C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\importlib\_bootstrap.py in _gcd_import(name, package, level)

C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\importlib\_bootstrap.py in _find_and_load(name, import_)

C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\importlib\_bootstrap.py in _find_and_load_unlocked(name, import_)

C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\importlib\_bootstrap.py in _load_unlocked(spec)

C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\importlib\_bootstrap.py in module_from_spec(spec)

C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\importlib\_bootstrap_external.py in create_module(self, spec)

C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\importlib\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     40     sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_GLOBAL)
---> 41   from tensorflow.python.pywrap_tensorflow_internal import *
     42   from tensorflow.python.pywrap_tensorflow_internal import __version__

C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
     20             return importlib.import_module('_pywrap_tensorflow_internal')
---> 21     _pywrap_tensorflow_internal = swig_import_helper()
     22     del swig_import_helper

C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     19         except ImportError:
---> 20             return importlib.import_module('_pywrap_tensorflow_internal')
     21     _pywrap_tensorflow_internal = swig_import_helper()

C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\importlib\__init__.py in import_module(name, package)
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127 

ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-3-feaf27c67465> in <module>()
----> 1 import tensorflow as tf
      2 import numpy as np
      3 
      4 IM_SIZE_PX = 50
      5 SLICE_COUNT = 20

C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\__init__.py in <module>()
     22 
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26 

C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\__init__.py in <module>()
     49 import numpy as np
     50 
---> 51 from tensorflow.python import pywrap_tensorflow
     52 
     53 # Protocol buffers

C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     50 for some common reasons and solutions.  Include the entire stack trace
     51 above this error message when asking for help."""""" % traceback.format_exc()
---> 52   raise ImportError(msg)
     53 
     54 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\dines\Anaconda3\envs\tensorflow-gpu\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.`"
9764,how to call AttentionWrapper?,"I am trying to write a simple seq2seq model with attention. But  it gets the following error: 

```
attn_cell = tf.contrib.seq2seq.AttentionWrapper(
AttributeError: 'module' object has no attribute 'AttentionWrapper'
```
 How should I call AttentionWrapper?

 Here is my code:

```
    T=1000
    N=100
    input = tf.placeholder(tf.float32, shape=(N, T, 512), name=""input_matrix"")
    seq_lengths = tf.placeholder(tf.int32, shape=(N), name=""input_lengths"")

    cell= MultiRNNCell([DeviceWrapper(ResidualWrapper(LSTMCell(num_units=512)),device='/gpu:%d' %(i+1)) for i in range(2)])
    encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(cell,input, parallel_iterations=32, swap_memory=True, dtype=tf.float32)

    # Attention Mechanisms. Bahdanau is additive style attention
    attn_mech = tf.contrib.seq2seq.BahdanauAttention(
        num_units = 100, # depth of query mechanism
        memory = encoder_outputs, # hidden states to attend (output of RNN)
        #memory_sequence_length= T,#tf.sequence_mask(seq_lengths, T), # masks false memories
        normalize=False, # normalize energy term
        name='BahdanauAttention')

    cell_out= MultiRNNCell([DeviceWrapper(ResidualWrapper(LSTMCell(num_units=512)),device='/gpu:%d' %(i+1)) for i in range(2)])
        # Attention Wrapper: adds the attention mechanism to the cell

    # Attention Wrapper: adds the attention mechanism to the cell
    attn_cell = tf.contrib.seq2seq.AttentionWrapper(
        cell = cell,# Instance of RNNCell
        attention_mechanism = attn_mech, # Instance of AttentionMechanism
        attention_size = 100, # Int, depth of attention (output) tensor
        attention_history=False, # whether to store history in final output
        name=""attention_wrapper"")
 
    # TrainingHelper does no sampling, only uses inputs
    helper = tf.contrib.seq2seq.TrainingHelper(
        inputs = x, # decoder inputs
        sequence_length = seq_len_dec, # decoder input length
        name = ""decoder_training_helper"")
 
    # Decoder setup
    decoder = tf.contrib.seq2seq.BasicDecoder(
              cell = attn_cell,
              helper = helper, # A Helper instance
              initial_state = encoder_final_state, # initial state of decoder
              output_layer = None) # instance of tf.layers.Layer, like Dense
 
    # Perform dynamic decoding with decoder object
    outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)
```"
9763,"Android libandroid_tensorflow_inference - ""No OpKernel was registered to support Op 'FloorMod' with these attrs"" when using tf.nn.atrous_conv2d","hi, everyone,

We are trying to load a pb file inside Android. The pb file is generated with python 2.7.0 and Tensorflow 1.0.1.   We have this following error:

```
exception triggered java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'FloorMod' with these attrs.  Registered devices: [CPU], Registered kernels:
                                                                    <no registered kernels>                                                                  
                                                                  	 [[Node: WCon/atrous_conv2d/mod = FloorMod[T=DT_INT32, _device=""/device:CPU:0""](WCon/atrous_conv2d/add_1, WCon/atrous_conv2d/mod/y)]]
```

The network definition is using tf.nn.atrous_conv2d(i, k, dilation_rate, padding=padding)

And our freezing graph code is also pretty standard.

We have no problem running this pb files for inference with python under Tensorflow 1.0.1

But when running on Android phone, we will see the above error, I tried several nightly build TF Android Inference library including the latest from https://ci.tensorflow.org/view/Nightly/job/nightly-android/    The result is the same

My feeling is that the inference on Tensorflow python and Tensorflow Android has a huge gap?

Thanks for any ideas!


"
9761,Can't launch tensorboard,"When i try to launch tensorboard i get this error message : 

tensorboard --logdir=/home/mejdi/Desktop/CNN/logs Traceback (most recent call last): File ""/usr/local/bin/tensorboard"", line 7, in from tensorflow.tensorboard.tensorboard import main File ""/usr/local/lib/python3.5/dist-packages/tensorflow/tensorboard/tensorboard.py"", line 33, in from tensorflow.tensorboard.backend import application File ""/usr/local/lib/python3.5/dist-packages/tensorflow/tensorboard/backend/application.py"", line 47, in from tensorflow.tensorboard.plugins.projector import projector_plugin File ""/usr/local/lib/python3.5/dist-packages/tensorflow/tensorboard/plugins/projector/projector_plugin.py"", line 28, in from tensorflow.contrib.tensorboard.plugins.projector import PROJECTOR_FILENAME File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/init.py"", line 37, in from tensorflow.contrib import keras File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/keras/init.py"", line 26, in from tensorflow.contrib.keras.api.keras import * File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/keras/api/keras/init.py"", line 25, in from tensorflow.contrib.keras.api.keras import activations File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/keras/api/keras/activations/init.py"", line 22, in from tensorflow.contrib.keras.python.keras.activations import elu File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/keras/python/keras/init.py"", line 21, in from tensorflow.contrib.keras.python.keras import activations File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/keras/python/keras/activations.py"", line 23, in from tensorflow.contrib.keras.python.keras import backend as K File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/keras/python/keras/backend.py"", line 3601, in assert _image_data_format in {'channels_last', 'channels_first'} AssertionError

What's wrong ?"
9758,"I use the bazel to generate the android so library on mac, but i met an issue :'ERROR: /Users/nankaiming/Desktop/tensorflow-master/tensorflow/core/kernels/BUILD:3869:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed: Process exited with status 1 [sandboxed].'","the gcc version on my mac is
![image](https://cloud.githubusercontent.com/assets/18348154/25809650/3b40c944-3440-11e7-9235-ff6293194a6f.png)
the log of the process is 
![image](https://cloud.githubusercontent.com/assets/18348154/25809685/5cacd0aa-3440-11e7-9a2e-263a7120d762.png)
the WORKSPACE i edited is 
![image](https://cloud.githubusercontent.com/assets/18348154/25809807/c00e6c9e-3440-11e7-9424-76187b008c4d.png)


"
9757,Large page fault causes slow performance while using gpu,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary, using `pip install tensorflow-gpu==1.0.1`
- **TensorFlow version (use command below)**:
1.0.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
8.0/5.1
- **GPU model and memory**:
nvidia gtx1080, 8g

### Describe the problem
I have observed a large amount of page fault while running the provided sample code on gpu, and this causes a serious performance drawdown. 

The key parts of the output of `/usr/bin/time -v python sample.py` are:

    System time (seconds): 7.28  
    Percent of CPU this job got: 85%  
    Elapsed (wall clock) time (h:mm:ss or m:ss): 0:22.41 
    Minor (reclaiming a frame) page faults: 684695  
    Involuntary context switches: 164 
    File system inputs: 0  
    File system outputs: 8  

There are 684k page faults,  and the `gpu-volatile usage` is only about 30%. 

I am very hesitating to ask for help here, because on another system with exact os, software and gpu, this issue does not appears, I have posted on stackoverflow to compare two systems [here](http://stackoverflow.com/questions/43842731/two-exactly-same-systems-have-very-different-performances-when-running-tensorflo) 
Is that possible that tensorflow handles different hardwares differently? It looks to me that the gpu-cpu I/O may have caused this issue, and I suspect that I need to configure my hardware settings somewhere, but don't know how.

Things I have tried:

1. Upgrade BIOS to the latest version and reset default settings.
2. Call Asus(my motherboard and gpu vendor) customer service for help.
3. Inject LD_PRELOAD=""/usr/lib/libtcmalloc.so"" to .bashrc file.
 
### Source code / logs

Here is the sample code I used to test

    import tensorflow as tf
    import numpy as np
    from tqdm import trange
  
    np.random.seed(111)
    h,w = 3000, 2000
    steps = 1000

    x = tf.placeholder(dtype=tf.float32, shape=[h, w], name='x')
    t = tf.constant(np.random.random(size=[w, w]), dtype=tf.float32)
    m = tf.matmul(x,t)

    x0 = np.random.random(size=[h, w])
    sess = tf.Session()
    for i in trange(steps):
        x0 = sess.run(m, feed_dict={x: x0})

The attachment contains: `Nvidia-smi` output, `/usr/bin/time -v` output, hardware specs in html format, chrome trace timeline.
[sysB.zip](https://github.com/tensorflow/tensorflow/files/983550/sysB.zip)



"
9756,Accessing Array based on subscript?,"How I get the result using tensorflow?
```
import numpy as np
a = np.random.rand(2,10)
b = np.random.randint(0,10,(4,2))
c = a[:,b]
```
This code can be implemented by Nympy and Theano. But how to do that on tensorflow?
```
ind = np.random.randint(0,10,(4,))
x = K.placeholder(shape = (2,10),dtype = 'int64')
y = x[:,ind]
f = K.function([x],[y])

x_ = np.random.rand(2,10)
y_ = f([x_])[0]
print(x_)
print(y_)
```"
9755,Bazel unable to build on Jetson TX1 with --config=cuda option?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 LTS
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.1 (latest master)
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: 8.0/5.1
- **GPU model and memory**: Tegra X1
- **Exact command to reproduce**:
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package 


### Describe the problem
Using the bazel build command above, I am unable to find a valid toolchain to support the aarch64 architecture in the Jetson TX1. However, by removing the `--config=cuda` option, the error is gone, although I am still unable to finish building the pip file (probably due to the fact that I configured TF for GPU using `./configure`). 

Note: I changed the bazel files as seen in this guide: http://zhiyisun.github.io/2017/02/15/Running-Google-Machine-Learning-Library-Tensorflow-On-ARM-64-bit-Platform.html , in order to get bazel to build for aarch64.

Is there anything more that I have to change? using the suggested command of `bazel build -c opt --copt=""-funsafe-math-optimizations"" --copt=""-ftree-vectorize"" --copt=""-fomit-frame-pointer"" --verbose_failures tensorflow/tools/pip_package:build_pip_package` I get the same error too.

I saw this issue over here: https://github.com/bazelbuild/bazel/issues/1855
and it says it has something related to the cuda crosstool. Is there a way to fix this to configure TensorFlow-GPU on the Jetson TX1?

Thank you for your help. :D

### Source code / logs

Here is the error I got:

```
ubuntu@tegra-ubuntu:~/tensorflow$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
ERROR: No toolchain found for cpu 'aarch64'. Valid cpus are: [
  k8,
  piii,
  arm,
  darwin,
  ppc,
].
INFO: Elapsed time: 1.589s

```"
9754,TensorFlow weight initialization taking 99% of total run time,"### System information
- **Have I written custom code**: No (but custom data)
- **OS Platform and Distribution**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: tensorflow-gpu (1.0.0 and 1.1) (Python 3.5)
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: GTX 1080
- **Exact command to reproduce**: 

### Describe the problem
I noticed that one of my networks was running slow and nvidia-smi was reporting only around ~10% GPU usage. After running the profiler, I saw that `TruncatedNormal` process was taking the vast majority of running time (see photo).

[Profiler report](https://i.stack.imgur.com/Ymup2.png)

### Source code / logs
Weight declaration function (from MNIST tutorial):
```

def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)
```
Code in action:
```
 # First Layer
    with tf.name_scope('input'):
        x = tf.placeholder(tf.float32, [None, Nvars])
    w1 = weight_variable([Nvars, 8])
    b1 = bias_variable([8])
    y1 = tf.nn.relu(tf.matmul(x, w1) + b1)
```
Please note this question was also asked on StackOverflow. [Link](http://stackoverflow.com/q/43793066)"
9753,Much time cost from worker to ps server,"I try to test the distribute speed, with one machine of 4 GPUs. I calculate one batch on each GPU, and add the four batch on the worker's cpu , and then average the four batch on ps CPU and update the parameters, I saved the timeline file, but there are no operations from workers CPU to ps CPU with a long time span in the timeline, I don`t know where the time gone? because of data transfer? but this is test in only one machine. the timeline is as follows, and the right up operation is average four data batch and updata parameters
![webwxgetmsgimg](https://cloud.githubusercontent.com/assets/16236576/25800351/87f2acb4-341b-11e7-86a1-bfb888405d72.jpeg)




"
9752,contrib/verbs causes compile error on master,"Recently merged RDMA-supporting feature is based on r1.0, and incompatible with the latest master.

> ERROR: /data/chuangchen/workspace/tensorflow/tensorflow/contrib/verbs/BUILD:104:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 151 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc: In constructor 'tensorflow::RdmaRemoteRendezvous::RdmaRemoteRendezvous(const tensorflow::WorkerEnv*, tensorflow::int64, tensorflow::RdmaMgr*)':
tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:34:35: error: 'worker_name' was not declared in this scope
       : BaseRemoteRendezvous(env, worker_name, step_id, true),
                                   ^
tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc: In constructor 'tensorflow::RdmaRendezvousMgr::RdmaRendezvousMgr(const tensorflow::WorkerEnv*, const string&, tensorflow::WorkerCacheInterface*)':
tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:139:41: error: no matching function for call to 'tensorflow::BaseRendezvousMgr::BaseRendezvousMgr(const tensorflow::WorkerEnv*&, const string&)'
     : BaseRendezvousMgr(env, worker_name) {}
                                         ^
tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:139:41: note: candidate is:
In file included from ./tensorflow/contrib/verbs/rdma_rendezvous_mgr.h:22:0,
                 from tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:18:
./tensorflow/core/distributed_runtime/base_rendezvous_mgr.h:62:12: note: tensorflow::BaseRendezvousMgr::BaseRendezvousMgr(const tensorflow::WorkerEnv*)
   explicit BaseRendezvousMgr(const WorkerEnv* worker_env);
            ^
./tensorflow/core/distributed_runtime/base_rendezvous_mgr.h:62:12: note:   candidate expects 1 argument, 2 provided
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
"
9750,"building error, tensorflow r1.0, with bazel 0.4.5, Ubuntu 16.04.1 LTS armv7 board","

Update from #9632, @andydavis1 suggest upgrading to 1.1, but it seems to be worse:

````
odroid@odroid:~/local_DT_project/tensorflow_git$ git checkout r1.1
Branch r1.1 set up to track remote branch r1.1 from origin.
Switched to a new branch 'r1.1'
odroid@odroid:~/local_DT_project/tensorflow_git$ ./configure
Please specify the location of python. [Default is /usr/bin/python]:
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]:
Do you wish to use jemalloc as the malloc implementation? [Y/n]
jemalloc enabled
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N]
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N]
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]
No XLA support will be enabled for TensorFlow
Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]

Using python library path: /usr/local/lib/python2.7/dist-packages
Do you wish to build TensorFlow with OpenCL support? [y/N]
No OpenCL support will be enabled for TensorFlow 
Do you wish to build TensorFlow with CUDA support? [y/N]
No CUDA support will be enabled for TensorFlow   
Configuration finished
......................................................................................................
INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
.....................................................................................................
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_dialog//': Error downloading [https://github.com/polymerelements/paper-dialog/archive/v1.0.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_dialog/v1.0.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_form_element_behavior//': Error downloading [https://github.com/polymerelements/iron-form-element-behavior/archive/v1.0.6.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_form_element_behavior/v1.0.6.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_collapse//': Error downloading [https://github.com/polymerelements/iron-collapse/archive/v1.0.8.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_collapse/v1.0.8.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_fit_behavior//': Error downloading [https://github.com/polymerelements/iron-fit-behavior/archive/v1.2.5.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_fit_behavior/v1.2.5.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_flex_layout//': Error downloading [https://github.com/polymerelements/iron-flex-layout/archive/v1.3.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_flex_layout/v1.3.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_dropdown//': Error downloading [https://github.com/polymerelements/iron-dropdown/archive/v1.4.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_dropdown/v1.4.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_checked_element_behavior//': Error downloading [https://github.com/polymerelements/iron-checked-element-behavior/archive/v1.0.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_checked_element_behavior/v1.0.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_behaviors//': Error downloading [https://github.com/polymerelements/iron-behaviors/archive/v1.0.17.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_behaviors/v1.0.17.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_autogrow_textarea//': Error downloading [https://github.com/polymerelements/iron-autogrow-textarea/archive/v1.0.12.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_autogrow_textarea/v1.0.12.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.  
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_ajax//': Error downloading [https://github.com/polymerelements/iron-ajax/archive/v1.2.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_ajax/v1.2.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@dagre//': Error downloading [https://github.com/cpettitt/dagre/archive/v0.7.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/dagre/v0.7.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@font_roboto//': Error downloading [https://github.com/polymerelements/font-roboto/archive/v1.0.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/font_roboto/v1.0.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_a11y_keys_behavior//': Error downloading [https://github.com/polymerelements/iron-a11y-keys-behavior/archive/v1.1.8.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_a11y_keys_behavior/v1.1.8.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'. 
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_a11y_announcer//': Error downloading [https://github.com/polymerelements/iron-a11y-announcer/archive/v1.0.5.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_a11y_announcer/v1.0.5.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@graphlib//': Error downloading [https://github.com/cpettitt/graphlib/archive/v1.0.7.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/graphlib/v1.0.7.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@es6_promise//': Error downloading [https://github.com/components/es6-promise/archive/v2.1.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/es6_promise/v2.1.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@d3//': Error downloading [https://github.com/mbostock-bower/d3-bower/archive/v3.5.15.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/d3/v3.5.15.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@three_js_orbitcontrols_js//file': Error downloading [https://raw.githubusercontent.com/mrdoob/three.js/r77/examples/js/controls/OrbitControls.js] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/three_js_orbitcontrols_js/OrbitControls.js: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@web_animations_js//': Error downloading [https://github.com/web-animations/web-animations-js/archive/2.2.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/web_animations_js/2.2.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@weblas_weblas_js//file': Error downloading [https://raw.githubusercontent.com/waylonflinn/weblas/v0.9.0/dist/weblas.js] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/weblas_weblas_js/weblas.js: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@webcomponentsjs//': Error downloading [https://github.com/webcomponents/webcomponentsjs/archive/v0.7.22.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/webcomponentsjs/v0.7.22.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@three_js_three_min_js//file': Error downloading [https://raw.githubusercontent.com/mrdoob/three.js/r77/build/three.min.js] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/three_js_three_min_js/three.min.js: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@promise_polyfill//': Error downloading [https://github.com/polymerlabs/promise-polyfill/archive/v1.0.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/promise_polyfill/v1.0.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@polymer//': Error downloading [https://github.com/polymer/polymer/archive/v1.7.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/polymer/v1.7.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@plottable//': Error downloading [https://github.com/palantir/plottable/archive/v1.16.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/plottable/v1.16.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_tooltip//': Error downloading [https://github.com/polymerelements/paper-tooltip/archive/v1.1.2.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_tooltip/v1.1.2.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_toolbar//': Error downloading [https://github.com/polymerelements/paper-toolbar/archive/v1.1.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_toolbar/v1.1.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_toggle_button//': Error downloading [https://github.com/polymerelements/paper-toggle-button/archive/v1.2.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_toggle_button/v1.2.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_toast//': Error downloading [https://github.com/polymerelements/paper-toast/archive/v1.3.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_toast/v1.3.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_tabs//': Error downloading [https://github.com/polymerelements/paper-tabs/archive/v1.7.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_tabs/v1.7.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_styles//': Error downloading [https://github.com/polymerelements/paper-styles/archive/v1.1.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_styles/v1.1.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_toggle_button//': Error downloading [https://github.com/polymerelements/paper-toggle-button/archive/v1.2.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_toggle_button/v1.2.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_toast//': Error downloading [https://github.com/polymerelements/paper-toast/archive/v1.3.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_toast/v1.3.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_tabs//': Error downloading [https://github.com/polymerelements/paper-tabs/archive/v1.7.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_tabs/v1.7.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_styles//': Error downloading [https://github.com/polymerelements/paper-styles/archive/v1.1.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_styles/v1.1.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_spinner//': Error downloading [https://github.com/polymerelements/paper-spinner/archive/v1.1.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_spinner/v1.1.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_slider//': Error downloading [https://github.com/polymerelements/paper-slider/archive/v1.0.10.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_slider/v1.0.10.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_ripple//': Error downloading [https://github.com/polymerelements/paper-ripple/archive/v1.0.5.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_ripple/v1.0.5.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_radio_group//': Error downloading [https://github.com/polymerelements/paper-radio-group/archive/v1.0.9.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_radio_group/v1.0.9.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_radio_button//': Error downloading [https://github.com/polymerelements/paper-radio-button/archive/v1.1.2.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_radio_button/v1.1.2.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_progress//': Error downloading [https://github.com/polymerelements/paper-progress/archive/v1.0.9.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_progress/v1.0.9.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_menu_button//': Error downloading [https://github.com/polymerelements/paper-menu-button/archive/v1.5.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_menu_button/v1.5.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_menu//': Error downloading [https://github.com/polymerelements/paper-menu/archive/v1.2.2.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_menu/v1.2.2.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_material//': Error downloading [https://github.com/polymerelements/paper-material/archive/v1.0.6.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_material/v1.0.6.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_listbox//': Error downloading [https://github.com/polymerelements/paper-listbox/archive/v1.1.2.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_listbox/v1.1.2.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_item//': Error downloading [https://github.com/polymerelements/paper-item/archive/v1.1.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_item/v1.1.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_input//': Error downloading [https://github.com/polymerelements/paper-input/archive/v1.1.18.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_input/v1.1.18.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_icon_button//': Error downloading [https://github.com/polymerelements/paper-icon-button/archive/v1.1.3.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_icon_button/v1.1.3.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_header_panel//': Error downloading [https://github.com/polymerelements/paper-header-panel/archive/v1.1.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_header_panel/v1.1.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_dropdown_menu//': Error downloading [https://github.com/polymerelements/paper-dropdown-menu/archive/v1.4.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_dropdown_menu/v1.4.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_dialog_behavior//': Error downloading [https://github.com/polymerelements/paper-dialog-behavior/archive/v1.2.5.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_dialog_behavior/v1.2.5.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_dialog//': Error downloading [https://github.com/polymerelements/paper-dialog/archive/v1.0.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_dialog/v1.0.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_checkbox//': Error downloading [https://github.com/polymerelements/paper-checkbox/archive/v1.4.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_checkbox/v1.4.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_button//': Error downloading [https://github.com/polymerelements/paper-button/archive/v1.0.11.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_button/v1.0.11.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_behaviors//': Error downloading [https://github.com/polymerelements/paper-behaviors/archive/v1.0.12.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_behaviors/v1.0.12.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@numericjs_numeric_min_js//file': Error downloading [https://cdnjs.cloudflare.com/ajax/libs/numeric/1.2.6/numeric.min.js] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/numericjs_numeric_min_js/numeric.min.js: sun.security.validator.ValidatorException: PKIX path validation failed: java.security.cert.CertPathValidatorException: signature check failed and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@neon_animation//': Error downloading [https://github.com/polymerelements/neon-animation/archive/v1.2.2.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/neon_animation/v1.2.2.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@lodash//': Error downloading [https://github.com/lodash/lodash/archive/3.8.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/lodash/3.8.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_validatable_behavior//': Error downloading [https://github.com/polymerelements/iron-validatable-behavior/archive/v1.1.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_validatable_behavior/v1.1.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_selector//': Error downloading [https://github.com/polymerelements/iron-selector/archive/v1.5.2.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_selector/v1.5.2.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_scroll_target_behavior//': Error downloading [https://github.com/polymerelements/iron-scroll-target-behavior/archive/v1.0.3.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_scroll_target_behavior/v1.0.3.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_resizable_behavior//': Error downloading [https://github.com/polymerelements/iron-resizable-behavior/archive/v1.0.3.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_resizable_behavior/v1.0.3.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'. 
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_range_behavior//': Error downloading [https://github.com/polymerelements/iron-range-behavior/archive/v1.0.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_range_behavior/v1.0.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_overlay_behavior//': Error downloading [https://github.com/polymerelements/iron-overlay-behavior/archive/v1.10.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_overlay_behavior/v1.10.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_meta//': Error downloading [https://github.com/polymerelements/iron-meta/archive/v1.1.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_meta/v1.1.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_menu_behavior//': Error downloading [https://github.com/polymerelements/iron-menu-behavior/archive/v1.1.10.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_menu_behavior/v1.1.10.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_list//': Error downloading [https://github.com/polymerelements/iron-list/archive/v1.3.9.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_list/v1.3.9.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_input//': Error downloading [https://github.com/polymerelements/iron-input/archive/1.0.10.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_input/1.0.10.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_iconset_svg//': Error downloading [https://github.com/polymerelements/iron-iconset-svg/archive/v1.1.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_iconset_svg/v1.1.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_icons//': Error downloading [https://github.com/polymerelements/iron-icons/archive/v1.1.3.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_icons/v1.1.3.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_icon//': Error downloading [https://github.com/polymerelements/iron-icon/archive/v1.0.11.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_icon/v1.0.11.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: Evaluation of query ""deps(((//tensorflow/... - //tensorflow/contrib/nccl/...) - //tensorflow/examples/android/...))"" failed: errors were encountered while computing transitive closure.
````

Any help will be appreciated."
9749,tensorflow install for go,"bojuzideMacBook-Pro:~ manman$ go get github.com/tensorflow/tensorflow/tensorflow/go github.com/tensorflow/tensorflow/tensorflow/go
ld: library not found for -ltensorflow
clang: error: linker command failed with exit code 1 (use -v to see invocation)
bojuzideMacBook-Pro:~ manman$ 
 "
9748,Segmentation fault on help(tf.train.SequenceExample),"I was trying to try and test the tf.train.SequenceExample class, but it wasn't in the official docs and using help crashed Python. Does anyone know how to use it or how to solve this issue?"
9747,'Tensor' object has no attribute 'initializer' after import from meta graph,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution**: Darwin Austins-MBP 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64
Mac OS X 10.12.4
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:v1.1.0-rc0-61-g1ec6ed5 1.1.0
- **Bazel version (if compiling from source)**:0.4.5
- **CUDA/cuDNN version**:None
- **GPU model and memory**:None
- **Exact command to reproduce**: Ref to Codes

### tensorflow import 
    tf.VERSION = 1.1.0
    tf.GIT_VERSION = v1.1.0-rc0-61-g1ec6ed5
    tf.COMPILER_VERSION = v1.1.0-rc0-61-g1ec6ed5
    Sanity check: array([1], dtype=int32)



### Describe the problem
After export and import a meta graph with uninitialized local variables,
You can not inittialize them with sess.run(tf. local_variables_initializer()), cause
TF do not register variable's proto function with key 'LOCAL_VARIABLES' and when 
export meta graph to protobuf, source code can not find to_proto function from repository.


### Source code / logs
```python
import tensorflow as tf

graph = tf.Graph()
with graph.as_default():
    x = tf.Variable(1, collections=[tf.GraphKeys.LOCAL_VARIABLES])
    y = tf.Variable(1)
    z = x + y
origin_meta_graph = tf.train.export_meta_graph(graph=graph)
new_graph = tf.Graph()
with new_graph.as_default():
    tf.train.import_meta_graph(origin_meta_graph)
    init = tf.local_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
```

```bash
Traceback (most recent call last):
  File ""/Users/austin/workspace/aip/3rd/tensorflow/tensorflow/test.py"", line 12, in <module>
    init = tf.local_variables_initializer()
  File ""/Users/austin/workspace/aip/3rd/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 1184, in local_variables_initializer
    return variables_initializer(local_variables())
  File ""/Users/austin/workspace/aip/3rd/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 1149, in variables_initializer
    return control_flow_ops.group(*[v.initializer for v in var_list], name=name)
  File ""/Users/austin/workspace/aip/3rd/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 1149, in <listcomp>
    return control_flow_ops.group(*[v.initializer for v in var_list], name=name)
AttributeError: 'Tensor' object has no attribute 'initializer'
```

```python
import tensorflow as tf

graph = tf.Graph()
with graph.as_default():
    x = tf.Variable(1, collections=[tf.GraphKeys.LOCAL_VARIABLES])
    y = tf.Variable(1)
    z = x + y
origin_meta_graph = tf.train.export_meta_graph(graph=graph)
new_graph = tf.Graph()
with new_graph.as_default():
    tf.train.import_meta_graph(origin_meta_graph)
print(graph.get_collection(tf.GraphKeys.LOCAL_VARIABLES))
print(new_graph.get_collection(tf.GraphKeys.LOCAL_VARIABLES))
```

```bash
[<tf.Variable 'Variable:0' shape=() dtype=int32_ref>]
[<tf.Tensor 'Variable:0' shape=() dtype=int32_ref>]
```
As it show above, in origin graph local_variable collection is a list of **tf.Variable**
but in the new graph, is a list of **tf.Tensor**

### Work around
Add following registration in your model core
OR Ref to this [PR](https://github.com/tensorflow/tensorflow/pull/9674)

```python
from tensorflow.core.framework import variable_pb2
from tensorflow.python.framework import ops
from tensorflow.python.ops import variables
from tensorflow.python.framework.ops import register_proto_function

register_proto_function(
    ops.GraphKeys.LOCAL_VARIABLES,
    proto_type=variable_pb2.VariableDef,
    to_proto=variables.Variable.to_proto,
    from_proto=variables.Variable.from_proto)
```"
9745,Feature request: distance between each pair of the two collections of inputs. Analog of scipy.spatial.distance.cdist,"is it possible to add a function to calculate a distance between each pair of the two collections of inputs. Scipy has it implemented in [distance.sdist](https://docs.scipy.org/doc/scipy/reference/generated/generated/scipy.spatial.distance.cdist.html) and supports many distance metrics.

Is it possible to have this function in TF? I have made an implementation for [Eucledian distance](http://stackoverflow.com/a/43839605/1090562) but have no idea how to implement it for other metrics.
"
9744,Adding a random seed parameter for tensorflow.layer.dense,"Hi,
I noticed that the `tensorflow.layer.dense` wrapper does not have a `seed` parameter, and I was wondering what you think about adding one. E.g., this seed would be used to initialize the random weights and bias units (if they are not initialized to zero).

Related to the points above,  it is currently also not clear how the weights are initialized. I.e., what distribution the random numbers are drawn from (if not zero). Does someone have some info on this? -- maybe this could be added to the API docs. 

What do you think? (I am happy to contribute a random seed implementation and a doc update if that's desirable.)"
9742,Optimized compiled Tensorflow uses almost 2x more memory than non-optimized binary,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Both
- **TensorFlow version (use command below)**: ('v1.0.0-65-g4763edf-dirty', '1.0.1') (compiled at the same git commit of the binary version)
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: N/A (CPU only)
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```python
import pdb
import numpy as np
import tensorflow as tf

def get_layer(input_size, output_size, name):
    # W_val is loaded from a file using numpy.load
    W_val = np.random.normal(scale=0.1, size=(input_size, output_size)).astype(np.float32)
    W = tf.get_variable(name='W_{}'.format(name), shape=(input_size, output_size),
                        initializer=tf.constant_initializer(value=W_val, dtype=tf.float32),
                        dtype=tf.float32)
    b = tf.get_variable(name='b_{}'.format(name), shape=(output_size,),
                        initializer=tf.constant_initializer(value=0.1, dtype=tf.float32),
                        dtype=tf.float32)
    return W, b

sessions = []
for i in range(3):
    g = tf.Graph()
    with g.as_default():
        W1, b1 = get_layer(158238, 900, '1')
        W2, b2 = get_layer(900, 1000, '2')
        W3, b3 = get_layer(1000, 1, '3')

        init = tf.global_variables_initializer()
    session = tf.Session(graph=g)
    session.run(init)
    print 'Loaded {}'.format(i)
    sessions.append(session)

pdb.set_trace()
```

### Describe the problem
After running the code snippet under the non-optimized binary Tensorflow installation, the used memory is ~6GB. However, when the same snippet is run with Tensorflow compiled with `-c opt --copt=-march=native` directives, the memory usage is ~11GB (1.8x larger).

Note that there's no memory-usage difference when I use `tf.truncated_normal(stddev=0.1...)` instead of `tf.constant_initializer` with a numpy array.

Not sure if this is a bug, or a side-effect of the optimized version or if there's something I can do to optimize memory usage?

### Source code / logs
I can attach chrome traces if necessary."
9741,run own model on android,"**question:**
i run tensorflow SSD model on android. The android studio didn't throw error. But i can't detect anything.I test my pb file, it's seems right. I don't know why. Anyone who can help me? Thanks!!

**code:**
because the model is based on tensorflow demo. so i just paste own code.

```
public List<Recognition> recognizeImage(final Bitmap bitmap) {
    final SplitTimer timer = new SplitTimer(""recognizeImage"");

    // Log this method so that it can be analyzed with systrace.
    Trace.beginSection(""recognizeImage"");

    Trace.beginSection(""preprocessBitmap"");
    // Preprocess the image data from 0-255 int to normalized float based
    // on the provided parameters.
    bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());

    for (int i = 0; i < intValues.length; ++i) {
      floatValues[i * 3 + 0] = ((intValues[i] >> 16) & 0xFF) / 255.0f;
      floatValues[i * 3 + 1] = ((intValues[i] >> 8) & 0xFF) / 255.0f;
      floatValues[i * 3 + 2] = (intValues[i] & 0xFF) / 255.0f;
    }
    Trace.endSection(); // preprocessBitmap

    // Copy the input data into TensorFlow.
    Trace.beginSection(""fillNodeFloat"");
    inferenceInterface.fillNodeFloat(
            inputName, new int[] {1, inputSize, inputSize, 3}, floatValues);
    Trace.endSection();

    timer.endSplit(""ready for inference"");

    // Run the inference call.
    Trace.beginSection(""runInference"");
    final int resultCode = inferenceInterface.runInference(outputNames);
    if (resultCode != 0) {
      throw new RuntimeException(""Bad result code from inference: "" + resultCode);
    }
    Trace.endSection();

    timer.endSplit(""ran inference"");

    // Copy the output Tensor back into the output array.
    Trace.beginSection(""readNodeFloat"");
    //推理得到的结果

    final float[] outputs=new float[feat_shapes[0][0] * feat_shapes[0][1] * (anchor_sizes[0].length+anchor_ratios[0].length) * 4+
                                          feat_shapes[1][0] * feat_shapes[1][1] * (anchor_sizes[1].length+anchor_ratios[1].length) * 4+
                                          feat_shapes[2][0] * feat_shapes[2][1] * (anchor_sizes[2].length+anchor_ratios[2].length) * 4+
                                          feat_shapes[3][0] * feat_shapes[3][1] * (anchor_sizes[3].length+anchor_ratios[3].length) * 4+
                                          feat_shapes[4][0] * feat_shapes[4][1] * (anchor_sizes[4].length+anchor_ratios[4].length) * 4+
                                          feat_shapes[5][0] * feat_shapes[5][1] * (anchor_sizes[5].length+anchor_ratios[5].length) * 4+
                                          feat_shapes[0][0] * feat_shapes[0][1] * (anchor_sizes[0].length+anchor_ratios[0].length) * NUM_CLASSES+
                                          feat_shapes[1][0] * feat_shapes[1][1] * (anchor_sizes[1].length+anchor_ratios[1].length) * NUM_CLASSES+
                                          feat_shapes[2][0] * feat_shapes[2][1] * (anchor_sizes[2].length+anchor_ratios[2].length) * NUM_CLASSES+
                                          feat_shapes[3][0] * feat_shapes[3][1] * (anchor_sizes[3].length+anchor_ratios[3].length) * NUM_CLASSES+
                                          feat_shapes[4][0] * feat_shapes[4][1] * (anchor_sizes[4].length+anchor_ratios[4].length) * NUM_CLASSES+
                                          feat_shapes[5][0] * feat_shapes[5][1] * (anchor_sizes[5].length+anchor_ratios[5].length) * NUM_CLASSES];

    inferenceInterface.readNodeFloat(outputNames[0], outputs);
    LOGGER.i(""recognizeImage: ""+outputs);
    Trace.endSection();

    // Find the best detections.
    final PriorityQueue<Recognition> pq =
            new PriorityQueue<Recognition>(
                    1,
                    new Comparator<Recognition>() {
                      @Override
                      public int compare(final Recognition lhs, final Recognition rhs) {
                        // Intentionally reversed to put high confidence at the head of the queue.
                        return Float.compare(rhs.getConfidence(), lhs.getConfidence());
                      }
                    });

    int num=0,offest=0;
    float cx,cy,cw,ch,xpos,ypos,h,w;
    for(int i=0;i<6;i++)
    {
      for(int y=0;y<feat_shapes[i][0];y++)
      {
        for(int x=0;x<feat_shapes[i][1];x++)
        {
          ypos=(y+anchor_offset)*anchor_steps[i]/img_shape[0];
          xpos=(x+anchor_offset)*anchor_steps[i]/img_shape[1];
          //预测框
          for(int l=0;l<(anchor_ratios[i].length+2);l++)
          {
            if(l==0)
            {
              h=anchor_sizes[i][0]/img_shape[0];
              w=anchor_sizes[i][0]/img_shape[1];
            }
            else if(l==1)
            {
              h=Float.parseFloat(String.valueOf(Math.sqrt(anchor_sizes[i][0]*anchor_sizes[i][1])/img_shape[0]));
              w=Float.parseFloat(String.valueOf(Math.sqrt(anchor_sizes[i][0]*anchor_sizes[i][1])/img_shape[1]));
            }
            else
            {
              h=Float.parseFloat(String.valueOf(anchor_sizes[i][0]/img_shape[0]/Math.sqrt(anchor_ratios[i][l-2])));
              w=Float.parseFloat(String.valueOf(anchor_sizes[i][0]/img_shape[1]/Math.sqrt(anchor_ratios[i][l-2])));
            }
            //解码
            cx=outputs[offest]*w*prior_scaling[0]+xpos;
            offest++;
            cy=outputs[offest]*h*prior_scaling[1]+ypos;
            offest++;
            cw=Float.parseFloat(String.valueOf(w*Math.exp(outputs[offest]*prior_scaling[2])));
            offest++;
            ch=Float.parseFloat(String.valueOf(h*Math.exp(outputs[offest]*prior_scaling[3])));
            offest++;
            final RectF rect =
                    new RectF(
                            Math.max(0, cx - cw / 2),
                            Math.max(0, cy - ch / 2),
                            Math.min(1, cx + cw / 2),
                            Math.min(1, cy + ch / 2));
            //挑选大于select_threshold的框
            offest++;
            for(int m=1;m<NUM_CLASSES;m++)
            {
              if(outputs[offest]>select_threshold)
              {
                LOGGER.i(
                        ""%s (%d) %f %s"", LABELS[m], m, outputs[offest], rect);
                pq.add(new Recognition("""" + num, LABELS[m], outputs[offest], rect));
                num++;
              }
              offest++;
            }
          }
        }
      }
    }
    timer.endSplit(""decoded results"");

    final ArrayList<Recognition> sort = new ArrayList<Recognition>();
    for (int i = 0; i < Math.min(pq.size(), top_k); ++i) {
      sort.add(pq.poll());
    }
    timer.endSplit(""sorted results"");

    final ArrayList<Recognition> nms = new ArrayList<Recognition>();
    float []keep_bboxes=new float[sort.size()];
    for(int i=0;i<sort.size();i++)
    {
      keep_bboxes[i]=1;
    }
    float overlap;
    for(int i=0;i<sort.size();i++)
    {
      if(keep_bboxes[i]==1)
      {
        for(int j=i+1;j<sort.size();j++)
        {
          overlap=bboxes_jaccard(sort.get(i).getLocation(),sort.get(j).getLocation());
          if(overlap<nms_threshold||sort.get(i).getTitle()!=sort.get(j).getTitle())
          {
            nms.add(sort.get(i));
          }
          else
            keep_bboxes[j]=0;
        }
      }
    }
    timer.endSplit(""nms results"");

    final ArrayList<Recognition> recognitions = new ArrayList<Recognition>();
    for (int i = 0; i < Math.min(nms.size(), MAX_RESULTS); ++i) {
      recognitions.add(nms.get(i));
    }
    Trace.endSection(); // ""recognizeImage""

    timer.endSplit(""processed results"");

    return recognitions;
  }
```"
9739,404 Error When Installing 1.1.0 GPU Python 3.x version,"~~~
HTTP error 404 while getting https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.1.0-py3-none-any.whl
  Could not install requirement tensorflow-gpu==1.1.0 from https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.1.0-py3-none-any.whl because of error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.1.0-py3-none-any.whl
Could not install requirement tensorflow-gpu==1.1.0 from https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.1.0-py3-none-any.whl because of HTTP error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.1.0-py3-none-any.whl for URL https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.1.0-py3-none-any.whl
~~~
Ran same command to successfully install version 1.0.0 GPU py3. I thought this would be the best place to bring it to the Google team's attention, but let me know if not!"
9738,OpenCL support for Window,"Scheduled to be compatible with OpenCL for Window
Currently bazel and cmake can not build on OpenCL on Windows
Is there no plan to support OpenCL on Windows?
If so, when will the period be?"
9737,Why？,"I have successfully install the tensorflow-gpu-1.0.0 in windows 10, but when I train a model , it appears this error:
InternalError: Failed launching MaxPoolForwardNoMask"
9735,Tensorflow searching for outdated version of cudnn64_6.dll (Import Error),"So I tried my first install of tensorflow-gpu on Window 7x64 with a GTX1080 and was met with a DLL import error python exception that wasn't very helpful at all (that message needs to be updated to indicate what DLL it can't find!).

Anyways, I decided to attempt the import tensorflow function with proc_mon watching all the file I/O from python... Turns out, Tensorflow is searching for 'cudnn64_5.dll' ... The latest 'cudnn-8.0-windows7-x64-v6.0' installs the following dll: 'cudnn64_6.dll' ... By copying the newer DLL to the DLL name that tensorflow expects, I was able to get past this issue and run tensorflow.

Tensorflow needs to be updated to support the latest cudnn64_6 dll that NVidia provides by default."
9734,`tf.while_loop` Bug,"I found that `tf.while_loop(cond...)` has issues on its termination condition which `cond` is set by using method `tf.logical_or(t1, t2)`. If one of conditions `t1` or `t2` is satisfied, then while loop should break. But I noticed this is not true. The following is the code for testing and reproducing:

```
import tensorflow as tf


def condition(i, val):
    # Termination condition.
    return tf.logical_or(tf.less(i, 10), tf.less(val, 1))


def body(i, val):
    # Mimic the body loop.
    i += 1
    val -= 0.2
    return i, val

# Build up graph.
i = tf.Variable(0, trainable=False)
value = tf.Variable(0.0, trainable=False)
train_ops = tf.while_loop(condition, body, [i, value])
init_op = tf.global_variables_initializer()

# Run tf session.
sess = tf.Session()
sess.run(init_op)
print(sess.run(train_ops))
```
The code logic is quite simple: Inside the termination condition, either `i >= 10` or `value >= 1` is satisfied, the loop should be terminated. Since `i` increments by 1 each iteration with initial `0`, and second condition is always `True`, so it should terminate after the 10th iteration. However, if you run the above code, you could see the while loop goes infinity. I also tried other termination condition like `tf.logical_or(tf.less(i, 10), tf.less(val, -0.5))` and got the same result (but should terminate after the third iteration, in which `val` decreases by 0.2 each iteration). But if you try `cond` like`tf.logical_or(tf.less(i, 10), tf.less(val, -100))`, it would terminate at the 10th iteration which is also not expected, because `val` initially is 0, which is greater -100 at the beginning. This means that one of conditions for `tf.logical_or(t1, t2)` may have bugs when it works with `tf.while_loop()`.

P.S The above code runs on MacOS 10.12.4 with Python 3.5, and the issue can be reproduced with Tensorflow 0.12/1.0/1.1"
9731,"More description for optimizers Adagrad, Adadelta, FTRL...","Is it necessary to add more description for optimizers such as Adagrad, Adadelta, FTRL and so on just as what [Adam](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) does? Since these are several quite new optimizers and I think it's better to show users more details about these optimizers so that they can understand why do these optimizers work better than SGD in some situations.

If more descriptions are welcomed, I'm glad to make new PRs to do this.

Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
9729,AttributeError: module 'tensorflow.contrib.seq2seq' has no attribute 'BeamSearchDecoder',"I don't know why I am getting this issue:

 ```
AttributeError: module 'tensorflow.contrib.seq2seq' has no attribute 'BeamSearchDecoder'
```

I am using version 1.1.0, and BeamSearchDecoder is in the API master for Python: https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/seq2seq/BeamSearchDecoder"
9728,Error code 1 : While installing TensorFlow on windows 10 with pip,"I was trying to install TensorFlow  on windows 10 with pip 
When I type : 
>pip3 install tensorflow 

I get the following error : 
>ImportError : cannot import name 'setup'
>Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\Dell\AppData\Local\Temp\pip-build-qzzubrm_\protobuf\
"
9727,Python package for macOS missing.,"The python 3 gpu package listed on: https://www.tensorflow.org/install/install_mac#the_url_of_the_tensorflow_python_package is missing.

```
$ curl https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.1.0-py3-none-any.whl
<?xml version='1.0' encoding='UTF-8'?><Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message></Error>
```
"
9726,Import errors ,"### System information
- Trying to run tutorial code
- Win8.1
- pip3 install --upgrade tensorflow 
- can't run programm to write version (can't run any code with import tensorflow)

Traceback (most recent call last):
  File ""C:\Users\home-pc\AppData\Local\Programs\Python\Python35-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\home-pc\AppData\Local\Programs\Python\Python35-32\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\home-pc\AppData\Local\Programs\Python\Python35-32\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\home-pc\AppData\Local\Programs\Python\Python35-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\home-pc\AppData\Local\Programs\Python\Python35-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/home-pc/PycharmProjects/untitled6/b.py"", line 1, in <module>
    import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)
  File ""C:\Users\home-pc\AppData\Local\Programs\Python\Python35-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\home-pc\AppData\Local\Programs\Python\Python35-32\lib\site-packages\tensorflow\python\__init__.py"", line 60, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\home-pc\AppData\Local\Programs\Python\Python35-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\home-pc\AppData\Local\Programs\Python\Python35-32\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\home-pc\AppData\Local\Programs\Python\Python35-32\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\home-pc\AppData\Local\Programs\Python\Python35-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\home-pc\AppData\Local\Programs\Python\Python35-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ImportError: No module named '_pywrap_tensorflow'


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there."
9724,Freeze_graph results in very poor accuracy compared to manually exporting and freezing the graph?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 LTS
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.1
- **Bazel version (if compiling from source)**: 4.5
- **CUDA/cuDNN version**: 8.0/5.1
- **GPU model and memory**: GTX 860M

### Describe the problem
From using the official freeze_graph.py file from TF, I am getting a very low accuracy in prediction as compared to manually exporting the graph using a file I wrote called `write_pb.py`, I get a much higher accuracy.

To be specific, here are the differences:

**Differences:**
1. Using `write_pb.py` to manually export the graph converted way more variables to constants, even with the same checkpoint files.
2.  It takes a long, long time for `freeze_graph.py` to actually complete the exporting.
3. Very importantly: I get a very low accuracy from using freeze_graph. Meanwhile, by exporting the graph manually, I get a nearly identical accuracy as if I predicted an image right from the checkpoint without freezing.
4. Manually exporting the graph results in a smaller file size (just 1-2MB of difference).
5. The manually exported graph has a faster inference time than the graph obtained from `freeze_graph.py`.

This is the freeze_graph.py file I got from the TF repo: https://gist.github.com/kwotsin/b9dae8246a30371a1a10690e2fa27cb7

This is the `write_pb` file I wrote:
https://gist.github.com/kwotsin/8e43f5db4815e1f1af37da70d0933d8b


### Source code / logs
Using freeze_graph with this command: `python freeze_graph.py --input_checkpoint=model.ckpt-18279 --input_graph=graph.pbtxt --output_graph=frozen_model_from_freeze_graph.pb --output_node_names=InceptionResnetV2/Logits/Predictions`

I get this output:
```
2017-05-06 22:27:22.967898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-05-06 22:27:22.968268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: GeForce GTX 860M
major: 5 minor: 0 memoryClockRate (GHz) 1.0195
pciBusID 0000:01:00.0
Total memory: 3.95GiB
Free memory: 3.34GiB
2017-05-06 22:27:22.968283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-05-06 22:27:22.968288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-05-06 22:27:22.968306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0)
2017-05-06 22:27:33.499491: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-05-06 22:27:33.499519: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-05-06 22:27:33.509659: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x36ced220 executing computations on platform Host. Devices:
2017-05-06 22:27:33.509678: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>
2017-05-06 22:27:33.509827: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-05-06 22:27:33.509837: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-05-06 22:27:33.512265: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x36ce6210 executing computations on platform CUDA. Devices:
2017-05-06 22:27:33.512279: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): GeForce GTX 860M, Compute Capability 5.0
Converted 490 variables to const ops.
7871 ops in the final graph.
```
Meanwhile, if I manually freeze the graph using `write_pb.py`,

I get this output:
```
2017-05-06 22:39:00.197711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-05-06 22:39:00.198096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: GeForce GTX 860M
major: 5 minor: 0 memoryClockRate (GHz) 1.0195
pciBusID 0000:01:00.0
Total memory: 3.95GiB
Free memory: 3.29GiB
2017-05-06 22:39:00.198120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-05-06 22:39:00.198125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-05-06 22:39:00.198143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0)
2017-05-06 22:39:00.709417: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-05-06 22:39:00.709452: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-05-06 22:39:00.710238: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0xef16320 executing computations on platform Host. Devices:
2017-05-06 22:39:00.710252: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>
2017-05-06 22:39:00.710374: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-05-06 22:39:00.710384: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-05-06 22:39:00.710645: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0xef18180 executing computations on platform CUDA. Devices:
2017-05-06 22:39:00.710654: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): GeForce GTX 860M, Compute Capability 5.0
Exporting graph...
Converted 898 variables to const ops.
```





"
9722,How to know which will be supported wheel for my platform? (Installing Tensorflow),"C:\Users\Sudhit>pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.1.0-cp35-cp35m-win_amd64.whl
tensorflow-1.1.0-cp35-cp35m-win_amd64.whl is not a supported wheel on this platform.

how will I know which will be supported wheel for my platform?"
9721,Feature request: Add 'axis' option for 'tf.boolean_mask()',"`tf.boolean_mask()` is extremely useful when it comes to training.

However, the present `tf.boolean_mask()` does not have an `axis` option, thus masking some specific dimension of an array could be troublesome. 

Appreciate your consideration."
9720,"tf.while_loop swap_memory=True when not running train_op, does it always swap?","From the documentation of `tf.while_loop`:
> For training, TensorFlow remembers the tensors that are produced in the forward inference but needed in back propagation. These tensors can be a main source of memory consumption and often cause OOM problems when training on GPUs. When the flag swap_memory is true, we swap out these tensors from GPU to CPU. This for example allows us to train RNN models with very long sequences and large batches.

But **after** having trained, is TensorFlow's engine smart enough to avoid swapping even if `swap_memory=True` or should [optimize_for_inference.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py) flip swap_memory (i.e. make the equivalent modifications to the graph)?

Naturally this only applies to running inference on a GPU.

@yuanbyu"
9718,compile error C2064 when I use cmake(vs2015) to make windows binary,"I follows the instructions (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake  ) to compile the tensorflow in windows ( compiler vs2015)

I use 
    cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=D:/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=""D:/Program Files/Python 3.5/python.exe"" -DPYTHON_LIBRARIES=""D:/Program Files/Python 3.5/libs/python35.lib"" -Dtensorflow_ENABLE_GPU=OFF -Dtensorflow_ENABLE_GRPC_SUPPORT=OFF  -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX
to generate project files.

after: MSBuild /p:Configuration=Release tf_tutorials_example_trainer.vcxproj

it complains following :

[in Chinese , the cl output]
h:\tensorflow_c\tensorflow-1.1.0\tensorflow\core\lib\gtl\array_slice_internal
.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 H:\tensorflow_c
\tensorflow-1.1.0\tensorflow\core\lib\strings\str_util.cc) [H:\tensorflow_c\tensorflow-1.1.0\tensorflow\contrib\cmake\build_vc\tf_core_lib.vcxproj]

[in English ]
h:\tensorflow_c\tensorflow-1.1.0\tensorflow\core\lib\gtl\array_slice_internal
.h(89): error C2064: term does not evaluate to a function taking 0 arguments ( compiling  H:\tensorflow_c
\tensorflow-1.1.0\tensorflow\core\lib\strings\str_util.cc) [H:\tensorflow_c\tensorflow-1.1.0\tensorflow\contrib\cmake\build_vc\tf_core_lib.vcxproj]
  "
9717,feature request: support placeholder for parameter k in tf.nn.in_top_k,"Currently [`tf.nn.in_top_k`](https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k) requires an int as `k`, while [`tf.nn.top_k`](https://www.tensorflow.org/api_docs/python/tf/nn/top_k) does support a 0-D int32 Tensor as parameter `k`. This is kind of inconsistent and inconvenient.

Is it possible to allow `k` to be a 0-D int32 Tensor in `tf.nn.in_top_k`?"
9716,applying different batch_sizes for each buckets in bucketd_rnn?,"I've found an obvious fact that, in seq2seq model with buckets(especially in NMT applications), the shorter buckets takes extremely smaller memory size in GPU than the longer buckets in same batch_size in both training and inferencing; i.e 64 sentences for (5, 10) takes extremely smaller memory size than 64 sentences for (30, 40).

Then why don't we have different bucket sizes for each bucket? 

Actually i've run my experiments in these way and it really helps cutting down my training time.

There may be some issues in having different bucket sizes like sampling frequency for each buckets, the issue from recent paper in ICLR that claiming the smaller batch size make the model better in generalization, etc...

Yet, my preemptive concern was the training time so i managed different batch sizes like 1024 sentences for (5, 10) bucket and 64 sentences for (30, 40) bucket that maximize my GPU's utility.

(absolutely this cannot be done with dynamic_rnn structure.)

what do you guys think about this idea?"
9715,[inputs] instead of inputs for rnn_cell?,"In current rnn_cell implementation, It gets inputs as 2-d tensor for cell inputs,

Hence embedding layer before the actual rnn_cell gets a little bit annoying for making 'decoder'.

I really like the ""wrapper"" concept in current implementation that expand by each layer with embedding, residual net, or dropouts. 

If rnn_cell gets list of '2-d' tensor like [inputs1, inputs2, ...], then the concept of wrapper can be fully used in decoder as well as simple rnn or encoder;

for a little more concretely, 
if ""embedding wrapped cell"" gets input list [inputs, context_inputs_from_attention],
then the ""embedding wrapped cell"" can just pass [embedded_inputs, context_inputs_from_attention] to the real ""LSTM_cell"" and ""LSTM_cell"" can weight sum those inputs in inputs list.

what do you think about this idea?"
9714,Tensorflow causes SIGSEGV in numpy,"x_train Examples Loaded = (55000, 784)
y_train Examples Loaded = (55000, 10)

[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]
[New Thread 0x7ffff2211700 (LWP 6011)]
[New Thread 0x7ffff2a12700 (LWP 6012)]
[New Thread 0x7fffd33a4700 (LWP 6013)]
[Thread 0x7fffd33a4700 (LWP 6013) exited]
[Thread 0x7ffff2a12700 (LWP 6012) exited]
[Thread 0x7ffff2211700 (LWP 6011) exited]

Program received signal SIGSEGV, Segmentation fault.
0x00007ffff716602f in _int_free (av=0x7ffff74a8760 <main_arena>, 
    p=<optimized out>, have_lock=0) at malloc.c:3996
3996	malloc.c: No such file or directory.

gdb backtrace report

(gdb) bt
#0  0x00007ffff716602f in _int_free (av=0x7ffff74a8760 <main_arena>, 
    p=<optimized out>, have_lock=0) at malloc.c:3996
#1  0x00007ffff2ad9ef0 in ?? ()
   from /home/shreeranga/PP/Exp/venvs/lib/python2.7/site-packages/numpy/core/..0
#2  0x00007ffff2ad4770 in ?? ()
   from /home/shreeranga/PP/Exp/venvs/lib/python2.7/site-packages/numpy/core/..0
#3  0x00007ffff2ad486a in ?? ()
   from /home/shreeranga/PP/Exp/venvs/lib/python2.7/site-packages/numpy/core/..0
#4  0x00007ffff2a28e09 in ?? ()
   from /home/shreeranga/PP/Exp/venvs/lib/python2.7/site-packages/numpy/core/..0
#5  0x00007ffff2a26e7f in ?? ()
   from /home/shreeranga/PP/Exp/venvs/lib/python2.7/site-packages/numpy/core/..0
#6  0x00000000009649e0 in ?? ()
#7  0x00007ffff521f1c8 in __frame_dummy_init_array_entry ()
   from /home/shreeranga/PP/Exp/venvs/lib/python2.7/site-packages/numpy/core/..o
#8  0x00007fffffffdc00 in ?? ()
#9  0x00007ffff2ae7cd1 in ?? ()
"
9713,Exception during running the graph: Unable to get element from the feed as bytes.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.1.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: 
gcloud ml-engine local predict \
    --model-dir=$MODEL_DIR \
    --json-instances=""$DATA_DIR/test2.json"" \

### Describe the problem
I'm using a beam pipeline to preprocess my text to integers bag of words, similar to this example https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/reddit_tft/reddit.py

      words = tft.map(tf.string_split, inputs[name])
      result[name + '_bow'] = tft.string_to_int(
          words, frequency_threshold=frequency_threshold)

Preprocessing and training seem to work fine. I train a simple linear model and point to the transform function and run an experiment. 

When I try to run a prediction I get an error, no idea what I'm doing wrong.

### Source code / logs
WARNING:root:MetaGraph has multiple signatures 2. Support for multiple signatures is
 limited. By default we select named signatures.
ERROR:root:Exception during running the graph: Unable to get element from the feed a
s bytes.
Traceback (most recent call last):
  File ""lib/googlecloudsdk/command_lib/ml_engine/local_predict.py"", line 136, in <mo
dule>
    main()
  File ""lib/googlecloudsdk/command_lib/ml_engine/local_predict.py"", line 131, in mai
n
    instances=instances)
  File ""/Users/xyz/Downloads/google-cloud-sdk/lib/third_party/cloud_ml_engin
e_sdk/prediction/prediction_lib.py"", line 656, in local_predict
    _, predictions = model.predict(instances)
  File ""/Users/xyz/Downloads/google-cloud-sdk/lib/third_party/cloud_ml_engin
e_sdk/prediction/prediction_lib.py"", line 553, in predict
    outputs = self._client.predict(columns, stats)
  File ""/Users/xyz/Downloads/google-cloud-sdk/lib/third_party/cloud_ml_engin
e_sdk/prediction/prediction_lib.py"", line 382, in predict
    ""Exception during running the graph: "" + str(e))
prediction_lib.PredictionError: (4, 'Exception during running the graph: Unable to g
et element from the feed as bytes.')

    def feature_columns(vocab_size=100000):
        result = []
        for key in TEXT_COLUMNS:
            column = tf.contrib.layers.sparse_column_with_integerized_feature(key, vocab_size, combiner='sum')
        result.append(column)
    return result

    model_fn = tf.contrib.learn.LinearClassifier(
          feature_columns=feature_columns(),
          n_classes=15,
          model_dir=output_dir
        )  

    def get_transformed_reader_input_fn(transformed_metadata,
                                        transformed_data_paths,
                                        batch_size,
                                        mode):
      """"""Wrap the get input features function to provide the runtime arguments.""""""
      return input_fn_maker.build_training_input_fn(
          metadata=transformed_metadata,
          file_pattern=(
              transformed_data_paths[0] if len(transformed_data_paths) == 1
              else transformed_data_paths),
          training_batch_size=batch_size,
          label_keys=[LABEL_COLUMN],
          reader=gzip_reader_fn,
          key_feature_name='key',
          reader_num_threads=4,
          queue_capacity=batch_size * 2,
          randomize_input=(mode != tf.contrib.learn.ModeKeys.EVAL),
          num_epochs=(1 if mode == tf.contrib.learn.ModeKeys.EVAL else None))


    transformed_metadata = metadata_io.read_metadata(
        args.transformed_metadata_path)
    raw_metadata = metadata_io.read_metadata(args.raw_metadata_path)

    train_input_fn = get_transformed_reader_input_fn(
        transformed_metadata, args.train_data_paths, args.batch_size,
        tf.contrib.learn.ModeKeys.TRAIN)

    eval_input_fn = get_transformed_reader_input_fn(
        transformed_metadata, args.eval_data_paths, args.batch_size,
        tf.contrib.learn.ModeKeys.EVAL)

    serving_input_fn = input_fn_maker.build_parsing_transforming_serving_input_fn(
            raw_metadata,
            args.transform_savedmodel,
            raw_label_keys=[],
            raw_feature_keys=model.TEXT_COLUMNS)

    export_strategy = tf.contrib.learn.utils.make_export_strategy(
        serving_input_fn,
        default_output_alternative_key=None,
        exports_to_keep=5,
        as_text=True)

    return Experiment(
        estimator=model_fn,
        train_input_fn=train_input_fn,
        eval_input_fn=eval_input_fn,
        export_strategies=export_strategy,
        eval_metrics=model.get_eval_metrics(),
        train_monitors=[],
        train_steps=args.train_steps,
        eval_steps=args.eval_steps,
        min_eval_frequency=1
    )

"
9711,eval() error about variable_scope(),"I run the code from official tutorial:
https://www.tensorflow.org/programmers_guide/variable_scope

```
with tf.variable_scope(""foo"", initializer=tf.constant_initializer(0.4)):
    v = tf.get_variable(""v"", [1])
    assert v.eval() == 0.4  # Default initializer as set above.
    w = tf.get_variable(""w"", [1], initializer=tf.constant_initializer(0.3))
    assert w.eval() == 0.3  # Specific initializer overrides the default.
    with tf.variable_scope(""bar""):
        v = tf.get_variable(""v"", [1])
        assert v.eval() == 0.4  # Inherited default initializer.
    with tf.variable_scope(""baz"", initializer=tf.constant_initializer(0.2)):
        v = tf.get_variable(""v"", [1])
        assert v.eval() == 0.2  # Changed default initializer
```
**the problem as below**:
Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`

```
with tf.Session():
    with tf.variable_scope(""foo"", initializer=tf.constant_initializer(0.4)):
        v = tf.get_variable(""v"", [1])
        assert v.eval() == 0.4  # Default initializer as set above.
        w = tf.get_variable(""w"", [1], initializer=tf.constant_initializer(0.3))
        assert w.eval() == 0.3  # Specific initializer overrides the default.
        with tf.variable_scope(""bar""):
            v = tf.get_variable(""v"", [1])
            assert v.eval() == 0.4  # Inherited default initializer.
        with tf.variable_scope(""baz"", initializer=tf.constant_initializer(0.2)):
            v = tf.get_variable(""v"", [1])
            assert v.eval() == 0.2  # Changed default initializer
```

**when i try with session the problem changed as below:**

Attempting to use uninitialized value foo/v

	 [[Node: foo/v/_0 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_4_foo/v"", _device=""/job:localhost/replica:0/task:0/gpu:0""](foo/v)]]

this problem in both windows and linux
the platform is :
windows10 , python3.5 , tf.__version__ 0.12.1
ubuntu, python2.7  , tf.__version__  1.1.0-rc2

how can I solve this problem?
"
9710,"Error restoring checkpoints using tf.estimator.Estimator or tf.contrib.learn.Estimator, If moved since training","
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/5.1
- **GPU model and memory**: Titan X
- **Exact command to reproduce**:

```python
output_dir = /location1
estimator = learn.Estimator(model_fn=model_fn, model_dir=output_dir,
                                params=params, config = config)
estimator.fit(input_fn=train_input_fn, steps = 1000)
```
```bash
$ cp -r /location1 /location2
$ rm -rf /location1
```
```python
output_dir = /location2
estimator = learn.Estimator(model_fn=model_fn, model_dir=output_dir,
                                params=params, config = config)
estimator.fit(input_fn=train_input_fn, steps = 1000)
```
```bash
InvalidArgumentError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to get matching files on /location1
```
### Describe the problem
I had a need to copy my checkpoints to continue training on another machine, without access to the original location. 
The root cause seems to be in tensorflow/python/training/session_manager.py  at line 177 where 
`ckpt = saver_mode.get_checkpoint_state(checkpoint_dir)` is used to find the checkpoint path used at training, and then line 188
`saver.restore(sess, ckpt.model_checkpoint_path)` is used to restore, instead of using, for example, the latest checkpoint available provided by the model_dir argument to estimator.fit, which is the behavior I expected. 
In any case, it would be nice to be able to point an estimator.fit() or .train() (v1.1) call to continue at a specific checkpoint file.

"
9709,Missing image in https://www.tensorflow.org/api_docs/python/tf/gather,"### System information
n/a

### Describe the problem

https://www.tensorflow.org/api_docs/python/tf/gather includes an image, https://www.tensorflow.org/api_docs/images/Gather.png, but this does not exist.  Gather.png doesn't seem to exist anywhere in the TF repository."
9708,tf.random_crop exception after upgrading to tf1.1 from tf1.0,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.4
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:v1.1.0-rc0-61-g1ec6ed5 1.1.0

- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/5.1
- **GPU model and memory**: Tesla m40 / 12 gb
- **Exact command to reproduce**: 

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
The tf.random_crop gives exception even when it receives input of valid size. assertion failed: [Need value.shape >= size, got ] [224 224 3] [224 224 3]

### Source code / logs
  File ""../foo.py"", line 1356, in _pp_augment
    aug = tf.random_crop(aug, [crop_size[0], crop_size[1], aug_dim3])
  File ""/usr/local/anaconda3/envs/tf1.1/lib/python3.5/site-packages/tensorflow/python/ops/random_ops.py"", line 303, in random_crop
    [""Need value.shape >= size, got "", shape, size])
  File ""/usr/local/anaconda3/envs/tf1.1/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 121, in Assert
    condition, data, summarize, name=""Assert"")
  File ""/usr/local/anaconda3/envs/tf1.1/lib/python3.5/site-packages/tensorflow/python/ops/gen_logging_ops.py"", line 39, in _assert
    summarize=summarize, name=name)
  File ""/usr/local/anaconda3/envs/tf1.1/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 768, in apply_op
    op_def=op_def)
  File ""/usr/local/anaconda3/envs/tf1.1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2336, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/anaconda3/envs/tf1.1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1228, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): assertion failed: [Need value.shape >= size, got ] [224 224 3] [224 224 3]
         [[Node: image_filters/train_tower_0/random_crop_1/Assert/Assert = Assert[T=[DT_STRING, DT_INT32, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/cpu:0""](image_filters/train_tower_0/random_crop_1/All/_29, image_filters/train_tower_0/random_crop_1/Assert/Assert/data_0, image_filters/train_tower_0/random_crop_1/Shape/_31, image_filters/train_tower_0/random_crop_1/size/_33)]]
         [[Node: image_filters/train_tower_0/DecodeRaw_1/_93 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_232_image_filters/train_tower_0/DecodeRaw_1"", tensor_type=DT_UINT8, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
"
9707,ar,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
9705,atrous_conv2d does not support NCHW format,"Any plans to support NCHW format for atrous_conv2d?  As per the performance guidelines on TensorFlow website,  ops using NCHW format is faster than NHWC format for GPUs."
9703,pkg-config file generation,"Hi! I was wondering if a pkg-config file for the tensorflow library (`libtensorflow.so`) could be added so that depending projects could use it more easily.

For the current stable version (`1.1.0`), if we installed it into `/usr` (which should be configurable), the generated pkg-config file `tensorflow.pc` should look something like:
```
prefix=/usr
exec_prefix=${prefix}
libdir=${exec_prefix}/lib
includedir=${prefix}/include
modules=1

Name: tensorflow
Version: 1.1.0
Description: Library for computation using data flow graphs for scalable machine learning
Requires:
Libs: -L${libdir} -ltensorflow -lstdc++
Cflags: -I${includedir}/tensorflow
```
Thanks in advance :)"
9702,Installation instructions for conda install tensorflow in the root environment,"The [instructions](https://www.tensorflow.org/install/install_linux#InstallingAnaconda) say to 
1. create a new **empty** environment 
2. activate it
3. install tensorflow via pip. 

But pip is not installed in the new environment, so the third command will call the first pip inside the PATH system variable, that usually is the pip installed in the root conda environment. The ultimate result is that tensorflow is installed in the root environment.

To solve this issue, it's sufficient to install pip in the new environment:
`conda create --name tensorflow pip`
"
9701,Tensorboard does not show any scalers or graphs ,"System information

    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
    TensorFlow installed from (source or binary): binary, installed via pip
    TensorFlow version (use command below): 1.1.0
    Bazel version (if compiling from source):na
    CUDA/cuDNN version: na
    GPU model and memory: na
    Exact command to reproduce:?


### Describe the problem
Initially I was working with tensorflow 1.0.0 and tensorboard seemed to work just fine. I have updated to 1.1.0 and now it appears it does not display anything anymore. However, I get the warning 'WARNING:tensorflow:path ../external\data/plugin/text/runs not found, sending 404'. I have also downgraded tensorflow to 1.0.0 and now it also stopped working. 

### Source code / logs
```
tensorboard --inspect --logdir=model_dir
Found event files in: model_dir

These tags are in model_dir:
audio -
histograms
   dnn/dnn/hiddenlayer_0_activation
   dnn/dnn/hiddenlayer_10_activation
   dnn/dnn/hiddenlayer_11_activation
   dnn/dnn/hiddenlayer_1_activation
   dnn/dnn/hiddenlayer_2_activation
   dnn/dnn/hiddenlayer_3_activation
   dnn/dnn/hiddenlayer_4_activation
   dnn/dnn/hiddenlayer_5_activation
   dnn/dnn/hiddenlayer_6_activation
   dnn/dnn/hiddenlayer_7_activation
   dnn/dnn/hiddenlayer_8_activation
   dnn/dnn/hiddenlayer_9_activation
   dnn/dnn/logits_activation
images -
scalars
   dnn/dnn/hiddenlayer_0_fraction_of_zero_values
   dnn/dnn/hiddenlayer_10_fraction_of_zero_values
   dnn/dnn/hiddenlayer_11_fraction_of_zero_values
   dnn/dnn/hiddenlayer_1_fraction_of_zero_values
   dnn/dnn/hiddenlayer_2_fraction_of_zero_values
   dnn/dnn/hiddenlayer_3_fraction_of_zero_values
   dnn/dnn/hiddenlayer_4_fraction_of_zero_values
   dnn/dnn/hiddenlayer_5_fraction_of_zero_values
   dnn/dnn/hiddenlayer_6_fraction_of_zero_values
   dnn/dnn/hiddenlayer_7_fraction_of_zero_values
   dnn/dnn/hiddenlayer_8_fraction_of_zero_values
   dnn/dnn/hiddenlayer_9_fraction_of_zero_values
   dnn/dnn/logits_fraction_of_zero_values
   loss
tensor -
======================================================================

Event statistics for model_dir:
audio -
graph
   first_step           0
   last_step            0
   max_step             0
   min_step             0
   num_steps            1
   outoforder_steps     []
histograms
   first_step           1
   last_step            1
   max_step             1
   min_step             1
   num_steps            1
   outoforder_steps     []
images -
scalars
   first_step           1
   last_step            1
   max_step             1
   min_step             1
   num_steps            1
   outoforder_steps     []
sessionlog:checkpoint
   first_step           1
   last_step            1
   max_step             1
   min_step             1
   num_steps            1
   outoforder_steps     []
sessionlog:start
   outoforder_steps     []
   steps                [1]
sessionlog:stop -
tensor -
======================================================================


tensorboard --logdir=model_dir --host=127.0.0.1
Starting TensorBoard b'47' at http://127.0.0.1:6006
(Press CTRL+C to quit)
WARNING:tensorflow:path ../external\data/plugin/text/runs not found, sending 404
WARNING:tensorflow:path ../external\data/plugin/text/runs not found, sending 404
WARNING:tensorflow:path ../external\data/plugin/text/runs not found, sending 404
WARNING:tensorflow:path ../external\data/plugin/text/runs not found, sending 404

```"
9699,RNN: InvalidArgumentError when adding InitialState,"Please go to Stack Overflow for help and support:

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.  The current stock examples are pre-TF1.0 and do not run.  I have already posted an issue regarding them. (https://github.com/tensorflow/tensorflow/issues/9294#issuecomment-294944970)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: TF1.1.0
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: 5.1
- **GPU model and memory**: p2-xlarge with 12GiB
- **Exact command to reproduce**:

When training a simple two layer RNN, if I do not include the initialstate, the graph trains properly, if I add the initialstate, then I get the following error.  
Brief code (full code below):
```
    initial_state = lstm_cells.zero_state(batch_size, tf.float32) 
    RNN_outputs, state = tf.contrib.rnn.static_rnn(lstm_cells, inputs=Xnew, dtype=tf.float32, initial_state=initial_state)
```
Error message (full message below):
```
InvalidArgumentError (see above for traceback): ConcatOp : Dimensions of inputs should match: shape[0] = [20,75] vs. shape[1] = [2500,75]
	 [[Node: RNN/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](RNN/split, RNN/MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros_1, RNN/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/concat/axis)]]
	 [[Node: Mean_1/_9 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_16414_Mean_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
```
If I print the `initial_state` variable, I get this:
```
(LSTMStateTuple(c=<tf.Tensor 'RNN/MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros:0' shape=(2500, 75) dtype=float32>, h=<tf.Tensor 'RNN/MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(2500, 75) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'RNN/MultiRNNCellZeroState/BasicLSTMCellZeroState_1/zeros:0' shape=(2500, 75) dtype=float32>, h=<tf.Tensor 'RNN/MultiRNNCellZeroState/BasicLSTMCellZeroState_1/zeros_1:0' shape=(2500, 75) dtype=float32>))
```
If I print the `state` producted by `static_rnn` call:
```
(LSTMStateTuple(c=<tf.Tensor 'RNN/rnn/multi_rnn_cell_299/cell_0/basic_lstm_cell/add_1:0' shape=(2500, 75) dtype=float32>, h=<tf.Tensor 'RNN/rnn/multi_rnn_cell_299/cell_0/basic_lstm_cell/mul_2:0' shape=(2500, 75) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'RNN/rnn/multi_rnn_cell_299/cell_1/basic_lstm_cell/add_1:0' shape=(2500, 75) dtype=float32>, h=<tf.Tensor 'RNN/rnn/multi_rnn_cell_299/cell_1/basic_lstm_cell/mul_2:0' shape=(2500, 75) dtype=float32>))
```
Here is the full message:
```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-15-5e6d1756b352> in <module>()
     90         Xin   : X_test,
     91         Ytrue : one_hot(y_test, LabelMax),
---> 92         keep_prob: 1.0
     93     }
     94 )

/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)
    776     try:
    777       result = self._run(None, fetches, feed_dict, options_ptr,
--> 778                          run_metadata_ptr)
    779       if run_metadata:
    780         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)
    980     if final_fetches or final_targets:
    981       results = self._do_run(handle, final_targets, final_fetches,
--> 982                              feed_dict_string, options, run_metadata)
    983     else:
    984       results = []

/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1030     if handle is None:
   1031       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
-> 1032                            target_list, options, run_metadata)
   1033     else:
   1034       return self._do_call(_prun_fn, self._session, handle, feed_dict,

/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)
   1050         except KeyError:
   1051           pass
-> 1052       raise type(e)(node_def, op, message)
   1053 
   1054   def _extend_graph(self):

InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [20,75] vs. shape[1] = [2500,75]
	 [[Node: RNN/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](RNN/split, RNN/MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros_1, RNN/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/concat/axis)]]
	 [[Node: Mean_1/_9 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_16414_Mean_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op u'RNN/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/concat', defined at:
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/ipykernel/__main__.py"", line 3, in <module>
    app.launch_new_instance()
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/ipykernel/kernelapp.py"", line 474, in start
    ioloop.IOLoop.instance().start()
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/zmq/eventloop/ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/tornado/ioloop.py"", line 887, in start
    handler_func(fd_obj, events)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/tornado/stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/tornado/stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 276, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 228, in dispatch_shell
    handler(stream, idents, msg)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 390, in execute_request
    user_expressions, allow_stdin)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/ipykernel/zmqshell.py"", line 501, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2717, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2821, in run_ast_nodes
    if self.run_code(code, result):
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-14-eaca1bbf91d8>"", line 48, in <module>
    RNN_outputs, state = tf.contrib.rnn.static_rnn(lstm_cells, inputs=Xnew, dtype=tf.float32, initial_state=initial_state)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py"", line 197, in static_rnn
    (output, state) = call_cell()
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py"", line 184, in <lambda>
    call_cell = lambda: cell(input_, state)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py"", line 953, in __call__
    cur_inp, new_state = cell(cur_inp, cur_state)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py"", line 241, in __call__
    concat = _linear([inputs, h], 4 * self._num_units, True)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py"", line 1048, in _linear
    res = math_ops.matmul(array_ops.concat(args, 1), weights)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 1034, in concat
    name=name)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 519, in _concat_v2
    name=name)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 768, in apply_op
    op_def=op_def)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2336, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/ubuntu/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1228, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): ConcatOp : Dimensions of inputs should match: shape[0] = [20,75] vs. shape[1] = [2500,75]
	 [[Node: RNN/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](RNN/split, RNN/MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros_1, RNN/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/concat/axis)]]
	 [[Node: Mean_1/_9 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_16414_Mean_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
```
Here is the network creation code:
```
# Clear the graph memory
tf.reset_default_graph()

# Graph input/output
Xin   = tf.placeholder(tf.float32, [None, n_steps, n_input] , name= ""Xin"")
Ytrue = tf.placeholder(tf.float32, [None, n_classes]        , name= ""Ytrue"")

# Graph weights
weights={}
weights['Pre' ] = tf.Variable(tf.random_normal([n_input     , n_hiddenPre ]), name=""Wght_Pre"") # Hidden layer weights
weights['LSTM'] = tf.Variable(tf.random_normal([n_hiddenPre , n_hidden    ]), name=""Wght_LSTM"") # Hidden layer weights
weights['Post'] = tf.Variable(tf.random_normal([n_hidden    , n_hiddenPost]), name=""Wght_Post"")
weights['out' ] = tf.Variable(tf.random_normal([n_hiddenPost, n_classes   ]), name=""Wght_Out"")

biases={}
biases['Pre' ] = tf.Variable(tf.random_normal([n_hiddenPre ]), name=""Bias_Pre"")
biases['LSTM'] = tf.Variable(tf.random_normal([n_hidden    ]), name=""Bias_LSTM"")
biases['Post'] = tf.Variable(tf.random_normal([n_hiddenPost]), name=""Bias_Post"")
biases['out' ] = tf.Variable(tf.random_normal([n_classes   ]), name=""Bias_Out"")

Xnew = prepareX(Xin)

with tf.name_scope('DensePre'):
    Xpre = tf.matmul(Xnew, weights['Pre']) + biases['Pre']
    keep_prob  = tf.placeholder(tf.float32)
    Xpre_drop = tf.nn.dropout(Xpre, keep_prob)

    # Linear activation
    Xnew = tf.matmul(Xpre_drop, weights['LSTM']) + biases['LSTM']

with tf.name_scope('RNN'): 
    # Split data because rnn cell needs a list of inputs for the RNN inner loop
    Xnew = tf.split(Xnew, n_steps, axis=0) 
    # new shape: n_steps * (batch_size, n_hidden)

    # Can use one or more LSTM layers
    lstm_cell1  = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)
    lstm_cell2  = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)
    lstm_cells  = tf.contrib.rnn.MultiRNNCell([lstm_cell1,lstm_cell2], state_is_tuple=True)
    #RNN_outputs, state = lstm_cells(Xnew, state=state)  # this code fails, so used line above without state
    
    # The following lines are for the bidirectional RNN, but effort was blocked due to an opaque error message
    #  lstm_cell_back   = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)
    #  lstm_cells_back  = tf.contrib.rnn.MultiRNNCell([lstm_cell_back] * 2, state_is_tuple=True)

    # Get LSTM cell output
    initial_state = lstm_cells.zero_state(batch_size, tf.float32) # Not used, but should be!
    RNN_outputs, state = tf.contrib.rnn.static_rnn(lstm_cells, inputs=Xnew, dtype=tf.float32, initial_state=initial_state)
    #RNN_outputs, state = tf.contrib.rnn.static_rnn(lstm_cells, inputs=Xnew, dtype=tf.float32)
    print(initial_state)
    print(state)

with tf.name_scope('DensePost'):
    Xnew = tf.matmul(RNN_outputs[-1], weights['Post']) + biases['Post']
    Xnew = tf.nn.dropout(Xnew, keep_prob)
     
with tf.name_scope('DenseOut'):
    # Linear activation
    Ypred        = tf.add(tf.matmul(Xnew, weights['out']),biases['out'], name=""Ypred_raw"")
    # Compute softmax result
    YpredSoftMax = tf.nn.softmax(Ypred   , dim =1, name=""prediction"")
    YpredIndex   = tf.argmax(YpredSoftMax, axis=1, name=""predIndex"" )
# Loss, optimizer and evaluation; Regularization term
l2 = lambda_loss_amount * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()) 
# L2 loss prevents this overkill neural network to overfit the data
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Ypred, labels=Ytrue)) + l2 # Softmax loss
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer

correct_pred = tf.equal(tf.argmax(Ypred,1), tf.argmax(Ytrue,1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
```"
9697,Tensorflow build from sources fails,"### System information
Tensorflow build from TF 1.1 sources cloned from GIT fails.  This was working a couple of days ago on the same machine.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04, kernel 4.4.0-75-generic
Hardware: Skylake server, nVidia P4

- **TensorFlow installed from (source or binary)**:
Source

- **TensorFlow version (use command below)**:
I believe is 1.1-rc2 ... cannot get it from TF itself because unable to build in the first place

- **Bazel version (if compiling from source)**:
Build label: 0.4.5
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Mar 16 12:19:38 2017 (1489666778)
Build timestamp: 1489666778
Build timestamp as int: 1489666778

- **CUDA/cuDNN version**:
CUDA 8.0
cuDNN 5.1.10

- **GPU model and memory**:
nVidia P4, 8GB

- **Exact command to reproduce**:
Followed exactly the steps to build from source from - https://www.tensorflow.org/versions/master/install/install_sources

Did the following:
./configure - used all defaults (see attached -  
[configure_nocuda.txt](https://github.com/tensorflow/tensorflow/files/979910/configure_nocuda.txt)
)

bazel build --verbose_failures --config=opt //tensorflow/tools/pip_package:build_pip_package
OR
bazel build --verbose_failures --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

Both fail with the same issue (see below for the non cuda build)

Output from the environment collection script is attached - 
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/979900/tf_env.txt)


### Describe the problem
Build fails.  I have tried it with --config=cuda also - same issue.

### Source code / logs
Here are the first couple of errors
### System information
Tensorflow build from TF 1.1 sources cloned from GIT fails.  This was working a couple of days ago on the same machine.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04, kernel 4.4.0-75-generic
Hardware: Skylake server, nVidia P4

- **TensorFlow installed from (source or binary)**:
Source

- **TensorFlow version (use command below)**:
I believe is 1.1-rc2 ... cannot get it from TF itself because unable to build in the first place

- **Bazel version (if compiling from source)**:
Build label: 0.4.5
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Mar 16 12:19:38 2017 (1489666778)
Build timestamp: 1489666778
Build timestamp as int: 1489666778

- **CUDA/cuDNN version**:
CUDA 8.0
cuDNN 5.1.10

- **GPU model and memory**:
nVidia P4, 8GB

- **Exact command to reproduce**:
Followed exactly the steps to build from source from - https://www.tensorflow.org/versions/master/install/install_sources

Did the following:
./configure - used all defaults (see attached -  
[configure_nocuda.txt](https://github.com/tensorflow/tensorflow/files/979910/configure_nocuda.txt)
)

bazel build --verbose_failures --config=opt //tensorflow/tools/pip_package:build_pip_package
OR
bazel build --verbose_failures --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

Both fail with the same issue (see below for the non cuda build)

Output from the environment collection script is attached - 
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/979900/tf_env.txt)


### Describe the problem
Build fails.  I have tried it with --config=cuda also - same issue.

### Source code / logs
Here are the first couple of errors in bold from a non-cuda build (compete build log is attached - 
[tf_build_nocuda.txt](https://github.com/tensorflow/tensorflow/files/979914/tf_build_nocuda.txt)):

ERROR: /home/rajka/tensorflow/tensorflow/core/kernels/BUILD:2093:1: C++ compilation of rule '//tensorflow/core/kernels:self_adjoint_eig_v2_op' failed: gcc failed: error executing command 
  (cd /home/rajka/.cache/bazel/_bazel_rajka/28f0a9835f793d5627ca9486394f31f2/execroot/tensorflow && \
  exec env - \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL=0 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-march=native' '-std=c++0x' '-march=native' '-march=native' -MD -MF bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/self_adjoint_eig_v2_op/tensorflow/core/kernels/self_adjoint_eig_v2_op.pic.d '-frandom-seed=bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/self_adjoint_eig_v2_op/tensorflow/core/kernels/self_adjoint_eig_v2_op.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -DSNAPPY -iquote . -iquote bazel-out/local-opt/genfiles -iquote external/jemalloc -iquote bazel-out/local-opt/genfiles/external/jemalloc -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -iquote external/protobuf -iquote bazel-out/local-opt/genfiles/external/protobuf -iquote external/eigen_archive -iquote bazel-out/local-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/local-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/local-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/local-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/local-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local-opt/genfiles/external/zlib_archive -iquote external/snappy -iquote bazel-out/local-opt/genfiles/external/snappy -isystem external/jemalloc/include -isystem bazel-out/local-opt/genfiles/external/jemalloc/include -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/protobuf/src -isystem bazel-out/local-opt/genfiles/external/protobuf/src -isystem external/eigen_archive -isystem bazel-out/local-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/local-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/local-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local-opt/genfiles/external/zlib_archive -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions -msse3 -pthread -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/core/kernels/self_adjoint_eig_v2_op.cc -o bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/self_adjoint_eig_v2_op/tensorflow/core/kernels/self_adjoint_eig_v2_op.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.

... snip snip ...

tensorflow/core/kernels/self_adjoint_eig_v2_op.cc:95:1:   required from here
**external/eigen_archive/Eigen/src/Core/util/BlasUtil.h:63:74: error: no type named 'ReturnType' in 'struct Eigen::ScalarBinaryOpTraits<__vector(8) double, std::complex<double>, Eigen::internal::scalar_product_op<__vector(8) double, std::complex<double> > >'
   typedef typename ScalarBinaryOpTraits<LhsScalar,RhsScalar>::ReturnType Scalar;**
                                                                          ^
In file included from external/eigen_archive/Eigen/Jacobi:27:0,
                 from external/eigen_archive/Eigen/Eigenvalues:16,
                 from ./third_party/eigen3/Eigen/Eigenvalues:1,
                 from tensorflow/core/kernels/self_adjoint_eig_v2_op.cc:19:
external/eigen_archive/Eigen/src/Jacobi/Jacobi.h: In instantiation of 'void Eigen::internal::apply_rotation_in_the_plane(Eigen::DenseBase<Derived>&, Eigen::DenseBase<Derived>&, const Eigen::JacobiRotation<OtherScalar>&) [with VectorX = Eigen::Block<Eigen::Map<Eigen::Matrix<std::complex<double>, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, true>; VectorY = Eigen::Block<Eigen::Map<Eigen::Matrix<std::complex<double>, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, true>; OtherScalar = double]':
external/eigen_archive/Eigen/src/Jacobi/Jacobi.h:297:40:   required from 'void Eigen::MatrixBase<Derived>::applyOnTheRight(Eigen::Index, Eigen::Index, const Eigen::JacobiRotation<OtherScalar>&) [with OtherScalar = double; Derived = Eigen::Map<Eigen::Matrix<std::complex<double>, -1, -1>, 0, Eigen::Stride<0, 0> >; Eigen::Index = long int]'
external/eigen_archive/Eigen/src/Eigenvalues/SelfAdjointEigenSolver.h:861:7:   required from 'void Eigen::internal::tridiagonal_qr_step(RealScalar*, RealScalar*, Index, Index, Scalar*, Index) [with int StorageOrder = 0; RealScalar = double; Scalar = std::complex<double>; Index = long int]'
external/eigen_archive/Eigen/src/Eigenvalues/SelfAdjointEigenSolver.h:520:87:   required from 'Eigen::ComputationInfo Eigen::internal::computeFromTridiagonal_impl(DiagType&, SubDiagType&, Eigen::Index, bool, MatrixType&) [with MatrixType = Eigen::Matrix<std::complex<double>, -1, -1>; DiagType = Eigen::Matrix<double, -1, 1>; SubDiagType = Eigen::Matrix<double, -1, 1>; Eigen::Index = long int]'
external/eigen_archive/Eigen/src/Eigenvalues/SelfAdjointEigenSolver.h:439:49:   required from 'Eigen::SelfAdjointEigenSolver<MatrixType>& Eigen::SelfAdjointEigenSolver<_MatrixType>::compute(const Eigen::EigenBase<OtherDerived>&, int) [with InputType = Eigen::Map<const Eigen::Matrix<std::complex<double>, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; _MatrixType = Eigen::Matrix<std::complex<double>, -1, -1, 1, -1, -1>]'
external/eigen_archive/Eigen/src/Eigenvalues/SelfAdjointEigenSolver.h:168:14:   required from 'Eigen::SelfAdjointEigenSolver<_MatrixType>::SelfAdjointEigenSolver(const Eigen::EigenBase<OtherDerived>&, int) [with InputType = Eigen::Map<const Eigen::Matrix<std::complex<double>, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; _MatrixType = Eigen::Matrix<std::complex<double>, -1, -1, 1, -1, -1>]'
tensorflow/core/kernels/self_adjoint_eig_v2_op.cc:66:73:   required from 'void tensorflow::SelfAdjointEigV2Op<Scalar>::ComputeMatrix(tensorflow::OpKernelContext*, const ConstMatrixMaps&, tensorflow::SelfAdjointEigV2Op<Scalar>::MatrixMaps*) [with Scalar = std::complex<double>; tensorflow::SelfAdjointEigV2Op<Scalar>::ConstMatrixMaps = tensorflow::gtl::InlinedVector<Eigen::Map<const Eigen::Matrix<std::complex<double>, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, 4>; tensorflow::SelfAdjointEigV2Op<Scalar>::MatrixMaps = tensorflow::gtl::InlinedVector<Eigen::Map<Eigen::Matrix<std::complex<double>, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, 4>]'
tensorflow/core/kernels/self_adjoint_eig_v2_op.cc:95:1:   required from here
external/eigen_archive/Eigen/src/Jacobi/Jacobi.h:359:24: **error: 'struct Eigen::internal::conj_helper<__vector(8) double, std::complex<double>, false, false>' has no member named 'pmul'
         pstore(px, padd(pm.pmul(pc,xi),pcj.pmul(ps,yi)));
                        ^
external/eigen_archive/Eigen/src/Jacobi/Jacobi.h:359:24: error: 'struct Eigen::internal::conj_helper<__vector(8) double, std::complex<double>, false, false>' has no member named 'pmul'**
 in bold from a non-cuda build (compete build log is attached - 
[tf_build_nocuda.txt](https://github.com/tensorflow/tensorflow/files/979914/tf_build_nocuda.txt)):

ERROR: /home/rajka/tensorflow/tensorflow/core/kernels/BUILD:2093:1: C++ compilation of rule '//tensorflow/core/kernels:self_adjoint_eig_v2_op' failed: gcc failed: error executing command 
  (cd /home/rajka/.cache/bazel/_bazel_rajka/28f0a9835f793d5627ca9486394f31f2/execroot/tensorflow && \
  exec env - \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL=0 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-march=native' '-std=c++0x' '-march=native' '-march=native' -MD -MF bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/self_adjoint_eig_v2_op/tensorflow/core/kernels/self_adjoint_eig_v2_op.pic.d '-frandom-seed=bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/self_adjoint_eig_v2_op/tensorflow/core/kernels/self_adjoint_eig_v2_op.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -DSNAPPY -iquote . -iquote bazel-out/local-opt/genfiles -iquote external/jemalloc -iquote bazel-out/local-opt/genfiles/external/jemalloc -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -iquote external/protobuf -iquote bazel-out/local-opt/genfiles/external/protobuf -iquote external/eigen_archive -iquote bazel-out/local-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/local-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/local-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/local-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/local-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local-opt/genfiles/external/zlib_archive -iquote external/snappy -iquote bazel-out/local-opt/genfiles/external/snappy -isystem external/jemalloc/include -isystem bazel-out/local-opt/genfiles/external/jemalloc/include -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/protobuf/src -isystem bazel-out/local-opt/genfiles/external/protobuf/src -isystem external/eigen_archive -isystem bazel-out/local-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/local-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/local-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local-opt/genfiles/external/zlib_archive -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions -msse3 -pthread -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/core/kernels/self_adjoint_eig_v2_op.cc -o bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/self_adjoint_eig_v2_op/tensorflow/core/kernels/self_adjoint_eig_v2_op.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.

... snip snip ...

tensorflow/core/kernels/self_adjoint_eig_v2_op.cc:95:1:   required from here
**external/eigen_archive/Eigen/src/Core/util/BlasUtil.h:63:74: error: no type named 'ReturnType' in 'struct Eigen::ScalarBinaryOpTraits<__vector(8) double, std::complex<double>, Eigen::internal::scalar_product_op<__vector(8) double, std::complex<double> > >'
   typedef typename ScalarBinaryOpTraits<LhsScalar,RhsScalar>::ReturnType Scalar;**
                                                                          ^
In file included from external/eigen_archive/Eigen/Jacobi:27:0,
                 from external/eigen_archive/Eigen/Eigenvalues:16,
                 from ./third_party/eigen3/Eigen/Eigenvalues:1,
                 from tensorflow/core/kernels/self_adjoint_eig_v2_op.cc:19:
external/eigen_archive/Eigen/src/Jacobi/Jacobi.h: In instantiation of 'void Eigen::internal::apply_rotation_in_the_plane(Eigen::DenseBase<Derived>&, Eigen::DenseBase<Derived>&, const Eigen::JacobiRotation<OtherScalar>&) [with VectorX = Eigen::Block<Eigen::Map<Eigen::Matrix<std::complex<double>, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, true>; VectorY = Eigen::Block<Eigen::Map<Eigen::Matrix<std::complex<double>, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, true>; OtherScalar = double]':
external/eigen_archive/Eigen/src/Jacobi/Jacobi.h:297:40:   required from 'void Eigen::MatrixBase<Derived>::applyOnTheRight(Eigen::Index, Eigen::Index, const Eigen::JacobiRotation<OtherScalar>&) [with OtherScalar = double; Derived = Eigen::Map<Eigen::Matrix<std::complex<double>, -1, -1>, 0, Eigen::Stride<0, 0> >; Eigen::Index = long int]'
external/eigen_archive/Eigen/src/Eigenvalues/SelfAdjointEigenSolver.h:861:7:   required from 'void Eigen::internal::tridiagonal_qr_step(RealScalar*, RealScalar*, Index, Index, Scalar*, Index) [with int StorageOrder = 0; RealScalar = double; Scalar = std::complex<double>; Index = long int]'
external/eigen_archive/Eigen/src/Eigenvalues/SelfAdjointEigenSolver.h:520:87:   required from 'Eigen::ComputationInfo Eigen::internal::computeFromTridiagonal_impl(DiagType&, SubDiagType&, Eigen::Index, bool, MatrixType&) [with MatrixType = Eigen::Matrix<std::complex<double>, -1, -1>; DiagType = Eigen::Matrix<double, -1, 1>; SubDiagType = Eigen::Matrix<double, -1, 1>; Eigen::Index = long int]'
external/eigen_archive/Eigen/src/Eigenvalues/SelfAdjointEigenSolver.h:439:49:   required from 'Eigen::SelfAdjointEigenSolver<MatrixType>& Eigen::SelfAdjointEigenSolver<_MatrixType>::compute(const Eigen::EigenBase<OtherDerived>&, int) [with InputType = Eigen::Map<const Eigen::Matrix<std::complex<double>, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; _MatrixType = Eigen::Matrix<std::complex<double>, -1, -1, 1, -1, -1>]'
external/eigen_archive/Eigen/src/Eigenvalues/SelfAdjointEigenSolver.h:168:14:   required from 'Eigen::SelfAdjointEigenSolver<_MatrixType>::SelfAdjointEigenSolver(const Eigen::EigenBase<OtherDerived>&, int) [with InputType = Eigen::Map<const Eigen::Matrix<std::complex<double>, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; _MatrixType = Eigen::Matrix<std::complex<double>, -1, -1, 1, -1, -1>]'
tensorflow/core/kernels/self_adjoint_eig_v2_op.cc:66:73:   required from 'void tensorflow::SelfAdjointEigV2Op<Scalar>::ComputeMatrix(tensorflow::OpKernelContext*, const ConstMatrixMaps&, tensorflow::SelfAdjointEigV2Op<Scalar>::MatrixMaps*) [with Scalar = std::complex<double>; tensorflow::SelfAdjointEigV2Op<Scalar>::ConstMatrixMaps = tensorflow::gtl::InlinedVector<Eigen::Map<const Eigen::Matrix<std::complex<double>, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, 4>; tensorflow::SelfAdjointEigV2Op<Scalar>::MatrixMaps = tensorflow::gtl::InlinedVector<Eigen::Map<Eigen::Matrix<std::complex<double>, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, 4>]'
tensorflow/core/kernels/self_adjoint_eig_v2_op.cc:95:1:   required from here
external/eigen_archive/Eigen/src/Jacobi/Jacobi.h:359:24: **error: 'struct Eigen::internal::conj_helper<__vector(8) double, std::complex<double>, false, false>' has no member named 'pmul'
         pstore(px, padd(pm.pmul(pc,xi),pcj.pmul(ps,yi)));
                        ^
external/eigen_archive/Eigen/src/Jacobi/Jacobi.h:359:24: error: 'struct Eigen::internal::conj_helper<__vector(8) double, std::complex<double>, false, false>' has no member named 'pmul'**

"
9696,Epoch is not working properly for tensorflow keras -  KerasRegressor,"I'm just getting start with Keras using tensorflow, In the code code I have specified **nb_epoch=100** but it's not running for 100 epoch, instead it's executing only for 10 epoch. I wonder is there any bug in my code. Also attaching the output in the bottom

**Code:**

```python

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import sklearn.metrics as metrics
from utils import encode_numeric_zscore, to_xy

dataset = pd.read_csv('data/boston-housing.csv', dtype=np.float32)

for col in dataset.columns:
    if col != 'medv':
        encode_numeric_zscore(dataset, col)

features, target = to_xy(dataset, 'medv')

X_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size=0.25, random_state=42)


def chart_regression(pred, y):
    t = pd.DataFrame({'pred': pred, 'y': Y_test.flatten()})
    t.sort_values(by=['y'], inplace=True)
    a = plt.plot(t['y'].tolist(), label='expected')
    b = plt.plot(t['pred'].tolist(), label='prediction')
    plt.ylabel('output')
    plt.legend()
    plt.show()


from tensorflow.contrib.keras.api.keras.models import Sequential
from tensorflow.contrib.keras.api.keras.layers import Dense, Dropout


def build_nn():
    model = Sequential()
    model.add(Dense(75, input_shape=(13,), activation=""relu"", kernel_initializer=""glorot_uniform""))
    model.add(Dense(55, activation=""relu""))
    model.add(Dropout(0.005))
    model.add(Dense(35, activation=""relu""))
    model.add(Dropout(0.005))
    model.add(Dense(11, activation=""relu""))
    model.add(Dropout(0.005))
    model.add(Dense(1, activation='relu', kernel_initializer=""normal""))
    model.compile(loss='mean_squared_error', optimizer='adam')
    return model


seed = 7
np.random.seed(seed)

from tensorflow.contrib.keras.api.keras.wrappers.scikit_learn import KerasRegressor

regressor = KerasRegressor(build_fn=build_nn, nb_epoch=100, batch_size=1, verbose=1)

regressor.fit(X_train, Y_train)

prediction = regressor.predict(X_test, batch_size=1)

score = metrics.mean_squared_error(Y_test, prediction)

print('\n')

print(""Final score (MSE): {}"".format(score))

print('\n')

score = np.sqrt(metrics.mean_squared_error(prediction, Y_test))
print(""Final score (RMSE): {}"".format(score))

chart_regression(prediction, Y_test)
```

**Output:**

```
Epoch 1/10
2017-05-05 15:30:37.711608: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2017-05-05 15:30:37.711981: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-05 15:30:37.712355: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-05 15:30:37.712721: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-05 15:30:37.713094: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-05 15:30:37.713463: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-05-05 15:30:37.713839: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-05 15:30:37.714213: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
  1/379 [..............................] - ETA: 107s - loss: 492.8400
 60/379 [===>..........................] - ETA: 1s - loss: 669.9998  
109/379 [=======>......................] - ETA: 0s - loss: 625.8113
172/379 [============>.................] - ETA: 0s - loss: 461.0169
233/379 [=================>............] - ETA: 0s - loss: 353.9863
286/379 [=====================>........] - ETA: 0s - loss: 294.3637
336/379 [=========================>....] - ETA: 0s - loss: 254.4181
379/379 [==============================] - 0s - loss: 230.0349     
Epoch 2/10
  1/379 [..............................] - ETA: 0s - loss: 8.0337
 55/379 [===>..........................] - ETA: 0s - loss: 26.7809
104/379 [=======>......................] - ETA: 0s - loss: 22.0314
171/379 [============>.................] - ETA: 0s - loss: 25.4361
219/379 [================>.............] - ETA: 0s - loss: 28.0947
285/379 [=====================>........] - ETA: 0s - loss: 26.3540
338/379 [=========================>....] - ETA: 0s - loss: 26.1902
379/379 [==============================] - 0s - loss: 24.4095     
Epoch 3/10
  1/379 [..............................] - ETA: 0s - loss: 9.8365
 42/379 [==>...........................] - ETA: 0s - loss: 20.2841
108/379 [=======>......................] - ETA: 0s - loss: 16.0308
161/379 [===========>..................] - ETA: 0s - loss: 18.4792
225/379 [================>.............] - ETA: 0s - loss: 18.9558
288/379 [=====================>........] - ETA: 0s - loss: 16.6939
379/379 [==============================] - 0s - loss: 18.3086     
Epoch 4/10
  1/379 [..............................] - ETA: 0s - loss: 13.7463
 59/379 [===>..........................] - ETA: 0s - loss: 12.0900
115/379 [========>.....................] - ETA: 0s - loss: 18.4202
181/379 [=============>................] - ETA: 0s - loss: 16.6087
244/379 [==================>...........] - ETA: 0s - loss: 15.0061
302/379 [======================>.......] - ETA: 0s - loss: 13.2268
379/379 [==============================] - 0s - loss: 16.0613     
Epoch 5/10
  1/379 [..............................] - ETA: 0s - loss: 25.8912
 65/379 [====>.........................] - ETA: 0s - loss: 28.8039
123/379 [========>.....................] - ETA: 0s - loss: 21.3528
199/379 [==============>...............] - ETA: 0s - loss: 17.8586
256/379 [===================>..........] - ETA: 0s - loss: 19.4171
327/379 [========================>.....] - ETA: 0s - loss: 17.9864
379/379 [==============================] - 0s - loss: 18.2714     
Epoch 6/10
  1/379 [..............................] - ETA: 0s - loss: 2.0872
 61/379 [===>..........................] - ETA: 0s - loss: 18.4408
115/379 [========>.....................] - ETA: 0s - loss: 16.9127
184/379 [=============>................] - ETA: 0s - loss: 16.9419
247/379 [==================>...........] - ETA: 0s - loss: 16.2630
314/379 [=======================>......] - ETA: 0s - loss: 14.9947
379/379 [==============================] - 0s - loss: 13.5761     
Epoch 7/10
  1/379 [..............................] - ETA: 0s - loss: 0.3704
 59/379 [===>..........................] - ETA: 0s - loss: 15.2191
118/379 [========>.....................] - ETA: 0s - loss: 11.8381
188/379 [=============>................] - ETA: 0s - loss: 10.9623
247/379 [==================>...........] - ETA: 0s - loss: 10.0409
313/379 [=======================>......] - ETA: 0s - loss: 12.4478
379/379 [==============================] - 0s - loss: 13.8573     
Epoch 8/10
  1/379 [..............................] - ETA: 0s - loss: 1.2173
 44/379 [==>...........................] - ETA: 0s - loss: 12.5116
 96/379 [======>.......................] - ETA: 0s - loss: 10.2578
170/379 [============>.................] - ETA: 0s - loss: 11.9057
227/379 [================>.............] - ETA: 0s - loss: 12.1961
287/379 [=====================>........] - ETA: 0s - loss: 14.4951
341/379 [=========================>....] - ETA: 0s - loss: 13.7772
379/379 [==============================] - 0s - loss: 13.2340     
Epoch 9/10
  1/379 [..............................] - ETA: 0s - loss: 0.1982
 53/379 [===>..........................] - ETA: 0s - loss: 9.8250
124/379 [========>.....................] - ETA: 0s - loss: 9.8112
175/379 [============>.................] - ETA: 0s - loss: 10.9075
238/379 [=================>............] - ETA: 0s - loss: 11.3589
303/379 [======================>.......] - ETA: 0s - loss: 13.8970
379/379 [==============================] - 0s - loss: 14.5469     
Epoch 10/10
  1/379 [..............................] - ETA: 0s - loss: 3.7164
 49/379 [==>...........................] - ETA: 0s - loss: 14.6596
116/379 [========>.....................] - ETA: 0s - loss: 15.5960
172/379 [============>.................] - ETA: 0s - loss: 13.6525
223/379 [================>.............] - ETA: 0s - loss: 12.9588
275/379 [====================>.........] - ETA: 0s - loss: 12.4905
347/379 [==========================>...] - ETA: 0s - loss: 13.5629
379/379 [==============================] - 0s - loss: 13.1696     
  1/127 [..............................] - ETA: 1s

Final score (MSE): 11.964310646057129


Final score (RMSE): 3.458946466445923

```"
9695,convert a tensor to matrix,"Hi,
 output of first layer cnn , ""if  input was a image with size[100,150] then the output layer is a tensor (1,100,150,32)""
for slicing The first image from all 32 output images, i used this code:
conv1 = tf.layers.conv2d( image, filters=32, kernel_size=[5, 5], padding=""same"", activation=tf.nn.relu)
first_image=slice(conv1,[0,0,0,0],[1, 100, 152, 1])

>>conv1 is  tensor (1, 100, 150 ,32)
>>first_image is tensor (1, 100 ,150 , 1)

has the Tensorflow  a command for convert a tensor to 2-D matrix directly ?, or how can convert a tensor to matrix in Tensorflow?


"
9694,Possible bug in TensorFlowInferenceInterface,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.1.0rc2
- **Bazel version (if compiling from source)**: 
- **CUDA/cuDNN version**: Not using
- **GPU model and memory**: Not using
- **Exact command to reproduce**: Need to run tensorflow code in Android

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I was testing a trained my style transfer model on android. It uses depthwise separable convolution to decrease the size of neural network. When working on square images, it gives good results. But when I work with it on rectangular images, it gives weird results. Please see the result images attached. I highly doubt that this is due to my code because it is giving good results on all kinds of images on my laptop.
The style image was the famous painting 'la muse'
Content image:
![content](https://cloud.githubusercontent.com/assets/6660192/25755856/3ad8a43c-31e2-11e7-91f7-9758177f2002.jpg)
Result image for rectangular input:
![rectangular_image_result](https://cloud.githubusercontent.com/assets/6660192/25755900/600d2638-31e2-11e7-9593-890a42360f61.png)
Result image for square input:
![square_image_result](https://cloud.githubusercontent.com/assets/6660192/25755899/60060d8a-31e2-11e7-89c2-ce554c831e7c.png)
 
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
This is the code I used for running the network on android using TensorflowInferenceInterface
  private void stylizeImage(final Bitmap bitmap) {
    bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());

    for (int i = 0; i < intValues.length; ++i) {
      final int val = intValues[i];
      floatValues[i * 3] = ((val >> 16) & 0xFF) / 255.0f;
      floatValues[i * 3 + 1] = ((val >> 8) & 0xFF) / 255.0f;
      floatValues[i * 3 + 2] = (val & 0xFF) / 255.0f;
    }

    inferenceInterface.feed(
        INPUT_NODE, floatValues, 1, bitmap.getWidth(), bitmap.getHeight(), 3);

    inferenceInterface.run(new String[] {OUTPUT_NODE}, false);
    inferenceInterface.fetch(OUTPUT_NODE, outValues);

    for (int i = 0; i < intValues.length; ++i) {
      intValues[i] =
          0xFF000000
              | ((outValues[i * 3]) << 16)
              | ((outValues[i * 3 + 1]) << 8)
              | (outValues[i * 3 + 2]);
    }

    bitmap.setPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());
  }
"
9692,request for updating readme in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx,"OS: Ubuntu 16.04 64bits
Android Version: 7.1 (Nougat)
NDK Version: android-ndk-r10b
HEXAGON SDK: 3.1
nnlib source: https://source.codeaurora.org/quic/hexagon_nn/nnlib

Following steps to get tensorflow up on hexagon from `https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx`
1) Please update `git clone https://source.codeaurora.org/quic/hexagon_nn/nnlib` with 
`git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git` in the readme

2) tensorflow/contrib/makefile/downloads/protobuf/autogen.sh:

below autogen.sh is modified gmock ->(with) googlemock
as well gtest ->(with) googletest.

as `m4_include(../googletest/m4/acx_pthread.m4)` in `tensorflow/contrib/makefile/downloads/protobuf/googlemock/configure.ac` is referring to googletest.


```
if test ! -e googlemock; then
   echo ""Google Mock not present.  Fetching gmock-1.7.0 from the web...""
   curl $curlopts -L -O https://github.com/google/googlemock/archive/release-1.7.0.zip
   unzip -q release-1.7.0.zip
   rm release-1.7.0.zip
   mv googlemock-release-1.7.0 googlemock
 
   curl $curlopts -L -O https://github.com/google/googletest/archive/release-1.7.0.zip
   unzip -q release-1.7.0.zip
   rm release-1.7.0.zip
   mv googletest-release-1.7.0 googlemock/googletest
 fi
```

please let me know gtest/gmock is getting reffered somewhere as it can be modified to googletest/googlemock in protobuf/autogen.sh


thanks."
9691,hexagon_graph_execution runs for only one test and gets stuck there. Cannot execute until restart of target.,"OS: Ubuntu 16.04 64bits
Android Version: 7.1 (Nougat)
NDK Version: android-ndk-10b
HEXAGON SDK: 3.1
nnlib source: https://source.codeaurora.org/quic/hexagon_nn/nnlib


```
$ adb shell 'LD_LIBRARY_PATH=/data/hex_tf:$LD_LIBRARY_PATH' /data/hex_tf/hexagon_graph_execution
 WARNING: linker: Warning: unable to normalize """"
 Running main() from test_main.cc
 [==========] Running 1 test from 1 test case.
 [----------] Global test environment set-up.
 [----------] 1 test from GraphTransferer
 [ RUN      ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime
 native : hexagon_graph_execution_test.cc:446 Fuse and run inception v3 on hexagon with tf runtime
 native : hexagon_graph_execution_test.cc:72 Hexagon controller version is 90
 native : hexagon_graph_execution_test.cc:122 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes
 native : hexagon_graph_execution_test.cc:128 header size = 54
 native : hexagon_graph_execution_test.cc:130 image size = 40
 native : hexagon_graph_execution_test.cc:132 width = 299
 native : hexagon_graph_execution_test.cc:134 height = -299
 native : hexagon_graph_execution_test.cc:458 loading image finished.
 native : hexagon_graph_execution_test.cc:465 Build fused graph
 can't determine number of CPU cores: assuming 4
 can't determine number of CPU cores: assuming 4
 native : op_def_util.cc:332 Op PlaceholderV2 is deprecated. It will cease to work in GraphDef version 23. Placeholder now behaves the same as PlaceholderV2..
 native : hexagon_graph_execution_test.cc:122 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes
 native : hexagon_graph_execution_test.cc:128 header size = 54
 native : hexagon_graph_execution_test.cc:130 image size = 40
 native : hexagon_graph_execution_test.cc:132 width = 299
 native : hexagon_graph_execution_test.cc:134 height = -299
 native : hexagon_graph_execution_test.cc:262 loading image finished.
 native : hexagon_graph_execution_test.cc:170 loading image finished.
 native : hexagon_graph_execution_test.cc:174 Copy data to tensor.
 native : hexagon_graph_execution_test.cc:284 Run graph
 Init hexagon with max attributes (Controller version = 92)
 native : hexagon_control_wrapper.cc:252 Setup graph completed
 Execute graph!
 Execution succeeded!
 native : hexagon_graph_execution_test.cc:291 Output byte size = 4032
 native : hexagon_graph_execution_test.cc:292 Output shape = [1,1008]
 native : graph_transfer_utils.cc:46 === Dump ranking ===
 native : graph_transfer_utils.cc:49 0: 59, Yorkshire terrier, 0.829043
 native : graph_transfer_utils.cc:49 1: 4, Australian terrier, 0.048217
 native : graph_transfer_utils.cc:49 2: 89, toy terrier, 0.00723796
 native : graph_transfer_utils.cc:49 3: 131, silky terrier, 0.00347867
 native : graph_transfer_utils.cc:49 4: 43, papillon, 0.00160137
 native : graph_transfer_utils.cc:49 5: 145, Norwich terrier, 0.00134778
 native : graph_transfer_utils.cc:49 6: 160, wire-haired fox terrier, 0.000875875
native : graph_transfer_utils.cc:49 7: 926, pickelhaube, 0.000647765
 native : graph_transfer_utils.cc:49 8: 173, Chihuahua, 0.00062044
 native : graph_transfer_utils.cc:49 9: 127, affenpinscher, 0.000569199
 Finalize hexagon
 [       OK ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime (5027 ms)
 [----------] 1 test from GraphTransferer (5027 ms total)
 
 [----------] Global test environment tear-down
 [==========] 1 test from 1 test case ran. (5027 ms total)
 [  PASSED  ] 1 test.
 
   YOU HAVE 4 DISABLED TESTS
 
```
 
---

After this log, hvx session doesn't respond, suspicion is dsp/rpc session is not getting closed properly.
kill the process and again try to run the process, it gets stuck at :


```
WARNING: linker: Warning: unable to normalize """"
Running main() from test_main.cc
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from GraphTransferer
[ RUN      ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime
native : hexagon_graph_execution_test.cc:446 Fuse and run inception v3 on hexagon with tf runtime
```

please try to reproduce.
"
9690,Make EIGEN_MAX_ALIGN_BYTES available from Python,"Removing unnecessary memcpys when feeding a Numpy array with a feed_dict is a [recurring feature request](https://github.com/tensorflow/tensorflow/issues/7951) that has large performance implications.

From what I can tell, @alextp [implemented changes](https://github.com/tensorflow/tensorflow/commit/0ffa40ee3d5fae4ff14b75c2525edcaa2f01ece7) that avoid the memcpy if the input array is ``EIGEN_MAX_ALIGN_BYTES`` aligned. For a user to definitely avoid memcpy's on feeding, they would need to make sure their arrays are aligned to ``EIGEN_MAX_ALIGN_BYTES``. Currently, there is no way to access ``EIGEN_MAX_ALIGN_BYTES`` from Python, so the user can't be sure what alignment is required. 

It would be nice if there was a C API call (and a corresponding Python one) to make ``EIGEN_MAX_ALIGN_BYTES`` available. This isn't a critical feature as the user can currently be very pessimistic on alignment requirements and (probably safely) 64-byte align their inputs."
9689,ImportError: No module named tensorflow,"ImportError                               Traceback (most recent call last)
<ipython-input-3-ac22e13aedab> in <module>()
      1 import numpy
----> 2 from keras.datasets import mnist
      3 from keras.models import Sequential
      4 from keras.layers import Dense
      5 from keras.layers import Dropout

C:\Users\Dilip\Anaconda2\lib\site-packages\keras\__init__.py in <module>()
      1 from __future__ import absolute_import
      2 
----> 3 from . import activations
      4 from . import applications
      5 from . import backend

C:\Users\Dilip\Anaconda2\lib\site-packages\keras\activations.py in <module>()
      2 import six
      3 import warnings
----> 4 from . import backend as K
      5 from .utils.generic_utils import deserialize_keras_object
      6 from .engine import Layer

C:\Users\Dilip\Anaconda2\lib\site-packages\keras\backend\__init__.py in <module>()
     71 elif _BACKEND == 'tensorflow':
     72     sys.stderr.write('Using TensorFlow backend.\n')
---> 73     from .tensorflow_backend import *
     74 else:
     75     raise ValueError('Unknown backend: ' + str(_BACKEND))

C:\Users\Dilip\Anaconda2\lib\site-packages\keras\backend\tensorflow_backend.py in <module>()
----> 1 import tensorflow as tf
      2 from tensorflow.python.training import moving_averages
      3 from tensorflow.python.ops import tensor_array_ops
      4 from tensorflow.python.ops import control_flow_ops
      5 from tensorflow.python.ops import functional_ops

ImportError: No module named tensorflow
"
9688,"Tensorboard ""Color by"" drop-down not showing up on multi-column .tsv","------------------------

### System information
- **using tensorboard to visualize some embeddings**:
- **OSX 10.12.4**:
- **installed from binary via pip**:
- **1.1.0**:
- **not using bazel for this install**:
- **no CUDA**:
- **no GPU**:

### Describe the problem
Tensorboard does not display the ```Color By``` dropdown menu on multi-columnar data. ```Label by``` and ```search by``` displaying columns normally.

![tensorboard dropdown issue](https://github.com/markostam/sandbox/blob/master/photos/Screenshot%202017-05-05%2011.39.26.png?raw=true)

### Source code / logs
Sample of ```metadata.tsv``` file:
```
Name	Genre
(Sandy) Alex G	Alternative/Indie Rock	
*NSYNC	Pop/Rock	
Acollective	Pop/Rock	
Ahmet Özhan	International	
Ahu	Club/Dance	
Alex Ferreira	Pop/Rock	
Alex Winston	Pop/Rock	
Ali Azimi	Pop/Rock	
Alphamama	Pop/Rock	
Amaryllis	International	
...
Yomo Toro	Latin
Youssou N'Dour	International
Zafra Negra	Latin
Zany	Electronic	
Zeki Müren	International
iSHi	Electronic	
```
Code to generate embeddings and metadata:
```python
def list_to_tsv(filenames, metadata_dir):
    with open(os.path.join(metadata_dir,'metadata.tsv'), 'w') as tsvfile:
        writer = csv.writer(tsvfile, delimiter = ""\t"")
        for record in filenames:
            writer.writerow(record)

def save_down_tensors(tensor_dir, name_and_embedding):
    embeddings = [i[2] for i in name_and_embedding] 
    names = [[i[0],i[1]] for i in name_and_embedding]
    names.insert(0,['Name','Genre'])
    with tf.Session() as sess:
        embeddings_tf = tf.Variable(np.array(embeddings), name = ""embeddings"")
        tf.global_variables_initializer().run()
        # save the tensor down
        saver = tf.train.Saver(tf.global_variables())
        saver.save(sess, tensor_dir, 0)
        # associate metadata with the embedding
        summary_writer = tf.summary.FileWriter(tensor_dir)
        config = projector.ProjectorConfig()
        embedding = config.embeddings.add()
        embedding.tensor_name = embeddings_tf.name
        #save filenames to tsv
        list_to_tsv(names, metadata_dir)
        embedding.metadata_path = os.path.join(metadata_dir, ""metadata.tsv"")
        # save a configuration file that TensorBoard will read during startup.
        projector.visualize_embeddings(summary_writer, config)
```"
9685,[Java][Suggestion] Add Enum with all Operations,"Currently it is hard for beginners to start with the Java API of TF. Mostly because the function names staed in the python tutorial do not apply to the Operation names used in Java. It would be nice if we had an Enum Containing all standart possible Operations.

`grapth.opBuilder(type, name)`

I am currently struggling to find out what the correct name of ""Multiply"" is.
I have digged my way thorgh the code and most of the operations are registered trought `REGISTER_OP` and I could trace down the java methods to ` TF_OperationDescription(TF_Graph* g, const char* op_type, const char* node_name)` in c_api_internal.h. But I absolutly dont know were the operations are storred. 

Also nice would be a table ""python function name"" to ""internal operation name"""
9683,"tf.avg_pool fails for data format ""NCHW"" on ppc64le","Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04 ppc64le
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
'v1.0.1-0-ge895d5c-dirty'
- **Bazel version (if compiling from source)**:
Build label: 0.4.4-2017-04-13 (@80a07b5)
- **CUDA/cuDNN version**:
No GPU
- **GPU model and memory**:
No GPU
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I'm trying to run tf.nn.avg_pool (https://www.tensorflow.org/api_docs/python/tf/nn/avg_pool) by passing in NCHW as the data format on a CPU (no GPU) but see the error: 

"" Executor failed to create kernel. Invalid argument: Default AvgPoolingOp only supports NHWC.""

The API documentation mentions both NHWC and NCHW are supported. Please see logs below for complete details

**Query: Does avg_pool support NCHW for CPUs?**
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Note: Below, NHWCToNCHW defined standard func to convert from one form to another (but not shown in this snippet)

>>> x
array([[[[  1.,   2.,   3.],
         [  4.,   5.,   6.],
         [  7.,   8.,   9.]],

        [[ 10.,  11.,  12.],
         [ 13.,  14.,  15.],
         [ 16.,  17.,  18.]]]], dtype=float32)
>>> t = tf.placeholder(tf.float32)
>>> t = NHWCToNCHW(t)
>>> t = tf.nn.avg_pool(t,ksize=[1,2,2,1],strides=[1,2,2,1],padding=""SAME"",data_format=""NCHW"")
>>> actual = sess.run(t, {inputs: x})
E tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: Default AvgPoolingOp only supports NHWC.
         [[Node: AvgPool_1 = AvgPool[T=DT_FLOAT, data_format=""NCHW"", ksize=[1, 2, 2, 1], padding=""SAME"", strides=[1, 2, 2, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""](AvgPool)]]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Default AvgPoolingOp only supports NHWC.
         [[Node: AvgPool_1 = AvgPool[T=DT_FLOAT, data_format=""NCHW"", ksize=[1, 2, 2, 1], padding=""SAME"", strides=[1, 2, 2, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""](AvgPool)]]

Caused by op u'AvgPool_1', defined at:
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 1765, in avg_pool
    name=name)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 50, in _avg_pool
    data_format=data_format, name=name)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2327, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1226, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Default AvgPoolingOp only supports NHWC.
         [[Node: AvgPool_1 = AvgPool[T=DT_FLOAT, data_format=""NCHW"", ksize=[1, 2, 2, 1], padding=""SAME"", strides=[1, 2, 2, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""](AvgPool)]]

"
9682,Unable to run GMM estimator in TF 1.1 but I can run in TF 1.0,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 64 bit
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.1 (tensorflow-1.1.0-cp35-cp35m-win_amd64)
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: Run the below code in Python IDLE
- Used PIP installation for CPU only

### Describe the problem
I have used following code in TF 1.0 and TF 1.1. In TF 1.0, it executes and crashes in TF 1.1. In TF 1.0, I dont know any way to get weights(or prior probabilities) for each component in a given GMM. Changes related to weights are added in TF 1.1. Hence I need to run TF 1.1
```
from tensorflow.contrib.factorization.python.ops import gmm as gmm_lib 
import random 
import numpy as np
import tensorflow as tf
mu, sigma = 0, 0.1
x_1d = 10*np.random.randn(100000, 1).astype('f') + 50
gmm = gmm_lib.GMM(1,random_seed=0) 
gmm.fit(x_1d)
```


### Source code / logs
I got the below crash in TF 1.1

WARNING:tensorflow:Using temporary folder as model directory: C:\Users\HVINAY~1.COR\AppData\Local\Temp\tmptpejli7d

Warning (from warnings module):
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\util\deprecation.py"", line 248
    equality = a == b
FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.
WARNING:tensorflow:From C:\Users\hvinay.CORP\Documents\TF\GMMEst.py:11: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.
Instructions for updating:
Estimator is decoupled from Scikit Learn interface by moving into
separate class SKCompat. Arguments x, y and batch_size are only
available in the SKCompat class, Estimator will only accept input_fn.
Example conversion:
  est = Estimator(...) -> est = SKCompat(Estimator(...))
Traceback (most recent call last):
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\client\session.py"", line 1039, in _do_call
    return fn(*args)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\client\session.py"", line 1021, in _run_fn
    status, run_metadata)
  File ""<Python Installed Directory>\lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input' with dtype float
	 [[Node: input = Placeholder[dtype=DT_FLOAT, shape=[], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\hvinay.CORP\Documents\TF\GMMEst.py"", line 11, in <module>
    gmm.fit(x_1d)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\util\deprecation.py"", line 281, in new_func
    return func(*args, **kwargs)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 414, in fit
    SKCompat(self).fit(x, y, batch_size, steps, max_steps, monitors)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 1317, in fit
    monitors=all_monitors)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\util\deprecation.py"", line 281, in new_func
    return func(*args, **kwargs)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 430, in fit
    loss = self._train_model(input_fn=input_fn, hooks=hooks)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 974, in _train_model
    config=config_pb2.ConfigProto(allow_soft_placement=True)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 333, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 627, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 456, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 800, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 805, in _create_session
    return self._sess_creator.create_session()
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 517, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 393, in create_session
    init_fn=self._scaffold.init_fn)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\training\session_manager.py"", line 262, in prepare_session
    sess.run(init_op, feed_dict=init_feed_dict)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\client\session.py"", line 778, in run
    run_metadata_ptr)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\client\session.py"", line 982, in _run
    feed_dict_string, options, run_metadata)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\client\session.py"", line 1032, in _do_run
    target_list, options, run_metadata)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\client\session.py"", line 1052, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input' with dtype float
	 [[Node: input = Placeholder[dtype=DT_FLOAT, shape=[], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op 'input', defined at:
  File ""<string>"", line 1, in <module>
  File ""<Python Installed Directory>\lib\idlelib\run.py"", line 124, in main
    ret = method(*args, **kwargs)
  File ""<Python Installed Directory>\lib\idlelib\run.py"", line 351, in runcode
    exec(code, self.locals)
  File ""C:\Users\hvinay.CORP\Documents\TF\GMMEst.py"", line 11, in <module>
    gmm.fit(x_1d)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\util\deprecation.py"", line 281, in new_func
    return func(*args, **kwargs)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 414, in fit
    SKCompat(self).fit(x, y, batch_size, steps, max_steps, monitors)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 1317, in fit
    monitors=all_monitors)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\util\deprecation.py"", line 281, in new_func
    return func(*args, **kwargs)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 430, in fit
    loss = self._train_model(input_fn=input_fn, hooks=hooks)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 925, in _train_model
    features, labels = input_fn()
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\contrib\learn\python\learn\learn_io\data_feeder.py"", line 430, in input_builder
    self._input_dtype, 'input')
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\contrib\learn\python\learn\learn_io\data_feeder.py"", line 426, in get_placeholder
    dtypes.as_dtype(dtype), [None] + shape[1:], name=name_prepend)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 1507, in placeholder
    name=name)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 1997, in _placeholder
    name=name)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 768, in apply_op
    op_def=op_def)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\framework\ops.py"", line 2336, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""<Python Installed Directory>\lib\site-packages\tensorflow\python\framework\ops.py"", line 1228, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'input' with dtype float
	 [[Node: input = Placeholder[dtype=DT_FLOAT, shape=[], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]"
9681,Tensorboard Graph vizualisation crashes with Chrome/Safari,"### Server system information
```
== dockerfile image =============================================
FROM tensorflow/tensorflow:latest-gpu
RUN pip install tensorflow --upgrade
RUN pip install tensorflow-gpu --upgrade

== cat /etc/issue ===============================================
Linux 479a65b403e2 4.4.0-59-generic #80-Ubuntu SMP Fri Jan 6 17:47:47 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""14.04.5 LTS, Trusty Tahr""
VERSION_ID=""14.04""

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4
Copyright (C) 2013 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

== uname -a =====================================================
Linux 479a65b403e2 4.4.0-59-generic #80-Ubuntu SMP Fri Jan 6 17:47:47 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.12.1)
protobuf (3.3.0)
tensorflow (1.1.0)
tensorflow-gpu (1.1.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.1.0
tf.GIT_VERSION = v1.1.0-rc0-61-g1ec6ed5
tf.COMPILER_VERSION = v1.1.0-rc0-61-g1ec6ed5
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri May  5 08:50:12 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN X (Pascal)    Off  | 0000:01:00.0     Off |                  N/A |
| 24%   40C    P2    56W / 250W |   1793MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  TITAN X (Pascal)    Off  | 0000:02:00.0     Off |                  N/A |
| 25%   45C    P2    55W / 250W |    312MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  TITAN X (Pascal)    Off  | 0000:03:00.0     Off |                  N/A |
| 24%   43C    P2    53W / 250W |    312MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  TITAN X (Pascal)    Off  | 0000:04:00.0     Off |                  N/A |
| 25%   44C    P2    53W / 250W |    312MiB / 12186MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.44
```

### Local machine information
```
MacOS Sierra 10.12.4 (16E195)
Google Chrome 58.0.3029.96 (64-bit)
Safari 10.1 (12603.1.30.0.34)
```

### Describe the problem
I'm running Tensorboard on a server and visualizing the graph on my local machine.

With the attached file that I generated using a custom script, Tensorboard crashes after a while of just moving the graph around or when I try to remove some modules from the main graph (100% crash after 4 modules removed).

More specifically:
- In Safari only the browser freezes but I can still close it.
- In Chrome the browser freezes and then the whole OS, I don't even have haptic feedback anymore. On the otherhand Tensorboard is still running on the server and the Tensorboard webpage can be accessed by other computers.


### Source code / logs
Here is the log-file containing just the graph that reproduces this issue 100% time on my end.
[events.out.tfevents.1493971052.c941b31be93a.zip](https://github.com/tensorflow/tensorflow/files/978532/events.out.tfevents.1493971052.c941b31be93a.zip)
"
9680,I am not able to install tensorflow,">pip install tensorflow
Collecting tensorflow
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow

For info I am using Window 10 and Python 3.5.2
please help"
9679,*tf_gen_op_libs* BUILD rule uses host toolchain even when crosstool wrappers are provided.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

     Yes. I have done the following changes.

     - I pulled the tensorflow branch - **v1.0.1**       
     - [An ugly hack to compile on Tegra X1 /w Jetpack 2.3.1 release.](https://github.com/rwightman/tensorflow/commit/a1cde1d55f76a1d4eb806ba81d7c63fe72466e6d) - Added this for trying to cross compile tensorflow on NVIDIA Jetson Tx1
      - Modified **BUILD.tpl** and **CROSSTOOL.tpl** in `third_party/gpus/crosstool/` for cross building for aarch64 as mentioned in [Bazel build wiki for custom toolchain](https://github.com/bazelbuild/bazel/wiki/Building-with-a-custom-toolchain)

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

    Linux Ubuntu 16.04.1 - x86_64

- **TensorFlow installed from (source or binary)**:

    Trying to cross compile for NVIDIA Jetson TX1 from source

- **TensorFlow version (use command below)**:

    v1.0.1 branch

- **Bazel version (if compiling from source)**:

    v 0.4.5

- **CUDA/cuDNN version**:

     CUDA - 8.0.34
     cuDNN - 5.1.5

- **Exact command to reproduce**:

### Problem description
I am trying to cross compile tensorflow with GPU support for NVIDIA Jetson TX1. I have setup the crosstool file for using the cross-build tools which I downloaded from the Linaro Website ( I followed the instructions from the bazel wiki on how to do so). My code compiles fine till it reaches the stage where the `tf_gen_op_libs` BUILD rule is reached (`tensorflow/cc/BUILD:314`). All of the ops mentioned in the rule fails to build. To be more precise, if fails in the linking stage with the following error
```
 /home/jetsontx1/Softwares/tensorflow/tensorflow/cc/BUILD:314:1: Couldn't build file tensorflow/cc/ops/no_op_gen_cc: Linking of rule '//tensorflow/cc:ops/no_op_gen_cc' failed: gcc failed: error executing command 
  (cd /home/jetsontx1/.cache/bazel/_bazel_jetsontx1/aebd32d6c3050c56aab9d4678f2e4fce/execroot/tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64: \
    PATH=/usr/local/cuda-8.0/bin:/home/jetsontx1/bin:/home/jetsontx1/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
  /usr/bin/gcc -o bazel-out/host/bin/tensorflow/cc/ops/no_op_gen_cc -Lbazel-out/host/bin/_solib_k8/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Slib '-Wl,-rpath,$ORIGIN/../../../_solib_k8/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Slib' -pthread -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 '-fuse-ld=gold' -Wl,-no-as-needed -Wl,-z,relro,-z,now -B/usr/bin -B/usr/bin '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -pass-exit-codes -Wl,-S -Wl,--gc-sections -Wl,@bazel-out/host/bin/tensorflow/cc/ops/no_op_gen_cc-2.params): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
/usr/bin/ld.gold: fatal error: bazel-out/host/bin/_solib_k8/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Slib/libcudart.so.8.0: unsupported ELF machine number 183
collect2: error: ld returned 1 exit status
```
So what I don't understand is, I have specified my crosstool toolchain to build all the tensorflow libs. But it is using the host compiler (/usr/bin/gcc) for this particular stage alone. Shouldn't it use the wrapper I specified in the crosstool file?

To put in another way - Why is the rule `tf_gen_op_libs` building the ops mentioned in the BUILD rule with the host compiler and not my crosstool? Is this a BUG?"
9678,"Convert magenta/image_stylization model on Android, but doesn't work","I try [TF Stylize on Android](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android) and it works perfectly.  Also I find the training code of this [image stylization model](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/StylizeActivity.java#L70). Its name is [magenta/image_stylization](https://github.com/tensorflow/magenta/tree/master/magenta/models/image_stylization) and provides two pre-trained models: [Monet](http://download.magenta.tensorflow.org/models/multistyle-pastiche-generator-monet.ckpt) and [Varied](http://download.magenta.tensorflow.org/models/multistyle-pastiche-generator-varied.ckpt). The first one had 10 styles and second has 32.

So my idea it to use them to replace [image stylization model](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/StylizeActivity.java#L74).

Here is what I did.
1 Save `*.ckpt` model to `*.pb`  
Save graph  
```
import tensorflow as tf
import model
import ops

num_styles = 10
imgWidth = 216
imgHeight = 216
channel = 3
checkpoint = ""models/multistyle-pastiche-generator-monet.ckpt""

inputImage = tf.placeholder(tf.float32,shape=[None,imgWidth,imgHeight,channel],name=""input"")
styles = tf.placeholder(tf.float32,shape=[num_styles],name=""style_num"")

with tf.name_scope(""""):
    transform = model.transform(inputImage,
                            normalizer_fn=ops.weighted_instance_norm,
                            normalizer_params={
                                # 'weights': tf.constant(mixture),
                                'weights' : styles,
                                'num_categories': num_styles,
                                'center': True,
                                'scale': True})

model_saver = tf.train.Saver(tf.global_variables())

with tf.Session() as sess:
    tf.train.write_graph(sess.graph_def, ""models/"", ""input.pb"")
```

Freeze Graph
```
bazel-bin/tensorflow/python/tools/freeze_graph \
 --input_graph=input.pb --input_checkpoint=multistyle-pastiche-generator-monet.ckpt \
 --output_node_names=transformer/expand/conv3/conv/Sigmoid --input_binary=False \
 --output_graph=frozen.pb
```

Inference
```
bazel-bin/tensorflow/python/tools/optimize_for_inference \
--input=frozen.pb --output=inference.pb \
--input_names=input --output_names=transformer/expand/conv3/conv/Sigmoid \
--frozen_graph=True
```

Quantize
```
bazel-bin/tensorflow/tools/quantization/quantize_graph \
--input=inference.pb \
--output=quantize_graph.pb \
--output_node_names=transformer/expand/conv3/conv/Sigmoid  \
--mode=weights_rounded
```

2 Replace model with `quantize_graph.pb`

Then I got an issue.
I can see there are 10 styles and they display on the screen :
![screenshot_2017-05-05-14-10-21](https://cloud.githubusercontent.com/assets/8957771/25735287/d52a1700-319c-11e7-86e1-642ad06539e7.jpg)

However, the image is not transformed.  There's no style on the image. It's the just original image.
TF version is 1.1.0. Android is 6.0.1

Anyone met the same issue or anyone knew how exactly to convert these two models  and use on mobile?

"
9676,Bug when trying to restore training from Inception-3 checkpoint with different trainable variables,"I have the pretty common use case of freezing the bottom layers of Inception and training only the first two layers, after which I lower the learning rate and fine tune the entire Inception model.

Here is my code for running the first part

```python
train_dir='/home/ubuntu/pynb/TF play/log-inceptionv3flowers'
with tf.Graph().as_default():
    tf.logging.set_verbosity(tf.logging.INFO)
    
    dataset = get_dataset()
    images, _, labels = load_batch(dataset, batch_size=32)
    
    # Create the model, use the default arg scope to configure the batch norm parameters.
    with slim.arg_scope(inception.inception_v3_arg_scope()):
        logits, _ = inception.inception_v3(images, num_classes=5, is_training=True)
        
    # Specify the loss function:
    one_hot_labels = slim.one_hot_encoding(labels, 5)
    tf.losses.softmax_cross_entropy(one_hot_labels, logits)
    total_loss = tf.losses.get_total_loss()

    # Create some summaries to visualize the training process:
    tf.summary.scalar('losses/Total Loss', total_loss)
  
    # Specify the optimizer and create the train op:
    optimizer = tf.train.RMSPropOptimizer(0.001, 0.9,
                                    momentum=0.9, epsilon=1.0)
    train_op = slim.learning.create_train_op(total_loss, optimizer, variables_to_train=get_variables_to_train())
    
    # Run the training:
    final_loss = slim.learning.train(
        train_op,
        logdir=train_dir,
        init_fn=get_init_fn(),
        number_of_steps=4500,
        save_summaries_secs=30,
        save_interval_secs=30,
        session_config=tf.ConfigProto(gpu_options=gpu_options))
        
print('Finished training. Last batch loss %f' % final_loss)
```

which runs properly, then my code for running the second part

```python
train_dir='/home/ubuntu/pynb/TF play/log-inceptionv3flowers'
with tf.Graph().as_default():
    tf.logging.set_verbosity(tf.logging.INFO)
    
    dataset = get_dataset()
    images, _, labels = load_batch(dataset, batch_size=32)
    
    # Create the model, use the default arg scope to configure the batch norm parameters.
    with slim.arg_scope(inception.inception_v3_arg_scope()):
        logits, _ = inception.inception_v3(images, num_classes=5, is_training=True)
        
    # Specify the loss function:
    one_hot_labels = slim.one_hot_encoding(labels, 5)
    tf.losses.softmax_cross_entropy(one_hot_labels, logits)
    total_loss = tf.losses.get_total_loss()

    # Create some summaries to visualize the training process:
    tf.summary.scalar('losses/Total Loss', total_loss)
  
    # Specify the optimizer and create the train op:
    optimizer = tf.train.RMSPropOptimizer(0.0001, 0.9,
                                    momentum=0.9, epsilon=1.0)
    train_op = slim.learning.create_train_op(total_loss, optimizer)
    
    # Run the training:
    final_loss = slim.learning.train(
        train_op,
        logdir=train_dir,
        init_fn=get_init_fn(),
        number_of_steps=10000,
        save_summaries_secs=30,
        save_interval_secs=30,
        session_config=tf.ConfigProto(gpu_options=gpu_options))
        
print('Finished training. Last batch loss %f' % final_loss)
```

Notice that in the second part, I do not pass anything into `create_train_op`'s `variables_to_train` parameter. This error is then shown

```
NotFoundError (see above for traceback): Key InceptionV3/Conv2d_4a_3x3/BatchNorm/beta/RMSProp not found in checkpoint
	 [[Node: save_1/RestoreV2_49 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save_1/Const_0, save_1/RestoreV2_49/tensor_names, save_1/RestoreV2_49/shape_and_slices)]]
	 [[Node: save_1/Assign_774/_1550 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_2911_save_1/Assign_774"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

```

I suspect that it's looking for the RMSProp variables for the InceptionV3/Conv2d_4a_3x3 layer, which is non-existent, because I didn't train that layer in the previous checkpoint. I'm not sure how to achieve what I want, as I can see no examples in the documentation about how to do this."
9675,The tfrecords file is 8 times larger than raw image data,"I try to write a tfrecords file, but the file is larger than raw data. 
```python (type)
img = Image.open('img_file')  # this image file size: 24 kb
b = img.tobytes()  # the len(b) is 24 kb, this is right
feats = tf.train.Features(feature={'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[2])), 
'image_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[b]))})
example_string = example.SerializeToString()
len(example_string) / 8 / 1024  # the output == 24.0057373046875 kb, look like well
```
but I write this 'example_string' to tfrecords file , the tfrecords file size become 192 kb, I cann`t understand why tfrecords file size serval times larger than 'example_string' and raw image data"
9672,tf.contrib.image.rotate errors under Windows,"### Description
Trying to use `tf.contrib.image.rotate` produces the error: `tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'ImageProjectiveTransform'`. This appears to happen on Windows using both CPU and GPU, and does not appear to happen on Linux (Ubuntu 14.0.4, Tensorflow 1.0.1).


### Test Case
```
import tensorflow as tf
images = tf.zeros([2, 2])
rotated_images = tf.contrib.image.rotate(images, 0)
```

### Traceback
```
Traceback (most recent call last):
  File ""test.py"", line 3, in <module>
    rotated_images = tf.contrib.image.rotate(images, 0)
  File ""C:\Users\tim\didi-car\tf_env\lib\site-packages\tensorflow\contrib\image\python\ops\image_ops.py"", line 72, in rotate
    angles_to_projective_transforms(angles, image_width, image_height))
  File ""C:\Users\tim\didi-car\tf_env\lib\site-packages\tensorflow\contrib\image\python\ops\image_ops.py"", line 166, in transform
    output = gen_image_ops.image_projective_transform(images, transforms)
  File ""C:\Users\tim\didi-car\tf_env\lib\site-packages\tensorflow\contrib\image\ops\gen_image_ops.py"", line 49, in image_projective_transform
    transforms=transforms, name=name)
  File ""C:\Users\tim\didi-car\tf_env\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 768, in apply_op
    op_def=op_def)
  File ""C:\Users\tim\didi-car\tf_env\lib\site-packages\tensorflow\python\framework\ops.py"", line 2338, in create_op
    set_shapes_for_outputs(ret)
  File ""C:\Users\tim\didi-car\tf_env\lib\site-packages\tensorflow\python\framework\ops.py"", line 1719, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""C:\Users\tim\didi-car\tf_env\lib\site-packages\tensorflow\python\framework\common_shapes.py"", line 610, in call_cpp_shape_fn
    debug_python_shape_fn, require_shape_fn)
  File ""C:\Users\tim\didi-car\tf_env\lib\site-packages\tensorflow\python\framework\common_shapes.py"", line 671, in _call_cpp_shape_fn_impl
    input_tensors_as_shapes, status)
  File ""c:\users\tim\anaconda3\Lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""C:\Users\tim\didi-car\tf_env\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'ImageProjectiveTransform'
```

### System information
- Windows 10 Pro
- TensorFlow 1.1.0 installed via pip into virtual environment
- Python 3.5.1 (Anaconda)
"
9670,"InvalidArgumentError even finished feeding the values, maybe hidden bug of variable dependency","I have defined two classes of models, `x` and `y`. 

    class x():
        def __init__(self, x_inp1, x_inp2):
            # do sth...

        def step(self, session, encoder_inputs):
            input_feed = {}
            for l in range(encoder_size):
                 input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]
            ...
            output_feed = [x_output]
            return session.run(x_output)

    class y():
        def __init__(self, y_inp1, y_inp2):
            # do sth...

        def step(self, encoder_inputs):
            input_feed = {}
            for l in range(encoder_size):
                 input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]
            ...

They have quite similar functions. And then I define another class to group them up.

    class gp():
        def __init__(self, x_inp1, x_inp2, y_inp1, y_inp2):
            with tf.variable_scope('x'):
                  self.x_model = x(x_inp1, x_inp2)
            with tf.variable_scope('y'):
                  self.y_model = y(y_inp1, y_inp2)
        def step(self, session, encoder_inputs):
            print('train x....')
            x_output = self.x_model.step(session, encoder_inputs)
            print('train y....')
            y_output = self.y_model.step(session, x_output)
            ...

Please notice that the `y_model` takes the output of `x_model` as input. And I run the `gp()` in the `main` function:
    
    with tf.Session() as sess:
         gp_m = gp(x_inp1, x_inp2, y_inp1, y_inp2)
         gp_m.step(sess, x_inp1, x_inp2, y_inp1, y_inp2)

And after running  `x_output = self.x_model.step(encoder_inputs)` and begin to do `y_output = self.y_model.step(x_output)`, I got such an error:
    
    InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'x/encoder0' with dtype int32
	 [[Node: x/encoder0 = Placeholder[dtype=DT_INT32, shape=[], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Please notice this error points to the `x_model` even the step function of it has been finished, i.e., both `train x...` and `train y...` are shown. The `x_output` should be python list and numpy array, and it is not Tensorflow object anymore. So maybe there is any bug of the variable dependency? Or I have any wrong operation that cause such a kind of error? 

Here are the information maybe useful:

### operation system:  mac os 10.10
### python: 3.5
### tensorflow: r1.0
### install: from binary
### install using pip"
9669,Unsound GPU driver version scheme assumption in StringToDriverVersion,"Tensorflow's `cuda_diagnostics.cc` [here](https://github.com/tensorflow/tensorflow/blob/076799bdfae0057723d96f47cf78cb623c8bcd57/tensorflow/stream_executor/cuda/cuda_diagnostics.cc#L82) assumes that the GPU driver version will match `%d.%d.%d` but that doesn't seem to be a safe assumption.

```
2017-05-04 09:48:39.543664: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Invalid argument: expected %d.%d or %d.%d.%d form for driver version; got ""378.05.05.05""
```

In `FindDsoVersion()`, TF is parsing the name of the loaded libcuda library to get the version number, and the current Nvidia web driver library is:

    /Library/Frameworks/CUDA.framework/Versions/A/Libraries/libcuda_378.05.05.05_mercury.dylib
"
9665,Android Inference AAR Release Builds,"### Android Release request
You currently have [nightly Android builds](https://ci.tensorflow.org/view/Nightly/job/nightly-android/).  Is it possible to also have a Jenkins job that publishes stable release (and maybe RC) builds for Android?

Or alternatively, can you publish release and RC builds of your Android Inference AAR out somewhere they can be pulled as dependencies by Gradle (Maven Central or wherever other Android dependencies are stored)?"
9664,Run half-precision models on Android,"Is it possible to run half-precision (float16) graphs on Android (arm64-v8a/AArch64 supports half-precision)? If so, what would be the approach to do that? Trying to run a float16 graph gives an exception saying that only INT32 and Float32 ops are supported.

Thank you in advance,"
9662,Fail to compile binary for Android,Is there simple way to compile a native binary for inference on Android? Now I can successfully build the [benchmark](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark) and run it on my Android device. But it does not contain any actions about ops. I want to do something similar to the [benchmark_model_test](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/benchmark_model_test.cc) but it fails to build. I think it now lacks a android version of //tensorflow/cc APIs. How can I tackle it?
9661,[feature] Support Cross Compiling with tfcompile,"Tensorflow (using XLA) is able to AOT compile a graph using `tfcompile`. There does not seem to be a way to, or it it not documented, cross compile the graph (ie compile on OS X for deployment on iOS). (Related [SO](http://stackoverflow.com/questions/43508105/using-tfcompile-to-aot-compile-tensorflow-graph-for-ios) question).

I suggest adding a means of performing this cross compilation."
9657,> gcc: error: $opt: No such file or directory,"
Hi, I am using Ubuntu 16.04 (GCC5.2) and CUDA 8 to compile TF. I meet the following problem. Does anyone knows how to solve it? Thanks!

> ERROR: /home/xxx/.cache/bazel/_bazel_root/88a2645c89e061c3b0fe8e82d8c21312/external/protobuf/BUILD:609:1: C++ compilation of rule '@protobuf//:python/google/protobuf/internal/_api_implementation.so' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 50 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
> gcc: error: $opt: No such file or directory
> gcc: error: $opt: No such file or directory
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
"
9656,add a image with summary for showing  in tensorboard,"Hi,
for add a image to graph for showing in tensorboard , i used below code. but the tensorflow error that :""TypeError: Parameter to MergeFrom() must be instance of same class: expected Summary got Tensor. for field Event.summary""

the code :
<<  _**writer =tf.summary.FileWriter('C:\Program Files\python3.5\logs',graph=tf.get_default_graph())**_
<v
<<tf.Tensor 'Slice_2:0' shape=(1, 152, 138, 1) dtype=float32>
<<**_imgsumarry=tf.summary.image('image', v, 3 ,collections=None)_**
<imgsumarry
<<tf.Tensor 'image_2:0' shape=() dtype=string>
<< **writer.add_summary(imgsumarry)**


full error:

Traceback (most recent call last):
  File ""C:\Program Files\python3.5\python-3.5.3.amd64\lib\site-packages\google\protobuf\internal\python_message.py"", line 516, in init
    copy.MergeFrom(new_val)
  File ""C:\Program Files\python3.5\python-3.5.3.amd64\lib\site-packages\google\protobuf\internal\python_message.py"", line 1208, in MergeFrom
    'expected %s got %s.' % (cls.__name__, msg.__class__.__name__))
TypeError: Parameter to MergeFrom() must be instance of same class: expected Summary got Tensor.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#579>"", line 1, in <module>
    writer.add_summary(imgsumarry)
  File ""C:\Program Files\python3.5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\summary\writer\writer.py"", line 113, in add_summary
    event = event_pb2.Event(summary=summary)
  File ""C:\Program Files\python3.5\python-3.5.3.amd64\lib\site-packages\google\protobuf\internal\python_message.py"", line 518, in init
    _ReraiseTypeErrorWithFieldName(message_descriptor.name, field_name)
  File ""C:\Program Files\python3.5\python-3.5.3.amd64\lib\site-packages\google\protobuf\internal\python_message.py"", line 446, in _ReraiseTypeErrorWithFieldName
    six.reraise(type(exc), exc, sys.exc_info()[2])
  File ""C:\Program Files\python3.5\python-3.5.3.amd64\lib\site-packages\six.py"", line 685, in reraise
    raise value.with_traceback(tb)
  File ""C:\Program Files\python3.5\python-3.5.3.amd64\lib\site-packages\google\protobuf\internal\python_message.py"", line 516, in init
    copy.MergeFrom(new_val)
  File ""C:\Program Files\python3.5\python-3.5.3.amd64\lib\site-packages\google\protobuf\internal\python_message.py"", line 1208, in MergeFrom
    'expected %s got %s.' % (cls.__name__, msg.__class__.__name__))
TypeError: Parameter to MergeFrom() must be instance of same class: expected Summary got Tensor. for field Event.summary
"
9655,why can't i recover cifar-100/cifar-10 image from binary files using tensorflow?,"**my code likes following:**

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
import tensorflow as tf
from matplotlib import pyplot as plt

def read_cifar_100_bin(batch_size):

    coarse_label_bytes = 1
    final_lable_bytes = 1
    image_bytes = 3072
    filenames = [""./cifar-100-binary/train.bin""]
    filename_queue = tf.train.string_input_producer(filenames, shuffle=False)
    reader = tf.FixedLengthRecordReader(record_bytes=3074)
    key,value = reader.read(filename_queue)
    record_bytes = tf.decode_raw(value, tf.uint8)
    coarse_label = tf.cast(tf.slice(record_bytes, [0], [coarse_label_bytes]), tf.uint8)   
    final_label = tf.cast(tf.slice(record_bytes, [1], [final_lable_bytes]), tf.uint8)  #final_label:0~99
    example = tf.reshape(tf.slice(record_bytes, [2], [image_bytes]),[3,32,32])
    example_tr = tf.transpose(example, [1, 2, 0])
    example_batch, coarse_label_batch,final_label_batch = tf.train.batch([example_tr, coarse_label,final_label],` batch_size=batch_size)

    # example_batch, coarse_label_batch,final_label_batch  = tf.train.shuffle_batch(
    #     [example, coarse_label,final_label],
    #     batch_size=batch_size,
    #     num_threads=4,
    #     capacity=50000,
    #     min_after_dequeue=10000)

    with tf.Session() as sess:
        coord = tf.train.Coordinator()  
        threads = tf.train.start_queue_runners(coord=coord)
        #sess.run(tf.global_variables_initializer())

        images_batch = example_batch.eval()
        coarse_labels_batch = coarse_label_batch.eval()
        final_labels_batch = final_label_batch.eval()
        plt.imshow(images_batch[0])
        plt.show()

        coord.request_stop()
        coord.join(threads)

    return images_batch,coarse_labels_batch,final_labels_batch
images, coarse_labels,final_labels = read_cifar_100_bin(5)
print(images[0])
/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
**my problem is: when i use plt to recover image from images_batch[0],it show like this:**
![figure_1 copy](https://cloud.githubusercontent.com/assets/11583292/25697490/ba6a2f2c-30ed-11e7-9af5-636b11607d76.png)


**Anyone knows why? Thanks a lot**

**Another question:**
  I want to choose random mini-batch data from whole dataset so i use tf.train.shuffle_batch() function you can see above,i wonder if it's a good way to realize it w.r.t speed and efficiency."
9654,Estimator's argument checking overzealous : model_fn has following not expected args: ['self'],"I'm using TensorFlow (1.1) high-level API Estimators to create my neural net. But I'm using it into a class and I have to call an instance of my class to generate the model of the neural network. (Here `self.a`)

```
class NeuralNetwork(object):
  def __init__(self):
    """""" Create neural net """"""
    regressor = tf.estimator.Estimator(model_fn=self.my_model_fn,
                                       model_dir=""/tmp/data"")
    // ...

  def my_model_fn(self, features, labels, mode):
  """""" Generate neural net model """"""
    self.a = a
    predictions = ...
    loss = ...
    train_op = ...
    return tf.estimator.EstimatorSpec(
      mode=mode,
      predictions=predictions,
      loss=loss,
      train_op=train_op)
```

But I get the error : ValueError: model_fn [...] has following not expected args: ['self']. I tried to remove the self for the args of my model but got another error TypeError: … got multiple values for keyword argument. 
Solution for now (as it was suggested on [StackOverflow](http://stackoverflow.com/questions/43755609/tensorflow-estimator-model-fn-has-following-not-expected-args-self)) is to use a lambda function to wrap my function `my_model_fn` (see below) but it will be nicer without it.
```
class NeuralNetwork(object):
  def __init__(self):
    """""" Create neural net """"""
    regressor = tf.estimator.Estimator(
        model_fn=lambda features, labels, mode: self.my_model_fn(features, labels, mode),
        model_dir=""/tmp/data"")
    // ...
```

"
9653,macOS 10.12.4 GPU Installing with native pip python3.6 (error: not loaded: @rpath/libcublas.8.0.dylib),"-- macOS 10.12.4
    python3.6
    ""native pip "" way
    gtx 960
    cuda is working

-- error:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 51, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: @rpath/libcublas.8.0.dylib
  Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
  Reason: image not found"
9651,crosstool_wrapper_driver_is_not_gcc failed: error executing command,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request. **This is a bug**
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: n/a, GitHub clone, 11:12pm CST May 3 2017.
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**:  8.0.61
- **GPU model and memory**: NVIDIA Titan X Pascal (12GB)
- **Exact command to reproduce**: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

This is a bug relating to installation.  I'm getting a similar error to #336  (https://github.com/tensorflow/serving/issues/336) except without any reference to nccl, similar to #8709 except I'm on Ubuntu 16.04 instead of 14, similar to #8790 (https://github.com/tensorflow/tensorflow/issues/8790) except that one seems to have been closed prematurely.


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures
.......
WARNING: /home/anaconda/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': Use SavedModel Builder instead.
WARNING: /home/anaconda/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': Use SavedModel instead.
INFO: Found 1 target...
ERROR: /home/anaconda/tensorflow/tensorflow/core/BUILD:1299:1: C++ compilation of rule '//tensorflow/core:lib_hash_crc32c_accelerate_internal' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/anaconda/.cache/bazel/_bazel_anaconda/8174be50bc0029f23c89ddd1973ff713/execroot/tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    PYTHON_BIN_PATH=/opt/anaconda/envs/py27/bin/python \
    PYTHON_LIB_PATH=/opt/anaconda/envs/py27/lib/python2.7/site-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_CUDA_VERSION='' \
    TF_CUDNN_VERSION='' \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '$opt' '-std=c++11' '$opt' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/lib_hash_crc32c_accelerate_internal/tensorflow/core/lib/hash/crc32c_accelerate.pic.d '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/lib_hash_crc32c_accelerate_internal/tensorflow/core/lib/hash/crc32c_accelerate.pic.o' -fPIC -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-DGOOGLE_CUDA=1' -msse3 -pthread -msse4.2 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c tensorflow/core/lib/hash/crc32c_accelerate.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/lib_hash_crc32c_accelerate_internal/tensorflow/core/lib/hash/crc32c_accelerate.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
gcc: error: $opt: No such file or directory
gcc: error: $opt: No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 15.639s, Critical Path: 6.51s
$


$ bazel version
Build label: 0.4.5
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Mar 16 12:19:38 2017 (1489666778)
Build timestamp: 1489666778
Build timestamp as int: 1489666778

$ cat /usr/local/cuda/version.txt 
CUDA Version 8.0.61

$ gcc --version
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```


On Ubuntu 16.04, building for Python 2.7 (from Anaconda). Running from bash.
I grabbed tensorflow from GitHub tonight, accepted all the the defaults when running configure, said yes CUDA support (more defaults) and used compute capability 6.1.
Tried bazel clean, re-configure, etc...same result.   

I've built this successfully for my Python 3.5 environment on this computer a couple months ago, just trying to add a Python 2.7 version and now it's failing."
9646,Memory leaking when session.run in a certain situation,"Hi,
Here I find a possible memory leaking bug.

### Describe the problem
When we doing 
`session.run( var )`

if we add operator before that variable, such as 
`session.run(-var )`

Memory will keep growing by the iteration, when we call the train function with -var thousands of times it will use up all the memory and also let the running time become much slower.

### Source code / logs
Here is a minimal example code:


**Normal situation:**

```
import tensorflow as tf
@profile
def run():
    a = tf.constant(5.0)
    b = tf.constant(6.0)
    c = a * b
    sess = tf.Session()
    for i in range(1000):
        b = sess.run(c) 
run()


Line     Mem usage    Increment   Line Contents
================================================
     4   80.816 MiB    0.000 MiB   @profile
     5                             def run():
     6   80.957 MiB    0.141 MiB       a = tf.constant(5.0)
     7   80.980 MiB    0.023 MiB       b = tf.constant(6.0)
     8   82.691 MiB    1.711 MiB       c = a * b
     9   82.879 MiB    0.188 MiB       sess = tf.Session()
    10                             
    11   83.570 MiB    0.691 MiB       for i in range(1000):
    12   83.570 MiB    0.000 MiB           b = sess.run(c)

```

**Memory leaking **


```
import tensorflow as tf

@profile
def run():
    a = tf.constant(5.0)
    b = tf.constant(6.0)
    c = a * b
    sess = tf.Session()

    for i in range(1000):
        b = sess.run(-c)

run()

 

Line     Mem usage    Increment   Line Contents
================================================
     4   80.805 MiB    0.000 MiB   @profile
     5                             def run():
     6   80.938 MiB    0.133 MiB       a = tf.constant(5.0)
     7   80.949 MiB    0.012 MiB       b = tf.constant(6.0)
     8   82.652 MiB    1.703 MiB       c = a * b
     9   82.848 MiB    0.195 MiB       sess = tf.Session()
    10                             
    11  134.258 MiB   51.410 MiB       for i in range(1000):
    12  134.258 MiB    0.000 MiB           b = sess.run(-c)

```



### System information

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: mac os 10.12.4  and Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: pip version  
- **TensorFlow version (use command below)**: ('v1.0.0-65-g4763edf-dirty', '1.0.1')
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A 
- **GPU model and memory**: N/A
- **Exact command to reproduce**: see above
"
9645,Gather/Slice/StridedSlice Gradients Support in the C++ API,"Hi,

I noticed that there is currently no gradient support in the C++ API for the gather, slice, and strided slice ops. Would it be too difficult to add support for those gradient ops? I am asking because they are ops very frequently used in the context of machine learning models and without them building ML models can be unnecessarily complicated.

Thank you!

P.S. I have noticed that the Python API implementation of the gather op gradient uses indexed slices, but I am not sure if that is entirely necessary and can thus probably also be supported in the C++ API."
9644,Indexed Slices Support,"Hi,

I am wondering what the reasoning behind IndexedSlices is and whether they are actually necessary. My understanding is that they are only constructed for the gradient of the gather op. Is there any other place in the Python API that IndexedSlices are being constructed? Now, even though they are only constructed there, there are special cases throughout the Python codebase for dealing with indexed slices. Is the performance benefit so important to justify all this special treatment code? I am asking because I am wondering if it is useful to implement that functionality in an API for a different language. 

And if I am to rephrase this question, should one put effort into adding support to the C++ API for indexed slices, or is their use a remnant of a design choice that is not that useful looking in retrospect?

Thank you!"
9643,bazel coverage build failure,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS 10.12
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 
('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')
- **Bazel version (if compiling from source)**: Build label: 
0.4.5
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Mar 16 12:50:12 2017 (1489668612)
Build timestamp: 1489668612
Build timestamp as int: 1489668612

- **CUDA/cuDNN version**: 7.0
- **GPU model and memory**: AMD Radeon R9 M370X 2048 MB
- **Exact command to reproduce**:

### Describe the problem

I'm trying to run `bazel coverage` on my Mac. But it fails to build.
```
$ bazel coverage //tensorflow/tensorboard/backend/... --verbose_failures
```
or
```
$ bazel build //tensorflow/tensorboard/backend/...   --verbose_failures  --collect_code_coverage
```

It returns error message like below
```
com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
ld: library not found for -lgcov
clang: error: linker command failed with exit code 1 (use -v to see invocation)
```

Is there any way to fix `ld: library not found for -lgcov` problem?


### Source code / logs
```
$ bazel coverage //tensorflow/tensorboard/backend/... --verbose_failures
..
INFO: Using default value for --instrumentation_filter: ""//tensorflow/tensorboard/backend"".
INFO: Override the above default with --instrumentation_filter
INFO: Found 14 targets and 9 test targets...
ERROR: /private/var/tmp/_bazel_Chris/b0f26c43826ae438107dcf403665fcf5/external/protobuf/BUILD:609:1: Linking of rule '@protobuf//:python/google/protobuf/internal/_api_implementation.so' failed: link_dynamic_library.sh failed: error executing command 
  (cd /private/var/tmp/_bazel_Chris/b0f26c43826ae438107dcf403665fcf5/execroot/tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda \
    PATH=/Users/Chris/.rvm/gems/ruby-2.0.0-p648/bin:/Users/Chris/.rvm/gems/ruby-2.0.0-p648@global/bin:/Users/Chris/.rvm/rubies/ruby-2.0.0-p648/bin:/Users/Chris/anaconda/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/share/dotnet:/Library/TeX/texbin:/Users/Chris/.rvm/bin:/Users/Chris/.rvm/bin \
    PYTHON_BIN_PATH=/Users/Chris/anaconda/bin/python \
    PYTHON_LIB_PATH=/Users/Chris/anaconda/lib/python2.7/site-packages \
    TF_CUDA_CLANG=1 \
    TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2 \
    TF_CUDA_VERSION='' \
    TF_CUDNN_VERSION='' \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL=0 \
    TMPDIR=/var/folders/tr/tl261hjj2wl4662x8msxh6fm0000gn/T/ \
  external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored external/local_config_cc/cc_wrapper.sh -shared -o bazel-out/local-opt/bin/external/protobuf/python/google/protobuf/internal/_api_implementation.so -Wl,-force_load,bazel-out/local-opt/bin/external/protobuf/_objs/python/google/protobuf/internal/_api_implementation.so/external/protobuf/python/google/protobuf/internal/api_implementation.pic.o -lstdc++ -lm -undefined dynamic_lookup -headerpad_max_install_names -lgcov): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
ld: library not found for -lgcov
clang: error: linker command failed with exit code 1 (use -v to see invocation)
INFO: Elapsed time: 17.559s, Critical Path: 2.03s
//tensorflow/tensorboard/backend/event_processing:directory_watcher_test NO STATUS
//tensorflow/tensorboard/backend/event_processing:event_accumulator_test NO STATUS
//tensorflow/tensorboard/backend/event_processing:event_file_inspector_test NO STATUS
//tensorflow/tensorboard/backend/event_processing:event_file_loader_test NO STATUS
//tensorflow/tensorboard/backend/event_processing:event_multiplexer_test NO STATUS
//tensorflow/tensorboard/backend/event_processing:plugin_asset_util_test NO STATUS
//tensorflow/tensorboard/backend/event_processing:reservoir_test      NO STATUS
//tensorflow/tensorboard/backend:http_util_test                       NO STATUS
//tensorflow/tensorboard/backend:json_util_test                       NO STATUS

Executed 0 out of 9 tests: 9 were skipped.

```



```
$ bazel build //tensorflow/tensorboard/backend/...   --verbose_failures  --collect_code_coverage
INFO: Found 23 targets...
ERROR: /private/var/tmp/_bazel_Chris/b0f26c43826ae438107dcf403665fcf5/external/protobuf/BUILD:609:1: Linking of rule '@protobuf//:python/google/protobuf/internal/_api_implementation.so' failed: link_dynamic_library.sh failed: error executing command 
  (cd /private/var/tmp/_bazel_Chris/b0f26c43826ae438107dcf403665fcf5/execroot/tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda \
    PATH=/Users/Chris/.rvm/gems/ruby-2.0.0-p648/bin:/Users/Chris/.rvm/gems/ruby-2.0.0-p648@global/bin:/Users/Chris/.rvm/rubies/ruby-2.0.0-p648/bin:/Users/Chris/anaconda/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/share/dotnet:/Library/TeX/texbin:/Users/Chris/.rvm/bin:/Users/Chris/.rvm/bin \
    PYTHON_BIN_PATH=/Users/Chris/anaconda/bin/python \
    PYTHON_LIB_PATH=/Users/Chris/anaconda/lib/python2.7/site-packages \
    TF_CUDA_CLANG=1 \
    TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2 \
    TF_CUDA_VERSION='' \
    TF_CUDNN_VERSION='' \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL=0 \
    TMPDIR=/var/folders/tr/tl261hjj2wl4662x8msxh6fm0000gn/T/ \
  external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored external/local_config_cc/cc_wrapper.sh -shared -o bazel-out/local-opt/bin/external/protobuf/python/google/protobuf/internal/_api_implementation.so -Wl,-force_load,bazel-out/local-opt/bin/external/protobuf/_objs/python/google/protobuf/internal/_api_implementation.so/external/protobuf/python/google/protobuf/internal/api_implementation.pic.o -lstdc++ -lm -undefined dynamic_lookup -headerpad_max_install_names -lgcov): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
ld: library not found for -lgcov
clang: error: linker command failed with exit code 1 (use -v to see invocation)
```
"
9638,Seg fault on session run when built with CMake and optimize for native arch enabled,"**System info: Ubuntu 16.04 64 bit, gcc 5.4.0, Intel i5 CPU**

Segmentation fault occurs in Eigen when a certain AVX instruction is performed (see stack trace below). This occurs during session run of several convolutional neural network graphs.

Tensorflow (checked out from the master branch today) is built using CMake with tensorflow_BUILD_SHARED_LIB enabled which generates a libtensorflow.so library file. This library file is linked to another C++ application which simply loads a graph and executes it.

Disabling the CMake option tensorflow_OPTIMIZE_FOR_NATIVE_ARCH removes the error, but probably also reduce performance.

Below is a nasty long stack trace, if you need any other info please let me know.

Stack trace:
```
#0 0x00007fffee19021b in _mm256_store_ps (__A=..., __P=0x7fff7c110cd0) at /usr/lib/gcc/x86_64-linux-gnu/5/include/avxintrin.h:854
#1 Eigen::internal::pstore<float, float __vector(8)>(float*, float __vector(8) const&) (to=0x7fff7c110cd0, from=...)
  at /home/smistad/workspace/FAST/build_Release/external/tensorflow/external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AVX/PacketMath.h:260
#2 0x00007fffefd255b5 in Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 24, 8, 0, false, false>::operator() (
  this=0x7fff98fd4a67, blockA=0x7fff7c110cd0, lhs=..., depth=9, rows=8, stride=0, offset=0)
  at /home/smistad/workspace/FAST/build_Release/external/tensorflow/external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/products/GeneralBlockPanelKernel.h:1767
#3 0x00007fffeff65692 in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 24, 8, 0, false, false>, Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 24, 4, false, false>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<float, long, 0, 0> >::pack_lhs (
  this=0x7fff937fbbd0, m=1, k=0) at /home/smistad/workspace/FAST/build_Release/external/tensorflow/external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContractionThreadPool.h:495
#4 0x00007fffeff63758 in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 24, 8, 0, false, false>, Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 24, 4, false, false>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<float, long, 0, 0> >::enqueue_packing_helper (this=0x7fff937fbbd0, start=1, end=2, k=0, rhs=false) at /home/smistad/workspace/FAST/build_Release/external/tensorflow/external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContractionThreadPool.h:624
#5 0x00007fffeff6369d in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 24, 8, 0, false, false>, Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 24, 4, false, false>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<float, long, 0, 0> >::enqueue_packing_helper(long, long, long, bool)::{lambda()#1}::operator()() const (__closure=0x7fff7c1aaca0)
  at /home/smistad/workspace/FAST/build_Release/external/tensorflow/external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContractionThreadPool.h:628
#6 0x00007fffeff73cca in std::_Bind<Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 24, 8, 0, false, false>, Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 24, 4, false, false>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<float, long, 0, 0> >::enqueue_packing_helper(long, long, long, bool)::{lambda()#1} ()>::__call<void>(std::tuple<>&&, std::_Index_tuple<>) (this=0x7fff7c1aaca0,
  __args=<unknown type in /home/smistad/workspace/FAST/build_Release/lib/libtensorflow.so, CU 0xf277508, DIE 0xf39a603>) at /usr/include/c++/5/functional:1074
#7 0x00007fffeff71a5d in std::_Bind<Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 24, 8, 0, false, false>, Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 24, 4, false, false>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<float, long, 0, 0> >::enqueue_packing_helper(long, long, long, bool)::{lambda()#1} ()>::operator()<, void>() (this=0x7fff7c1aaca0) at /usr/include/c++/5/functional:1133
#8 0x00007fffeff6d3ac in std::_Function_handler<void (), std::_Bind<Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::M---Type <return> to continue, or q <return> to quit---
akePointer>, 24, 8, 0, false, false>, Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 24, 4, false, false>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<float, long, 0, 0> >::enqueue_packing_helper(long, long, long, bool)::{lambda()#1} ()> >::_M_invoke(std::_Any_data const&) (__functor=...) at /usr/include/c++/5/functional:1871
#9 0x00007fffedeba4c8 in std::function<void ()>::operator()() const (this=0x7fff7c1aace0) at /usr/include/c++/5/functional:2267
#10 0x00007fffedeb9da7 in tensorflow::thread::EigenEnvironment::ExecuteTask (this=0x21bd7a8, t=...) at /home/smistad/workspace/FAST/build_Release/external/tensorflow/src/tensorflow/tensorflow/core/lib/core/threadpool.cc:81
#11 0x00007fffedebca71 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop (this=0x21bd7a0, thread_id=3)
  at /home/smistad/workspace/FAST/build_Release/external/tensorflow/external/eigen_archive/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:232
#12 0x00007fffedebade0 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::NonBlockingThreadPoolTempl(int, bool, tensorflow::thread::EigenEnvironment)::{lambda()#1}::operator()() const ()
  at /home/smistad/workspace/FAST/build_Release/external/tensorflow/external/eigen_archive/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:65
#13 0x00007fffedebe462 in std::_Function_handler<void (), Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::NonBlockingThreadPoolTempl(int, bool, tensorflow::thread::EigenEnvironment)::{lambda()#1}>::_M_invoke(std::_Any_data const&) (__functor=...) at /usr/include/c++/5/functional:1871
#14 0x00007fffedeba4c8 in std::function<void ()>::operator()() const (this=0x3333350) at /usr/include/c++/5/functional:2267
#15 0x00007fffedeb9aec in tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}::operator()() const (__closure=0x3333350)
  at /home/smistad/workspace/FAST/build_Release/external/tensorflow/src/tensorflow/tensorflow/core/lib/core/threadpool.cc:56
#16 0x00007fffedebbcb1 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) (__functor=...)
  at /usr/include/c++/5/functional:1871
#17 0x00007fffedeba4c8 in std::function<void ()>::operator()() const (this=0x3346398) at /usr/include/c++/5/functional:2267
#18 0x00007fffedef50ca in std::_Bind_simple<std::function<void ()> ()>::_M_invoke<>(std::_Index_tuple<>) (this=0x3346398) at /usr/include/c++/5/functional:1531
#19 0x00007fffedef5020 in std::_Bind_simple<std::function<void ()> ()>::operator()() (this=0x3346398) at /usr/include/c++/5/functional:1520
#20 0x00007fffedef4fb0 in std::thread::_Impl<std::_Bind_simple<std::function<void ()> ()> >::_M_run() (this=0x3346380) at /usr/include/c++/5/thread:115
#21 0x00007ffff6201c80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#22 0x00007ffff5d1d6ba in start_thread (arg=0x7fff98fd5700) at pthread_create.c:333
#23 0x00007ffff5a5382d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
```
"
9637,Convolutional Neural Networks Tutorial problem,"I am learning about Convolutional Neural Networks from the tutorial:
https://www.tensorflow.org/tutorials/deep_cnn
 
Inside it there is a link for getting the code but it is not working:
https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow_models/tutorials/image/cifar10/

how can I get the code?


"
9636,`reload(tensorflow)` fails,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.1.0
- **Bazel version (if compiling from source)**:
- **Exact command to reproduce**:

``` python
from importlib import reload
import tensorflow
reload(tensorflow)
```

fails with error

```
Traceback (most recent call last):
  File ""...\lib\importlib\__init__.py"", line 166, in reload
    _bootstrap._exec(spec, module)
  File ""<frozen importlib._bootstrap>"", line 626, in _exec
  File ""<frozen importlib._bootstrap_external>"", line 673, in exec_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
  File ""...\lib\site-packages\tensorflow\__init__.py"", line 50, in <module>
    del python
NameError: name 'python' is not defined
```"
9635,Implement of attention mechanisms,"Here is a question(maybe bug) about implements of different attention mechanisms, i.e. LuongAttention & BahdanauAttention, in contrib/seq2seq/python/ops/attention_wrapper.py. It seems that the only difference is their score functions(alignment models).

 In BahdanauAttention, the alignment model is as described in https://arxiv.org/abs/1412.7449

```
 score = math_ops.reduce_sum(v * math_ops.tanh(keys + processed_query),
                                    [2])
```

In LuongAttention, the alignment model is a dot function:
`score = math_ops.matmul(query, self.keys, transpose_b=True)`

Then, the context vector is used all in same way. However, there should be more differences between these two attention mechanisms. Is this a modified version of Bahdanau attention proposed in https://arxiv.org/abs/1409.0473?

Any response would be appreciated. Thanks!"
