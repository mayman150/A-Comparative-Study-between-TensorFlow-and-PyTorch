Issue Number,Issue Title,Issue Body
111936,[dynamo] `AutogradFunctionMethodHigherOrderVariable` may not properly guard kwargs,"### üêõ Describe the bug

We will run `AutogradFunctionMethodHigherOrderVariable` within dynamo even though its kwargs are constant but we do not ensure that we guard on those constants.

I'm not entirely sure how to construct a scenario in which we can provide args that are constant but not guarded (or whose constant value is not guarded).

For instance, this may be some constant attribute of `nn.Module` which I don't think we currently guard... :thinking: 

### Versions

main"
111934,[dynamo] `CondHigherOrderVariable` should handle kwargs by flattening to args,"### üöÄ The feature, motivation and pitch

Currently, I believe it is not flattened. We throw an error if len(args) != 4

However, users may do something like the following:

```python
torch.cond(pred=True, true_fn=lambda x: x +1, false_fn=lambda x: x - 1, operands=(torch.tensor([0]),))
```
Causing a graph break when it could have been trivially traced within dynamo
"
111932,Pruning/Compressing heads in attention blocks,"### üöÄ The feature, motivation and pitch

I've a conceptual question

BERT-base has a dimension of 768 for query, key and value and 12 heads (Hidden dimension=768, number of heads=12). The same is conveyed if we see the BERT-base architecture

```
(self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
)
```
Now, my question is:

Can I consider the first 64 neurons from the _out_features_ as the first-head, the next 64 neurons from the _out_features_ as the 2nd head and so on? (sec 3.2.2 from original paper; [Link](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf))

Basically, I am wondering if the Linear module representing query matrix; which is 768x768 can be thought as (768x64), (768x64)...12 times? The same for key and value modules

If so, is it possible to provide some starter code as I am unable to wrap around my head. Any help is appreciated (and I've some sample in the contribution section)

**P.S:** Here's the issue from StackOverflow ([link](https://datascience.stackexchange.com/questions/124233/understanding-multi-headed-attention-from-architecture-details))

### Alternatives

Example applications include papers from academia such as - [Paper1](https://lena-voita.github.io/posts/acl19_heads.html), [Paper2](https://github.com/pmichel31415/are-16-heads-really-better-than-1)

I referred to some of the previous posts ([link](https://datascience.stackexchange.com/questions/88330/how-do-the-linear-layers-in-the-attention-mechanism-work)), but I would appreciate any validation on this thought-process as it's similar but not same.

### Additional context

Here's a code which prunes a particular % in particular layer depending on _layer_index_ and _prune_percentage_
```
model = AutoModelForMaskedLM.from_pretrained(checkpoint)
linear_layers_list = []
for name, layer in model.named_modules():
    if name in model_layers_list:
        linear_layers_list.append(layer)
print(f""No of linear layers are: {len(linear_layers_list)}"")

layer = linear_layers_list[layer_index]
if prune_type == 'ln_structured':
    # Ln structured with n=1 i.e L1 pruning
    prune.ln_structured(layer, name='weight', amount=prune_percentage, dim=0, n=n)
```
I can understand that I can basically pass the Linear module and prune x% of weights.

Now, I would like to prune/remove one head in a similar fashion.

P.S: Raised a request in Huggingface as well, happy to close both ([Issue link](https://github.com/huggingface/transformers/issues/27044))
Thanks
```[tasklist]
### Tasks
```
"
111931,[opcheck] Faster gradcheck execution,"### üêõ Describe the bug

Running opcheck on gradcheck is pretty slow. We have a workaround for this in https://github.com/pytorch/FBGEMM/blob/f94254d24f8dc733a0a8c233e5ca368f0be04989/fbgemm_gpu/test/test_utils.py#L285 but it should be upstreamed to PT

### Versions

main

cc @zou3519"
111930,[opcheck] Way to reduce Hypothesis sampling when running opcheck,"### üêõ Describe the bug

Hypothesis sampling is very slow and typically it's pointless for a meta function test. It would greatly speedup opcheck tests if we can get rid of it.

@zou3519 says it's hard, because Hypothesis doesn't let you override settings. Maybe need to do some source code reading.

### Versions

main

cc @zou3519"
111928,DISABLED test_meta_inplace_addmm_cpu_complex64 (__main__.TestMetaCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_inplace_addmm_cpu_complex64&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17999233458).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_inplace_addmm_cpu_complex64`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

cc @ezyang @mruberry @Lezcano @peterbell10 @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111927,DISABLED test_abs_backward_cpu (__main__.TestNestedTensorAutogradCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_abs_backward_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17999233694).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_abs_backward_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111926,[opcheck] No easy way to initialize a blank failures dict,"### üêõ Describe the bug

When I say PYTORCH_OPCHECK_ACCEPT=1 and the failures dict is an empty file I expect it to initialize it with the default template, but instead it chokes saying that the json is invalid

### Versions

main

cc @zou3519"
111925,Resize warning in two argument torch.logical_* with broadcasting,"### üêõ Describe the bug

`torch.logical_*` operations that operate on two tensors raise a warning whenever the output has a larger number of elements than the first argument. The corresponding `torch.bitwise_*` operators do not raise this warning.

Example with `torch.logical_and` bellow:

```python
import torch

a = torch.randn(4, 2, 128) < 0
b = torch.randn(2, 128) < 0

# No warning
torch.bitwise_and(a, b)
# Raises warning
torch.bitwise_and(b, a)

# No warning
torch.logical_and(a, b)
# Raises warning
torch.logical_and(b, a)

# No warning
torch.logical_and(*torch.broadcast_tensors(b, a))
```


Result:
```
UserWarning: An output with one or more elements was resized since it had shape [2, 128], which does not match the required output shape [4, 2, 128]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1695392022560/work/aten/src/ATen/native/Resize.cpp:28.)
  torch.logical_and(b, a)
```

### Versions

PyTorch version: 2.1.0
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.6 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.27

Python version: 3.9.18 (main, Sep 11 2023, 13:41:44)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.4.0-1103-aws-x86_64-with-glibc2.27
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA A10G
Nvidia driver version: 470.182.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:        x86_64
CPU op-mode(s):      32-bit, 64-bit
Byte Order:          Little Endian
CPU(s):              64
On-line CPU(s) list: 0-63
Thread(s) per core:  2
Core(s) per socket:  32
Socket(s):           1
NUMA node(s):        1
Vendor ID:           AuthenticAMD
CPU family:          23
Model:               49
Model name:          AMD EPYC 7R32
Stepping:            0
CPU MHz:             3299.097
BogoMIPS:            5600.00
Hypervisor vendor:   KVM
Virtualization type: full
L1d cache:           32K
L1i cache:           32K
L2 cache:            512K
L3 cache:            16384K
NUMA node0 CPU(s):   0-63
Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr wbnoinvd arat npt nrip_save rdpid

Versions of relevant libraries:
[pip3] numpy==1.26.0
[pip3] torch==2.1.0
[pip3] torchaudio==2.1.0
[pip3] torchvision==0.16.0
[pip3] triton==2.1.0
[conda] blas                      1.0                         mkl
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch
[conda] mkl                       2023.1.0         h213fc3f_46343
[conda] mkl-service               2.4.0            py39h5eee18b_1
[conda] mkl_fft                   1.3.8            py39h5eee18b_0
[conda] mkl_random                1.2.4            py39hdb19cb5_0
[conda] numpy                     1.26.0           py39h5f9d8c6_0
[conda] numpy-base                1.26.0           py39hb5e798b_0
[conda] pytorch                   2.1.0           py3.9_cuda11.8_cudnn8.7.0_0    pytorch
[conda] pytorch-cuda              11.8                 h7e8668a_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                2.1.0                py39_cu118    pytorch
[conda] torchtriton               2.1.0                      py39    pytorch
[conda] torchvision               0.16.0               py39_cu118    pytorch"
111924,[opcheck] Cannot share failures_dict between multiple tests with differing sets of tests they run ,"### üêõ Describe the bug

The validation code tests that all keys are valid for every test, which means that if you have one test set with less running tests than another, that test will fail that the json is malformed because it sees test keys it doesn't recognize

### Versions

main

cc @zou3519"
111918,Graph break doesn't result in continuation function when break happens in if-statement expression without inline function call,"### üêõ Describe the bug

After https://github.com/pytorch/pytorch/pull/111919 I noticed that I can now trigger a graph break without Dynamo creating a continuation frame. Modify the test function as such:

```
diff --git a/test/dynamo/test_unspec.py b/test/dynamo/test_unspec.py
index b5c3bfd02ca..02b0d48bd8d 100644
--- a/test/dynamo/test_unspec.py
+++ b/test/dynamo/test_unspec.py
@@ -381,7 +381,7 @@ class UnspecTests(torch._dynamo.test_case.TestCase):
         def fn(x):
             x = x + 1
             y = x.item()
-            if test(y):
+            if y > 2:
                 return x * 2
             else:
                 return x * 3
```

(effectively inlining test.) This causes a continuation frame to no longer be created:

<img width=""808"" alt=""image"" src=""https://github.com/pytorch/pytorch/assets/13564/cca189bb-0c9e-459a-9737-e5ea08682c0f"">

If you move the graph break to an inner function call (as the test is written in the PR), this solves the problem. So it's probably some minor issue. I took a quick look though and it wasn't obvious what the issue was.

### Versions

main

cc @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111917,[dynamo] `ExecutorchCallDelegateHigherOrderVariable` may not preserve `FakeTensor` identity,"### üêõ Describe the bug

`ExecutorchCallDelegateHigherOrderVariable` will attempt to retrieve the real values as inputs to the executorch module, and then deep copy fake the result into new fake values. However, it is possible that the `executorch_call_delegate` function might perform in-place ops. 

The new FakeTensors would no longer share the same identity as the input fake tensors, violating our mechanism for in-graph aliasing detection. 

No repro as of yet, this is just speculative

Solution: manually dedup by matching the `fake <-> real <-> fake` relationship between inputs and outputs.

### Versions

main

cc @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan"
111914,vector loss autograd computation,"### üöÄ The feature, motivation and pitch

Hi, the default torch autograd only supports scalar loss, but if we need to compute vectorized loss, e.g., losses = [loss1, loss2, ... lossn], wrt. to weights, how to achieve this function efficiently without loop? Our current workaround is looping through all loss, 

grad_vectors = []
for l in losses:
    grad = torch.autograd.grad(l, self.weight, retain_graph=True)[0]
    grad_vectors.append(grad)


### Alternatives

_No response_

### Additional context

_No response_

cc @zou3519"
111909,DISABLED test_meta_outplace_fft_ihfft_cpu_float64 (__main__.TestMetaCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_outplace_fft_ihfft_cpu_float64&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17990893637).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_outplace_fft_ihfft_cpu_float64`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

ConnectionTimeoutError: Connect timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/main/test/test_meta.py -2 (connected: false, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 0)
headers: {}

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111908,Inconsistent Keyword Arguments behaviors in torch.triangular_solve(),"### üêõ Describe the bug

I encountered unexpected behavior and a documentation inconsistency when using torch.triangular_solve(). The documentation mentions that the function takes arguments 'A' and 'b', but when providing these as keyword arguments, an error message states that ""input"" is also required. Additionally, the function throws a runtime error related to tensor dimensions.

To reproduce :
```python
import torch

args = {'A': torch.randn([1]), 'b': torch.randn([2])}
res = torch.triangular_solve(**args)
```
it returns
```
TypeError: triangular_solve() missing 2 required positional argument: ""input"", ""A""
```

Second case: using 'A' and 'input'
```python
import torch
args = {'A': torch.randn([1]), 'input': torch.randn([2])}
res = torch.triangular_solve(**args)
```

Error message:

```
RuntimeError: torch.triangular_solve: Expected to have at least 2 dimensions, but it has 1 dimensions instead
```
Expected Behavior:

I expected the code to work based on the keyword arguments 'A' and 'b' as per the documentation.

Actual Behavior:

The code throws a TypeError and a RuntimeError related to missing or incorrect arguments.


### Versions

PyTorch Version: 2.2.0.dev20231023+cu118

cc @albanD"
111907,Migration from c10::variant to std::variant causes undefined symbols when linking against older pytorch,"### üêõ Describe the bug

# Context

We build our library against *nightly PyTorch wheel*.
We ship our library as a wheel package and expects customer to install PyTorch wheel along side with it. Our test infrastructure tests our pip wheel against both: 1. nightly pytorch wheel; and 2. pytorch 2.1 release wheel.

The above workflow has been working smoothly, until the recent change that breaks the compatibility, and our binary no-longer works across pytorch nightly and 2.1 release.

# Pitch

PR #109723 migrates to `std::variant` from `c10::variant`. This change is leaked in our API.
https://github.com/pytorch/pytorch/blob/4f79161452c876a76be0499f29c0b84bc64537e6/c10/util/Exception.h#L118-L130

Our library built against pytorch nightly has undefined symbols which is provided by torch.
```
root@cc2800f1e374:/opt/pytorch/nvfuser# nm -C nvfuser/lib/libnvfuser_codegen.so | grep c10::Warning
                 U c10::WarningUtils::get_warnAlways()
                 U c10::warn(c10::Warning const&)
                 U c10::Warning::Warning(std::variant<c10::Warning::UserWarning, c10::Warning::DeprecationWarning>, c10::SourceLocation, char const*, bool)
                 U c10::Warning::Warning(std::variant<c10::Warning::UserWarning, c10::Warning::DeprecationWarning>, c10::SourceLocation const&, std::string, bool)
```

However, this means that the library we have won't work against older PyTorch release. (i.e. either older pytorch nightly wheel from before that code change and more importantly 2.1 release). Since the library gives a different symbol using c10::variant instead of std::variant.

In a general case, `c10::Warning` could be only a pytorch internal bits. Targets that are *not* using `c10::Warning` explicitly should not care about an internal implementation change.
By marking c10::Warning as a weak symbol allows targets built against PyTorch to ignore the undefined symbol from the name change. (see PR #111860).

### Versions

Collecting environment information...
PyTorch version: 2.2.0a0+gitbabb6c6
Is debug build: False
CUDA used to build PyTorch: 12.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.27.7
Libc version: glibc-2.35

Python version: 3.10.11 (main, Oct 15 2023, 07:05:32) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-126-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.3.52
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA A100 80GB PCIe
Nvidia driver version: 525.105.17
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Address sizes:                   43 bits physical, 48 bits virtual
Byte Order:                      Little Endian
CPU(s):                          64
On-line CPU(s) list:             0-63
Vendor ID:                       AuthenticAMD
Model name:                      AMD Ryzen Threadripper PRO 3975WX 32-Cores
CPU family:                      23
Model:                           49
Thread(s) per core:              2
Core(s) per socket:              32
Socket(s):                       1
Stepping:                        0
BogoMIPS:                        6987.00
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate sme ssbd mba sev ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif umip rdpid overflow_recov succor smca
Virtualization:                  AMD-V
L1d cache:                       1 MiB (32 instances)
L1i cache:                       1 MiB (32 instances)
L2 cache:                        16 MiB (32 instances)
L3 cache:                        128 MiB (8 instances)
NUMA node(s):                    1
NUMA node0 CPU(s):               0-63
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Mmio stale data:   Not affected
Vulnerability Spec store bypass: Vulnerable
Vulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers
Vulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected

Versions of relevant libraries:
[pip3] numpy==1.26.1
[pip3] optree==0.9.2
[pip3] torch==2.2.0a0+gitbabb6c6
[pip3] torchvision==0.17.0a0+68161e9
[pip3] triton==2.1.0
[conda] Could not collect


cc @malfet @seemethere"
111905,Cannot build static windows libraries ,"### üêõ Describe the bug

üêõ Describe the bug

The torch_cpu.dll is ~200 Mb, which is way too large for us. Therefore, we'd like to build torch statically. I followed the procedures hinted in the README.

These are the commands I used:

~~~cmd
git clone https://github.com/pytorch/pytorch.git
cd pytorch && git checkout tags/v2.0.1
git submodule update --init --recursive
python -m venv .venv && .venv\Scripts\activate
pip install -r requirements.txt
pip install mkl mkl-include cmake


# additionally as found in the link above
curl https://s3.amazonaws.com/ossci-windows/mkl_2020.2.254.7z -k -O
mkdir mkl
# move 7z to mkl dir and right click, unpack here
set CMAKE_INCLUDE_PATH=D:\00_src\pytorch-fresh-install\mkl\include
set ""LIB=D:\00_src\pytorch-fresh-install\mkl\lib""
# end

cd .. && mkdir build-pytorch && cd build-pytorch
cmake -DBUILD_SHARED_LIBS:BOOL=OFF -DUSE_CUDA=0 -DCMAKE_BUILD_TYPE:STRING=MinSizeRel -DCMAKE_INSTALL_PREFIX:PATH=../pytorch-install ../pytorch
# use visual studio 2019 and ALL_BUILD with MinSizeRel to build
~~~

This builds for about an hour and fails with 1200 linker errors. Is building static libs not possible anymore?




### Versions

Windows 11 21H2, x64
using pip for python

cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @vladimir-aubrecht @iremyux @Blackhex @cristianPanaite"
111904,DISABLED test_nested_tensor_sum_dim_cpu_float32 (__main__.TestNestedTensorDeviceTypeCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_nested_tensor_sum_dim_cpu_float32&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17987305754).

Over the past 3 hours, it has been determined flaky in 8 workflow(s) with 24 failures and 8 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_nested_tensor_sum_dim_cpu_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111903,DISABLED test_meta_inplace_addmm_cpu_complex128 (__main__.TestMetaCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_inplace_addmm_cpu_complex128&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17987838015).

Over the past 3 hours, it has been determined flaky in 8 workflow(s) with 24 failures and 8 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_inplace_addmm_cpu_complex128`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

cc @ezyang @mruberry @Lezcano @peterbell10 @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111902,torchscript file can not be loaded if its saved form the export model produced by torch.export.export,"### üêõ Describe the bug

Hi, torchscript team guys,

 I tried the newest quantization tool pt2e and get the  final QAT model. But when I load the model after serializing,some error happens.

code snippet:
```
from torch.ao.quantization.quantizer.xnnpack_quantizer import (
    XNNPACKQuantizer,
    get_symmetric_quantization_config,
)

from torch._export import dynamic_dim
from torch._export import capture_pre_autograd_graph
from torch.ao.quantization.quantize_pt2e import (
    prepare_qat_pt2e,
    convert_pt2e,
)

from torch.export import dynamic_dim
import torch
from torch._export import capture_pre_autograd_graph
from torch.export import export, ExportedProgram

class Debug(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.conv_debug=torch.nn.Conv2d(in_channels=3, out_channels=5, kernel_size=(3,3), padding=0, stride=1, groups=1,bias=True)
        self.relu=torch.nn.ReLU()
    def forward(self,x):
        x=self.conv_debug(x)
        return self.relu(x)

if __name__=='__main__':
    print('The default quantized engine is {}'.format(torch.backends.quantized.engine))
    quantizer = XNNPACKQuantizer()
    qconfig=get_symmetric_quantization_config(is_qat=True)
    quantizer.set_global(qconfig)
    model=Debug()
    input=torch.rand(1,3,10,10)
    model_aot_graph= capture_pre_autograd_graph(model,(input,))
    model_qat = prepare_qat_pt2e(model_aot_graph, quantizer)
    model_qat_convert = convert_pt2e(model_qat)
    model_qat_ep=torch.export.export(model_qat_convert,(input,))


    model_jit=torch.jit.trace(model_qat_ep.module(),input)
    torch.jit.save(model_jit,'xnnpack_pt2e.pth')
```
if I load the model in terminalÔºåerror happens:
```
>>> import torch
>>> model_jit_load=torch.jit.load('xnnpack_pt2e.pth')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/sfadmin/anaconda3/envs/torch2.1/lib/python3.9/site-packages/torch/jit/_serialization.py"", line 162, in load
    cpp_module = torch._C.import_ir_module(cu, str(f), map_location, _extra_files, _restore_shapes)  # type: ignore[call-arg]
RuntimeError: 
Unknown builtin op: quantized_decomposed::quantize_per_tensor.
Could not find any similar ops to quantized_decomposed::quantize_per_tensor. This op may not exist or may not be currently supported in TorchScript.
:
  File ""code/__torch__/torch/fx/graph_module.py"", line 12
    _param_constant1 = self._param_constant1
    _param_constant0 = self._param_constant0
    _0 = ops.quantized_decomposed.quantize_per_tensor(arg_0, 1., 0, -128, 127, 1)
         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    _1 = ops.quantized_decomposed.dequantize_per_tensor(_0, 1., 0, -128, 127, 1)
    _2 = ops.quantized_decomposed.quantize_per_tensor(_param_constant0, 1., 0, -127, 127, 1)
```
It seems that torchscript does not support those new ops produced during using pt2e.
Is there some plan to support those operators , I hear that torchscript is in maintain mode and will not add new features.
thanks.


### Versions

Collecting environment information...
PyTorch version: 2.1.0+cu118
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.3 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.22.1
Libc version: glibc-2.31

Python version: 3.9.12 (main, Apr  5 2022, 06:56:58)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.4.0-155-generic-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: 11.3.58
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A100-PCIE-40GB
GPU 1: NVIDIA A100-PCIE-40GB
GPU 2: NVIDIA A100-PCIE-40GB
GPU 3: NVIDIA A100-PCIE-40GB

Nvidia driver version: 470.82.01
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   46 bits physical, 48 bits virtual
CPU(s):                          96
On-line CPU(s) list:             0-89
Off-line CPU(s) list:            90-95
Thread(s) per core:              1
Core(s) per socket:              24
Socket(s):                       2
NUMA node(s):                    2
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           85
Model name:                      Intel(R) Xeon(R) Gold 6248R CPU @ 3.00GHz
Stepping:                        7
CPU MHz:                         3600.003
CPU max MHz:                     4000.0000
CPU min MHz:                     1200.0000
BogoMIPS:                        6000.00
Virtualization:                  VT-x
L1d cache:                       768 KiB
L1i cache:                       768 KiB
L2 cache:                        24 MiB
L3 cache:                        35.8 MiB
NUMA node0 CPU(s):               0-23,48-71
NUMA node1 CPU(s):               24-47,72-95
Vulnerability Itlb multihit:     KVM: Mitigation: Split huge pages
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Retbleed:          Mitigation; Enhanced IBRS
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Mitigation; TSX disabled
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke avx512_vnni md_clear flush_l1d arch_capabilities

Versions of relevant libraries:
[pip3] numpy==1.22.3
[pip3] torch==2.1.0+cu118
[pip3] torch-pruning==1.0.0
[pip3] torch-scatter==2.1.0+pt112cu113
[pip3] torch-tb-profiler==0.4.0
[pip3] torchsummary==1.5.1
[pip3] torchvision==0.15.2
[pip3] torchviz==0.0.2
[pip3] triton==2.1.0
[conda] numpy                     1.22.3                   pypi_0    pypi
[conda] torch                     2.1.0+cu118              pypi_0    pypi
[conda] torch-pruning             1.0.0                    pypi_0    pypi
[conda] torch-scatter             2.1.0+pt112cu113          pypi_0    pypi
[conda] torch-tb-profiler         0.4.0                    pypi_0    pypi
[conda] torchsummary              1.5.1                    pypi_0    pypi
[conda] torchvision               0.15.2                   pypi_0    pypi
[conda] torchviz                  0.0.2                    pypi_0    pypi
[conda] triton                    2.1.0                    pypi_0    pypi

cc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @Xia-Weiwen @leslie-fang-intel @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @EikanWang @wenzhe-nrv @sanchitintel"
111901,Multiprocess. DataLoader worker  is killed by signal: Segmentation fault.,"### üêõ Describe the bug

```-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File ""/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/torch/multiprocessing/spawn.py"", line 74, in _wrap
    fn(i, *args)
  File ""/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/easytorch/launcher/dist_wrap.py"", line 43, in dist_func
    func(*args, **kwargs)
  File ""/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/easytorch/launcher/launcher.py"", line 35, in training_func
    raise e
  File ""/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/easytorch/launcher/launcher.py"", line 31, in training_func
    runner.train(cfg)
  File ""/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/easytorch/core/runner.py"", line 349, in train
    self.on_epoch_end(epoch)
  File ""/home/seyed/PycharmProjects/step/STEP/basicts/runners/base_runner.py"", line 151, in on_epoch_end
    self.validate(train_epoch=epoch)
  File ""/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/easytorch/utils/dist.py"", line 102, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/easytorch/core/runner.py"", line 517, in validate
    for iter_index, data in enumerate(data_iter):
  File ""/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/tqdm/std.py"", line 1182, in __iter__
    for obj in iterable:
  File ""/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/torch/utils/data/dataloader.py"", line 630, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File ""/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/torch/utils/data/dataloader.py"", line 1317, in _next_data
    self._shutdown_workers()
  File ""/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/torch/utils/data/dataloader.py"", line 1442, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File ""/home/seyed/miniconda3/envs/env/lib/python3.11/multiprocessing/process.py"", line 149, in join
    res = self._popen.wait(timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/seyed/miniconda3/envs/env/lib/python3.11/multiprocessing/popen_fork.py"", line 40, in wait
    if not wait([self.sentinel], timeout):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/seyed/miniconda3/envs/env/lib/python3.11/multiprocessing/connection.py"", line 930, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/seyed/miniconda3/envs/env/lib/python3.11/selectors.py"", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py"", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 72618) is killed by signal: Segmentation fault.
```

### Versions

```
Collecting environment information...
PyTorch version: 2.1.0
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.22.1
Libc version: glibc-2.35

Python version: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.2.0-34-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.104.12
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      39 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             32
On-line CPU(s) list:                0-31
Vendor ID:                          GenuineIntel
Model name:                         13th Gen Intel(R) Core(TM) i9-13900F
CPU family:                         6
Model:                              183
Thread(s) per core:                 2
Core(s) per socket:                 24
Socket(s):                          1
Stepping:                           1
CPU max MHz:                        5600.0000
CPU min MHz:                        800.0000
BogoMIPS:                           3993.60
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi umip pku ospke waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize arch_lbr ibt flush_l1d arch_capabilities
Virtualization:                     VT-x
L1d cache:                          896 KiB (24 instances)
L1i cache:                          1.3 MiB (24 instances)
L2 cache:                           32 MiB (12 instances)
L3 cache:                           36 MiB (1 instance)
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-31
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        Not affected
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Not affected
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Not affected
Vulnerability Retbleed:             Not affected
Vulnerability Spec rstack overflow: Not affected
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] easy-torch==1.3.2
[pip3] numpy==1.26.0
[pip3] torch==2.1.0
[pip3] torch_geometric==2.4.0
[pip3] torchaudio==2.1.0
[pip3] torchvision==0.16.0
[pip3] triton==2.1.0
[conda] blas                      1.0                         mkl  
[conda] easy-torch                1.3.2                    pypi_0    pypi
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch
[conda] mkl                       2023.1.0         h213fc3f_46343  
[conda] mkl-service               2.4.0           py311h5eee18b_1  
[conda] mkl_fft                   1.3.8           py311h5eee18b_0  
[conda] mkl_random                1.2.4           py311hdb19cb5_0  
[conda] numpy                     1.26.0          py311h08b1b3b_0  
[conda] numpy-base                1.26.0          py311hf175353_0  
[conda] pytorch                   2.1.0           py3.11_cuda12.1_cudnn8.9.2_0    pytorch
[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torch-geometric           2.4.0                    pypi_0    pypi
[conda] torchaudio                2.1.0               py311_cu121    pytorch
[conda] torchtriton               2.1.0                     py311    pytorch
[conda] torchvision               0.16.0              py311_cu121    pytorch
```

cc @SsnL @VitalyFedyunin @ejguan @dzhulgakov"
111900,"OOM when saving model(lora adapter), seems the clause ""FullyShardedDataParallel(model,...)"" will directly cause the OOM.","### üêõ Describe the bug

I am using llama-recipes to finetune LLMs.
After training 34B-codellama2 with LoRA(the training went on well), I got OOM error when model.save_pretrained.
I tried different PEFT versions, from v 0.3.0 to source code, do not work.
It is said PEFT-0.2.0 will work, but llama-recipes can't work with PEFT-0.2.0

I don't know the root cause of the problem( llama-recipes, PEFT, or FSDP from pytorch ?) , so I submitted this issue to pytorch and llama-recipes. Hope someone can help!

I modified the code and added save_pretrained just after the model loaded.
It shows that OOM occurs just after FSDP(FullyShardedDataParallel) operation, it's nothing to do with the training process.


    from torch.distributed.fsdp import (
        FullyShardedDataParallel as FSDP,
    )
    ##some codes  omitted
    mixed_precision_policy, wrapping_policy = get_policies(fsdp_config, rank)
    my_auto_wrapping_policy = fsdp_auto_wrap_policy(model, LlamaDecoderLayer)
    ### add save model before FSDP
    print(""before FSDP save model"")
    model.save_pretrained(""./test_pretrained1"")
    model = FSDP(
        model,
        auto_wrap_policy= my_auto_wrapping_policy if train_config.use_peft else wrapping_policy,
        cpu_offload=CPUOffload(offload_params=True) if fsdp_config.fsdp_cpu_offload else None,
        mixed_precision=mixed_precision_policy if not fsdp_config.pure_bf16 else None,
        sharding_strategy=fsdp_config.sharding_strategy,
        device_id=torch.cuda.current_device(),
        limit_all_gathers=True,
        #add use_orig_params=True for adapter#
        #use_orig_params=True,
        sync_module_states=train_config.low_cpu_fsdp,
        param_init_fn=lambda module: module.to_empty(device=torch.device(""cuda""), recurse=False)
        if train_config.low_cpu_fsdp and rank != 0 else None,
    )
    ### add save model after FSDP
    print(""after FSDP save model"")
    model.save_pretrained(""./test_pretrained2"")

Error logs:

before FSDP save model
trainable params: 19,660,800 || all params: 33,763,631,104 || trainable%: 0.05823070373989121
before FSDP save model
trainable params: 19,660,800 || all params: 33,763,631,104 || trainable%: 0.05823070373989121
before FSDP save model
trainable params: 19,660,800 || all params: 33,763,631,104 || trainable%: 0.05823070373989121
before FSDP save model
^MLoading checkpoint shards: 0%| | 0/7 [00:00<?, ?it/s]^MLoading checkpoint shards: 14%|‚ñà‚ñç | 1/7 [00:07<00:45, 7.64s/it]^MLoading checkpoint shards: 29%|‚ñà‚ñà‚ñä | 2/7 [00:14<00:37, 7.48s/it]^MLoading checkpoint shards: 43%|‚ñà‚ñà‚ñà‚ñà‚ñé | 3/7 [00:22<00:29, 7.50s/it]^MLoading checkpoint shards: 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 4/7 [00:29<00:22, 7.47s/it]^MLoading checkpoint shards: 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 5/7 [00:37<00:15, 7.64s/it]^MLoading checkpoint shards: 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:45<00:07, 7.52s/it]^MLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:52<00:00, 7.42s/it]^MLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:52<00:00, 7.49s/it]
--> Model /mnt/shhg01/cyris/model_hub/codellama34B_chat/

--> /mnt/shhg01/cyris/model_hub/codellama34B_chat/ has 33743.970304 Million params

paras distr: 435 0
LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules=['q_proj', 'v_proj'], lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)
trainable params: 19,660,800 || all params: 33,763,631,104 || trainable%: 0.05823070373989121
bFloat16 enabled for mixed precision - using bfSixteen policy
before FSDP save model
after FSDP save model
after FSDP save model
Traceback (most recent call last):
File ""/mnt/shhg01/cyris/univista-llm-train/ft/lora/finetuning.py"", line 8, in
Traceback (most recent call last):
File ""/mnt/shhg01/cyris/univista-llm-train/ft/lora/finetuning.py"", line 8, in
fire.Fire(main)
File ""/mnt/shhg01/env/llama-recipes/lib/python3.10/site-packages/fire/core.py"", line 141, in Fire
fire.Fire(main)
File ""/mnt/shhg01/env/llama-recipes/lib/python3.10/site-packages/fire/core.py"", line 141, in Fire
component_trace = _Fire(component, args, parsed_flag_args, context, name)
File ""/mnt/shhg01/env/llama-recipes/lib/python3.10/site-packages/fire/core.py"", line 475, in _Fire
component_trace = _Fire(component, args, parsed_flag_args, context, name)
File ""/mnt/shhg01/env/llama-recipes/lib/python3.10/site-packages/fire/core.py"", line 475, in _Fire
component, remaining_args = _CallAndUpdateTrace(
File ""/mnt/shhg01/env/llama-recipes/lib/python3.10/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
component, remaining_args = _CallAndUpdateTrace(
File ""/mnt/shhg01/env/llama-recipes/lib/python3.10/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
component = fn(*varargs, **kwargs)
File ""/mnt/shhg01/cyris/univista-llm-train/ft/lora/llama_recipes/finetuning.py"", line 204, in main
component = fn(*varargs, **kwargs)
File ""/mnt/shhg01/cyris/univista-llm-train/ft/lora/llama_recipes/finetuning.py"", line 204, in main
model.save_pretrained(""./test_pretrained2"")

...
...
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 6 has a total capacty of 39.59 GiB of which 1.12 MiB is free. Process 3377172 has 39.58 GiB memory in use. Of the allocated memory 38.18 GiB is allocated by PyTorch, and 40.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation. See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Exception raised from malloc at ../c10/cuda/CUDACachingAllocator.cpp:1114 (most recent call first):


### Versions

Collecting environment information...
PyTorch version: 2.2.0.dev20231012+cu118
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.5 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.24.1
Libc version: glibc-2.31

Python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-1025-nvidia-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: 11.8.89
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration:
GPU 0: NVIDIA A100-SXM4-40GB
GPU 1: NVIDIA A100-SXM4-40GB
GPU 2: NVIDIA A100-SXM4-40GB
GPU 3: NVIDIA A100-SXM4-40GB
GPU 4: NVIDIA A100-SXM4-40GB
GPU 5: NVIDIA A100-SXM4-40GB
GPU 6: NVIDIA A100-SXM4-40GB
GPU 7: NVIDIA A100-SXM4-40GB

Nvidia driver version: 515.86.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.7.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   43 bits physical, 48 bits virtual
CPU(s):                          256
On-line CPU(s) list:             0-255
Thread(s) per core:              2
Core(s) per socket:              64
Socket(s):                       2
NUMA node(s):                    8
Vendor ID:                       AuthenticAMD
CPU family:                      23
Model:                           49
Model name:                      AMD EPYC 7742 64-Core Processor
Stepping:                        0
Frequency boost:                 enabled
CPU MHz:                         2250.000
CPU max MHz:                     2250.0000
CPU min MHz:                     1500.0000
BogoMIPS:                        4491.36
Virtualization:                  AMD-V
L1d cache:                       4 MiB
L1i cache:                       4 MiB
L2 cache:                        64 MiB
L3 cache:                        512 MiB
NUMA node0 CPU(s):               0-15,128-143
NUMA node1 CPU(s):               16-31,144-159
NUMA node2 CPU(s):               32-47,160-175
NUMA node3 CPU(s):               48-63,176-191
NUMA node4 CPU(s):               64-79,192-207
NUMA node5 CPU(s):               80-95,208-223
NUMA node6 CPU(s):               96-111,224-239
NUMA node7 CPU(s):               112-127,240-255
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Mmio stale data:   Not affected
Vulnerability Retbleed:          Mitigation; untrained return thunk; SMT enabled with STIBP protection
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es

Versions of relevant libraries:
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.26.0
[pip3] pytorch-triton==2.1.0+6e4932cda8
[pip3] torch==2.2.0.dev20231012+cu118
[pip3] torchaudio==2.2.0.dev20231012+cu118
[pip3] torchvision==0.17.0.dev20231012+cu118
[pip3] triton==2.1.0
[conda] numpy                     1.26.0                   pypi_0    pypi
[conda] pytorch-triton            2.1.0+6e4932cda8          pypi_0    pypi
[conda] torch                     2.2.0.dev20231012+cu118          pypi_0    pypi
[conda] torchaudio                2.2.0.dev20231012+cu118          pypi_0    pypi
[conda] torchvision               0.17.0.dev20231012+cu118          pypi_0    pypi
[conda] triton                    2.1.0                    pypi_0    pypi




cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin"
111897,DISABLED test_meta_outplace_fft_ihfft2_cpu_int8 (__main__.TestMetaCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_outplace_fft_ihfft2_cpu_int8&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17985969934).

Over the past 3 hours, it has been determined flaky in 8 workflow(s) with 24 failures and 8 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_outplace_fft_ihfft2_cpu_int8`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

cc @ezyang @mruberry @Lezcano @peterbell10 @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111896,ImportError: cannot import name 'external_utils' from partially initialized module 'torch._dynamo',"### üêõ Describe the bug

Hello, 
I pulled ""docker pull pytorch/pytorch:2.1.0-cuda12.1-cudnn8-devel"" this line and got a error in below. 
Can you help me fix it? 

- ImportError: cannot import name 'external_utils' from partially initialized module 'torch._dynamo'



### Error logs

```
    from . import config  File ""/opt/conda/lib/python3.10/site-packages/torch/_dynamo/config.py"", line 9, in <module>
    from . import external_utilsImportError: cannot import name 'external_utils' from partially initialized module 'torch._dynamo' (most likely due to a circular import) (/opt/conda/lib/python3.10/site-packages/torch/_dynamo/__init__.py)
```

### Minified repro

_No response_

### Versions

```
Collecting environment information...
PyTorch version: 2.1.0+cu118
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.10 (default, May 26 2023, 14:05:08)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.29
Is CUDA available: True
CUDA runtime version: 11.8.89
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA Graphics Device
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Byte Order:                         Little Endian
Address sizes:                      39 bits physical, 48 bits virtual
CPU(s):                             32
On-line CPU(s) list:                0-31
Thread(s) per core:                 1
Core(s) per socket:                 24
Socket(s):                          1
NUMA node(s):                       1
Vendor ID:                          GenuineIntel
CPU family:                         6
Model:                              183
Model name:                         13th Gen Intel(R) Core(TM) i9-13900KF
Stepping:                           1
CPU MHz:                            3000.000
CPU max MHz:                        5800.0000
CPU min MHz:                        800.0000
BogoMIPS:                           5990.40
Virtualization:                     VT-x
L1d cache:                          576 KiB
L1i cache:                          384 KiB
L2 cache:                           24 MiB
NUMA node0 CPU(s):                  0-31
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        Not affected
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Not affected
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Not affected
Vulnerability Retbleed:             Not affected
Vulnerability Spec rstack overflow: Not affected
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Not affected
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req umip pku ospke waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize arch_lbr flush_l1d arch_capabilities

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==2.1.0+cu118
[pip3] torchaudio==2.1.0+cu118
[pip3] torchvision==0.16.0+cu118
[pip3] triton==2.1.0
[conda] Could not collect
```

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111895,"Install torch2.1.0+cuda11.8, I get error cannot import name '_get_privateuse1_backend_name' from 'torch._C'","### üêõ Describe the bug

I build a docker image, base image is nvcr.io/nvidia/pytorch:22.12-py3.   In Dockerfile,  I install python3.10 and torch 2.1.0+cuda11.8,  But I get error: cannot import name '_get_privateuse1_backend_name' from 'torch._C'

The Dockerfile is: 
```
FROM nvcr.io/nvidia/pytorch:22.12-py3
RUN apt-get update --fix-missing
RUN apt-get upgrade -y
RUN apt-get update

RUN apt-get -y remove python3.8 && apt-get -y remove --auto-remove python3.8 && apt-get purge python3.8
# Install python3.10
RUN apt-get install software-properties-common -y
RUN add-apt-repository ppa:deadsnakes/ppa
RUN apt-get install  -y python3.10 python3.10-dev
RUN ln -s -f /usr/bin/python3.10-config /usr/bin/python3-config
RUN ln -s -f /usr/bin/python3.10 /usr/bin/python3

# Install torch 
RUN curl -sS https://bootstrap.pypa.io/get-pip.py | python3.10
RUN python3.10 -m pip install --upgrade pip
RUN pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

Run the image and import torch, has error :
```
root@f013972c3e90:/workspace# python3
Python 3.10.13 (main, Aug 25 2023, 13:20:03) [GCC 9.4.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import torch
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.10/dist-packages/torch/__init__.py"", line 1119, in <module>
    from ._tensor import Tensor
  File ""/usr/local/lib/python3.10/dist-packages/torch/_tensor.py"", line 12, in <module>
    import torch.utils.hooks as hooks
  File ""/usr/local/lib/python3.10/dist-packages/torch/utils/__init__.py"", line 6, in <module>
    from .backend_registration import rename_privateuse1_backend, generate_methods_for_privateuse1_backend
  File ""/usr/local/lib/python3.10/dist-packages/torch/utils/backend_registration.py"", line 2, in <module>
    from torch._C import _rename_privateuse1_backend, _get_privateuse1_backend_name
ImportError: cannot import name '_get_privateuse1_backend_name' from 'torch._C' (/usr/local/lib/python3.10/dist-packages/torch/_C.cpython-310-x86_64-linux-gnu.so)
```

### Versions

 Collecting environment information...
PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.10.13 (main, Aug 25 2023, 13:20:03) [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-164-generic-x86_64-with-glibc2.31
Is CUDA available: N/A
CUDA runtime version: 11.8.89
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: GPU 0: Quadro RTX 4000
Nvidia driver version: 515.105.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.7.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: N/A

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Byte Order:                         Little Endian
Address sizes:                      46 bits physical, 48 bits virtual
CPU(s):                             32
On-line CPU(s) list:                0-31
Thread(s) per core:                 2
Core(s) per socket:                 8
Socket(s):                          2
NUMA node(s):                       2
Vendor ID:                          GenuineIntel
CPU family:                         6
Model:                              85
Model name:                         Intel(R) Xeon(R) Silver 4208 CPU @ 2.10GHz
Stepping:                           7
CPU MHz:                            800.081
CPU max MHz:                        3200.0000
CPU min MHz:                        800.0000
BogoMIPS:                           4200.00
Virtualization:                     VT-x
L1d cache:                          512 KiB
L1i cache:                          512 KiB
L2 cache:                           16 MiB
L3 cache:                           22 MiB
NUMA node0 CPU(s):                  0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30
NUMA node1 CPU(s):                  1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31
Vulnerability Gather data sampling: Mitigation; Microcode
Vulnerability Itlb multihit:        KVM: Mitigation: Split huge pages
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Not affected
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Retbleed:             Mitigation; Enhanced IBRS
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Mitigation; TSX disabled
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke avx512_vnni md_clear flush_l1d arch_capabilities

Versions of relevant libraries:
[pip3] numpy==1.24.1
[pip3] torch==2.1.0+cu118
[pip3] torchaudio==2.1.0+cu118
[pip3] torchvision==0.16.0+cu118
[pip3] triton==2.1.0
[conda] Could not collect

cc @seemethere @malfet @osalpekar @atalman"
111892,DISABLED test_nested_tensor_indexing_cpu_float64 (__main__.TestNestedTensorDeviceTypeCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_nested_tensor_indexing_cpu_float64&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17979964194).

Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 21 failures and 7 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_nested_tensor_indexing_cpu_float64`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111891,DISABLED test_forward_ad_linalg_lu_factor_cuda_float32 (__main__.TestCompositeComplianceCUDA),"Platforms: linux

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_forward_ad_linalg_lu_factor_cuda_float32&suite=TestCompositeComplianceCUDA) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17981993578).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 3 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_forward_ad_linalg_lu_factor_cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_ops.py`"
111890,DISABLED test_cuda_stream_context_manager2 (__main__.CtxManagerTests),"Platforms: linux

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_cuda_stream_context_manager2&suite=CtxManagerTests) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17983700103).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 5 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_cuda_stream_context_manager2`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `dynamo/test_ctx_manager.py`

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng"
111889,Difference in simple l1 computation on MPS vs CPU,"### üêõ Describe the bug

Identical code for computing the l1 distance gives different values on CPU and MPS. MPS results don't have a diagonal zero matrix either:

```
import torch

# Set a deterministic seed
torch.manual_seed(42)

# Ensure MPS backend is available
if not torch.backends.mps.is_available():
    raise ValueError(""MPS backend is not available on this machine."")

# Create a random tensor
A = torch.randn(9, 360000)

# Define the pairwise L1 distance function
def compute_l1_distances(tensor):
    N = tensor.shape[0]
    l1_distances = torch.empty(N, N)
    for i in range(N):
        l1_distances[i] = torch.sum(torch.abs(tensor[i] - tensor), dim=1)
    return l1_distances

# Compute on CPU
A_cpu = A.cpu()
l1_cpu = compute_l1_distances(A_cpu)
print(""L1 distances (CPU):"")
print(l1_cpu)

# Compute on MPS
A_mps = A.to(torch.device(""mps""))
l1_mps = compute_l1_distances(A_mps)
print(""\nL1 distances (MPS):"")
print(l1_mps)

# Check for differences
diff = (l1_cpu.cpu() - l1_mps.cpu()).abs().max().item()
print(f""\nMaximum absolute difference: {diff}"")
```

gives

```
L1 distances (CPU):
tensor([[     0.0000, 406899.6562, 406053.1875, 406739.1875, 406423.7500,
         406206.3125, 405852.6875, 405990.3750, 407657.2812],
        [406899.6562,      0.0000, 405465.9375, 406305.9688, 406232.8750,
         406189.3125, 406208.1875, 406567.6875, 405604.2812],
        [406053.1875, 405465.9375,      0.0000, 406576.6875, 405615.8125,
         405975.8438, 405437.3750, 405609.8750, 406184.0000],
        [406739.1875, 406305.9688, 406576.6875,      0.0000, 406542.4375,
         405836.3438, 405988.3438, 406958.3125, 407160.5625],
        [406423.7500, 406232.8750, 405615.8125, 406542.4375,      0.0000,
         405538.4375, 405996.0000, 406366.0000, 407350.3750],
        [406206.3125, 406189.3125, 405975.8438, 405836.3438, 405538.4375,
              0.0000, 405020.6875, 405809.4688, 406394.8750],
        [405852.6875, 406208.1875, 405437.3750, 405988.3438, 405996.0000,
         405020.6875,      0.0000, 406079.0625, 406435.8750],
        [405990.3750, 406567.6875, 405609.8750, 406958.3125, 406366.0000,
         405809.4688, 406079.0625,      0.0000, 406834.7500],
        [407657.2812, 405604.2812, 406184.0000, 407160.5625, 407350.3750,
         406394.8750, 406435.8750, 406834.7500,      0.0000]])

L1 distances (MPS):
tensor([[407657.3125, 405604.3125, 406183.9688, 407160.5625, 407350.3750,
         406394.8750, 406435.9062, 406834.7500,      0.0000],
        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,
              0.0000,      0.0000,      0.0000,      0.0000],
...
        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,
              0.0000,      0.0000,      0.0000,      0.0000]])

Maximum absolute difference: 407657.3125
```

### Versions

```
PyTorch version: 2.1.0
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: macOS 13.5 (arm64)
GCC version: Could not collect
Clang version: 14.0.3 (clang-1403.0.22.14.1)
CMake version: version 3.27.7
Libc version: N/A

Python version: 3.11.3 (main, Apr 19 2023, 18:49:55) [Clang 14.0.6 ] (64-bit runtime)
Python platform: macOS-13.5-arm64-arm-64bit
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Apple M2 Ultra

Versions of relevant libraries:
[pip3] flake8==6.0.0
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.24.4
[pip3] numpydoc==1.5.0
[pip3] pytorch-lightning==2.0.5
[pip3] pytorch_revgrad==0.2.0
[pip3] torch==2.1.0
[pip3] torchaudio==2.1.0
[pip3] torchmetrics==1.1.0
[pip3] torchvision==0.15.2
[conda] numpy                     1.24.4                   pypi_0    pypi
[conda] numpydoc                  1.5.0           py311hca03da5_0
[conda] pytorch-lightning         2.0.5                    pypi_0    pypi
[conda] pytorch-revgrad           0.2.0                    pypi_0    pypi
[conda] torch                     2.1.0                    pypi_0    pypi
[conda] torchaudio                2.1.0                    pypi_0    pypi
[conda] torchmetrics              1.1.0                    pypi_0    pypi
[conda] torchvision               0.15.2                   pypi_0    pypi
```

cc @ezyang @gchanan @zou3519 @kadeng @kulinseth @albanD @malfet @DenisVieriu97 @razarmehr @abhudev"
111887,Overriding Kernel Registrations on XLA,"### üêõ Describe the bug

We implemented a torch_npu plugin that uses the key XLA to register the operator.
As shown below
```
import torch
import torch_npu
```
```
[W OperatorEntry.cpp:150] Warning: Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: aten::_foreach_floor(Tensor[] tensors) -> (Tensor[])
    registered at build/aten/src/ATen/RegisterSchema.cpp:6
  dispatch key: XLA
  previous kernel: registered at ./build/aten/src/ATen/RegisterCPU.cpp:21063
       new kernel: registered at ./torch_npu/csrc/aten/RegisterNPU.cpp:20012 (function registerKernel)
```
After running the above code, there will be a lot of WARNING logs for the operator. But the previous kernel shown is RegisterCPU, is this warning misleading. Also why does torch show that there are duplicate kernels registered on XLA, which are registered in a third party plugin. Does torch register some XLA operators?

cc @ezyang @bdhirsh 

### Versions

PyTorch version: 1.11.0+cpu
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.4 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: 15.0.5
CMake version: version 3.24.1
Libc version: glibc-2.10

Python version: 3.7.5 (default, Oct 25 2019, 15:51:11)  [GCC 7.3.0] (64-bit runtime)
Python platform: Linux-4.15.0-76-generic-x86_64-with-debian-buster-sid
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:        x86_64
CPU op-mode(s):      32-bit, 64-bit
Byte Order:          Little Endian
CPU(s):              72
On-line CPU(s) list: 0-71
Thread(s) per core:  2
Core(s) per socket:  18
Socket(s):           2
NUMA node(s):        2
Vendor ID:           GenuineIntel
CPU family:          6
Model:               85
Model name:          Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz
Stepping:            7
CPU MHz:             3300.004
BogoMIPS:            5200.00
Virtualization:      VT-x
L1d cache:           32K
L1i cache:           32K
L2 cache:            1024K
L3 cache:            25344K
NUMA node0 CPU(s):   0-17,36-53
NUMA node1 CPU(s):   18-35,54-71
Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke avx512_vnni md_clear flush_l1d arch_capabilities

Versions of relevant libraries:
[pip3] flake8==5.0.4
[pip3] numpy==1.21.6
[pip3] pytorch-sphinx-theme==0.0.24
[pip3] torch==1.11.0+cpu
[pip3] torch-npu==1.11.0.post5+gita2c727c
[pip3] torch-test-cpp-extension==0.0.0
[pip3] torchtext==0.6.0
[pip3] torchvision==0.12.0a0+9b5a3fe
[conda] mkl                       2023.0.0                 pypi_0    pypi
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] pytorch-sphinx-theme      0.0.24                   pypi_0    pypi
[conda] torch                     1.11.0+cpu               pypi_0    pypi
[conda] torch-npu                 1.11.0.post5+gita2c727c          pypi_0    pypi
[conda] torch-test-cpp-extension  0.0.0                    pypi_0    pypi
[conda] torchtext                 0.6.0                    pypi_0    pypi
[conda] torchvision               0.12.0a0+9b5a3fe          pypi_0    pypi
"
111884,Custom FFT implementation returns unexpected results when using torch.compile,"### üêõ Describe the bug

I am trying to implement a custom radix-16 FFT function, because `torch.compile` does not yet support complex data types. The basic idea is to basically represent FFT as a series of blocked 16 x 16 matrix multiplications using `torch.einsum`, similar in spirit to the BlockFFT approach proposed in the [H3 paper](https://arxiv.org/abs/2212.14052). 

The function can run normally and the results are correct without using `torch.compile`, which I have tested by comparing the results with the results outputted by `torch.fft.fft`. However, when applying `torch.compile` to this function, this is no longer the case, and the results are completely different, although there is no runtime error.

### Error logs

assert torch.allclose(input_f, to_complex(block_dft_compiled(input)), rtol=1e-3, atol=1e-3)  # this will fail

### Minified repro

```
import torch


def complex_matrix(real, imag):
    return torch.stack((
            torch.stack((real, -imag), dim=-1),
            torch.stack((imag, real), dim=-1)
        ), dim=-2)


to_complex = lambda x: torch.complex(x[..., 0, 0], x[..., 1, 0])


def block_dft(input):    
    input_block = input.view(16, 16, -1)
    last_dim_size = input_block.shape[-1]
    
    tau = 2 * torch.pi
    range_16 = torch.arange(16, device=input.device)
    range_large = torch.arange(16 * last_dim_size, device=input.device)
    range_small = torch.arange(last_dim_size, device=input.device)
    
    dft_real = torch.cos(-(range_16.unsqueeze(-1) * range_16) / 16 * tau)
    dft_imag = torch.sin(-(range_16.unsqueeze(-1) * range_16) / 16 * tau)
    dft = complex_matrix(dft_real, dft_imag)
    
    dft_small_real = torch.cos(-(range_small.unsqueeze(-1) * range_small) / last_dim_size * tau)
    dft_small_imag = torch.sin(-(range_small.unsqueeze(-1) * range_small) / last_dim_size * tau)
    dft_small = complex_matrix(dft_small_real, dft_small_imag)
    
    twid_real = torch.cos(-(range_16.unsqueeze(-1) * range_large) / (16 * 16 * last_dim_size) * tau).reshape(16, 16, -1)
    twid_imag = torch.sin(-(range_16.unsqueeze(-1) * range_large) / (16 * 16 * last_dim_size) * tau).reshape(16, 16, -1)
    twid = complex_matrix(twid_real, twid_imag)
    
    twid_small_real = torch.cos(-(range_16.unsqueeze(-1) * range_small) / (16 * last_dim_size) * tau)
    twid_small_imag = torch.sin(-(range_16.unsqueeze(-1) * range_small) / (16 * last_dim_size) * tau)
    twid_small = complex_matrix(twid_small_real, twid_small_imag)
    
    input_block = complex_matrix(input_block, torch.zeros_like(input_block))
    
    return torch.einsum('xyzAB,xfBC,ygDE,zhFG,fyzCD,gzEF->hgfAG', 
                        input_block, 
                        dft, dft, dft_small, 
                        twid, twid_small).flatten(0, 2)


input = torch.rand(1024)
input_f = torch.fft.fft(input)

assert torch.allclose(input_f, to_complex(block_dft(input)), rtol=1e-3, atol=1e-3)  # this will pass

block_dft_compiled = torch.compile(block_dft)
assert torch.allclose(input_f, to_complex(block_dft_compiled(input)), rtol=1e-3, atol=1e-3)  # this will fail
```

### Versions

PyTorch version: 2.1.0
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: 11.5.119
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A30
GPU 1: NVIDIA A30

Nvidia driver version: 535.86.10
cuDNN version: Probably one of the following:
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn.so.8.1.1
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.1.1
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.1.1
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.1.1
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.1.1
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.1.1
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.1.1
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8.1.1
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.1.1
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.1.1
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.1.1
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.1.1
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.1.1
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.1.1
/usr/local/cuda-11.5/targets/x86_64-linux/lib/libcudnn.so.8.8.1
/usr/local/cuda-11.5/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.8.1
/usr/local/cuda-11.5/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.8.1
/usr/local/cuda-11.5/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.8.1
/usr/local/cuda-11.5/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.8.1
/usr/local/cuda-11.5/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.8.1
/usr/local/cuda-11.5/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.8.1
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Byte Order:                         Little Endian
Address sizes:                      48 bits physical, 48 bits virtual
CPU(s):                             64
On-line CPU(s) list:                0-63
Thread(s) per core:                 2
Core(s) per socket:                 32
Socket(s):                          1
NUMA node(s):                       1
Vendor ID:                          AuthenticAMD
CPU family:                         25
Model:                              1
Model name:                         AMD EPYC 7543P 32-Core Processor
Stepping:                           1
Frequency boost:                    enabled
CPU MHz:                            1500.000
CPU max MHz:                        3737.8899
CPU min MHz:                        1500.0000
BogoMIPS:                           5600.03
Virtualization:                     AMD-V
L1d cache:                          1 MiB
L1i cache:                          1 MiB
L2 cache:                           16 MiB
L3 cache:                           256 MiB
NUMA node0 CPU(s):                  0-63
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        Not affected
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Not affected
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Not affected
Vulnerability Retbleed:             Not affected
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Not affected
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] pytorch-quantization==2.1.2
[pip3] torch==2.1.0
[pip3] torchaudio==2.1.0
[pip3] torcheval==0.0.7
[pip3] torchvision==0.16.0
[pip3] triton==2.1.0
[conda] blas                      1.0                         mkl  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0           py310h7f8727e_0  
[conda] mkl_fft                   1.3.1           py310hd6ae3a3_0  
[conda] mkl_random                1.2.2           py310h00e6091_0  
[conda] numpy                     1.24.3          py310hd5efca6_0  
[conda] numpy-base                1.24.3          py310h8e6c178_0  
[conda] pytorch                   2.1.0           py3.10_cuda11.8_cudnn8.7.0_0    pytorch
[conda] pytorch-cuda              11.8                 h7e8668a_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] pytorch-quantization      2.1.2                    pypi_0    pypi
[conda] torchaudio                2.1.0               py310_cu118    pytorch
[conda] torcheval                 0.0.7                    pypi_0    pypi
[conda] torchtriton               2.1.0                     py310    pytorch
[conda] torchvision               0.16.0              py310_cu118    pytorch

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111880,DISABLED test_forward_ad_stft_cuda_float32 (__main__.TestCompositeComplianceCUDA),"Platforms: linux

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_forward_ad_stft_cuda_float32&suite=TestCompositeComplianceCUDA) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17979958785).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_forward_ad_stft_cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_ops.py`"
111876,Update torch.load examples to encourage best security practices,"`torch.load` without setting `weights_only` is unsafe.
See https://github.com/pytorch/pytorch/issues/111806 and linked items there for some discussion.

This task is to update examples at https://pytorch.org/docs/stable/generated/torch.load.html to use `weights_only=True` where possible, and explicit `weights_only=False` with a comment about unsafety only where needed (you need to verify if `weights_only=True` works for a particular example)."
111874,Coalescing manager does not work w/device from torch.cuda.current_device(),"### üêõ Describe the bug

Can be repro'd with the following - 

```
pg = _get_default_group()
dev = torch.cuda.current_device()
pg._start_coalescing(dev)
```

### Versions

main

cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu"
111873,[dynamo] Should `wrap_to_fake_tensor_and_record` be idempotent with regard to `FakeTensor`s?,"### üêõ Describe the bug

`wrap_to_fake_tensor_and_record` will actually wrap an existing `FakeTensor` within another layer of `FakeTensor`. What use does this have?

Shouldn't this function be idempotent - return the same `FakeTensor`, and not wrap it further? Further, shouldn't we forbid wrapping one fake tensor in another? 

Perhaps there are use cases, since one layer of fake tensor may have metadata that another would not?

Nevertheless, for the purpose of Dynamo, perhaps we should not allow nested fake tensor.

This is because nested FakeTensor could lead to weird aliasing bugs, as we are now shifting to using `FakeTensor` object identity to identify aliasing when it comes to user code in Dynamo. (See: https://github.com/pytorch/pytorch/issues/111585 https://github.com/pytorch/pytorch/pull/111196)

### Versions

main

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @eellison @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng"
111869,[dynamo] `get_fake_value` doesn't always return fake values,"### üêõ Describe the bug

In particular, when you run `get_fake_value` on a `getattr_var`, it will return the real tensor. This has got to be wrong. If only because the function is named `get_fake_value`.

Related: https://github.com/pytorch/pytorch/pull/111864

### Versions

main

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @eellison @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng"
111863,Jit scripting support for `|` and mixing `typing`.,"### üöÄ The feature, motivation and pitch

Jit scripting does not support describing unions with the new syntax `|`. Ideally we would be able to jit script this method:
```
import torch

def f(x: list[torch.Tensor] | None):
    if x is None:
        return torch.randn(2)
    return torch.cat(x)

torch.jit.script(f)
```
But it errors out with: 
```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
[..]
/python3.10/site-packages/torch/jit/_script.py in script(obj, optimize, _frames_up, _rcb, example_inputs)
   1339         if _rcb is None:
   1340             _rcb = _jit_internal.createResolutionCallbackFromClosure(obj)
-> 1341         fn = torch._C._jit_script_compile(
   1342             qualified_name, ast, _rcb, get_default_args(obj)
   1343         )

RuntimeError: 
Expression of type | cannot be used in a type expression:

def f(x: list[torch.Tensor] | None):
         ~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
```

We can't mix imports from the `typing` module and the new typings neither, like:
```
from typing import Optional
import torch

def f(x: Optional[list[torch.Tensor]]):
    if x is None:
        return torch.randn(2)
    return torch.cat(x)

torch.jit.script(f)
```
Errors out with: 
```
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
[...]/python3.10/site-packages/torch/jit/_script.py in script(obj, optimize, _frames_up, _rcb, example_inputs)
   1339         if _rcb is None:
   1340             _rcb = _jit_internal.createResolutionCallbackFromClosure(obj)
-> 1341         fn = torch._C._jit_script_compile(
   1342             qualified_name, ast, _rcb, get_default_args(obj)
   1343         )

[..]/python3.10/site-packages/torch/jit/annotations.py in try_ann_to_type(ann, loc)
    349         valid_type = try_ann_to_type(contained, loc)
    350         msg = ""Unsupported annotation {} could not be resolved because {} could not be resolved.""
--> 351         assert valid_type, msg.format(repr(ann), repr(contained))
    352         return OptionalType(valid_type)
    353     if is_union(ann):

AssertionError: Unsupported annotation typing.Optional[list[torch.Tensor]] could not be resolved because list[torch.Tensor] could not be resolved.
```

If we use `def f(x: Optional[List[torch.Tensor]]):` it works fine, but most code-bases are trying to get rid of as many of the `typing` imports as possible.

### Alternatives

_No response_

### Additional context

_No response_

cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel"
111855,Inductor torch.Tensor.item() implementation breaks when there are more than one tensors,"### üêõ Describe the bug

Updated the code of `test_item_nobreak` to be
```
    @torch._dynamo.config.patch(capture_scalar_outputs=True)
    def test_item_nobreak(self, device):
        @torch.compile(fullgraph=True)
        def f(x, x2):
            y = x.item()
            y2 = x2.item()
            return y

        t1 = torch.tensor([3], device=device)
        t2 = torch.tensor([4], device=device)

        f(t1, t2)
```
fails with 
```
    return (i6, )
            ^^
NameError: name 'i6' is not defined
```
in the output_code.

I have only discovered this while writing something else that also uses NoneLayout and it also works when there's only one call but when there are two calls it also breaks.

### Versions

master

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111851,DISABLED test_nested_tensor_indexing_cpu_float32 (__main__.TestNestedTensorDeviceTypeCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_nested_tensor_indexing_cpu_float32&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17974582867).

Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 9 failures and 3 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_nested_tensor_indexing_cpu_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111850,DISABLED test_cond_side_effects (__main__.MiscTests),"Platforms: linux

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_cond_side_effects&suite=MiscTests) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17973670342).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_cond_side_effects`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `dynamo/test_misc.py`

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng"
111849,DISABLED test_meta_outplace_fft_ihfft2_cpu_int64 (__main__.TestMetaCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_outplace_fft_ihfft2_cpu_int64&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17974674006).

Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 18 failures and 6 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_outplace_fft_ihfft2_cpu_int64`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

cc @ezyang @mruberry @Lezcano @peterbell10 @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111848,DISABLED test_cuda_stream_context_manager2_dynamic_shapes (__main__.DynamicShapesCtxManagerTests),"Platforms: linux, rocm

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_cuda_stream_context_manager2_dynamic_shapes&suite=DynamicShapesCtxManagerTests) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17972546325).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 3 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_cuda_stream_context_manager2_dynamic_shapes`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `dynamo/test_dynamic_shapes.py`

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng"
111841,"Fix docstring errors in _VF.py, __config__.py, _lobpcg.py, random.py, _linalg_utils.py, _namedtensor_internals.py, torch_version.py, __future__.py, _classes.py, _sources.py, _lowrank.py, _vmap_internals.py, _storage_docs.py, quasirandom.py, _appdirs.py","- **File**: `torch/__config__.py`, **Line**: 5, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/__config__.py`, **Line**: 5, **Description**: First line should end with a period (not 'e')
- **File**: `torch/__config__.py`, **Line**: 16, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/__config__.py`, **Line**: 21, **Description**: First line should end with a period (not 's')
- **File**: `torch/__config__.py`, **Line**: 21, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/__future__.py`, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/__future__.py`, **Line**: 1, **Description**: First line should end with a period (not 's')
- **File**: `torch/_appdirs.py`, **Line**: 8, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_appdirs.py`, **Line**: 8, **Description**: First line should end with a period (not 'm')
- **File**: `torch/_appdirs.py`, **Line**: 501, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_appdirs.py`, **Line**: 501, **Description**: First line should end with a period (not 'e')
- **File**: `torch/_appdirs.py`, **Line**: 501, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/_lowrank.py`, **Line**: 1, **Description**: One-line docstring should fit on one line with quotes (found 2)
- **File**: `torch/_lowrank.py`, **Line**: 17, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/_lowrank.py`, **Line**: 17, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_lowrank.py`, **Line**: 17, **Description**: First line should end with a period (not 'h')
- **File**: `torch/_lowrank.py`, **Line**: 90, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_lowrank.py`, **Line**: 90, **Description**: First line should end with a period (not ',')
- **File**: `torch/_lowrank.py`, **Line**: 194, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/_lowrank.py`, **Line**: 194, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_lowrank.py`, **Line**: 194, **Description**: First line should end with a period (not 'k')
- **File**: `torch/_lowrank.py`, **Line**: 194, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/_lowrank.py`, **Line**: 194, **Description**: No blank lines allowed between a section header and its content ('Args')
- **File**: `torch/_storage_docs.py`, **Line**: 1, **Description**: First line should end with a period (not 's')
- **File**: `torch/torch_version.py`, **Line**: 7, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/torch_version.py`, **Line**: 7, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/torch_version.py`, **Line**: 7, **Description**: First line should end with a period (not 'n')
- **File**: `torch/torch_version.py`, **Line**: 42, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/torch_version.py`, **Line**: 42, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/torch_version.py`, **Line**: 42, **Description**: First line should end with a period (not '!')
- **File**: `torch/_VF.py`, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_VF.py`, **Line**: 1, **Description**: First line should end with a period (not 's')
- **File**: `torch/_classes.py`, **Line**: 34, **Description**: First line should be in imperative mood (perhaps 'Load', not 'Loads')
- **File**: `torch/_linalg_utils.py`, **Line**: 1, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/_linalg_utils.py`, **Line**: 12, **Description**: First line should end with a period (not 'r')
- **File**: `torch/_lobpcg.py`, **Line**: 1, **Description**: One-line docstring should fit on one line with quotes (found 2)
- **File**: `torch/_lobpcg.py`, **Line**: 75, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/_lobpcg.py`, **Line**: 75, **Description**: First line should be in imperative mood; try rephrasing (found 'A')
- **File**: `torch/_lobpcg.py`, **Line**: 106, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/_lobpcg.py`, **Line**: 106, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_lobpcg.py`, **Line**: 106, **Description**: First line should be in imperative mood (perhaps 'Evaluate', not 'Evaluates')
- **File**: `torch/_lobpcg.py`, **Line**: 126, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/_lobpcg.py`, **Line**: 126, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_lobpcg.py`, **Line**: 126, **Description**: First line should be in imperative mood (perhaps 'Evaluate', not 'Evaluates')
- **File**: `torch/_lobpcg.py`, **Line**: 362, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/_lobpcg.py`, **Line**: 362, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_lobpcg.py`, **Line**: 362, **Description**: First line should end with a period (not 'g')
- **File**: `torch/_lobpcg.py`, **Line**: 362, **Description**: No blank lines allowed between a section header and its content ('Returns')
- **File**: `torch/_lobpcg.py`, **Line**: 362, **Description**: No blank lines allowed between a section header and its content ('References')
- **File**: `torch/_lobpcg.py`, **Line**: 776, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_lobpcg.py`, **Line**: 776, **Description**: First line should end with a period (not 'e')
- **File**: `torch/_lobpcg.py`, **Line**: 851, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/_lobpcg.py`, **Line**: 894, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/_lobpcg.py`, **Line**: 945, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_lobpcg.py`, **Line**: 945, **Description**: First line should end with a period (not 'z')
- **File**: `torch/_lobpcg.py`, **Line**: 1003, **Description**: No blank lines allowed between a section header and its content ('Returns')
- **File**: `torch/_lobpcg.py`, **Line**: 1064, **Description**: No blank lines allowed between a section header and its content ('Returns')
- **File**: `torch/_namedtensor_internals.py`, **Line**: 19, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_namedtensor_internals.py`, **Line**: 19, **Description**: Multi-line docstring closing quotes should be on a separate line
- **File**: `torch/_namedtensor_internals.py`, **Line**: 19, **Description**: First line should end with a period (not 'd')
- **File**: `torch/_namedtensor_internals.py`, **Line**: 19, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/_namedtensor_internals.py`, **Line**: 75, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/_namedtensor_internals.py`, **Line**: 75, **Description**: First line should be in imperative mood (perhaps 'Expand', not 'Expands')
- **File**: `torch/_namedtensor_internals.py`, **Line**: 116, **Description**: First line should end with a period (not '
- **File**: `torch/_sources.py`, **Line**: 15, **Description**: First line should be in imperative mood; try rephrasing (found 'Wrapper')
- **File**: `torch/_sources.py`, **Line**: 38, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_sources.py`, **Line**: 38, **Description**: Use r"""""" if any backslashes in a docstring
- **File**: `torch/_sources.py`, **Line**: 38, **Description**: First line should end with a period (not 'e')
- **File**: `torch/_sources.py`, **Line**: 38, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/_vmap_internals.py`, **Line**: 194, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/quasirandom.py`, **Line**: 6, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/quasirandom.py`, **Line**: 6, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/quasirandom.py`, **Line**: 6, **Description**: First line should end with a period (not 'g')
- **File**: `torch/quasirandom.py`, **Line**: 73, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/quasirandom.py`, **Line**: 73, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/quasirandom.py`, **Line**: 109, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/quasirandom.py`, **Line**: 109, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/quasirandom.py`, **Line**: 133, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/quasirandom.py`, **Line**: 133, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/quasirandom.py`, **Line**: 141, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/quasirandom.py`, **Line**: 141, **Description**: First line should end with a period (not 'y')
- **File**: `torch/quasirandom.py`, **Line**: 141, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/random.py`, **Line**: 10, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')

cc @carljparker"
111840,Multiplying the same tensor as part of a batch results in numerically different outputs,"### üêõ Describe the bug

Multiplying a tensor A of shape [1, 4, 1024] by tensor B [1024, 2048] and then slicing the index [:, 0:1, :] of the output results in numerically different output than if you slice tensor A first and then do the multiplication.

```
# Code to reproduce the issue:
import torch


torch.set_printoptions(precision=8)

a = torch.randn((1, 4, 1024), dtype=torch.float32)
b = torch.randn((1024, 2048), dtype=torch.float32)

out1 = torch.matmul(a, b)[:, 0:1, :]
print(out1, out1.shape, out1.dtype)
a2 = a[:, 0:1, :]
out2 = torch.matmul(a2, b)
print(out2, out2.shape, out2.dtype)

print(""all close:"", torch.allclose(out1, out2))
```

Output:
```
tensor([[[  0.65285444, 119.79240417, -89.60567474,  ...,  13.46691799,
           -8.47685242,   8.73911667]]]) torch.Size([1, 1, 2048]) torch.float32
tensor([[[  0.65286279, 119.79228973, -89.60571289,  ...,  13.46692467,
           -8.47685051,   8.73912144]]]) torch.Size([1, 1, 2048]) torch.float32
all close: False
```

### Versions

Collecting environment information...
PyTorch version: 2.0.0
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.2 LTS (x86_64)
GCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0
Clang version: Could not collect
CMake version: version 3.22.1
Libc version: glibc-2.35

Python version: 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] (64-bit runtime)
Python platform: Linux-5.15.0-1045-aws-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.1.105
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration:
GPU 0: NVIDIA H100 80GB HBM3
GPU 1: NVIDIA H100 80GB HBM3
GPU 2: NVIDIA H100 80GB HBM3
GPU 3: NVIDIA H100 80GB HBM3
GPU 4: NVIDIA H100 80GB HBM3
GPU 5: NVIDIA H100 80GB HBM3
GPU 6: NVIDIA H100 80GB HBM3
GPU 7: NVIDIA H100 80GB HBM3

Nvidia driver version: 535.104.12
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.1
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      48 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             96
On-line CPU(s) list:                0-95
Vendor ID:                          AuthenticAMD
Model name:                         AMD EPYC 7R13 Processor
CPU family:                         25
Model:                              1
Thread(s) per core:                 1
Core(s) per socket:                 48
Socket(s):                          2
Stepping:                           1
BogoMIPS:                           5300.00
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save vaes vpclmulqdq rdpid
Hypervisor vendor:                  KVM
Virtualization type:                full
L1d cache:                          3 MiB (96 instances)
L1i cache:                          3 MiB (96 instances)
L2 cache:                           48 MiB (96 instances)
L3 cache:                           384 MiB (12 instances)
NUMA node(s):                       2
NUMA node0 CPU(s):                  0-47
NUMA node1 CPU(s):                  48-95
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        Not affected
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Not affected
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Not affected
Vulnerability Retbleed:             Not affected
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] numpy==1.22.2
[pip3] onnx==1.13.1rc2
[pip3] pytorch-quantization==2.1.2
[pip3] torch==2.0.0
[pip3] torch-tensorrt==1.4.0.dev0
[pip3] torchdata==0.6.0
[pip3] torchtext==0.15.1
[pip3] torchvision==0.15.1
[pip3] triton==2.0.0
[conda] Could not collect"
111837,[dynamo] UnspecializedNNModuleVariable does not implement object identity,"### üêõ Describe the bug

called from `nn.Modules.named_modules()` where one of the modules are initialized within the compile region.
```
[2023-10-23 14:05:10,051] [1/0] torch._dynamo.symbolic_convert: [DEBUG] break_graph_if_unsupported triggered compile
[2023-10-23 14:05:10,051] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: call_method SetVariable() __contains__ [UnspecializedNNModuleVariable(Linear)] {} from user code at:
[2023-10-23 14:05:10,051] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File ""/home/jonch/Desktop/sdpa.py"", line 911, in <resume in train>
[2023-10-23 14:05:10,051] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     m = M(linear, encode=encode)
[2023-10-23 14:05:10,051] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File ""/home/jonch/Desktop/sdpa.py"", line 893, in __init__
[2023-10-23 14:05:10,051] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     self.linear.requires_grad_(False)
[2023-10-23 14:05:10,051] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File ""/home/jonch/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 2439, in requires_grad_
[2023-10-23 14:05:10,051] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     for p in self.parameters():
[2023-10-23 14:05:10,051] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File ""/home/jonch/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 2192, in parameters
[2023-10-23 14:05:10,051] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     for name, param in self.named_parameters(recurse=recurse):
[2023-10-23 14:05:10,051] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File ""/home/jonch/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 2223, in named_parameters
[2023-10-23 14:05:10,051] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     gen = self._named_members(
[2023-10-23 14:05:10,051] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File ""/home/jonch/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 2159, in _named_members
[2023-10-23 14:05:10,051] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     modules = self.named_modules(prefix=prefix, remove_duplicate=remove_duplicate) if recurse else [(prefix, self)]
[2023-10-23 14:05:10,051] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File ""/home/jonch/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 2369, in named_modules
[2023-10-23 14:05:10,051] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     if self not in memo:
```

### Versions

main

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng"
111834,"Numerical inaccuracies in ""ddp_apply_optim_in_backward"" unit tests","### üêõ Describe the bug

After some experiments in https://github.com/pytorch/pytorch/pull/111791 I have replicated an accuracy issue with the gloo backend relating to ddp models using ""apply_optim_in_backwards"" instead of .step() on the CI. This occurs both for CUDA and ROCm.

There are already unit tests in https://github.com/pytorch/pytorch/blob/main/torch/testing/_internal/distributed/distributed_test.py that track this behavior but I have found that torchvision is not present in the distributed CI job causing these tests to only run for a simple linear model in which the bug is not present.

This can be replicated with TOT PyTorch with the following unit test (as long as torchvision is installed):
```
BACKEND=gloo WORLD_SIZE=2 PYTORCH_TEST_WITH_ROCM=1 HIP_VISIBLE_DEVICES=0,1 python3 test/distributed/test_distributed_spawn.py TestDistBackendWithSpawn.test_ddp_apply_optim_in_backward_grad_as_bucket_view_false
```



### Versions

This can be replicated with both CUDA and ROCm CI environments in the distributed workflow if we modify the job to install torchvision as seen here:
https://hud.pytorch.org/pr/111791

cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @ezyang @albanD @zou3519 @pearu @nikitaved @soulitzer @Lezcano @Varal7 @vincentqb @jbschlosser @janeyx99 @crcrpar"
111830,[aotinductor]14k models: AttributeError: 'int' object has no attribute 'device',"```
python main.py --compile_mode aot_inductor -e ./generated/test_chengchunhsu_EveryPixelMatters.py:FCOSDiscriminator
```

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111826,DISABLED test_nested_tensor_indexing_cpu_float16 (__main__.TestNestedTensorDeviceTypeCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_nested_tensor_indexing_cpu_float16&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17964762574).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_nested_tensor_indexing_cpu_float16`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

ResponseTimeoutError: Response timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/main/test/test_nestedtensor.py 200 (connected: true, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 1)
headers: {""connection"":""keep-alive"",""content-length"":""138378"",""cache-control"":""max-age=300"",""content-security-policy"":""default-src 'none'; style-src 'unsafe-inline'; sandbox"",""content-type"":""text/plain; charset=utf-8"",""etag"":""\""70598adce2f7ea59edca2428ad327477a96eef2dedf5f0a03a7d6652f963cacf\"""",""strict-transport-security"":""max-age=31536000"",""x-content-type-options"":""nosniff"",""x-frame-options"":""deny"",""x-xss-protection"":""1; mode=block"",""x-github-request-id"":""B9FA:1CF1:134794:17BEC8:6536BDE5"",""accept-ranges"":""bytes"",""date"":""Mon, 23 Oct 2023 18:39:34 GMT"",""via"":""1.1 varnish"",""x-served-by"":""cache-sjc1000104-SJC"",""x-cache"":""MISS"",""x-cache-hits"":""0"",""x-timer"":""S1698086374.139982,VS0,VE192"",""vary"":""Authorization,Accept-Encoding,Origin"",""access-control-allow-origin"":""*"",""cross-origin-resource-policy"":""cross-origin"",""x-fastly-request-id"":""1296da4d8db7f56738313a7143ecc245638da458"",""expires"":""Mon, 23 Oct 2023 18:44:34 GMT"",""source-age"":""0""}

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111825,DISABLED test_meta_outplace_fft_ihfft2_cpu_int16 (__main__.TestMetaCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_outplace_fft_ihfft2_cpu_int16&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17964762574).

Over the past 3 hours, it has been determined flaky in 8 workflow(s) with 24 failures and 8 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_outplace_fft_ihfft2_cpu_int16`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

ResponseTimeoutError: Response timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/main/test/test_meta.py -1 (connected: true, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 0)
headers: {}

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111824,GroupNorm & InstanceNorm does not handle channels_last correctly,"### üêõ Describe the bug

GroupNorm does not return channels_last tensor.

```python
norm = nn.GroupNorm(8, 32).to(device, memory_format=torch.channels_last)
x = torch.randn([4, 32, 24, 24], device=device).to(memory_format=torch.channels_last)

print(x.stride())
assert x.is_contiguous(memory_format=torch.channels_last) # Pass

y = norm(x)

print(y.stride())
assert y.is_contiguous(memory_format=torch.channels_last) # Fail
```

I had to implement groupnorm manually. Somehow this works great and fast.

```python
class GroupNorm(nn.GroupNorm):
    def forward(self, x):
        dtype = x.dtype
        x = x.float()
        x = rearrange(x, ""b (g c) h w -> b g c h w"", g=self.num_groups)

        mean = x.mean(dim=[2,3,4], keepdim=True)
        var = x.var(dim=[2,3,4], keepdim=True)

        x = (x - mean) * (var + self.eps).rsqrt()
        x = rearrange(x, ""b g c h w -> b (g c) h w"")

        if self.affine:
            weight = rearrange(self.weight, ""c -> 1 c 1 1"")
            bias = rearrange(self.bias, ""c -> 1 c 1 1"")
            x = x * weight + bias

        x = x.type(dtype)
        return x
```



### Versions

Collecting environment information...
PyTorch version: 2.1.0+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Debian GNU/Linux 11 (bullseye) (x86_64)
GCC version: (Debian 10.2.1-6) 10.2.1 20210110
Clang version: Could not collect
CMake version: version 3.18.4
Libc version: glibc-2.31

Python version: 3.9.2 (default, Feb 28 2021, 17:03:44)  [GCC 10.2.1 20210110] (64-bit runtime)
Python platform: Linux-5.4.56.bsk.11-amd64-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: 12.2.140
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA A100-SXM4-80GB
Nvidia driver version: 470.129.06
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.4
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   46 bits physical, 48 bits virtual
CPU(s):                          96
On-line CPU(s) list:             0-95
Thread(s) per core:              2
Core(s) per socket:              24
Socket(s):                       2
NUMA node(s):                    2
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           85
Model name:                      Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
Stepping:                        7
CPU MHz:                         3599.940
BogoMIPS:                        5999.99
Hypervisor vendor:               KVM
Virtualization type:             full
L1d cache:                       1.5 MiB
L1i cache:                       1.5 MiB
L2 cache:                        48 MiB
L3 cache:                        71.5 MiB
NUMA node0 CPU(s):               0-23,48-71
NUMA node1 CPU(s):               24-47,72-95
Vulnerability Itlb multihit:     KVM: Vulnerable
Vulnerability L1tf:              Mitigation; PTE Inversion
Vulnerability Mds:               Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown
Vulnerability Meltdown:          Vulnerable
Vulnerability Spec store bypass: Vulnerable
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Vulnerable, STIBP: disabled
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke

Versions of relevant libraries:
[pip3] byted-torch==2.1.0.post0
[pip3] byted-torch-monitor==0.0.1
[pip3] numpy==1.26.1
[pip3] torch==2.1.0
[pip3] torchaudio==2.1.0+cu121
[pip3] torchvision==0.16.0
[pip3] triton==2.1.0
[conda] Could not collect

cc @albanD @mruberry @jbschlosser @walterddr @mikaylagawarecki"
111819,__slots__ + inheriting from torch.Tensor,"# Sumary
Inheriting from torch.tensor causes the attribute checking from slots to not raise anymore 

``` Python
class Example():
    __slots__ = ['a', 'b']

class Example_2(torch.Tensor):
    __slots__ = ['a', 'b']

def main():
    ex1 = Example()
    ex2 = Example_2()
    except_raised = False
    try:
        ex1.c = 1
    except AttributeError:
        except_raised = True
    assert except_raised
    # No exception raised
    ex2.c = 1
  
main()
```

I imagine it is because torch.Tensor creates a __dict__ and we inherit this. I am curious if it is best practice to do something like this on the subclass, or would this cause other things to not work?

``` Python
    def __setattr__(self, name, value):
        if name not in self.__slots__:
            raise AttributeError(f""'{type(self).__name__}' object has no attribute '{name}'"")
        super().__setattr__(name, value)

    def __getattr__(self, name):
        if name not in self.__slots__:
            raise AttributeError(f""'{type(self).__name__}' object has no attribute '{name}'"")
        return super().__getattr__(name)
        ```

cc @ezyang @msaroufim @albanD"
111818,[aotinductor] 14k models: AssertionError,"Repro:
```
python main.py --compile_mode aot_inductor -e  ./generated/test_ruotianluo_self_critical_pytorch.py:SublayerConnection
```

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111813,[aotinductor]14k models: RuntimeError: t == DeviceType::CUDA INTERNAL,"Repro:
```
python main.py --compile_mode export -e ./generated/test_Vious_LBAM_Pytorch.py:GaussActivation
```

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111809,torch.utils.flop_counter modifies model signatures when it shouldn't ,"### üêõ Describe the bug

`torch.utils.flop_counter` has hooks that can undesirably change some model signatures, which in turn can lead to errors. This happens when a call returns a tuple with only one element. When using the flop counter, the call instead starts to return the element itself instead of a tuple, which can cause downstream errors.

A simple example is shown below (need to `pip install open_clip_torch` to run it). The code below (without the flop counter) works fine.

```python
import open_clip
import torch
model = open_clip.create_model('xlm-roberta-base-ViT-B-32', force_custom_text=True, pretrained_hf=False).text
example_input = torch.ones((1, 514), dtype=torch.int64)
model(example_input)
```

When adding the flop counter, the code now fails.

```python
import open_clip
import torch
from torch.utils.flop_counter import FlopCounterMode

model = open_clip.create_model('xlm-roberta-base-ViT-B-32', force_custom_text=True, pretrained_hf=False).text
example_input = torch.ones((1, 514), dtype=torch.int64)
flop_counter = FlopCounterMode(model)
with flop_counter:
    model(example_input)
```

Error:

```
Module                           FLOP    % Total
--------------------------  ---------  ---------
HFTextEncoder               8087.679M    100.00%
 - aten.addmm               7276.069M     89.96%
 - aten.bmm                  811.610M     10.04%
 HFTextEncoder.transformer  8087.679M    100.00%
  - aten.addmm              7276.069M     89.96%
  - aten.bmm                 811.610M     10.04%
Traceback (most recent call last):
  File ""/home/open_clip/test.py"", line 17, in <module>
    model(example_input)
  File ""/home/miniconda3/envs/oc/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/miniconda3/envs/oc/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File ""/home/open_clip/src/open_clip/hf_model.py"", line 156, in forward
    out = self.transformer(input_ids=x, attention_mask=attn_mask)
  File ""/home/miniconda3/envs/oc/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/miniconda3/envs/oc/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File ""/home/miniconda3/envs/oc/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py"", line 850, in forward
    encoder_outputs = self.encoder(
  File ""/home/miniconda3/envs/oc/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/miniconda3/envs/oc/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File ""/home/miniconda3/envs/oc/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py"", line 534, in forward
    layer_outputs = layer_module(
  File ""/home/miniconda3/envs/oc/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/miniconda3/envs/oc/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File ""/home/miniconda3/envs/oc/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py"", line 463, in forward
    outputs = (layer_output,) + outputs
TypeError: can only concatenate tuple (not ""Tensor"") to tuple
```


I believe this is because of how the code currently normalizes outputs to tuples, and then attempts to reverse this at https://github.com/pytorch/pytorch/blob/f7401de1bbf29ac488aac6a2c472c79353a7533d/torch/utils/flop_counter.py#L334. However, we should only reverse this if the original signature wasn't a tuple with one element.  

### Versions

```
PyTorch version: 2.1.0+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.10.8 (main, Nov  4 2022, 13:48:29) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-1037-aws-x86_64-with-glibc2.31
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   46 bits physical, 48 bits virtual
CPU(s):                          36
On-line CPU(s) list:             0-35
Thread(s) per core:              2
Core(s) per socket:              18
Socket(s):                       1
NUMA node(s):                    1
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           85
Model name:                      Intel(R) Xeon(R) Platinum 8124M CPU @ 3.00GHz
Stepping:                        4
CPU MHz:                         3241.206
BogoMIPS:                        6000.01
Hypervisor vendor:               KVM
Virtualization type:             full
L1d cache:                       576 KiB
L1i cache:                       576 KiB
L2 cache:                        18 MiB
L3 cache:                        24.8 MiB
NUMA node0 CPU(s):               0-35
Vulnerability Itlb multihit:     KVM: Mitigation: VMX unsupported
Vulnerability L1tf:              Mitigation; PTE Inversion
Vulnerability Mds:               Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown
Vulnerability Meltdown:          Mitigation; PTI
Vulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown
Vulnerability Retbleed:          Vulnerable
Vulnerability Spec store bypass: Vulnerable
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Retpolines, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke

Versions of relevant libraries:
[pip3] numpy==1.23.4
[pip3] open-clip-torch==2.22.0
[pip3] torch==2.1.0
[pip3] torchvision==0.16.0
[pip3] triton==2.1.0
[conda] numpy                     1.23.4                   pypi_0    pypi
[conda] open-clip-torch           2.22.0                    dev_0    <develop>
[conda] torch                     2.1.0                    pypi_0    pypi
[conda] torchvision               0.16.0                   pypi_0    pypi
[conda] triton                    2.1.0                    pypi_0    pypi
```

cc @robieta @chaekit @aaronenyeshi @nbcsm @guotuofeng @guyang3532 @gaoteng-git @tiffzhaofb @dzhulgakov @davidberard98"
111806,Revisit security implications of #31875,"### üêõ Describe the bug

https://github.com/pytorch/pytorch/issues/31875 was closed by adding a warning to the documentation of `load`.

This is unhelpful because by the time you come to load the model, it's too late. Your choice is either run the binary blob or... not use the model at all.

I would like to suggest moving to a default serialization format which does not rely on arbitrary code execution. Ideally the format would also be based on some standard so that it could be easily read by other tools.

A `load_insecure()` function could be retained for backwards compatibility with older models, but the important thing is that new models be saved to a safer format by default.

I did notice that the docs for `torch.save()` mention a new ""zip-file based serialization format"". However, it is not documented whether this new format is also vulnerable to RCE. Furthermore, there doesn't appear to be an option in `pytorch.load()` to reject the older *definitely insecure* format.

### Versions

N/A"
111804,Dynamo - more closely tracker class type in UserDefinedObjectVariable,"### üêõ Describe the bug

Ideally we shouldn't need the `or ConstantVariable ...` part in here https://github.com/pytorch/pytorch/pull/110794/files#diff-4a52059570bb96333d8383ce6a9d01bbb114c5e34aff6028f820899ca39b5a26R214. That code makes userdefinedobjectvariables possibly slightly inaccurate.

We should try removing this and see how it goes.

### Error logs

_No response_

### Minified repro

_No response_

### Versions

main

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng"
111799,DISABLED test_meta_outplace_fft_ihfft2_cpu_float64 (__main__.TestMetaCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_outplace_fft_ihfft2_cpu_float64&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17950794814).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_outplace_fft_ihfft2_cpu_float64`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

ResponseTimeoutError: Response timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/main/test/test_meta.py 200 (connected: true, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 1)
headers: {""connection"":""keep-alive"",""content-length"":""64727"",""cache-control"":""max-age=300"",""content-security-policy"":""default-src 'none'; style-src 'unsafe-inline'; sandbox"",""content-type"":""text/plain; charset=utf-8"",""etag"":""\""334feb635e18b4862f10cbe65cc0f11826a4857dca465b01cd88aa57042f09b3\"""",""strict-transport-security"":""max-age=31536000"",""x-content-type-options"":""nosniff"",""x-frame-options"":""deny"",""x-xss-protection"":""1; mode=block"",""x-github-request-id"":""BBDA:3DA5:607F1D:79FDEE:65366B04"",""accept-ranges"":""bytes"",""date"":""Mon, 23 Oct 2023 12:45:58 GMT"",""via"":""1.1 varnish"",""x-served-by"":""cache-sjc10026-SJC"",""x-cache"":""MISS"",""x-cache-hits"":""0"",""x-timer"":""S1698065158.156407,VS0,VE246"",""vary"":""Authorization,Accept-Encoding,Origin"",""access-control-allow-origin"":""*"",""cross-origin-resource-policy"":""cross-origin"",""x-fastly-request-id"":""ce7cd4de92420cdebf75036b1eacb3dced3b5267"",""expires"":""Mon, 23 Oct 2023 12:50:58 GMT"",""source-age"":""0""}

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111794,torch2.1.0 compile+amp+ddp cause NotImplementedError,"### üêõ Describe the bug

When I try to compile the ddp + amp model using torch.compile, I get the following errorÔºö

```
NotImplementedError: argument of type: <class 'torch.amp.autocast_mode.autocast'>
```

Can you help me with this error?


### Error logs

```
eager train time 0: 2.062772216796875 loss: 12.34566593170166
eager train time 29: 0.12792012786865234 loss: 12.378190040588379
~~~~~~~~~~

[rank0]:[2023-10-20 15:17:41,173] [1/0] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
Traceback (most recent call last):
  File ""elastic_ddp.py"", line 347, in <module>
    demo_basic()
  File ""elastic_ddp.py"", line 330, in demo_basic
    loss, compile_time = timed(
  File ""elastic_ddp.py"", line 288, in timed
    result = fn()
  File ""elastic_ddp.py"", line 331, in <lambda>
    lambda: train_opt(ddp_model, inp, opt, grad_scaler))
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py"", line 328, in _fn
    return fn(*args, **kwargs)
  File ""elastic_ddp.py"", line 267, in train
    opt.zero_grad(True)
  File ""elastic_ddp.py"", line 269, in <resume in train>
    predict = model(data[0])
  File ""/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py"", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py"", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File ""/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py"", line 487, in catch_errors
    return hijacked_callback(frame, cache_entry, hooks, frame_state)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py"", line 641, in _convert_frame
    result = inner_convert(frame, cache_size, hooks, frame_state)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py"", line 133, in _fn
    return fn(*args, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py"", line 389, in _convert_frame_assert
    return _compile(
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py"", line 569, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/utils.py"", line 189, in time_wrapper
    r = func(*args, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py"", line 491, in compile_inner
    out_code = transform_code_object(code, transform)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/bytecode_transformation.py"", line 1028, in transform_code_object
    transformations(instructions, code_options)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py"", line 458, in transform
    tracer.run()
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py"", line 2074, in run
    super().run()
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py"", line 724, in run
    and self.step()
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py"", line 688, in step
    getattr(self, inst.opname)(inst)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py"", line 2162, in RETURN_VALUE
    self.output.compile_subgraph(
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/output_graph.py"", line 833, in compile_subgraph
    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
  File ""/opt/conda/lib/python3.8/contextlib.py"", line 75, in inner
    return func(*args, **kwds)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/output_graph.py"", line 957, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/utils.py"", line 189, in time_wrapper
    r = func(*args, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/output_graph.py"", line 1024, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/output_graph.py"", line 1009, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/backends/distributed.py"", line 436, in compile_fn
    submod_compiler.run(*example_inputs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/fx/interpreter.py"", line 138, in run
    self.env[node] = self.run_node(node)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/backends/distributed.py"", line 417, in run_node
    compiled_submod_real = self.compile_submod(
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/backends/distributed.py"", line 361, in compile_submod
    self.compiler(input_mod, args),
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/repro/after_dynamo.py"", line 117, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/__init__.py"", line 1568, in __call__
    return compile_fx(model_, inputs_, config_patches=self.config)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_inductor/compile_fx.py"", line 961, in compile_fx
    return compile_fx(
  File ""/opt/conda/lib/python3.8/site-packages/torch/_inductor/compile_fx.py"", line 1150, in compile_fx
    return aot_autograd(
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/backends/common.py"", line 55, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py"", line 3891, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/utils.py"", line 189, in time_wrapper
    r = func(*args, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py"", line 3429, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py"", line 2212, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py"", line 2392, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py"", line 2804, in aot_dispatch_autograd
    fx_g = aot_dispatch_autograd_graph(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py"", line 2781, in aot_dispatch_autograd_graph
    fx_g = create_functionalized_graph(
  File ""/opt/conda/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py"", line 1420, in create_functionalized_graph
    fx_g = make_fx(helper, decomposition_table=aot_config.decompositions)(*args)
  File ""/opt/conda/lib/python3.8/site-packages/torch/fx/experimental/proxy_tensor.py"", line 809, in wrapped
    t = dispatch_trace(wrap_key(func, args, fx_tracer, pre_dispatch), tracer=fx_tracer, concrete_args=tuple(phs))
  File ""/opt/conda/lib/python3.8/site-packages/torch/_compile.py"", line 24, in inner
    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py"", line 328, in _fn
    return fn(*args, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/external_utils.py"", line 17, in inner
    return fn(*args, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/fx/experimental/proxy_tensor.py"", line 468, in dispatch_trace
    graph = tracer.trace(root, concrete_args)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py"", line 328, in _fn
    return fn(*args, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/_dynamo/external_utils.py"", line 17, in inner
    return fn(*args, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/fx/_symbolic_trace.py"", line 817, in trace
    (self.create_arg(fn(*args)),),
  File ""/opt/conda/lib/python3.8/site-packages/torch/fx/experimental/proxy_tensor.py"", line 451, in create_arg
    return super().create_arg(a)
  File ""/opt/conda/lib/python3.8/site-packages/torch/fx/_symbolic_trace.py"", line 385, in create_arg
    return super().create_arg(a)
  File ""/opt/conda/lib/python3.8/site-packages/torch/fx/proxy.py"", line 255, in create_arg
    return type(a)(self.create_arg(elem) for elem in a)
  File ""/opt/conda/lib/python3.8/site-packages/torch/fx/proxy.py"", line 255, in <genexpr>
    return type(a)(self.create_arg(elem) for elem in a)
  File ""/opt/conda/lib/python3.8/site-packages/torch/fx/experimental/proxy_tensor.py"", line 451, in create_arg
    return super().create_arg(a)
  File ""/opt/conda/lib/python3.8/site-packages/torch/fx/_symbolic_trace.py"", line 385, in create_arg
    return super().create_arg(a)
  File ""/opt/conda/lib/python3.8/site-packages/torch/fx/proxy.py"", line 291, in create_arg
    raise NotImplementedError(f""argument of type: {type(a)}"")
torch._dynamo.exc.BackendCompilerFailed: backend='compile_fn' raised:
NotImplementedError: argument of type: <class 'torch.amp.autocast_mode.autocast'>

While executing %submod_0 : [num_users=3] = call_module[target=submod_0](args = (%l_x_,), kwargs = {})
Original traceback:
None

Set TORCH_LOGS=""+dynamo"" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

233522-10ea61f2-6f10-11ee-9682-4ee5ea5d12cf-6mprp:6107:6116 [0] NCCL INFO [Service thread] Connection closed by localRank 0
233522-10ea61f2-6f10-11ee-9682-4ee5ea5d12cf-6mprp:6107:6107 [0] NCCL INFO comm 0x55b1609f2ad0 rank 0 nranks 1 cudaDev 0 busId b1000 - Abort COMPLETE

```

### Minified repro

The code for `elastic_ddp.py` is shown below.

```python
# coding=utf-8
import numpy as np
import torch
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn as nn
import torch.optim as optim
from torch.nn.parallel import DistributedDataParallel as DDP
from torchvision.models import resnet18, resnet152

using_ckpt = False


def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):
    """"""3x3 convolution with padding""""""
    return nn.Conv2d(in_planes,
                     out_planes,
                     kernel_size=3,
                     stride=stride,
                     padding=dilation,
                     groups=groups,
                     bias=False,
                     dilation=dilation)


def conv1x1(in_planes, out_planes, stride=1):
    """"""1x1 convolution""""""
    return nn.Conv2d(in_planes,
                     out_planes,
                     kernel_size=1,
                     stride=stride,
                     bias=False)


class IBasicBlock(nn.Module):
    expansion = 1

    def __init__(self,
                 inplanes,
                 planes,
                 stride=1,
                 downsample=None,
                 groups=1,
                 base_width=64,
                 dilation=1):
        super(IBasicBlock, self).__init__()
        if groups != 1 or base_width != 64:
            raise ValueError(
                'BasicBlock only supports groups=1 and base_width=64')
        if dilation > 1:
            raise NotImplementedError(
                ""Dilation > 1 not supported in BasicBlock"")
        self.bn1 = nn.BatchNorm2d(
            inplanes,
            eps=1e-05,
        )
        self.conv1 = conv3x3(inplanes, planes)
        self.bn2 = nn.BatchNorm2d(
            planes,
            eps=1e-05,
        )
        self.prelu = nn.PReLU(planes)
        self.conv2 = conv3x3(planes, planes, stride)
        self.bn3 = nn.BatchNorm2d(
            planes,
            eps=1e-05,
        )
        self.downsample = downsample
        self.stride = stride

    def forward_impl(self, x):
        identity = x
        out = self.bn1(x)
        out = self.conv1(out)
        out = self.bn2(out)
        out = self.prelu(out)
        out = self.conv2(out)
        out = self.bn3(out)
        if self.downsample is not None:
            identity = self.downsample(x)
        out += identity
        return out

    def forward(self, x):
        if self.training and using_ckpt:
            return checkpoint(self.forward_impl, x)
        else:
            return self.forward_impl(x)


class IResNet(nn.Module):
    fc_scale = 7 * 7

    def __init__(self,
                 block,
                 layers,
                 dropout=0,
                 num_features=512,
                 zero_init_residual=False,
                 groups=1,
                 width_per_group=64,
                 replace_stride_with_dilation=None,
                 fp16=False):
        super(IResNet, self).__init__()
        self.extra_gflops = 0.0
        self.fp16 = fp16
        self.inplanes = 64
        self.dilation = 1
        if replace_stride_with_dilation is None:
            replace_stride_with_dilation = [False, False, False]
        if len(replace_stride_with_dilation) != 3:
            raise ValueError(""replace_stride_with_dilation should be None ""
                             ""or a 3-element tuple, got {}"".format(
                                 replace_stride_with_dilation))
        self.groups = groups
        self.base_width = width_per_group
        self.conv1 = nn.Conv2d(3,
                               self.inplanes,
                               kernel_size=3,
                               stride=1,
                               padding=1,
                               bias=False)
        self.bn1 = nn.BatchNorm2d(self.inplanes, eps=1e-05)
        self.prelu = nn.PReLU(self.inplanes)
        self.layer1 = self._make_layer(block, 64, layers[0], stride=2)
        self.layer2 = self._make_layer(block,
                                       128,
                                       layers[1],
                                       stride=2,
                                       dilate=replace_stride_with_dilation[0])
        self.layer3 = self._make_layer(block,
                                       256,
                                       layers[2],
                                       stride=2,
                                       dilate=replace_stride_with_dilation[1])
        self.layer4 = self._make_layer(block,
                                       512,
                                       layers[3],
                                       stride=2,
                                       dilate=replace_stride_with_dilation[2])
        self.bn2 = nn.BatchNorm2d(
            512 * block.expansion,
            eps=1e-05,
        )
        self.dropout = nn.Dropout(p=dropout, inplace=True)
        self.fc = nn.Linear(512 * block.expansion * self.fc_scale, num_features)
        self.features = nn.BatchNorm1d(num_features, eps=1e-05)
        nn.init.constant_(self.features.weight, 1.0)
        self.features.weight.requires_grad = False

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.normal_(m.weight, 0, 0.1)
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

        if zero_init_residual:
            for m in self.modules():
                if isinstance(m, IBasicBlock):
                    nn.init.constant_(m.bn2.weight, 0)

    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):
        downsample = None
        previous_dilation = self.dilation
        if dilate:
            self.dilation *= stride
            stride = 1
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                conv1x1(self.inplanes, planes * block.expansion, stride),
                nn.BatchNorm2d(
                    planes * block.expansion,
                    eps=1e-05,
                ),
            )
        layers = []
        layers.append(
            block(self.inplanes, planes, stride, downsample, self.groups,
                  self.base_width, previous_dilation))
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(
                block(self.inplanes,
                      planes,
                      groups=self.groups,
                      base_width=self.base_width,
                      dilation=self.dilation))

        return nn.Sequential(*layers)

    def forward(self, x):
        with torch.cuda.amp.autocast(True):
            x = self.conv1(x)
            x = self.bn1(x)
            x = self.prelu(x)
            x = self.layer1(x)
            x = self.layer2(x)
            x = self.layer3(x)
            x = self.layer4(x)
            x = self.bn2(x)
            x = torch.flatten(x, 1)
            x = self.dropout(x)
        x = self.fc(x.float())
        x = self.features(x)
        return x


def _iresnet(arch, block, layers, pretrained, progress, **kwargs):
    model = IResNet(block, layers, **kwargs)
    if pretrained:
        raise ValueError()
    return model


def iresnet18(pretrained=False, progress=True, **kwargs):
    return _iresnet('iresnet18', IBasicBlock, [2, 2, 2, 2], pretrained,
                    progress, **kwargs)


def iresnet34(pretrained=False, progress=True, **kwargs):
    return _iresnet('iresnet34', IBasicBlock, [3, 4, 6, 3], pretrained,
                    progress, **kwargs)


def iresnet50(pretrained=False, progress=True, **kwargs):
    return _iresnet('iresnet50', IBasicBlock, [3, 4, 14, 3], pretrained,
                    progress, **kwargs)


def iresnet100(pretrained=False, progress=True, **kwargs):
    return _iresnet('iresnet100', IBasicBlock, [3, 13, 30, 3], pretrained,
                    progress, **kwargs)


def iresnet200(pretrained=False, progress=True, **kwargs):
    return _iresnet('iresnet200', IBasicBlock, [6, 26, 60, 6], pretrained,
                    progress, **kwargs)


def generate_data(b, device_id=0):
    channel = 3
    height = 112
    width = 112
    return (
        torch.randn(b, channel, height, width).to(torch.float32).cuda(),
        torch.randint(256, (b,)).cuda(device_id),
    )


def init_model(amp=False, device_id=0):
    # return resnet152(num_classes=256).to(torch.float32).cuda(device_id)
    return iresnet200(num_features=256).cuda(device_id)


def train(model, data, opt, grad_scaler):
    opt.zero_grad(True)
    # with torch.autocast(device_type=""cuda"", enabled=True):
    predict = model(data[0])
    loss = torch.nn.CrossEntropyLoss()(predict, data[1])
    loss = grad_scaler.scale(loss)
    loss.backward()
    opt.step()
    return loss


def eval(model, data):
    with torch.cuda.amp.autocast(enabled=True):
        predict = model(data[0])
        loss = torch.nn.CrossEntropyLoss()(predict, data[1])
    return loss


def timed(fn):
    start = torch.cuda.Event(enable_timing=True)
    end = torch.cuda.Event(enable_timing=True)
    start.record()
    result = fn()
    end.record()
    torch.cuda.synchronize()
    return result, start.elapsed_time(end) / 1000


def demo_basic():
    N_ITERS = 30
    dist.init_process_group(""nccl"")
    rank = dist.get_rank()
    print(f""Start running basic DDP example on rank {rank}."")

    # create model and move it to GPU with id rank
    device_id = rank % torch.cuda.device_count()
    # model = ToyModel().to(device_id)
    dynamo.reset()
    model = init_model(device_id=device_id)
    ddp_model = DDP(model, device_ids=[device_id])

    opt = optim.SGD(ddp_model.parameters(), lr=0.001)
    # data = generate_data(16, device_id=device_id)
    grad_scaler = torch.cuda.amp.GradScaler(init_scale=2.0)
    eager_times = []
    for i in range(N_ITERS):
        inp = generate_data(16)
        loss, eager_time = timed(
            lambda: train(ddp_model, inp, opt, grad_scaler))
        eager_times.append(eager_time)
        print(f""eager train time {i}: {eager_time}"", ""loss:"", loss.item())
    print(""~"" * 10)
    dynamo.reset()
    model = init_model(device_id=device_id)
    ddp_model = DDP(model, device_ids=[device_id])
    opt = optim.SGD(ddp_model.parameters(), lr=0.001)
    grad_scaler = torch.cuda.amp.GradScaler(init_scale=2.0)
    train_opt = torch.compile(train,
                              options={""triton.cudagraphs"": True},
                              backend=""inductor"")

    compile_times = []
    for i in range(N_ITERS):
        inp = generate_data(16)
        loss, compile_time = timed(
            lambda: train_opt(ddp_model, inp, opt, grad_scaler))
        compile_times.append(compile_time)
        print(f""compile train time {i}: {compile_time}"", ""loss:"", loss.item())
    print(""~"" * 10)

    eager_med = np.median(eager_times)
    compile_med = np.median(compile_times)
    speedup = eager_med / compile_med
    print(
        f""(train) eager median: {eager_med}, compile median: {compile_med}, speedup: {speedup}x""
    )
    print(""~"" * 10)
    dist.destroy_process_group()


if __name__ == ""__main__"":
    demo_basic()

```

The running commands are as follows:


```shell
torchrun --nnodes=1 --nproc_per_node=1 --node_rank=0 --master_addr=127.0.0.1 elastic_ddp.py
```


### Versions

```
Collecting environment information...
PyTorch version: 2.1.0+cu118
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.2 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.22.1
Libc version: glibc-2.31

Python version: 3.8.12 (default, Oct 12 2021, 13:49:34)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-3.10.0-1062.el7.x86_64-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.6.124
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: Tesla V100-SXM2-32GB
Nvidia driver version: 510.47.03
cuDNN version: Probably one of the following:
/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn.so.8.4.0
/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.4.0
/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.4.0
/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.4.0
/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.4.0
/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.4.0
/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.4.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:        x86_64
CPU op-mode(s):      32-bit, 64-bit
Byte Order:          Little Endian
Address sizes:       46 bits physical, 48 bits virtual
CPU(s):              96
On-line CPU(s) list: 0-95
Thread(s) per core:  2
Core(s) per socket:  24
Socket(s):           2
NUMA node(s):        2
Vendor ID:           GenuineIntel
CPU family:          6
Model:               85
Model name:          Intel(R) Xeon(R) Gold 6240R CPU @ 2.40GHz
Stepping:            7
CPU MHz:             3170.947
CPU max MHz:         4000.0000
CPU min MHz:         1000.0000
BogoMIPS:            4800.00
Virtualization:      VT-x
L1d cache:           1.5 MiB
L1i cache:           1.5 MiB
L2 cache:            48 MiB
L3 cache:            71.5 MiB
NUMA node0 CPU(s):   0-23,48-71
NUMA node1 CPU(s):   24-47,72-95
Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_ppin intel_pt ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear spec_ctrl intel_stibp flush_l1d arch_capabilities

Versions of relevant libraries:
[pip3] numpy==1.22.4
[pip3] torch==2.1.0+cu118
[pip3] torchaudio==2.1.0+cu118
[pip3] torchdata==0.7.0
[pip3] torchtext==0.16.0+cpu
[pip3] torchvision==0.16.0+cu118
[pip3] triton==2.1.0
[conda] numpy                     1.22.4                   pypi_0    pypi
[conda] torch                     2.1.0+cu118              pypi_0    pypi
[conda] torchaudio                2.1.0+cu118              pypi_0    pypi
[conda] torchdata                 0.7.0                    pypi_0    pypi
[conda] torchtext                 0.16.0+cpu               pypi_0    pypi
[conda] torchvision               0.16.0+cu118             pypi_0    pypi
[conda] triton                    2.1.0                    pypi_0    pypi
```

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng"
111792,torch.jit.trace is not able to trace torch extension,"### üöÄ The feature, motivation and pitch

https://pytorch.org/tutorials/advanced/cpp_extension.html#integrating-a-c-cuda-operation-with-pytorch give a way to write torch extension, but those cpp_extension can not be traced into torchscript.

### Alternatives

_No response_

### Additional context

_No response_

cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel"
111790,DISABLED test_nested_tensor_chunk_cpu_float64 (__main__.TestNestedTensorDeviceTypeCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_nested_tensor_chunk_cpu_float64&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17948126395).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_nested_tensor_chunk_cpu_float64`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111789,Precisely monitor the collective communication tasks,"### üöÄ The feature, motivation and pitch

I have noticed that [https://github.com/pytorch/pytorch/pull/111072](https://github.com/pytorch/pytorch/pull/111072) has provided 'PG observability hooks' to obtain the time-consuming nature of collection communication. However, it seems that the CUDA event-based method for collecting communication time statistics is unable to capture the specific start and end times of the task. Additionally, if the task is not yet finished, users cannot obtain any information about it, including the start time and duration. Is it possible to use  ['cudaLaunchHostFunc'](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EXECUTION.html#group__CUDART__EXECUTION_1g05841eaa5f90f27124241baafb3e856f) to implement these new functionalities?

### Alternatives

_No response_

### Additional context

We aim to accurately monitor the collective communication operations during training.

cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @robieta @chaekit @aaronenyeshi @nbcsm @guotuofeng @guyang3532 @gaoteng-git @tiffzhaofb @dzhulgakov @davidberard98"
111786,pytorch support for cuda 12.2 ?,"### üöÄ The feature, motivation and pitch

.

### Alternatives

.

### Additional context

.

cc @seemethere @malfet @osalpekar @atalman @ptrblck"
111785,torch.compile precision bug when the attr object changes,"### üêõ Describe the bug

Hello, I ran into an accuracy issue, when using a recent version.  
I found that there is not accuracy issue when using the eager mode. And when I use ```torch.compile```  based on ```eager`` backend, this problem arises.The specific case to reproduce the problem is as follows.

cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @wconstab @bdhirsh @anijain2305 

### Test Case

```
import torch
class Model2(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.register_buffer(""test_buffer"", torch.zeros((4,6), dtype=torch.float32))
        self.cur_buffer = self.test_buffer
    def select_context_kv(self, idx:int):
        self.cur_buffer = self.test_buffer[idx : idx+1]
    def forward(self, src):
        self.cur_buffer.copy_(src)
def func(mode, src):
  for i in range(4):
      model.select_context_kv(i)
      src_in = src + i
      model_output = model(src_in)

      print(""      {} mode, result_{} test_buffer: "".format(mode, i))
      print(model.test_buffer)

src = torch.ones([1,6])
model = Model2()
func(""eager  "", src)
model = Model2()
model = torch.compile(model, backend=""eager"")
func(""compile"", src)
```

It is expected that the execution results of the two modes are the same, but the actual output is as follows:
```
   eager   mode, result_0 test_buffer:
tensor([[1., 1., 1., 1., 1., 1.],
        [0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0.]])
   eager   mode, result_1 test_buffer:
tensor([[1., 1., 1., 1., 1., 1.],
        [2., 2., 2., 2., 2., 2.],
        [0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0.]])
   eager   mode, result_2 test_buffer:
tensor([[1., 1., 1., 1., 1., 1.],
        [2., 2., 2., 2., 2., 2.],
        [3., 3., 3., 3., 3., 3.],
        [0., 0., 0., 0., 0., 0.]])
   eager   mode, result_3 test_buffer:
tensor([[1., 1., 1., 1., 1., 1.],
        [2., 2., 2., 2., 2., 2.],
        [3., 3., 3., 3., 3., 3.],
        [4., 4., 4., 4., 4., 4.]])
   compile mode, result_0 test_buffer:
tensor([[1., 1., 1., 1., 1., 1.],
        [0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0.]])
   compile mode, result_1 test_buffer:
tensor([[2., 2., 2., 2., 2., 2.],
        [0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0.]])
   compile mode, result_2 test_buffer:
tensor([[3., 3., 3., 3., 3., 3.],
        [0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0.]])
   compile mode, result_3 test_buffer:
tensor([[4., 4., 4., 4., 4., 4.],
        [0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0.]])

```


### Versions

[pip3] numpy==1.24.3
[pip3] torch==2.1.0+cpu
[pip3] triton==2.0.0
[conda] numpy                     1.24.3                   pypi_0    pypi
[conda] torch                     2.1.0+cpu                pypi_0    pypi
[conda] triton                    2.0.0                    pypi_0    pypi
"
111784,DISABLED test_meta_outplace_fft_ihfft2_cpu_float32 (__main__.TestMetaCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_outplace_fft_ihfft2_cpu_float32&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17946712088).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_outplace_fft_ihfft2_cpu_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

cc @ezyang @mruberry @Lezcano @peterbell10 @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111776,Add regex matching to Inductor all2all collective unit tests,"As part of https://github.com/pytorch/pytorch/pull/110195, we added support for tracing and compiling all2all collectives, specifically passing in a list of integers for `input_split_sizes` / `output_split_sizes` args in `torch.ops.c10d_functional.all_to_all_single()`. And we expect that in the Inductor generated code, we are passing symints like `i0`, `i1` to the `input_split_sizes` / `output_split_sizes` args of `all_to_all_single()`. Values of these symints are not hardcoded and instead are populated at runtime.

The above mechanism already works today. But current unit tests only check if ""all_to_all_single"" exists in the generated code:
```python
FileCheck() \
.check(""all_to_all_single"") \
.run(code)
```
while the actual generated code looks like:
```python
buf5_work = dist.all_to_all_single(buf5[0], buf5_inputs[0], output_split_sizes=[i14, i15], input_split_sizes=[i12, i13], group=buf5_pg, async_op=True)
```
(notice i14, i15, i12, i13 are the symints)

We want to update the unit test code to check that symints are indeed used in `output_split_sizes` and `input_split_sizes` args, to prevent functionality regression. We want to do something similar to:
```python
FileCheck() \
.check_regex(""all_to_all_single(buf${buf_id}[0], buf${buf_id}_inputs[0], output_split_sizes=[i${symint_id_0}, i${symint_id_1}], input_split_sizes=[i${symint_id_2}, i${symint_id_3}], "") \
.run(code)
```
where the actual value of buf_id / symint_id_0 / symint_id_1 / symint_id_2 / symint_id_3 can be anything. We can likely achieve this via some sort of regular expression matching.

### Alternatives

_No response_

### Additional context

_No response_

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler"
111769,DISABLED test_nested_tensor_chunk_cpu_float32 (__main__.TestNestedTensorDeviceTypeCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_nested_tensor_chunk_cpu_float32&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17941971978).

Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 9 failures and 3 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_nested_tensor_chunk_cpu_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

ResponseTimeoutError: Response timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/main/test/test_nestedtensor.py -1 (connected: true, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 0)
headers: {}

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111768,DISABLED test_meta_outplace_fft_ihfft2_cpu_bool (__main__.TestMetaCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_outplace_fft_ihfft2_cpu_bool&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17942194968).

Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 18 failures and 6 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_outplace_fft_ihfft2_cpu_bool`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

ResponseTimeoutError: Response timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/main/test/test_meta.py -1 (connected: true, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 0)
headers: {}

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111767,DISABLED test_list_inputs (__main__.ActivationCheckpointingViaTagsTests),"Platforms: linux

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_list_inputs&suite=ActivationCheckpointingViaTagsTests) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17942473710).

Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 9 failures and 3 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_list_inputs`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `dynamo/test_activation_checkpointing.py`

ResponseTimeoutError: Response timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/main/test/dynamo/test_activation_checkpointing.py -1 (connected: true, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 0)
headers: {}"
111764,"Segmentation fault in `torch.stft`, `cufftXtMakePlanMany`","### üêõ Describe the bug

I have not created a reproducing simple test case yet. This was in a larger project, and the same code also has run fine before, so maybe it's not really a PyTorch bug but I messed sth up with my environment. But a quick search did not reveal any similar crashes, so that's why I'm reporting it here.

In this training run, epoch 151 was loaded from an earlier checkpoint, so this happened very early after startup.

It seems quite deterministic. After I run it again in GDB, I got the crash exactly in the same step.

Log:
```
...
ep 151 train, step 151, ce 1.088, ctc_4 1.368, ctc_8 1.175, fer 0.234                                                   
ep 151 train, step 152, ce 1.359, ctc_4 1.829, ctc_8 1.635, fer 0.256                                                   
ep 151 train, step 153, ce 1.193, ctc_4 1.786, ctc_8 1.466, fer 0.242                                                   
ep 151 train, step 154, ce 1.120, ctc_4 1.587, ctc_8 1.355, fer 0.232                                                   
ep 151 train, step 155, ce 1.162, ctc_4 1.554, ctc_8 1.305, fer 0.220                                                   
ep 151 train, step 156, ce 1.398, ctc_4 1.901, ctc_8 1.710, fer 0.273                                                   
                                                                                                                        
Thread 1 ""python3.10"" received signal SIGSEGV, Segmentation fault.                                                      
0x00007fffbb1cb2f2 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1                                                 
(gdb) bt                                                                                                                
#0  0x00007fffbb1cb2f2 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1                                             
#1  0x00007fffdb0b935c in ?? ()                                                                                         
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/../../nvidia/cufft/lib/libcufft.so.10                    
#2  0x00007fffdae1b14f in ?? ()                                                                                         
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/../../nvidia/cufft/lib/libcufft.so.10                    
#3  0x00007fffdae43b75 in ?? ()                                                                                         
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/../../nvidia/cufft/lib/libcufft.so.10     
#4  0x00007fffdae468a7 in ?? ()                                                                                         
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/../../nvidia/cufft/lib/libcufft.so.10                    
#5  0x00007fffdae3344c in ?? ()                                                                                         
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/../../nvidia/cufft/lib/libcufft.so.10                    
#6  0x00007fffdae3519b in ?? ()                                                                                         
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/../../nvidia/cufft/lib/libcufft.so.10                    
#7  0x00007fffdae35ca4 in ?? ()                                                                                         
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/../../nvidia/cufft/lib/libcufft.so.10                    
#8  0x00007fffdae3ec25 in ?? ()                                                                                         
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/../../nvidia/cufft/lib/libcufft.so.10
#9  0x00007fffdae3f1e0 in ?? ()
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/../../nvidia/cufft/lib/libcufft.so.10
#10 0x00007fffdae7c6dd in ?? ()
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/../../nvidia/cufft/lib/libcufft.so.10
#11 0x00007fffdae7d38c in ?? ()
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/../../nvidia/cufft/lib/libcufft.so.10
#12 0x00007fffdae72196 in ?? ()
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/../../nvidia/cufft/lib/libcufft.so.10
#13 0x00007fffdae26d35 in ?? ()
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/../../nvidia/cufft/lib/libcufft.so.10
#14 0x00007fffdae6e855 in cufftXtMakePlanMany ()
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/../../nvidia/cufft/lib/libcufft.so.10
#15 0x00007fff7bd037b4 in at::native::detail::CuFFTConfig::CuFFTConfig(c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::detail::CuFFTTransformType, c10::ScalarType) ()
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so
#16 0x00007fff7bcfc69f in at::native::(anonymous namespace)::_exec_fft(at::Tensor&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, bool) ()
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so
#17 0x00007fff7bcfd45e in at::native::_fft_r2c_cufft(at::Tensor const&, c10::ArrayRef<long>, long, bool) ()
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so
#18 0x00007fff7d9fbe74 in at::(anonymous namespace)::(anonymous namespace)::wrapper_CUDA___fft_r2c(at::Tensor const&, c10::ArrayRef<long>, long, bool) ()
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so
#19 0x00007fff7d9fbefe in c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor (at::Tensor const&, c10::ArrayRef<long>, long, bool), &at::(anonymous namespace)::(anonymous namespace)::wrapper_CUDA___fft_r2c>, at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, c10::ArrayRef<long>, long, bool> >, at::Tensor (at::Tensor const&, c10::ArrayRef<long>, long, bool)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>, long, bool) ()
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so
#20 0x00007fffa383195e in at::_ops::_fft_r2c::redispatch(c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>, long, bool) ()
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so
#21 0x00007fffa512bb75 in torch::autograd::VariableType::(anonymous namespace)::_fft_r2c(c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>, long, bool) ()
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so
#22 0x00007fffa512c245 in c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor (c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>, long, bool), &torch::autograd::VariableType::(anonymous namespace)::_fft_r2c>, at::Tensor, c10::guts::typelist::typelist<c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>, long, bool> >, at::Tensor (c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>, long, bool)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>, long, bool) ()
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so
--Type <RET> for more, q to quit, c to continue without paging--
#23 0x00007fffa387b1e9 in at::_ops::_fft_r2c::call(at::Tensor const&, c10::ArrayRef<long>, long, bool) ()
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so
#24 0x00007fffa2e937c1 in at::native::stft(at::Tensor const&, long, c10::optional<long>, c10::optional<long>, c10::optional<at::Tensor> const&, bool, c10::basic_string_view<char>, bool, c10::optional<bool>, c10::optional<bool>) ()
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so
...
#30 0x00007fffba39d9e2 in torch::autograd::THPVariable_stft(_object*, _object*, _object*) ()                               
   from /u/zeyer/.local/lib/python3.10/site-packages/torch/lib/libtorch_python.so                                       
...
```

### Versions

The collect_env script failed for some reason:
```
Collecting environment information...
Traceback (most recent call last):
  File ""/u/zeyer/pytorch-collect-env.py"", line 612, in <module>
    main()
  File ""/u/zeyer/pytorch-collect-env.py"", line 595, in main
    output = get_pretty_env_info()
  File ""/u/zeyer/pytorch-collect-env.py"", line 590, in get_pretty_env_info
    return pretty_str(get_env_info())
  File ""/u/zeyer/pytorch-collect-env.py"", line 428, in get_env_info
    pip_version, pip_list_output = get_pip_packages(run_lambda)
  File ""/u/zeyer/pytorch-collect-env.py"", line 400, in get_pip_packages
    out = run_with_pip([sys.executable, '-mpip'])
  File ""/u/zeyer/pytorch-collect-env.py"", line 384, in run_with_pip
    for line in out.splitlines()
AttributeError: 'NoneType' object has no attribute 'splitlines'
```

Anyway, some manually collected info:

* PyTorch 2.0.1+cu117
* Ubuntu Linux 22.04
* NVIDIA GeForce GTX 1080 Ti

**Edit** Ok, it seems PIP was somehow broken in this environment. After fixing this, I get this output from the script:
```
Collecting environment information...                                                                                   
PyTorch version: 2.0.1+cu117                                
Is debug build: False                                       
CUDA used to build PyTorch: 11.7                       
ROCM used to build PyTorch: N/A                      
                                                            
OS: Ubuntu 22.04.2 LTS (x86_64)   
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: 15.0.7                                       
CMake version: version 3.26.3                                                                                           
Libc version: glibc-2.35                                                                                                
                                                                                                                        
Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-5.15.0-39-generic-x86_64-with-glibc2.35                
Is CUDA available: True                                                                                                 
CUDA runtime version: Could not collect                                                                                 
CUDA_MODULE_LOADING set to: LAZY                                                                                        
GPU models and configuration:                
GPU 0: NVIDIA GeForce GTX 1080 Ti                                                                                       
GPU 1: NVIDIA GeForce GTX 1080 Ti
GPU 2: NVIDIA GeForce GTX 1080 Ti
GPU 3: NVIDIA GeForce GTX 1080 Ti
                               
Nvidia driver version: 530.41.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Address sizes:                   46 bits physical, 48 bits virtual
Byte Order:                      Little Endian
CPU(s):                          32
On-line CPU(s) list:             0-31
Vendor ID:                       GenuineIntel
Model name:                      Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz
CPU family:                      6
Model:                           79
Thread(s) per core:              2
Core(s) per socket:              8
Socket(s):                       2
Stepping:                        1
CPU(s) scaling MHz:              54%
CPU max MHz:                     3000.0000
CPU min MHz:                     1200.0000
BogoMIPS:                        4200.44
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d
Virtualization:                  VT-x
L1d cache:                       512 KiB (16 instances)
L1i cache:                       512 KiB (16 instances)
L2 cache:                        4 MiB (16 instances)
L3 cache:                        40 MiB (2 instances)
NUMA node(s):                    2
NUMA node0 CPU(s):               0-7,16-23
NUMA node1 CPU(s):               8-15,24-31
Vulnerability Itlb multihit:     KVM: Mitigation: VMX disabled
Vulnerability L1tf:              Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable
Vulnerability Mds:               Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Meltdown:          Mitigation; PTI
Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT vulnerable

Versions of relevant libraries:
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.23.5
[pip3] torch==2.0.1
[pip3] torchaudio==2.0.2
[pip3] torchdata==0.6.1
[pip3] triton==2.0.0
[conda] Could not collect
```

I think the CUDA libs might be relevant as well:
```
 % python3.10 -m pip freeze | grep nvidia
nvidia-cublas-cu11==11.10.3.66
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cudnn-cu11==8.5.0.96
nvidia-cufft-cu11==10.9.0.58
nvidia-curand-cu11==10.2.10.91
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusparse-cu11==11.7.4.91
nvidia-nccl-cu11==2.14.3
nvidia-nvtx-cu11==11.7.91
```
"
111762,error during install from source of Pytorch 2.1.0 ,"### üêõ Describe the bug

# problem
commands i have used:
```bash
conda create -n ptr python=3.10
conda activate ptr

git clone https://github.com/pytorch/pytorch
cd pytorch
git checkout 7bcf7da3a268b435777fe87c7794c382f444e86d
git submodule sync
git submodule update --init --recursive

conda install cmake ninja
pip install -r requirements.txt
conda install mkl mkl-include
conda install -c pytorch magma-cuda110
make triton
pip install typing_extensions
pip install pyyaml
export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}
python setup.py develop
```
but i failed in the ending of install process, error info is below
```
FAILED: bin/kernel_function_legacy_test 
: && /usr/bin/c++ -D_GLIBCXX_USE_CXX11_ABI=1 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow -DHAVE_AVX2_CPU_DEFINITION -O3 -DNDEBUG -DNDEBUG -rdynamic -pthread -Wl,-rpath-link,/usr/lib/x86_64-linux-gnu caffe2/CMakeFiles/kernel_function_legacy_test.dir/__/aten/src/ATen/core/boxing/impl/kernel_function_legacy_test.cpp.o -o bin/kernel_function_legacy_test -L/lib/intel64   -L/lib/intel64_win   -L/lib/win-x64 -Wl,-rpath,/lib/intel64:/lib/intel64_win:/lib/win-x64:/home/me/miniconda3/envs/ptr/lib:/home/me/llm_workspace/path/to/pytorch/build/lib:/home/me/cuda-11.8/lib64:  lib/libgtest_main.a  -Wl,--no-as-needed,""/home/me/llm_workspace/path/to/pytorch/build/lib/libtorch.so"" -Wl,--as-needed  -Wl,--no-as-needed,""/home/me/llm_workspace/path/to/pytorch/build/lib/libtorch_cpu.so"" -Wl,--as-needed  lib/libprotobuf.a  /home/me/miniconda3/envs/ptr/lib/libmkl_intel_lp64.so  /home/me/miniconda3/envs/ptr/lib/libmkl_gnu_thread.so  /home/me/miniconda3/envs/ptr/lib/libmkl_core.so  -fopenmp  /usr/lib/x86_64-linux-gnu/libpthread.so  -lm  /usr/lib/x86_64-linux-gnu/libdl.so  -Wl,--no-as-needed,""/home/me/llm_workspace/path/to/pytorch/build/lib/libtorch_cuda.so"" -Wl,--as-needed  lib/libc10_cuda.so  lib/libc10.so  /home/me/cuda-11.8/lib64/libcudart.so  /home/me/cuda-11.8/lib64/libnvToolsExt.so  /home/me/cuda-11.8/lib64/libcufft.so  /home/me/cuda-11.8/lib64/libcurand.so  /home/me/cuda-11.8/lib64/libcublas.so  /home/me/cuda-11.8/lib64/libcublasLt.so  lib/libgtest.a  -pthread && :
/usr/bin/ld: /home/me/llm_workspace/path/to/pytorch/build/lib/libtorch_cuda.so: undefined reference to `cudaGraphInstantiateWithFlags'
/usr/bin/ld: /home/me/llm_workspace/path/to/pytorch/build/lib/libtorch_cuda.so: undefined reference to `cudaLaunchKernelExC'
collect2: error: ld returned 1 exit status
ninja: build stopped: subcommand failed.
```
cuda11.1 will failed for the same reason
what'more, I have tried to use cuda12.1 to install this version but failed too because of missing some cuda libxxx.so

### Versions

# Environmet:
cuda11.8
cudnnv8
nccl_2.18.5-1+cuda11.0
ubuntu 20.04.1
pytorch - 7bcf7da3a268b435777fe87c7794c382f444e86d"
111759,`allow_non_fake_inputs=True` of `make_fx()` is not effective in fake tracing mode,"### üêõ Describe the bug

```
    gm = make_fx(
        gm,
        tracing_mode=""fake"",
        _allow_non_fake_inputs=True,
    )(*inputs)
```

In this code above, we specify allow non fake input tensors to allow, but unexpected exception happens like below

```
Exception: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.eq.Scalar(Parameter containing:
```

We're using custom backend and same code had worked well in 2.0, but 2.1 (and master) have regression now. 

After some investigation, I found the FakeTensorMode is not created using the `_allow_non_fake_inputs` options at all because `make_fx()` will use existed one from context manager: https://github.com/pytorch/pytorch/blob/main/torch/fx/experimental/proxy_tensor.py#L776  

The TracingContext from context manager seems to be added in: https://github.com/pytorch/pytorch/pull/100043, https://github.com/pytorch/pytorch/pull/96054



### Error logs

(pytorch) npu-tools$ python main.py
Traceback (most recent call last):
  File ""/home/ys/Source/Furiosa/compiler/npu-tools/main.py"", line 105, in <module>
    model(x)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/nn/modules/module.py"", line 1519, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/nn/modules/module.py"", line 1528, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/eval_frame.py"", line 401, in _fn
    return fn(*args, **kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/nn/modules/module.py"", line 1519, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/nn/modules/module.py"", line 1528, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/eval_frame.py"", line 549, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/convert_frame.py"", line 683, in _convert_frame
    result = inner_convert(frame, cache_entry, hooks, frame_state)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/convert_frame.py"", line 148, in _fn
    return fn(*args, **kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/convert_frame.py"", line 402, in _convert_frame_assert
    return _compile(
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/convert_frame.py"", line 610, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/utils.py"", line 221, in time_wrapper
    r = func(*args, **kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/convert_frame.py"", line 527, in compile_inner
    out_code = transform_code_object(code, transform)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/bytecode_transformation.py"", line 1028, in transform_code_object
    transformations(instructions, code_options)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/convert_frame.py"", line 497, in transform
    tracer.run()
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/symbolic_convert.py"", line 2102, in run
    super().run()
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/symbolic_convert.py"", line 742, in run
    and self.step()
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/symbolic_convert.py"", line 705, in step
    getattr(self, inst.opname)(inst)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/symbolic_convert.py"", line 2212, in RETURN_VALUE
    self.output.compile_subgraph(
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/output_graph.py"", line 858, in compile_subgraph
    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
  File ""/home/ys/.miniconda3/envs/pytorch/lib/python3.9/contextlib.py"", line 79, in inner
    return func(*args, **kwds)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/output_graph.py"", line 986, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/utils.py"", line 221, in time_wrapper
    r = func(*args, **kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/output_graph.py"", line 1057, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/output_graph.py"", line 1038, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/repro/after_dynamo.py"", line 117, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/__init__.py"", line 1643, in __call__
    return self.compiler_fn(model_, inputs_, **self.kwargs)
  File ""/home/ys/Source/Furiosa/compiler/npu-tools/main.py"", line 97, in backend
    gm = make_fx(
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/fx/experimental/proxy_tensor.py"", line 867, in wrapped
    t = dispatch_trace(wrap_key(func, args, fx_tracer, pre_dispatch), tracer=fx_tracer, concrete_args=tuple(phs))
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_compile.py"", line 24, in inner
    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/eval_frame.py"", line 401, in _fn
    return fn(*args, **kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/external_utils.py"", line 17, in inner
    return fn(*args, **kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/fx/experimental/proxy_tensor.py"", line 478, in dispatch_trace
    graph = tracer.trace(root, concrete_args)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/eval_frame.py"", line 401, in _fn
    return fn(*args, **kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_dynamo/external_utils.py"", line 17, in inner
    return fn(*args, **kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/fx/_symbolic_trace.py"", line 817, in trace
    (self.create_arg(fn(*args)),),
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/fx/experimental/proxy_tensor.py"", line 514, in wrapped
    out = f(*tensors)
  File ""<string>"", line 1, in <lambda>
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/fx/graph_module.py"", line 728, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/fx/graph_module.py"", line 307, in __call__
    raise e
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/fx/graph_module.py"", line 294, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/fx/_symbolic_trace.py"", line 795, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/fx/experimental/proxy_tensor.py"", line 435, in call_module
    return forward(*args, **kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/fx/_symbolic_trace.py"", line 788, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/nn/modules/module.py"", line 1519, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/nn/modules/module.py"", line 1528, in _call_impl
    return forward_call(*args, **kwargs)
  File ""<eval_with_key>.0"", line 7, in forward
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/utils/_stats.py"", line 20, in wrapper
    return fn(*args, **kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/fx/experimental/proxy_tensor.py"", line 591, in __torch_dispatch__
    return self.inner_torch_dispatch(func, types, args, kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/fx/experimental/proxy_tensor.py"", line 626, in inner_torch_dispatch
    return proxy_call(self, func, self.pre_dispatch, args, kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/fx/experimental/proxy_tensor.py"", line 371, in proxy_call
    out = func(*args, **kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_ops.py"", line 516, in __call__
    return self._op(*args, **kwargs or {})
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/utils/_stats.py"", line 20, in wrapper
    return fn(*args, **kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_subclasses/fake_tensor.py"", line 1324, in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_subclasses/fake_tensor.py"", line 1456, in dispatch
    ) = self.validate_and_convert_non_fake_tensors(func, converter, args, kwargs)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_subclasses/fake_tensor.py"", line 1678, in validate_and_convert_non_fake_tensors
    args, kwargs = tree_map_only(
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/utils/_pytree.py"", line 388, in tree_map_only
    return tree_map(map_only(ty)(fn), pytree)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/utils/_pytree.py"", line 314, in tree_map
    return tree_unflatten([fn(i) for i in flat_args], spec)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/utils/_pytree.py"", line 314, in <listcomp>
    return tree_unflatten([fn(i) for i in flat_args], spec)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/utils/_pytree.py"", line 369, in inner
    return f(x)
  File ""/home/ys/Source/Projects/cpp/pytorch/torch/_subclasses/fake_tensor.py"", line 1668, in validate
    raise Exception(
torch._dynamo.exc.BackendCompilerFailed: backend='backend' raised:
Exception: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.eq.Scalar(Parameter containing:
tensor([...], size=(32, 6), dtype=torch.int8), 1)

Set TORCH_LOGS=""+dynamo"" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True



### Minified repro

```
import torch
import torch.nn as nn
from torch.fx.experimental.proxy_tensor import make_fx



model = nn.Linear(20, 30)
x = torch.randn(128, 20)


def backend(gm, inputs):
    gm = make_fx(
        gm,
        tracing_mode=""fake"",
        _allow_non_fake_inputs=True,
    )(*inputs)
    return gm

model = torch.compile(model, backend=backend)
model(x)

```


### Versions

Collecting environment information...
PyTorch version: 2.2.0a0+gite6d9350
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: 14.0.0-1ubuntu1.1
CMake version: version 3.27.4
Libc version: glibc-2.35

Python version: 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:58:50)  [GCC 10.3.0] (64-bit runtime)
Python platform: Linux-5.15.0-53-generic-x86_64-with-glibc2.35
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Address sizes:                   39 bits physical, 48 bits virtual
Byte Order:                      Little Endian
CPU(s):                          12
On-line CPU(s) list:             0-11
Vendor ID:                       GenuineIntel
Model name:                      Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz
CPU family:                      6
Model:                           165
Thread(s) per core:              2
Core(s) per socket:              6
Socket(s):                       1
Stepping:                        2
CPU max MHz:                     5000.0000
CPU min MHz:                     800.0000
BogoMIPS:                        5199.98
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp pku ospke md_clear flush_l1d arch_capabilities
Virtualization:                  VT-x
L1d cache:                       192 KiB (6 instances)
L1i cache:                       192 KiB (6 instances)
L2 cache:                        1.5 MiB (6 instances)
L3 cache:                        12 MiB (1 instance)
NUMA node(s):                    1
NUMA node0 CPU(s):               0-11
Vulnerability Itlb multihit:     KVM: Mitigation: VMX disabled
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Retbleed:          Mitigation; Enhanced IBRS
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
Vulnerability Srbds:             Mitigation; Microcode
Vulnerability Tsx async abort:   Not affected

Versions of relevant libraries:
[pip3] flake8==4.0.1
[pip3] mypy==0.960
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.23.3
[pip3] onnx==1.12.0
[pip3] onnx-simplifier==0.4.8
[pip3] onnxoptimizer==0.3.1
[pip3] onnxruntime==1.12.1
[pip3] torch==2.2.0a0+gite6d9350
[pip3] torch-xla==1.0
[pip3] torchvision==0.15.2
[pip3] triton==2.0.0
[conda] mkl                       2022.0.1           h06a4308_117
[conda] mkl-include               2022.1.0           h84fe81f_915    conda-forge
[conda] numpy                     1.23.3                   pypi_0    pypi
[conda] torch                     2.2.0a0+gite6d9350           dev_0    <develop>
[conda] torch-xla                 1.0                      pypi_0    pypi
[conda] torchvision               0.15.2                   pypi_0    pypi
[conda] triton                    2.0.0                    pypi_0    pypi


cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111757,Einsum empty output corner case,"### üêõ Describe the bug

```python
import torch
x = torch.ones((1, 0))
y = torch.ones((1,))
z = torch.ones((1,))
# works; b is just a zero-sized axis as expected
print(torch.einsum('ab,a->b', x, y))

x = torch.ones((1, 1, 0))
y = torch.ones((1,))
z = torch.ones((1,))
# no similar luck with an extra argument in the mix; we get an error
#     self.speedup = self.naive_cost / self.opt_cost
#     decimal.InvalidOperation: [<class 'decimal.DivisionUndefined'>]
print(torch.einsum('abc,a,b->c', x, y, z))

# numpy and JAX are a-ok with these scenarios, as expected
import numpy as np
print(np.einsum('abc,a,b->c', x, y, z))
```

### Versions

Script failed; but this is torch 2.0.0

cc @albanD"
111756,"""userwarning: loky-backed parallel loops cannot be called in a multiprocessing"" when using two dataloaders with num_workers=1","### üêõ Describe the bug

Setting num_workers=1 speeds dataloader a lot, but it doesn't seem to work when I have more than 1 dataloader. When I only have one, no warning appears and the enumeration only takes 0.3s. However, when I have 2 dataloader (train and val), the warning starts to appear on every iteration and it now takes 100s to enumerate the data loader, like when using spawn or without worker. It also happens when I just recreate that 1 dataloader. 

/usr/local/lib/python3.10/dist-packages/joblib/parallel.py:1332: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1

Environment:
Google Colab
Python 3.10.12
torch==1.13.1

I also tried upgrading to torch 2.1.0 but it's the same.

### Versions

--2023-10-22 05:26:33--  https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 21737 (21K) [text/plain]
Saving to: ‚Äòcollect_env.py‚Äô

collect_env.py      100%[===================>]  21.23K  --.-KB/s    in 0s      

2023-10-22 05:26:33 (114 MB/s) - ‚Äòcollect_env.py‚Äô saved [21737/21737]

Collecting environment information...
PyTorch version: 1.13.1+cu117
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.2 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: 14.0.0-1ubuntu1.1
CMake version: version 3.27.7
Libc version: glibc-2.35

Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-5.15.120+-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 11.8.89
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: Tesla T4
Nvidia driver version: 525.105.17
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Address sizes:                   46 bits physical, 48 bits virtual
Byte Order:                      Little Endian
CPU(s):                          2
On-line CPU(s) list:             0,1
Vendor ID:                       GenuineIntel
Model name:                      Intel(R) Xeon(R) CPU @ 2.00GHz
CPU family:                      6
Model:                           85
Thread(s) per core:              2
Core(s) per socket:              1
Socket(s):                       1
Stepping:                        3
BogoMIPS:                        4000.34
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities
Hypervisor vendor:               KVM
Virtualization type:             full
L1d cache:                       32 KiB (1 instance)
L1i cache:                       32 KiB (1 instance)
L2 cache:                        1 MiB (1 instance)
L3 cache:                        38.5 MiB (1 instance)
NUMA node(s):                    1
NUMA node0 CPU(s):               0,1
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Mitigation; PTE Inversion
Vulnerability Mds:               Vulnerable; SMT Host state unknown
Vulnerability Meltdown:          Vulnerable
Vulnerability Mmio stale data:   Vulnerable
Vulnerability Retbleed:          Vulnerable
Vulnerability Spec store bypass: Vulnerable
Vulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers
Vulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Not affected
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Vulnerable

Versions of relevant libraries:
[pip3] numpy==1.26.1
[pip3] torch==1.13.1
[pip3] torchaudio==2.1.0+cu118
[pip3] torchdata==0.7.0
[pip3] torchinfo==1.8.0
[pip3] torchmetrics==1.2.0
[pip3] torchsummary==1.5.1
[pip3] torchtext==0.16.0
[pip3] torchvision==0.14.1
[pip3] triton==2.1.0
[conda] Could not collect"
111754,[dynamo] Better determinism of `ConfigModule` by walking using pytree,"### üöÄ The feature, motivation and pitch

https://github.com/pytorch/pytorch/pull/111318

Currently, validation only occurs at the root. However, we should walk the pytree of each object to ensure types are respected.

In particular, we can do conversions of unfriendly types
- function objects into `""f{__module__}{__name__}""` strings
- sort sets and walk their objects to make them deterministic. 
    - There isn't a canonical order, however; sets have `__hash__`, but this hash is based on `id` which is non-deterministic. So sorting by hash is non-deterministic. Further, not all objects implement `>` operator. 
      - In other words, there is no feasible way to make deterministic representations of all sets.
    - Nevertheless, we can forbid those for which objects are not inequality comparable, and sort the rest.



cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng"
111753,[dynamo] AutogradFunctionMethodHigherOrderVariable check for new guards is broken,"AutogradFunctionMethodHigherOrderVariable has a check for new guards being added in the following places:
https://github.com/pytorch/pytorch/blob/f0cde8613c4c8814e157c0a742187a91aa72a009/torch/_dynamo/variables/higher_order_ops.py#L1091
https://github.com/pytorch/pytorch/blob/f0cde8613c4c8814e157c0a742187a91aa72a009/torch/_dynamo/variables/higher_order_ops.py#L1115
https://github.com/pytorch/pytorch/blob/f0cde8613c4c8814e157c0a742187a91aa72a009/torch/_dynamo/variables/higher_order_ops.py#L1122-L1123

As written, this check does nothing because we are storing a *reference* in pre_guards that just gets mutated.  So `pre_guards is post_guards` evaluates `True`.

The following will make the code do as intended:
```patch
diff --git a/torch/_dynamo/variables/higher_order_ops.py b/torch/_dynamo/variables/higher_order_ops.py
index 2292c71f048..5d7d87f810c 100644
--- a/torch/_dynamo/variables/higher_order_ops.py
+++ b/torch/_dynamo/variables/higher_order_ops.py
@@ -1088,7 +1088,7 @@ class AutogradFunctionMethodHigherOrderVariable(TorchHigherOrderOperatorVariable
         else:
             fn = TorchVariable(self.value)
         checkpoint = tx.copy_graphstate()
-        pre_guards = tx.output.guards
+        pre_guards = tx.output.guards.clone()
         graph_checkpoint = tx.output.graph
 
         # TODO: Support kwargs
diff --git a/torch/_guards.py b/torch/_guards.py
index e532a32cdd2..4f9e874476e 100644
--- a/torch/_guards.py
+++ b/torch/_guards.py
@@ -483,6 +483,8 @@ class GuardsSet:
         for o in others:
             for g in o:
                 self.add(g, skip=1)

+    def clone(self):
+        return GuardsSet(set(self.inner))
 
 
 class GuardsContext(Checkpointable[GuardsCheckpointState]):
```

However, it causes a test to fail:
```
____________________________________________ ReproTests.test_hf_xsoftmax_training ____________________________________________
Traceback (most recent call last):
  File ""/home/jansel/conda/envs/pytorch/lib/python3.10/unittest/case.py"", line 59, in testPartExecutor
    yield
  File ""/home/jansel/conda/envs/pytorch/lib/python3.10/unittest/case.py"", line 591, in run
    self._callTestMethod(testMethod)
  File ""/home/jansel/conda/envs/pytorch/lib/python3.10/unittest/case.py"", line 549, in _callTestMethod
    method()
  File ""/home/jansel/pytorch/torch/testing/_internal/common_utils.py"", line 2453, in wrapper
    method(*args, **kwargs)
  File ""/home/jansel/pytorch/test/dynamo/test_repros.py"", line 3163, in test_hf_xsoftmax_training
    self.assertEqual(dict(counters[""frames""]), {""total"": 1, ""ok"": 1})
  File ""/home/jansel/pytorch/torch/testing/_internal/common_utils.py"", line 3356, in assertEqual
    raise error_metas.pop()[0].to_error(
AssertionError: Scalars are not equal!

Expected 1 but got 2.
Absolute difference: 1
Relative difference: 1.0

The failure occurred for item ['ok']

To execute this test, run the following from the base repo dir:
     python test/dynamo/test_repros.py -k test_hf_xsoftmax_training
```

I think this piece of code needs to be revisited, as I am not sure if the graph break it adds is correct.




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng @anijain2305 "
111752,Is it a good time to switch to CXX11_ABI?,"### üöÄ The feature, motivation and pitch

Now most of CI jobs use g++>=9 except Android jobs which use g++-8. Given this situation, is it possible to always use CXX11_ABI and get rid of the many checks in build systems?

### Alternatives

_No response_

### Additional context

_No response_

cc @seemethere @malfet @osalpekar @atalman"
111744,ninja: build stopped: subcommand failed,"### üêõ Describe the bug

When I try to build pytorch from source in linux, I face a confusing problem when I run the 'python setup.py install'.

Here are the error logs when I run the 'python setup.py install' the second time.

### Error logs

(pytorch_install) [root@cn0 pytorch-1.7]# python setup.py installBuilding wheel torch-1.7.0a0
-- Building version 1.7.0a0
cmake --build . --target install --config Release -- -j 96
[3/2123] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx2.dir/common_avx2.cc.o
FAILED: caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx2.dir/common_avx2.cc.o 
/opt/rh/devtoolset-7/root/usr/bin/c++ -DHAVE_MALLOC_USABLE_SIZE=1 -DHAVE_MMAP=1 -DHAVE_SHM_OPEN=1 -DHAVE_SHM_UNLINK=1 -DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -DONNXIFI_ENABLE_EXT=1 -DONNX_ML=1 -DONNX_NAMESPACE=onnx_torch -DTH_BLAS_MKL -DUSE_EXTERNAL_MZCRC -D_FILE_OFFSET_BITS=64 -Iaten/src -I../aten/src -I. -I../ -I../cmake/../third_party/benchmark/include -Icaffe2/contrib/aten -I../third_party/onnx -Ithird_party/onnx -I../third_party/foxi -Ithird_party/foxi -I../c10/.. -isystem third_party/gloo -isystem ../cmake/../third_party/gloo -isystem ../cmake/../third_party/googletest/googlemock/include -isystem ../cmake/../third_party/googletest/googletest/include -isystem ../third_party/protobuf/src -isystem /home/mpi_share/env/bz/anaconda3/envs/pytorch_install/include -isystem ../third_party/gemmlowp -isystem ../third_party/neon2sse -isystem ../third_party/XNNPACK/include -isystem ../third_party -isystem ../cmake/../third_party/eigen -isystem /home/mpi_share/env/bz/anaconda3/envs/pytorch_install/include/python3.8 -isystem ../cmake/../third_party/pybind11/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/hwloc/hwloc201/hwloc/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/event/libevent2022/libevent -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/event/libevent2022/libevent/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include -isystem ../cmake/../third_party/cub -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow -DHAVE_AVX_CPU_DEFINITION -DHAVE_AVX2_CPU_DEFINITION -O3 -DNDEBUG -DNDEBUG -fPIC -DCAFFE2_USE_GLOO -DCUDA_HAS_FP16=1 -DHAVE_GCC_GET_CPUID -DUSE_AVX -DUSE_AVX2 -DTH_HAVE_THREAD -mavx2 -mfma -mavx -mf16c -std=gnu++14 -MD -MT caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx2.dir/common_avx2.cc.o -MF caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx2.dir/common_avx2.cc.o.d -o caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx2.dir/common_avx2.cc.o -c ../caffe2/perfkernels/common_avx2.cc
../caffe2/perfkernels/common_avx2.cc:17:2: error: #error ( ""You found a build system error: __AVX2__ is defined (via e.g. -mavx2) "" ""but CAFFE2_PERF_WITH_AVX2 is not defined."");
 #error( \
  ^~~~~
[4/2123] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx512.dir/common_avx512.cc.o
FAILED: caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx512.dir/common_avx512.cc.o 
/opt/rh/devtoolset-7/root/usr/bin/c++ -DHAVE_MALLOC_USABLE_SIZE=1 -DHAVE_MMAP=1 -DHAVE_SHM_OPEN=1 -DHAVE_SHM_UNLINK=1 -DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -DONNXIFI_ENABLE_EXT=1 -DONNX_ML=1 -DONNX_NAMESPACE=onnx_torch -DTH_BLAS_MKL -DUSE_EXTERNAL_MZCRC -D_FILE_OFFSET_BITS=64 -Iaten/src -I../aten/src -I. -I../ -I../cmake/../third_party/benchmark/include -Icaffe2/contrib/aten -I../third_party/onnx -Ithird_party/onnx -I../third_party/foxi -Ithird_party/foxi -I../c10/.. -isystem third_party/gloo -isystem ../cmake/../third_party/gloo -isystem ../cmake/../third_party/googletest/googlemock/include -isystem ../cmake/../third_party/googletest/googletest/include -isystem ../third_party/protobuf/src -isystem /home/mpi_share/env/bz/anaconda3/envs/pytorch_install/include -isystem ../third_party/gemmlowp -isystem ../third_party/neon2sse -isystem ../third_party/XNNPACK/include -isystem ../third_party -isystem ../cmake/../third_party/eigen -isystem /home/mpi_share/env/bz/anaconda3/envs/pytorch_install/include/python3.8 -isystem ../cmake/../third_party/pybind11/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/hwloc/hwloc201/hwloc/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/event/libevent2022/libevent -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/event/libevent2022/libevent/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include -isystem ../cmake/../third_party/cub -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow -DHAVE_AVX_CPU_DEFINITION -DHAVE_AVX2_CPU_DEFINITION -O3 -DNDEBUG -DNDEBUG -fPIC -DCAFFE2_USE_GLOO -DCUDA_HAS_FP16=1 -DHAVE_GCC_GET_CPUID -DUSE_AVX -DUSE_AVX2 -DTH_HAVE_THREAD -mavx512f -mavx512dq -mavx512vl -mavx2 -mfma -mavx -mf16c -std=gnu++14 -MD -MT caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx512.dir/common_avx512.cc.o -MF caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx512.dir/common_avx512.cc.o.d -o caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx512.dir/common_avx512.cc.o -c ../caffe2/perfkernels/common_avx512.cc
../caffe2/perfkernels/common_avx512.cc:18:2: error: #error ( ""You found a build system error: __AVX512F__, __AVX512DQ__, __AVX512VL__ "" ""is defined (via e.g. -mavx512f, -mavx512dq, and -mavx512vl) "" ""but CAFFE2_PERF_WITH_AVX512 is not defined."");
 #error( \
  ^~~~~
[5/2123] Building CXX object caffe2/CMakeFiles/torch_cpu.dir/onnx/backend.cc.o
FAILED: caffe2/CMakeFiles/torch_cpu.dir/onnx/backend.cc.o 
/opt/rh/devtoolset-7/root/usr/bin/c++ -DCPUINFO_SUPPORTED_PLATFORM=1 -DFMT_HEADER_ONLY=1 -DFXDIV_USE_INLINE_ASSEMBLY=0 -DHAVE_MALLOC_USABLE_SIZE=1 -DHAVE_MMAP=1 -DHAVE_SHM_OPEN=1 -DHAVE_SHM_UNLINK=1 -DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -DNNP_CONVOLUTION_ONLY=0 -DNNP_INFERENCE_ONLY=0 -DONNXIFI_ENABLE_EXT=1 -DONNX_ML=1 -DONNX_NAMESPACE=onnx_torch -DTH_BLAS_MKL -DUSE_DISTRIBUTED -DUSE_EXTERNAL_MZCRC -DUSE_RPC -DUSE_TENSORPIPE -D_FILE_OFFSET_BITS=64 -Dtorch_cpu_EXPORTS -Iaten/src -I../aten/src -I. -I../ -I../cmake/../third_party/benchmark/include -Icaffe2/contrib/aten -I../third_party/onnx -Ithird_party/onnx -I../third_party/foxi -Ithird_party/foxi -I../torch/csrc/api -I../torch/csrc/api/include -I../caffe2/aten/src/TH -Icaffe2/aten/src/TH -Icaffe2/aten/src -Icaffe2/../aten/src -Icaffe2/../aten/src/ATen -I../torch/csrc -I../third_party/miniz-2.0.8 -I../aten/src/TH -I../aten/../third_party/catch/single_include -I../aten/src/ATen/.. -Icaffe2/aten/src/ATen -I../caffe2/core/nomnigraph/include -I../third_party/FXdiv/include -I../c10/.. -I../third_party/pthreadpool/include -I../third_party/cpuinfo/include -I../third_party/QNNPACK/include -I../aten/src/ATen/native/quantized/cpu/qnnpack/include -I../aten/src/ATen/native/quantized/cpu/qnnpack/src -I../third_party/cpuinfo/deps/clog/include -I../third_party/NNPACK/include -I../third_party/fbgemm/include -I../third_party/fbgemm -I../third_party/fbgemm/third_party/asmjit/src -I../third_party/FP16/include -I../third_party/tensorpipe -Ithird_party/tensorpipe -I../third_party/tensorpipe/third_party/libnop/include -I../third_party/fmt/include -isystem third_party/gloo -isystem ../cmake/../third_party/gloo -isystem ../cmake/../third_party/googletest/googlemock/include -isystem ../cmake/../third_party/googletest/googletest/include -isystem ../third_party/protobuf/src -isystem /home/mpi_share/env/bz/anaconda3/envs/pytorch_install/include -isystem ../third_party/gemmlowp -isystem ../third_party/neon2sse -isystem ../third_party/XNNPACK/include -isystem ../third_party -isystem ../cmake/../third_party/eigen -isystem /home/mpi_share/env/bz/anaconda3/envs/pytorch_install/include/python3.8 -isystem ../cmake/../third_party/pybind11/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/hwloc/hwloc201/hwloc/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/event/libevent2022/libevent -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/event/libevent2022/libevent/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include -isystem ../cmake/../third_party/cub -isystem include -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow -DHAVE_AVX_CPU_DEFINITION -DHAVE_AVX2_CPU_DEFINITION -O3 -DNDEBUG -DNDEBUG -fPIC -DCAFFE2_USE_GLOO -DCUDA_HAS_FP16=1 -DHAVE_GCC_GET_CPUID -DUSE_AVX -DUSE_AVX2 -DTH_HAVE_THREAD -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-unknown-pragmas -Wno-missing-braces -Wno-maybe-uninitialized -fvisibility=hidden -O2 -fopenmp -DCAFFE2_BUILD_MAIN_LIB -pthread -DASMJIT_STATIC -std=gnu++14 -MD -MT caffe2/CMakeFiles/torch_cpu.dir/onnx/backend.cc.o -MF caffe2/CMakeFiles/torch_cpu.dir/onnx/backend.cc.o.d -o caffe2/CMakeFiles/torch_cpu.dir/onnx/backend.cc.o -c ../caffe2/onnx/backend.cc
../caffe2/onnx/backend.cc:11:10: fatal error: onnx/optimizer/optimize.h: No such file or directory
 #include ""onnx/optimizer/optimize.h""
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
[16/2123] Building CXX object caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/xnnpack/Convolution.cpp.o
FAILED: caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/xnnpack/Convolution.cpp.o 
/opt/rh/devtoolset-7/root/usr/bin/c++ -DCPUINFO_SUPPORTED_PLATFORM=1 -DFMT_HEADER_ONLY=1 -DFXDIV_USE_INLINE_ASSEMBLY=0 -DHAVE_MALLOC_USABLE_SIZE=1 -DHAVE_MMAP=1 -DHAVE_SHM_OPEN=1 -DHAVE_SHM_UNLINK=1 -DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -DNNP_CONVOLUTION_ONLY=0 -DNNP_INFERENCE_ONLY=0 -DONNXIFI_ENABLE_EXT=1 -DONNX_ML=1 -DONNX_NAMESPACE=onnx_torch -DTH_BLAS_MKL -DUSE_DISTRIBUTED -DUSE_EXTERNAL_MZCRC -DUSE_RPC -DUSE_TENSORPIPE -D_FILE_OFFSET_BITS=64 -Dtorch_cpu_EXPORTS -Iaten/src -I../aten/src -I. -I../ -I../cmake/../third_party/benchmark/include -Icaffe2/contrib/aten -I../third_party/onnx -Ithird_party/onnx -I../third_party/foxi -Ithird_party/foxi -I../torch/csrc/api -I../torch/csrc/api/include -I../caffe2/aten/src/TH -Icaffe2/aten/src/TH -Icaffe2/aten/src -Icaffe2/../aten/src -Icaffe2/../aten/src/ATen -I../torch/csrc -I../third_party/miniz-2.0.8 -I../aten/src/TH -I../aten/../third_party/catch/single_include -I../aten/src/ATen/.. -Icaffe2/aten/src/ATen -I../caffe2/core/nomnigraph/include -I../third_party/FXdiv/include -I../c10/.. -I../third_party/pthreadpool/include -I../third_party/cpuinfo/include -I../third_party/QNNPACK/include -I../aten/src/ATen/native/quantized/cpu/qnnpack/include -I../aten/src/ATen/native/quantized/cpu/qnnpack/src -I../third_party/cpuinfo/deps/clog/include -I../third_party/NNPACK/include -I../third_party/fbgemm/include -I../third_party/fbgemm -I../third_party/fbgemm/third_party/asmjit/src -I../third_party/FP16/include -I../third_party/tensorpipe -Ithird_party/tensorpipe -I../third_party/tensorpipe/third_party/libnop/include -I../third_party/fmt/include -isystem third_party/gloo -isystem ../cmake/../third_party/gloo -isystem ../cmake/../third_party/googletest/googlemock/include -isystem ../cmake/../third_party/googletest/googletest/include -isystem ../third_party/protobuf/src -isystem /home/mpi_share/env/bz/anaconda3/envs/pytorch_install/include -isystem ../third_party/gemmlowp -isystem ../third_party/neon2sse -isystem ../third_party/XNNPACK/include -isystem ../third_party -isystem ../cmake/../third_party/eigen -isystem /home/mpi_share/env/bz/anaconda3/envs/pytorch_install/include/python3.8 -isystem ../cmake/../third_party/pybind11/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/hwloc/hwloc201/hwloc/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/event/libevent2022/libevent -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/event/libevent2022/libevent/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include -isystem ../cmake/../third_party/cub -isystem include -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow -DHAVE_AVX_CPU_DEFINITION -DHAVE_AVX2_CPU_DEFINITION -O3 -DNDEBUG -DNDEBUG -fPIC -DCAFFE2_USE_GLOO -DCUDA_HAS_FP16=1 -DHAVE_GCC_GET_CPUID -DUSE_AVX -DUSE_AVX2 -DTH_HAVE_THREAD -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-unknown-pragmas -Wno-missing-braces -Wno-maybe-uninitialized -fvisibility=hidden -O2 -fopenmp -DCAFFE2_BUILD_MAIN_LIB -pthread -DASMJIT_STATIC -std=gnu++14 -MD -MT caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/xnnpack/Convolution.cpp.o -MF caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/xnnpack/Convolution.cpp.o.d -o caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/xnnpack/Convolution.cpp.o -c ../aten/src/ATen/native/xnnpack/Convolution.cpp
../aten/src/ATen/native/xnnpack/Convolution.cpp: In function ‚Äòat::native::xnnpack::ContextConv2D at::native::xnnpack::internal::convolution2d::create(const at::Tensor&, const c10::optional<at::Tensor>&, c10::IntArrayRef, c10::IntArrayRef, c10::IntArrayRef, c10::IntArrayRef, int64_t, bool, float, float)‚Äô:
../aten/src/ATen/native/xnnpack/Convolution.cpp:236:22: error: cannot convert ‚Äòxnn_operator**‚Äô to ‚Äòxnn_caches_t {aka const xnn_caches*}‚Äô for argument ‚Äò21‚Äô to ‚Äòxnn_status xnn_create_deconvolution2d_nhwc_f32(uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, size_t, size_t, size_t, size_t, const float*, const float*, float, float, uint32_t, xnn_caches_t, xnn_operator**)‚Äô
       &convolution_op);                                               // operator
                      ^
../aten/src/ATen/native/xnnpack/Convolution.cpp:264:22: error: cannot convert ‚Äòxnn_operator**‚Äô to ‚Äòxnn_caches_t {aka const xnn_caches*}‚Äô for argument ‚Äò21‚Äô to ‚Äòxnn_status xnn_create_convolution2d_nhwc_f32(uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, size_t, size_t, size_t, size_t, const float*, const float*, float, float, uint32_t, xnn_caches_t, xnn_operator**)‚Äô
       &convolution_op);                                               // operator
                      ^
[24/2123] Building CXX object caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/xnnpack/Linear.cpp.o
FAILED: caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/xnnpack/Linear.cpp.o 
/opt/rh/devtoolset-7/root/usr/bin/c++ -DCPUINFO_SUPPORTED_PLATFORM=1 -DFMT_HEADER_ONLY=1 -DFXDIV_USE_INLINE_ASSEMBLY=0 -DHAVE_MALLOC_USABLE_SIZE=1 -DHAVE_MMAP=1 -DHAVE_SHM_OPEN=1 -DHAVE_SHM_UNLINK=1 -DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -DNNP_CONVOLUTION_ONLY=0 -DNNP_INFERENCE_ONLY=0 -DONNXIFI_ENABLE_EXT=1 -DONNX_ML=1 -DONNX_NAMESPACE=onnx_torch -DTH_BLAS_MKL -DUSE_DISTRIBUTED -DUSE_EXTERNAL_MZCRC -DUSE_RPC -DUSE_TENSORPIPE -D_FILE_OFFSET_BITS=64 -Dtorch_cpu_EXPORTS -Iaten/src -I../aten/src -I. -I../ -I../cmake/../third_party/benchmark/include -Icaffe2/contrib/aten -I../third_party/onnx -Ithird_party/onnx -I../third_party/foxi -Ithird_party/foxi -I../torch/csrc/api -I../torch/csrc/api/include -I../caffe2/aten/src/TH -Icaffe2/aten/src/TH -Icaffe2/aten/src -Icaffe2/../aten/src -Icaffe2/../aten/src/ATen -I../torch/csrc -I../third_party/miniz-2.0.8 -I../aten/src/TH -I../aten/../third_party/catch/single_include -I../aten/src/ATen/.. -Icaffe2/aten/src/ATen -I../caffe2/core/nomnigraph/include -I../third_party/FXdiv/include -I../c10/.. -I../third_party/pthreadpool/include -I../third_party/cpuinfo/include -I../third_party/QNNPACK/include -I../aten/src/ATen/native/quantized/cpu/qnnpack/include -I../aten/src/ATen/native/quantized/cpu/qnnpack/src -I../third_party/cpuinfo/deps/clog/include -I../third_party/NNPACK/include -I../third_party/fbgemm/include -I../third_party/fbgemm -I../third_party/fbgemm/third_party/asmjit/src -I../third_party/FP16/include -I../third_party/tensorpipe -Ithird_party/tensorpipe -I../third_party/tensorpipe/third_party/libnop/include -I../third_party/fmt/include -isystem third_party/gloo -isystem ../cmake/../third_party/gloo -isystem ../cmake/../third_party/googletest/googlemock/include -isystem ../cmake/../third_party/googletest/googletest/include -isystem ../third_party/protobuf/src -isystem /home/mpi_share/env/bz/anaconda3/envs/pytorch_install/include -isystem ../third_party/gemmlowp -isystem ../third_party/neon2sse -isystem ../third_party/XNNPACK/include -isystem ../third_party -isystem ../cmake/../third_party/eigen -isystem /home/mpi_share/env/bz/anaconda3/envs/pytorch_install/include/python3.8 -isystem ../cmake/../third_party/pybind11/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/hwloc/hwloc201/hwloc/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/event/libevent2022/libevent -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/event/libevent2022/libevent/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include -isystem ../cmake/../third_party/cub -isystem include -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow -DHAVE_AVX_CPU_DEFINITION -DHAVE_AVX2_CPU_DEFINITION -O3 -DNDEBUG -DNDEBUG -fPIC -DCAFFE2_USE_GLOO -DCUDA_HAS_FP16=1 -DHAVE_GCC_GET_CPUID -DUSE_AVX -DUSE_AVX2 -DTH_HAVE_THREAD -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-unknown-pragmas -Wno-missing-braces -Wno-maybe-uninitialized -fvisibility=hidden -O2 -fopenmp -DCAFFE2_BUILD_MAIN_LIB -pthread -DASMJIT_STATIC -std=gnu++14 -MD -MT caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/xnnpack/Linear.cpp.o -MF caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/xnnpack/Linear.cpp.o.d -o caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/xnnpack/Linear.cpp.o -c ../aten/src/ATen/native/xnnpack/Linear.cpp
../aten/src/ATen/native/xnnpack/Linear.cpp: In function ‚Äòat::native::xnnpack::ContextLinear at::native::xnnpack::internal::linear::create(const at::Tensor&, const c10::optional<at::Tensor>&, float, float)‚Äô:
../aten/src/ATen/native/xnnpack/Linear.cpp:100:17: error: cannot convert ‚Äòxnn_operator**‚Äô to ‚Äòxnn_caches_t {aka const xnn_caches*}‚Äô for argument ‚Äò10‚Äô to ‚Äòxnn_status xnn_create_fully_connected_nc_f32(size_t, size_t, size_t, size_t, const float*, const float*, float, float, uint32_t, xnn_caches_t, xnn_operator**)‚Äô
       &linear_op);                                                    // operator
                 ^
[53/2123] Building CXX object caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp.o
FAILED: caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp.o 
/opt/rh/devtoolset-7/root/usr/bin/c++ -DCPUINFO_SUPPORTED_PLATFORM=1 -DFMT_HEADER_ONLY=1 -DFXDIV_USE_INLINE_ASSEMBLY=0 -DHAVE_MALLOC_USABLE_SIZE=1 -DHAVE_MMAP=1 -DHAVE_SHM_OPEN=1 -DHAVE_SHM_UNLINK=1 -DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -DNNP_CONVOLUTION_ONLY=0 -DNNP_INFERENCE_ONLY=0 -DONNXIFI_ENABLE_EXT=1 -DONNX_ML=1 -DONNX_NAMESPACE=onnx_torch -DTH_BLAS_MKL -DUSE_DISTRIBUTED -DUSE_EXTERNAL_MZCRC -DUSE_RPC -DUSE_TENSORPIPE -D_FILE_OFFSET_BITS=64 -Dtorch_cpu_EXPORTS -Iaten/src -I../aten/src -I. -I../ -I../cmake/../third_party/benchmark/include -Icaffe2/contrib/aten -I../third_party/onnx -Ithird_party/onnx -I../third_party/foxi -Ithird_party/foxi -I../torch/csrc/api -I../torch/csrc/api/include -I../caffe2/aten/src/TH -Icaffe2/aten/src/TH -Icaffe2/aten/src -Icaffe2/../aten/src -Icaffe2/../aten/src/ATen -I../torch/csrc -I../third_party/miniz-2.0.8 -I../aten/src/TH -I../aten/../third_party/catch/single_include -I../aten/src/ATen/.. -Icaffe2/aten/src/ATen -I../caffe2/core/nomnigraph/include -I../third_party/FXdiv/include -I../c10/.. -I../third_party/pthreadpool/include -I../third_party/cpuinfo/include -I../third_party/QNNPACK/include -I../aten/src/ATen/native/quantized/cpu/qnnpack/include -I../aten/src/ATen/native/quantized/cpu/qnnpack/src -I../third_party/cpuinfo/deps/clog/include -I../third_party/NNPACK/include -I../third_party/fbgemm/include -I../third_party/fbgemm -I../third_party/fbgemm/third_party/asmjit/src -I../third_party/FP16/include -I../third_party/tensorpipe -Ithird_party/tensorpipe -I../third_party/tensorpipe/third_party/libnop/include -I../third_party/fmt/include -isystem third_party/gloo -isystem ../cmake/../third_party/gloo -isystem ../cmake/../third_party/googletest/googlemock/include -isystem ../cmake/../third_party/googletest/googletest/include -isystem ../third_party/protobuf/src -isystem /home/mpi_share/env/bz/anaconda3/envs/pytorch_install/include -isystem ../third_party/gemmlowp -isystem ../third_party/neon2sse -isystem ../third_party/XNNPACK/include -isystem ../third_party -isystem ../cmake/../third_party/eigen -isystem /home/mpi_share/env/bz/anaconda3/envs/pytorch_install/include/python3.8 -isystem ../cmake/../third_party/pybind11/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/hwloc/hwloc201/hwloc/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/event/libevent2022/libevent -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/event/libevent2022/libevent/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include -isystem ../cmake/../third_party/cub -isystem include -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow -DHAVE_AVX_CPU_DEFINITION -DHAVE_AVX2_CPU_DEFINITION -O3 -DNDEBUG -DNDEBUG -fPIC -DCAFFE2_USE_GLOO -DCUDA_HAS_FP16=1 -DHAVE_GCC_GET_CPUID -DUSE_AVX -DUSE_AVX2 -DTH_HAVE_THREAD -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-unknown-pragmas -Wno-missing-braces -Wno-maybe-uninitialized -fvisibility=hidden -O2 -fopenmp -DCAFFE2_BUILD_MAIN_LIB -pthread -DASMJIT_STATIC -std=gnu++14 -MD -MT caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp.o -MF caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp.o.d -o caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp.o -c ../aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp
../aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp: In function ‚Äòat::Tensor at::native::{anonymous}::qembeddingbag_byte_unpack(const at::Tensor&)‚Äô:
../aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp:77:11: error: ‚ÄòFused8BitRowwiseQuantizedSBFloatToFloat‚Äô is not a member of ‚Äòfbgemm‚Äô
   fbgemm::Fused8BitRowwiseQuantizedSBFloatToFloat(
           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp:77:11: note: suggested alternative: ‚ÄòFused8BitRowwiseQuantizedSBFloatToFloatOrHalf‚Äô
   fbgemm::Fused8BitRowwiseQuantizedSBFloatToFloat(
           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           Fused8BitRowwiseQuantizedSBFloatToFloatOrHalf
../aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp: In function ‚Äòat::Tensor at::native::{anonymous}::_qembeddingbag_nbit_unpack_helper(const at::Tensor&, int)‚Äô:
../aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp:116:11: error: ‚ÄòFusedNBitRowwiseQuantizedSBHalfToFloat‚Äô is not a member of ‚Äòfbgemm‚Äô
   fbgemm::FusedNBitRowwiseQuantizedSBHalfToFloat(
           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp:116:11: note: suggested alternative: ‚ÄòFusedNBitRowwiseQuantizedSBHalfToFloatOrHalf‚Äô
   fbgemm::FusedNBitRowwiseQuantizedSBHalfToFloat(
           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           FusedNBitRowwiseQuantizedSBHalfToFloatOrHalf
[55/2123] Building CXX object caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp.o
FAILED: caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp.o 
/opt/rh/devtoolset-7/root/usr/bin/c++ -DCPUINFO_SUPPORTED_PLATFORM=1 -DFMT_HEADER_ONLY=1 -DFXDIV_USE_INLINE_ASSEMBLY=0 -DHAVE_MALLOC_USABLE_SIZE=1 -DHAVE_MMAP=1 -DHAVE_SHM_OPEN=1 -DHAVE_SHM_UNLINK=1 -DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -DNNP_CONVOLUTION_ONLY=0 -DNNP_INFERENCE_ONLY=0 -DONNXIFI_ENABLE_EXT=1 -DONNX_ML=1 -DONNX_NAMESPACE=onnx_torch -DTH_BLAS_MKL -DUSE_DISTRIBUTED -DUSE_EXTERNAL_MZCRC -DUSE_RPC -DUSE_TENSORPIPE -D_FILE_OFFSET_BITS=64 -Dtorch_cpu_EXPORTS -Iaten/src -I../aten/src -I. -I../ -I../cmake/../third_party/benchmark/include -Icaffe2/contrib/aten -I../third_party/onnx -Ithird_party/onnx -I../third_party/foxi -Ithird_party/foxi -I../torch/csrc/api -I../torch/csrc/api/include -I../caffe2/aten/src/TH -Icaffe2/aten/src/TH -Icaffe2/aten/src -Icaffe2/../aten/src -Icaffe2/../aten/src/ATen -I../torch/csrc -I../third_party/miniz-2.0.8 -I../aten/src/TH -I../aten/../third_party/catch/single_include -I../aten/src/ATen/.. -Icaffe2/aten/src/ATen -I../caffe2/core/nomnigraph/include -I../third_party/FXdiv/include -I../c10/.. -I../third_party/pthreadpool/include -I../third_party/cpuinfo/include -I../third_party/QNNPACK/include -I../aten/src/ATen/native/quantized/cpu/qnnpack/include -I../aten/src/ATen/native/quantized/cpu/qnnpack/src -I../third_party/cpuinfo/deps/clog/include -I../third_party/NNPACK/include -I../third_party/fbgemm/include -I../third_party/fbgemm -I../third_party/fbgemm/third_party/asmjit/src -I../third_party/FP16/include -I../third_party/tensorpipe -Ithird_party/tensorpipe -I../third_party/tensorpipe/third_party/libnop/include -I../third_party/fmt/include -isystem third_party/gloo -isystem ../cmake/../third_party/gloo -isystem ../cmake/../third_party/googletest/googlemock/include -isystem ../cmake/../third_party/googletest/googletest/include -isystem ../third_party/protobuf/src -isystem /home/mpi_share/env/bz/anaconda3/envs/pytorch_install/include -isystem ../third_party/gemmlowp -isystem ../third_party/neon2sse -isystem ../third_party/XNNPACK/include -isystem ../third_party -isystem ../cmake/../third_party/eigen -isystem /home/mpi_share/env/bz/anaconda3/envs/pytorch_install/include/python3.8 -isystem ../cmake/../third_party/pybind11/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/hwloc/hwloc201/hwloc/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/event/libevent2022/libevent -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include/openmpi/opal/mca/event/libevent2022/libevent/include -isystem /home/mpi_share/env/mlnx_sharp/hpcx-v2.7.0-gcc-MLNX_OFED_LINUX-5.1-0.6.6.0-redhat7.8-x86_64/ompi/include -isystem ../cmake/../third_party/cub -isystem include -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow -DHAVE_AVX_CPU_DEFINITION -DHAVE_AVX2_CPU_DEFINITION -O3 -DNDEBUG -DNDEBUG -fPIC -DCAFFE2_USE_GLOO -DCUDA_HAS_FP16=1 -DHAVE_GCC_GET_CPUID -DUSE_AVX -DUSE_AVX2 -DTH_HAVE_THREAD -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-unknown-pragmas -Wno-missing-braces -Wno-maybe-uninitialized -fvisibility=hidden -O2 -fopenmp -DCAFFE2_BUILD_MAIN_LIB -pthread -DASMJIT_STATIC -std=gnu++14 -MD -MT caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp.o -MF caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp.o.d -o caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp.o -c ../aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp
../aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp: In function ‚Äòat::Tensor at::native::{anonymous}::qembeddingbag_byte_prepack(const at::Tensor&)‚Äô:
../aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:116:11: error: ‚ÄòFloatToFused8BitRowwiseQuantizedSBFloat‚Äô is not a member of ‚Äòfbgemm‚Äô
   fbgemm::FloatToFused8BitRowwiseQuantizedSBFloat(
           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:116:11: note: suggested alternative: ‚ÄòFloatOrHalfToFused8BitRowwiseQuantizedSBFloat‚Äô
   fbgemm::FloatToFused8BitRowwiseQuantizedSBFloat(
           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           FloatOrHalfToFused8BitRowwiseQuantizedSBFloat
../aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp: In function ‚Äòat::Tensor at::native::{anonymous}::_qembeddingbag_nbit_prepack_helper(const at::Tensor&, int, bool)‚Äô:
../aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:187:13: error: ‚ÄòFloatToFusedNBitRowwiseQuantizedSBHalf‚Äô is not a member of ‚Äòfbgemm‚Äô
     fbgemm::FloatToFusedNBitRowwiseQuantizedSBHalf(
             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:187:13: note: suggested alternative: ‚ÄòFloatOrHalfToFusedNBitRowwiseQuantizedSBHalf‚Äô
     fbgemm::FloatToFusedNBitRowwiseQuantizedSBHalf(
             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
             FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf
[98/2123] Building CXX object caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/Functions.cpp.o
ninja: build stopped: subcommand failed.
Traceback (most recent call last):
  File ""setup.py"", line 717, in <module>
    build_deps()
  File ""setup.py"", line 308, in build_deps
    build_caffe2(version=version,
  File ""/home/mpi_share/env/bz/pytorch-1.7/tools/build_pytorch_libs.py"", line 62, in build_caffe2
    cmake.build(my_env)
  File ""/home/mpi_share/env/bz/pytorch-1.7/tools/setup_helpers/cmake.py"", line 345, in build
    self.run(build_args, my_env)
  File ""/home/mpi_share/env/bz/pytorch-1.7/tools/setup_helpers/cmake.py"", line 141, in run
    check_call(command, cwd=self.build_dir, env=env)
  File ""/home/mpi_share/env/bz/anaconda3/envs/pytorch_install/lib/python3.8/subprocess.py"", line 364, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '96']' returned non-zero exit status 1.

### Minified repro

_No response_

### Versions

sry, there is also a bug when I use the given commands.

pytorch 1.7.0
cuda 10.2
cmake 3.18.4
ninja 1.10.2
python 3.8.2

There are some difficulties to update my cuda version, so I just build the v1.7.0 pytorch to match the cuda version.

If there is something you need to know, please tell me.

Thanks ^^

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111740,GPU computation is not equivalent,"### üêõ Describe the bug

GPU computation is not equivalent, but it is equivalent on CPU. Why? And how can I avoid this?
```python
import torch
import torch.nn as nn

hidden_states = torch.randn([4, 2048, 512])
v_proj = nn.Linear(512, 128, bias=False)
value_states = v_proj(hidden_states)

h1, h2 = torch.chunk(hidden_states, 2, dim=0)
v1 = v_proj(h1)

assert h1.equal(hidden_states[:2])
print(v1[0,0,0].item())
print(value_states[0,0,0].item())
assert v1.equal(value_states[:2])

hidden_states = torch.randn([4, 2048, 512]).cuda()
v_proj = nn.Linear(512, 128, bias=False).cuda()
value_states = v_proj(hidden_states)

h1, h2 = torch.chunk(hidden_states, 2, dim=0)
v1 = v_proj(h1)

assert h1.equal(hidden_states[:2])
print(v1[0,0,0].item())
print(value_states[0,0,0].item())
assert v1.equal(value_states[:2])
```
running results
```python
0.429298460483551
0.429298460483551
0.3757566213607788
0.37575680017471313
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
```

### Versions

```python
PyTorch version: 2.1.0+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 11 Pro
GCC version: Could not collect
Clang version: Could not collect
CMake version: Could not collect
Libc version: N/A

Python version: 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.22635-SP0
Is CUDA available: True
CUDA runtime version: 12.1.105
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080 Ti Laptop GPU
Nvidia driver version: 531.14
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture=9
CurrentClockSpeed=2300
DeviceID=CPU0
Family=207
L2CacheSize=14336
L2CacheSpeed=
Manufacturer=GenuineIntel
MaxClockSpeed=2300
Name=12th Gen Intel(R) Core(TM) i9-12900HX
ProcessorType=3
Revision=

Versions of relevant libraries:
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.23.3
[pip3] torch==2.1.0+cu121
[pip3] torch-tb-profiler==0.4.3
[pip3] torchaudio==2.1.0+cu121
[pip3] torchvision==0.16.0+cu121
[pip3] torchviz==0.0.2
[conda] Could not collect
```"
111739,grad is inf/nan when using torch.amp,"### üêõ Describe the bug

Below is a very simple for using torch.amp, but the gradients are inf/nan.
```python
import torch
from torch.cuda.amp import GradScaler
from torch import optim
scaler = GradScaler()
a = torch.randn(2, 2, requires_grad=True, device=""cuda"")
b = torch.randn(2, 2, requires_grad=True, device=""cuda"")
optimizer = optim.Adam([a, b], lr=0.1)
with torch.autocast(device_type='cuda'):
    c = a @ b
    loss = c.sum()
scaler.scale(loss).backward()
scaler.unscale_(optimizer)
print(a.grad)
```
running results:
```python
tensor([[-inf, nan],
        [-inf, nan]], device='cuda:0')
```

### Versions

```python
PyTorch version: 2.1.0+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 11 Pro
GCC version: Could not collect
Clang version: Could not collect
CMake version: Could not collect
Libc version: N/A

Python version: 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.22635-SP0
Is CUDA available: True
CUDA runtime version: 12.1.105
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080 Ti Laptop GPU
Nvidia driver version: 531.14
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture=9
CurrentClockSpeed=2300
DeviceID=CPU0
Family=207
L2CacheSize=14336
L2CacheSpeed=
Manufacturer=GenuineIntel
MaxClockSpeed=2300
Name=12th Gen Intel(R) Core(TM) i9-12900HX
ProcessorType=3
Revision=

Versions of relevant libraries:
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.23.3
[pip3] torch==2.1.0+cu121
[pip3] torch-tb-profiler==0.4.3
[pip3] torchaudio==2.1.0+cu121
[pip3] torchvision==0.16.0+cu121
[pip3] torchviz==0.0.2
[conda] Could not collect
```

cc @mcarilli @ptrblck @leslie-fang-intel @jgong5"
111736,Implementation of Lion Optimizer.,"### üöÄ The feature, motivation and pitch

Lion Optimizer is becoming a great alterative to AdamW and Adam Optimizer. It is more efficient as it does not use second order moments and instead uses sign operations in order to update the weights. This saves on memory and decreases training time. In  some cases it is better than Adam and AdamW as given in the paper.
The original paper for this is : https://arxiv.org/pdf/2302.06675.pdf

The RFCS PR for this is: https://github.com/pytorch/rfcs/pull/60

### Alternatives

_No response_

### Additional context

_No response_

cc @vincentqb @jbschlosser @albanD @janeyx99 @crcrpar"
111734,Is the index_add_ function differentiable?,"### üöÄ The feature, motivation and pitch

```python
        verts_normals = torch.zeros_like(cornea_vertex)
        vertices_faces = cornea_vertex[face_index]

        faces_normals = torch.cross(
            vertices_faces[:, 2] - vertices_faces[:, 1],
            vertices_faces[:, 0] - vertices_faces[:, 1],
            dim=-1,
        )
        unit_faces_normals = safe_normalize(faces_normals)
        verts_normals.index_add_(0, face_index[:, 0], unit_faces_normals)
        verts_normals.index_add_(0, face_index[:, 1], unit_faces_normals)
        verts_normals.index_add_(0, face_index[:, 2], unit_faces_normals)
```
### Alternatives

_No response_

### Additional context

_No response_
```[tasklist]
### Tasks
```
"
111733,Bug: torch.compile fails to compile torch.func.vmap with reduction functions and raw python numbers,"### üêõ Describe the bug

`torch.compile` fails to compile vmap transformation with reduction functions and native python numbers. This bug was only found when using reduction functions, and there are several workarounds as shown in the following examples:

```python
import torch

torch._dynamo.reset()
torch._dynamo.config.capture_func_transforms=True

def foo(x):
  return torch.vmap(lambda x: torch.sum(x) + 1e-2)(x)  # Error
  # return torch.vmap(lambda x: torch.mean(x) + 1e-2)(x)  # Error
  # return torch.vmap(lambda x: torch.std(x) + 1e-2)(x)  # Error
  # return torch.vmap(lambda x: torch.sum(x) + torch.tensor(1e-2))(x) # OK
  # return torch.vmap(lambda x: torch.sum(x, 0, keepdim=True) + 1e-2)(x) # OK
  # return torch.vmap(lambda x: torch.square(x) + 1e-2)(x) # OK
  # return torch.vmap(lambda x: x + 1e-2)(x) # OK


torch.compile(foo, fullgraph=True)(torch.randn((3, 3), device='cuda:0'))
# foo(torch.randn((3, 3), device='cuda:0'))   # OK
```

Error messages:

```
BackendCompilerFailed: backend='inductor' raised:
AssertionError: While executing %call : [num_users=1] = call_method[target=__call__](args = (%vmap_proxy, %l_x_), kwargs = {})
Original traceback:
  File ""/tmp/ipykernel_672249/1649664715.py"", line 7, in foo
    return torch.vmap(lambda x: torch.sum(x) + 1e-2)(x)  # Error
  File ""/data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_functorch/apis.py"", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)



You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True
```

Traceback:

```
---------------------------------------------------------------------------
BackendCompilerFailed                     Traceback (most recent call last)
Cell In[66], line 16
      7   return torch.vmap(lambda x: torch.sum(x) + 1e-2)(x)  # Error
      8   # return torch.vmap(lambda x: torch.mean(x) + 1e-2)(x)  # Error
      9   # return torch.vmap(lambda x: torch.std(x) + 1e-2)(x)  # Error
     10   # return torch.vmap(lambda x: torch.sum(x) + torch.tensor(1e-2))(x) # OK
     11   # return torch.vmap(lambda x: torch.sum(x, 0, keepdim=True) + 1e-2)(x) # OK
     12   # return torch.vmap(lambda x: torch.square(x) + 1e-2)(x) # OK
     13   # return torch.vmap(lambda x: x + 1e-2)(x) # OK
---> 16 torch.compile(foo, fullgraph=True)(torch.randn((3, 3), device='cuda:0'))
     17 # foo(torch.randn((3, 3), device='cuda:0'))   # OK

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:328, in _TorchDynamoContext.__call__.<locals>._fn(*args, **kwargs)
    326 dynamic_ctx.__enter__()
    327 try:
--> 328     return fn(*args, **kwargs)
    329 finally:
    330     set_eval_frame(prior)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:490, in catch_errors_wrapper.<locals>.catch_errors(frame, cache_entry, frame_state)
    487             return hijacked_callback(frame, cache_entry, hooks, frame_state)
    489 with compile_lock, _disable_current_modes():
--> 490     return callback(frame, cache_entry, hooks, frame_state)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:133, in wrap_convert_context.<locals>._fn(*args, **kwargs)
    131 cleanup = setup_compile_debug()
    132 try:
--> 133     return fn(*args, **kwargs)
    134 finally:
    135     cleanup.close()

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:389, in convert_frame_assert.<locals>._convert_frame_assert(frame, cache_entry, hooks, frame_state)
    376 compile_id = CompileId(frame_id, frame_compile_id)
    378 signpost_event(
    379     ""dynamo"",
    380     ""_convert_frame_assert._compile"",
   (...)
    386     },
    387 )
--> 389 return _compile(
    390     frame.f_code,
    391     frame.f_globals,
    392     frame.f_locals,
    393     frame.f_builtins,
    394     compiler_fn,
    395     one_graph,
    396     export,
    397     export_constraints,
    398     hooks,
    399     cache_size,
    400     frame,
    401     frame_state=frame_state,
    402     compile_id=compile_id,
    403 )

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:569, in _compile(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id)
    567 with compile_context(CompileContext(compile_id)):
    568     try:
--> 569         guarded_code = compile_inner(code, one_graph, hooks, transform)
    570         return guarded_code
    571     except (
    572         Unsupported,
    573         TorchRuntimeError,
   (...)
    578         ValidationException,
    579     ) as e:

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/utils.py:189, in dynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper(*args, **kwargs)
    187 with torch.profiler.record_function(f""{key} (dynamo_timed)""):
    188     t0 = time.time()
--> 189     r = func(*args, **kwargs)
    190     time_spent = time.time() - t0
    191 compilation_time_metrics[key].append(time_spent)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:491, in _compile.<locals>.compile_inner(code, one_graph, hooks, transform)
    489 for attempt in itertools.count():
    490     try:
--> 491         out_code = transform_code_object(code, transform)
    492         orig_code_map[out_code] = code
    493         break

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py:1028, in transform_code_object(code, transformations, safe)
   1025 instructions = cleaned_instructions(code, safe)
   1026 propagate_line_nums(instructions)
-> 1028 transformations(instructions, code_options)
   1029 return clean_and_assemble_instructions(instructions, keys, code_options)[1]

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:458, in _compile.<locals>
.transform(instructions, code_options)
    456 try:
    457     with tracing(tracer.output.tracing_context):
--> 458         tracer.run()
    459 except (exc.RestartAnalysis, exc.SkipFrame):
    460     raise

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2074, in InstructionTranslator.run(self)
   2073 def run(self):
-> 2074     super().run()

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:724, in InstructionTranslatorBase.run(self)
    719 try:
    720     self.output.push_tx(self)
    721     while (
    722         self.instruction_pointer is not None
    723         and not self.output.should_exit
--> 724         and self.step()
    725     ):
    726         pass
    727 except BackendCompilerFailed:

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:688, in InstructionTranslatorBase.step(self)
    684         unimplemented(f""missing: {inst.opname}"")
    685     TracingContext.set_current_loc(
    686         self.f_code.co_filename, self.lineno, self.f_code.co_name
    687     )
--> 688     getattr(self, inst.opname)(inst)
    690     return inst.opname != ""RETURN_VALUE""
    691 except Unsupported:

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2162, in InstructionTranslator.RETURN_VALUE(self, inst)
   2157 _step_logger()(
   2158     logging.INFO,
   2159     f""torchdynamo done tracing {self.f_code.co_name} (RETURN_VALUE)"",
   2160 )
   2161 log.debug(""RETURN_VALUE triggered compile"")
-> 2162 self.output.compile_subgraph(
   2163     self,
   2164     reason=GraphCompileReason(
   2165         ""return_value"", [self.frame_summary()], graph_break=False
   2166     ),
   2167 )
   2168 self.output.add_output_instructions([create_instruction(""RETURN_VALUE"")])

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:833, in OutputGraph.compile_subgraph(self, tx, partial_convert, reason)
    830     append_prefix_insts()
    831     # optimization to generate better code in a common case
    832     self.add_output_instructions(
--> 833         self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
    834         + [create_instruction(""UNPACK_SEQUENCE"", arg=len(stack_values))]
    835     )
    836 else:
    837     graph_output_var = self.new_var(""graph_out"")

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/contextlib.py:81, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)
     78 @wraps(func)
     79 def inner(*args, **kwds):
     80     with self._recreate_cm():
---> 81         return func(*args, **kwds)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:957, in OutputGraph.compile_and_call_fx_graph(self, tx, rv, root)
    952 graph_tabular_log.debug(""%s"", lazy_format_graph_tabular(name, gm))
953 graph_sizes_log.debug(
    954     ""%s"", LazyString(lambda: self.get_graph_sizes_log_str(name))
    955 )
--> 957 compiled_fn = self.call_user_compiler(gm)
    958 compiled_fn = disable(compiled_fn)
    960 counters[""stats""][""unique_graphs""] += 1

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/utils.py:189, in dynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper(*args, **kwargs)
    187 with torch.profiler.record_function(f""{key} (dynamo_timed)""):
    188     t0 = time.time()
--> 189     r = func(*args, **kwargs)
    190     time_spent = time.time() - t0
    191 compilation_time_metrics[key].append(time_spent)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1024, in OutputGraph.call_user_compiler(self, gm)
   1022     unimplemented_with_warning(e, self.root_tx.f_code, msg)
   1023 except Exception as e:
-> 1024     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
   1025         e.__traceback__
   1026     ) from None
   1028 signpost_event(
   1029     ""dynamo"",
   1030     ""OutputGraph.call_user_compiler"",
   (...)
   1036     },
   1037 )
   1039 return compiled_fn

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1009, in OutputGraph.call_user_compiler(self, gm)
   1007 if config.verify_correctness:
   1008     compiler_fn = WrapperBackend(compiler_fn)
-> 1009 compiled_fn = compiler_fn(gm, self.example_inputs())
   1010 _step_logger()(logging.INFO, f""done compiler function {name}"")
   1011 assert callable(compiled_fn), ""compiler_fn did not return callable""

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py:117, in wrap_backend_debug.<locals>.debug_wrapper(gm, example_inputs, **kwargs)
    115             raise
    116 else:
--> 117     compiled_gm = compiler_fn(gm, example_inputs)
    119 return compiled_gm

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py:117, in wrap_backend_debug.<locals>.debug_wrapper(gm, example_inputs, **kwargs)
    115             raise
    116 else:
--> 117     compiled_gm = compiler_fn(gm, example_inputs)
    119 return compiled_gm

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/__init__.py:1568, in _TorchCompileInductorWrapper.__call__(self, model_, inputs_)
   1565 def __call__(self, model_, inputs_):
   1566     from torch._inductor.compile_fx import compile_fx
-> 1568     return compile_fx(model_, inputs_, config_patches=self.config)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1150, in compile_fx(model_, example_inputs_, inner_compile, config_patches, decompositions)
   1143 tracing_context = (
   1144     torch._guards.TracingContext.get() or torch._guards.TracingContext(fake_mode)
   1145 )
   1147 with V.set_fake_mode(fake_mode), torch._guards.tracing(  # type: ignore[call-arg]
   1148     tracing_context
   1149 ), compiled_autograd.disable():
-> 1150     return aot_autograd(
   1151         fw_compiler=fw_compiler,
   1152         bw_compiler=bw_compiler,
   1153         inference_compiler=inference_compiler,
   1154         decompositions=decompositions,
   1155         partition_fn=partition_fn,
   1156         keep_inference_input_mutations=True,
   1157     )(model_, example_inputs_)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/backends/common.py:55, in aot_autograd
.<locals>.compiler_fn(gm, example_inputs)
     52 try:
     53     # NB: NOT cloned!
     54     with enable_aot_logging(), patch_config:
---> 55         cg = aot_module_simplified(gm, example_inputs, **kwargs)
     56         counters[""aot_autograd""][""ok""] += 1
     57         return disable(cg)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:3891, in aot_module_simplified(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler)
   3875 aot_config = AOTConfig(
   3876     fw_compiler=fw_compiler,
   3877     bw_compiler=bw_compiler,
   (...)
   3887     no_tangents=False,
   3888 )
   3890 with compiled_autograd.disable():
-> 3891     compiled_fn = create_aot_dispatcher_function(
   3892         functional_call,
   3893         full_args,
   3894         aot_config,
   3895     )
   3897 # TODO: There is something deeply wrong here; compiled_fn running with
   3898 # the boxed calling convention, but aot_module_simplified somehow
   3899 # historically returned a function that was not the boxed calling
   3900 # convention.  This should get fixed...
   3901 def forward(*runtime_args):

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/utils.py:189, in dynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper(*args, **kwargs)
    187 with torch.profiler.record_function(f""{key} (dynamo_timed)""):
    188     t0 = time.time()
--> 189     r = func(*args, **kwargs)
    190     time_spent = time.time() - t0
    191 compilation_time_metrics[key].append(time_spent)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:3429, in create_aot_dispatcher_function(flat_fn, flat_args, aot_config)
   3426 compiler_fn = partial(aot_wrapper_dedupe, compiler_fn=compiler_fn)
   3427 # You can put more passes here
-> 3429 compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
   3430 if aot_config.is_export:
   3432     mutated_user_inp_locs = [
   3433         idx - aot_config.num_params_buffers
   3434         for idx in fw_metadata.mutated_inp_indices
   3435         if idx >= aot_config.num_params_buffers
   3436     ]

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:2212, in aot_wrapper_dedupe(flat_fn, flat_args, aot_config, compiler_fn, fw_metadata)
   2209         break
   2211 if ok:
-> 2212     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
   2214 # export path: ban duplicate inputs for now, add later if requested.
   2215 if aot_config.is_export:

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:2392, in aot_wrapper_synthetic_base(flat_fn, flat_args, aot_config, fw_metadata, needs_autograd, compiler_fn)
   2390 # Happy path: we don't need synthetic bases
   2391 if synthetic_base_info is None:
-> 2392     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
   2394 # export path: ban synthetic bases for now, add later if requested.
   2395 if aot_config.is_export:

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1558, in aot_dispatch_base(flat_fn, flat_args, aot_config, fw_metadata)
   1557 def aot_dispatch_base(flat_fn, flat_args: List[Tensor], aot_config: AOTConfig, *, fw_metadata: ViewAndMutationMeta):
-> 1558     fw_module = aot_dispatch_base_graph(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
   1560     disable_amp = torch._C._is_any_autocast_enabled()
   1561     context = torch._C._DisableAutocast if disable_amp else nullcontext

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1533, in aot_dispatch_base_graph(flat_fn, flat_args, aot_config, fw_metadata)
   1526 keep_mutations = aot_config.keep_inference_input_mutations
   1527 fn_to_trace = fn_input_mutations_to_outputs(
   1528     flat_fn,
   1529     fw_metadata,
   1530     keep_data_input_mutations=aot_config.keep_inference_input_mutations,
   1531 )
-> 1533 fw_module = create_functionalized_graph(
   1534     fn_to_trace,
   1535     flat_args,
   1536     meta=fw_metadata,
   1537     aot_config=aot_config,
   1538     trace_joint=False,
   1539 )
   1541 # As long as we opted to remove input mutations, then
   1542 # there should be *NO* mutating ops in the graph at this point.
   1543 copy_count = assert_functional_graph(fw_module.graph, allow_input_mutations=aot_config.keep_inference_input_mutations)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1420, in create_functionalized_graph(fn, args, meta, aot_config, trace_joint)
   1417     helper, args = create_functionalized_rng_ops_wrapper(helper, args, trace_joint)
   1419 with enable_python_dispatcher():
-> 1420     fx_g = make_fx(helper, decomposition_table=aot_config.decompositions)(*args)
   1422 return fx_g

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:809, in make_fx.<locals>.wrapped(*args)
    801 # We disable the autocast cache as the autocast cache causes type conversions on parameters to
    802 # check a cache, which introduces untracked tensors into the graph
    803 #
    804 # We also disable tracing by any other tensor proxy-based tracers except the current. The
    805 # purpose of `make_fx` is to produce graphmodules as a side effect; its internal execution is
    806 # thus irrelevant to any external functional trace.
    807 with decompose(decomposition_table), fake_tensor_mode, python_dispatcher_mode, pre_dispatch_mode, proxy_function_mode, \
    808      sym_mode, proxy_mode, disable_autocast_cache(), disable_proxy_modes_tracing(enable_current=True):
--> 809     t = dispatch_trace(wrap_key(func, args, fx_tracer, pre_dispatch), tracer=fx_tracer, concrete_args=tuple(phs))
    811 # TODO: kind of a bad way to do it, should maybe figure out a better way
    812 if tracing_mode == ""symbolic"":

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_compile.py:24, in _disable_dynamo.<locals>.inner(*args, **kwargs)
     20 @functools.wraps(fn)
     21 def inner(*args, **kwargs):
     22     import torch._dynamo
---> 24     return torch._dynamo.disable(fn, recursive)(*args, **kwargs)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:328, in _TorchDynamoContext.__call__.<locals>._fn(*args, **kwargs)
    326 dynamic_ctx.__enter__()
    327 try:
--> 328     return fn(*args, **kwargs)
    329 finally:
    330     set_eval_frame(prior)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:17, in wrap_inline.<locals>.inner(*args, **kwargs)
     15 @functools.wraps(fn)
     16 def inner(*args, **kwargs):
---> 17     return fn(*args, **kwargs)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:468, in dispatch_trace(root, tracer, concrete_args)
    462 @torch._disable_dynamo
    463 def dispatch_trace(
    464         root: Union[torch.nn.Module, Callable],
    465         tracer: Tracer,
    466         concrete_args: Optional[Tuple[Any, ...]] = None,
    467 ) -> GraphModule:
--> 468     graph = tracer.trace(root, concrete_args)
    469     name = root.__class__.__name__ if isinstance(root, torch.nn.Module) else root.__name__
    470     return GraphModule(tracer.root, graph, name)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:328, in _TorchDynamoContext.__call__.<locals>._fn(*args, **kwargs)
    326 dynamic_ctx.__enter__()
    327 try:
--> 328     return fn(*args, **kwargs)
    329 finally:
    330     set_eval_frame(prior)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:17, in wrap_inline.<locals>.inner(*args, **kwargs)
     15 @functools.wraps(fn)
     16 def inner(*args, **kwargs):
---> 17     return fn(*args, **kwargs)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py:817, in Tracer.trace(self, root, concrete_args)
    810         for module in self._autowrap_search:
    811             _autowrap_check(
    812                 patcher, module.__dict__, self._autowrap_function_ids
    813             )
    814         self.create_node(
    815             ""output"",
    816             ""output"",
--> 817             (self.create_arg(fn(*args)),),
    818             {},
    819             type_expr=fn.__annotations__.get(""return"", None),
    820         )
    822     self.submodule_paths = None
    823 finally:

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:485, in wrap_key.<locals>.wrapped(*proxies)
    482     assert isinstance(m, ProxyTorchDispatchMode)
    483     track_tensor_tree(flat_tensors, flat_proxies, constant=None, tracer=tracer)
--> 485 out = f(*tensors)
    486 out = pytree.tree_map_only(
    487     torch.Tensor,
    488     lambda t: get_proxy_slot(t, tracer, t, lambda x: x.proxy),
    489     out
    490 )
    491 out = pytree.tree_map_only(
    492     (SymInt, SymFloat, SymBool),
    493     lambda t: get_proxy_slot(t.node, tracer)(),
    494     out
    495 )

File <string>:1, in <lambda>(arg0)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1412, in create_functionalized_graph.<locals>.fwd_helper(*args)
   1411 def fwd_helper(*args):
-> 1412     return functionalized_f_helper(*args)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1363, in create_functionalized_graph.<locals>.functionalized_f_helper(*args)
   1360 torch._enable_functionalization(reapply_views=True)
   1361 try:
   1362     # Run the joint
-> 1363     f_outs = fn(*f_args)
   1364 finally:
   1365     torch._disable_functionalization()

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1165, in fn_input_mutations_to_outputs.<locals>.inner_fn(*args)
   1164 def inner_fn(*args):
-> 1165     outs = fn(*args)
   1166     assert len(meta.output_info) == len(outs)
   1167     # The compiled fw will return mutated input tensors, *including* metadata-only mutation.
   1168     # However, if keep_data_input_mutations is set, the compiled fw only needs to return metadata-mutated inputs.
   1169     # (because data-only input mutations are handled directly in the compiled graph)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:3496, in create_functional_call.<locals>.functional_call(*args, **kwargs)
   3492         warnings.filterwarnings(
   3493             ""ignore"", ""Anomaly Detection has been enabled.""
   3494         )
   3495         with torch.autograd.detect_anomaly(check_nan=False):
-> 3496             out = Interpreter(mod).run(*args[params_len:], **kwargs)
   3497 else:
   3498     out = mod(*args[params_len:], **kwargs)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/fx/interpreter.py:138, in Interpreter.run(self, initial_env, enable_io_processing, *args)
    135     continue
    137 try:
--> 138     self.env[node] = self.run_node(node)
    139 except Exception as e:
    140     if self.extra_traceback:

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/fx/interpreter.py:195, in Interpreter.run_node(self, n)
    193 assert isinstance(args, tuple)
    194 assert isinstance(kwargs, dict)
--> 195 return getattr(self, n.op)(n.target, args, kwargs)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/fx/interpreter.py:289, in Interpreter.call_method(self, target, args, kwargs)
    287 # Execute the method and return the result
    288 assert isinstance(target, str)
--> 289 return getattr(self_obj, target)(*args_tail, **kwargs)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_functorch/apis.py:188, in vmap.<locals>.wrapped(*args, **kwargs)
    187 def wrapped(*args, **kwargs):
--> 188     return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_functorch/vmap.py:266, in vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
    262     return _chunked_vmap(func, flat_in_dims, chunks_flat_args,
    263                          args_spec, out_dims, randomness, **kwargs)
    265 # If chunk_size is not specified.
--> 266 return _flat_vmap(
    267     func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs
    268 )

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_functorch/vmap.py:38, in doesnt_support_saved_tensors_hooks.<locals>.fn(*args, **kwargs)
     35 @functools.wraps(f)
     36 def fn(*args, **kwargs):
     37     with torch.autograd.graph.disable_saved_tensors_hooks(message):
---> 38         return f(*args, **kwargs)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_functorch/vmap.py:379, in _flat_vmap(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)
    377 try:
    378     batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
--> 379     batched_outputs = func(*batched_inputs, **kwargs)
    380     return _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)
    381 finally:

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/fx/graph_module.py:678, in GraphModule.recompile.<locals>.call_wrapped(self, *args, **kwargs)
    677 def call_wrapped(self, *args, **kwargs):
--> 678     return self._wrapped_call(self, *args, **kwargs)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/fx/graph_module.py:284, in _WrappedCall.__call__(self, obj, *args, **kwargs)
    282     raise e.with_traceback(None)
    283 else:
--> 284     raise e

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/fx/graph_module.py:274, in _WrappedCall.__call__(self, obj, *args, **kwargs)
    272         return self.cls_call(obj, *args, **kwargs)
    273     else:
--> 274         return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
    275 except Exception as e:
    276     assert e.__traceback__

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py:795, in Tracer.trace.<locals>.module_call_wrapper(mod, *args, **kwargs)
    788     return _orig_module_call(mod, *args, **kwargs)
    790 _autowrap_check(
    791     patcher,
    792     getattr(getattr(mod, ""forward"", mod), ""__globals__"", {}),
    793     self._autowrap_function_ids,
    794 )
--> 795 return self.call_module(mod, forward, args, kwargs)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:425, in PythonKeyTracer.call_module(self, m, forward, args, kwargs)
    422 def call_module(
    423         self, m: torch.nn.Module, forward: Callable[..., Any], args: Tuple[Any, ...], kwargs: Dict[str, Any]
    424 ) -> Any:
--> 425     return forward(*args, **kwargs)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py:788, in Tracer.trace.<locals>.module_call_wrapper.<locals>.forward(*args, **kwargs)
    787 def forward(*args, **kwargs):
--> 788     return _orig_module_call(mod, *args, **kwargs)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/nn/modules/module.py:1518, in Module._wrapped_call_impl(self, *args, **kwargs)
   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1517 else:
-> 1518     return self._call_impl(*args, **kwargs)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)
   1522 # If we don't have any hooks, we want to skip the rest of the logic in
   1523 # this function, and just call forward.
   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1525         or _global_backward_pre_hooks or _global_backward_hooks
   1526         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1527     return forward_call(*args, **kwargs)
   1529 try:
   1530     result = None

File <eval_with_key>.429:6, in forward(self, select)
      4 def forward(self, select):
      5     sum_1 = torch.sum(select);  select = None
----> 6     add = sum_1 + 0.01;  sum_1 = None
      7     return add

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/utils/_stats.py:20, in count.<locals>.wrapper(*args, **kwargs)
     18     simple_call_counter[fn.__qualname__] = 0
     19 simple_call_counter[fn.__qualname__] = simple_call_counter[fn.__qualname__] + 1
---> 20 return fn(*args, **kwargs)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:555, in ProxyTorchDispatchMode.__torch_dispatch__(self, func, types, args, kwargs)
    552 @count
    553 def __torch_dispatch__(self, func, types, args=(), kwargs=None):
    554     with self.sym_mode.enable(False), set_original_aten_op(func):
--> 555         return self.inner_torch_dispatch(func, types, args, kwargs)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:580, in ProxyTorchDispatchMode.inner_torch_dispatch(self, func, types, args, kwargs)
    577 if func in [prim.device.default]:
    578     return func(*args, **kwargs)
--> 580 return proxy_call(self, func, self.pre_dispatch, args, kwargs)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:262, in proxy_call(proxy_mode, func, pre_dispatch, args, kwargs)
    260 if func in CURRENT_DECOMPOSITION_TABLE:
    261     with proxy_mode:
--> 262         r = CURRENT_DECOMPOSITION_TABLE[func](*args, **kwargs)
    263         if r is not NotImplemented:
    264             return r

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_decomp/decompositions.py:1737, in _to_copy(x, dtype, layout, device, pin_memory, non_blocking, memory_format)
   1735     x = torch._prims.device_put(x, device)
   1736 if dtype is not None and not dtype_converted:
-> 1737     x = torch._prims.convert_element_type(x, dtype)
   1738     dtype_converted = True
   1739 # In case of dtype promotion, faketensor converted into tensor.
   1740 # Need to convert into faketensor if input was a faketensor.

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_ops.py:448, in OpOverload.__call__(self, *args, **kwargs)
    447 def __call__(self, *args, **kwargs):
--> 448     return self._op(*args, **kwargs or {})

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/utils/_stats.py:20, in count.<locals>.wrapper(*args, **kwargs)
     18     simple_call_counter[fn.__qualname__] = 0
     19 simple_call_counter[fn.__qualname__] = simple_call_counter[fn.__qualname__] + 1
---> 20 return fn(*args, **kwargs)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:555, in ProxyTorchDispatchMode.__torch_dispatch__(self, func, types, args, kwargs)
    552 @count
    553 def __torch_dispatch__(self, func, types, args=(), kwargs=None):
    554     with self.sym_mode.enable(False), set_original_aten_op(func):
--> 555         return self.inner_torch_dispatch(func, types, args, kwargs)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:580, in ProxyTorchDispatchMode.inner_torch_dispatch(self, func, types, args, kwargs)
    577 if func in [prim.device.default]:
    578     return func(*args, **kwargs)
--> 580 return proxy_call(self, func, self.pre_dispatch, args, kwargs)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:361, in proxy_call(proxy_mode, func, pre_dispatch, args, kwargs)
    358     else:
    359         args[0].proxy = proxy_out
--> 361 out = func(*args, **kwargs)
    363 # In some circumstances, we will be tracing in a situation where a tensor
    364 # is *statically* known to be a constant (currently, this only happens if
    365 # you run torch.tensor; deterministic factory functions like torch.arange
   (...)
    382 # propagating const-ness.  Similarly, we don't require the constant to
    383 # live on CPU, but we could.
    384 any_constant = pytree.tree_any_only(_ProxyTensor, lambda t: t.constant is not None, (f_args, f_kwargs))

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_ops.py:448, in OpOverload.__call__(self, *args, **kwargs)
    447 def __call__(self, *args, **kwargs):
--> 448     return self._op(*args, **kwargs or {})

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/utils/_stats.py:20, in count.<locals>.wrapper(*args, **kwargs)
     18     simple_call_counter[fn.__qualname__] = 0
     19 simple_call_counter[fn.__qualname__] = simple_call_counter[fn.__qualname__] + 1
---> 20 return fn(*args, **kwargs)

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1250, in FakeTensorMode.__torch_dispatch__(self, func, types, args, kwargs)
   1248 assert self not in _get_current_dispatch_mode_stack(), func
   1249 try:
-> 1250     return self.dispatch(func, types, args, kwargs)
   1251 except TypeError:
   1252     log.exception(""fake tensor raised TypeError"")

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1470, in FakeTensorMode.dispatch(self, func, types, args, kwargs)
   1464 if (
   1465     ""prims::"" in func._schema.name
   1466     and hasattr(func, ""prim_meta_impl"")
   1467     and not stride_incorrect_op(func)
   1468 ):
   1469     with self:
-> 1470         return func.prim_meta_impl(*args, **kwargs)
   1472 # Users can register FakeTensor rules for custom operators
   1473 # Call them if they exist.
   1474 if func.name() in torch._custom_op.impl.global_registry:

File /data/shurui.gui/mambaforge/envs/CSR/lib/python3.11/site-packages/torch/_prims/__init__.py:1993, in _convert_element_type_meta(a, dtype)
   1991 def _convert_element_type_meta(a: TensorLikeType, dtype: torch.dtype) -> TensorLikeType:
   1992     # Type checks
-> 1993     assert isinstance(a, TensorLike)
   1994     assert isinstance(dtype, torch.dtype)
   1996     # dtype conversion preserves dense strides
```

### Versions

PyTorch version: 2.1.0
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.4 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.11.6 | packaged by conda-forge | (main, Oct  3 2023, 10:40:35) [GCC 12.3.0] (64-bit runtime)
Python platform: Linux-5.15.0-69-generic-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: 12.3.52
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration:
GPU 0: NVIDIA A100 80GB PCIe
GPU 1: NVIDIA A100 80GB PCIe
GPU 2: NVIDIA A100 80GB PCIe
GPU 3: NVIDIA A100 80GB PCIe
GPU 4: NVIDIA A100 80GB PCIe
GPU 5: NVIDIA A100 80GB PCIe
GPU 6: NVIDIA A100 80GB PCIe
GPU 7: NVIDIA A100 80GB PCIe

Nvidia driver version: 510.60.02
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   46 bits physical, 48 bits virtual
CPU(s):                          112
On-line CPU(s) list:             0-111
Thread(s) per core:              2
Core(s) per socket:              28
Socket(s):                       2
NUMA node(s):                    2
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           85
Model name:                      Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
Stepping:                        7
CPU MHz:                         3399.999
CPU max MHz:                     4000.0000
CPU min MHz:                     1000.0000
BogoMIPS:                        5400.00
Virtualization:                  VT-x
L1d cache:                       1.8 MiB
L1i cache:                       1.8 MiB
L2 cache:                        56 MiB
L3 cache:                        77 MiB
NUMA node0 CPU(s):               0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110
NUMA node1 CPU(s):               1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111
Vulnerability Itlb multihit:     KVM: Mitigation: VMX disabled
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Retbleed:          Mitigation; Enhanced IBRS
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Mitigation; TSX disabled
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke avx512_vnni md_clear flush_l1d arch_capabilities

Versions of relevant libraries:
[pip3] numpy==1.26.0
[pip3] torch==2.1.0
[pip3] torch_geometric==2.4.0
[pip3] torchaudio==2.1.0
[pip3] torchvision==0.16.0
[pip3] triton==2.1.0
[conda] blas                      2.116                       mkl    conda-forge
[conda] blas-devel                3.9.0            16_linux64_mkl    conda-forge
[conda] cudatoolkit               11.8.0              h4ba93d1_12    conda-forge
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] libblas                   3.9.0            16_linux64_mkl    conda-forge
[conda] libcblas                  3.9.0            16_linux64_mkl    conda-forge
[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch
[conda] liblapack                 3.9.0            16_linux64_mkl    conda-forge
[conda] liblapacke                3.9.0            16_linux64_mkl    conda-forge
[conda] mkl                       2022.1.0           h84fe81f_915    conda-forge
[conda] mkl-devel                 2022.1.0           ha770c72_916    conda-forge
[conda] mkl-include               2022.1.0           h84fe81f_915    conda-forge
[conda] numpy                     1.26.0          py311h64a7726_0    conda-forge
[conda] pyg                       2.4.0           py311_torch_2.1.0_cu118    pyg
[conda] pytorch                   2.1.0           py3.11_cuda11.8_cudnn8.7.0_0    pytorch
[conda] pytorch-cuda              11.8                 h7e8668a_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                2.1.0               py311_cu118    pytorch
[conda] torchtriton               2.1.0                     py311    pytorch
[conda] torchvision               0.16.0              py311_cu118    pytorch

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @Chillee @samdow @kshitij12345 @janeyx99"
111729,An OOM where there should not be any OOM.,"### üêõ Describe the bug

I see similar type of errors being asked about in quite a few places, with advice given being usually useless. The suggestion below to muck around with an environment variable is similarly useless.

What is confounding to me is that memory allocation is tiny in comparison to still available space. Why is this happening?

¬†
Traceback (most recent call last):
  File ""fine-tune.py"", line 338, in <module>
    train()
  File ""fine-tune.py"", line 331, in train
    trainer.train()
  File ""/home/developer/mambaforge/envs/FinGPT/lib/python3.8/site-packages/transformers/trainer.py"", line 1591, in train
    return inner_training_loop(
  File ""/home/developer/mambaforge/envs/FinGPT/lib/python3.8/site-packages/transformers/trainer.py"", line 1726, in _inner_training_loop
    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
  File ""/home/developer/mambaforge/envs/FinGPT/lib/python3.8/site-packages/accelerate/accelerator.py"", line 1280, in prepare
    result = self._prepare_deepspeed(*args)
  File ""/home/developer/mambaforge/envs/FinGPT/lib/python3.8/site-packages/accelerate/accelerator.py"", line 1662, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File ""/home/developer/mambaforge/envs/FinGPT/lib/python3.8/site-packages/deepspeed/__init__.py"", line 171, in initialize
    engine = DeepSpeedEngine(args=args,
  File ""/home/developer/mambaforge/envs/FinGPT/lib/python3.8/site-packages/deepspeed/runtime/engine.py"", line 304, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File ""/home/developer/mambaforge/envs/FinGPT/lib/python3.8/site-packages/deepspeed/runtime/engine.py"", line 1212, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File ""/home/developer/mambaforge/envs/FinGPT/lib/python3.8/site-packages/deepspeed/runtime/engine.py"", line 1473, in _configure_zero_optimizer
    optimizer = DeepSpeedZeroOptimizer(
  File ""/home/developer/mambaforge/envs/FinGPT/lib/python3.8/site-packages/deepspeed/runtime/zero/stage_1_and_2.py"", line 509, in __init__
    self.initialize_optimizer_states()
  File ""/home/developer/mambaforge/envs/FinGPT/lib/python3.8/site-packages/deepspeed/runtime/zero/stage_1_and_2.py"", line 644, in initialize_optimizer_states
    self.optimizer.step()
  File ""/home/developer/pytorch/torch/optim/lr_scheduler.py"", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File ""/home/developer/pytorch/torch/optim/optimizer.py"", line 280, in wrapper
    out = func(*args, **kwargs)
  File ""/home/developer/pytorch/torch/optim/optimizer.py"", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File ""/home/developer/pytorch/torch/optim/adamw.py"", line 171, in step
    adamw(
  File ""/home/developer/pytorch/torch/optim/adamw.py"", line 321, in adamw
    func(
  File ""/home/developer/pytorch/torch/optim/adamw.py"", line 564, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 134.00 MiB (GPU 0; 11.93 GiB total capacity; 4.48 GiB already allocated; 6.69 GiB free; 4.77 GiB allowed; 4.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


### Versions

UBuntu 20.04
Python 3.8
Anaconda environment
Inside a docker
"
111728,Not Implemented Issue,"### üöÄ The feature, motivation and pitch

NotImplementedError: The operator 'aten::_unique2' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.

### Alternatives

Please add this

### Additional context

_No response_

cc @kulinseth @albanD @malfet @DenisVieriu97 @razarmehr @abhudev"
111718,Wrong way of checking if CustomModule is a subclass of torch.nn.Module ,"### üêõ Describe the bug

When I build my custom module and try to add it to a sequential text processing, with ""[torchtext.transforms.Sequential](https://pytorch.org/text/stable/transforms.html#torchtext.transforms.Sequential)"" It raises an error even though I'm doing the sub classing correctly.

This is a fragment of the error stack trace:
![image](https://github.com/pytorch/pytorch/assets/147768729/a5e62c5e-7cc8-40b4-aa3d-73bdfdd14da2)

When I go directly to the code to see what happens I find that the method used for checking if the provided module is a subclass of torch.nn.Module I find this:
![image](https://github.com/pytorch/pytorch/assets/147768729/a9939b8c-9270-4c81-90ee-e25470f3d92e)

This is a mistake because the function for doing the subclassing check is 'issubclass' instead of 'isinstance'. I changed the code and it worked as needed, so please check this bug out. 



### Versions

Collecting environment information...
PyTorch version: 2.1.0+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Manjaro Linux (x86_64)
GCC version: (GCC) 13.2.1 20230801
Clang version: 16.0.6
CMake version: version 3.27.5
Libc version: glibc-2.38

Python version: 3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801] (64-bit runtime)
Python platform: Linux-6.1.53-1-MANJARO-x86_64-with-glibc2.38
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1050 Ti
Nvidia driver version: 535.104.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      43 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             12
On-line CPU(s) list:                0-11
Vendor ID:                          AuthenticAMD
Model name:                         AMD Ryzen 5 3600X 6-Core Processor
CPU family:                         23
Model:                              113
Thread(s) per core:                 2
Core(s) per socket:                 6
Socket(s):                          1
Stepping:                           0
Frequency boost:                    enabled
CPU(s) scaling MHz:                 70%
CPU max MHz:                        4408,5928
CPU min MHz:                        2200,0000
BogoMIPS:                           7603,86
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sev sev_es
Virtualization:                     AMD-V
L1d cache:                          192 KiB (6 instances)
L1i cache:                          192 KiB (6 instances)
L2 cache:                           3 MiB (6 instances)
L3 cache:                           32 MiB (2 instances)
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-11
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        Not affected
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Not affected
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Not affected
Vulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection
Vulnerability Spec rstack overflow: Mitigation; safe RET
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] numpy==1.26.1
[pip3] pytorch-lightning==2.1.0
[pip3] torch==2.1.0
[pip3] torchaudio==2.1.0
[pip3] torchdata==0.7.0
[pip3] torchmetrics==1.2.0
[pip3] torchtext==0.16.0
[pip3] torchvision==0.16.0
[pip3] triton==2.1.0
[conda] Could not collect

cc @albanD @mruberry @jbschlosser @walterddr @mikaylagawarecki @ezyang @msaroufim"
111716,Cannot pip install torch 2.0.1,"### üêõ Describe the bug

I was trying to follow the instruction on the [webpage](https://pytorch.org/get-started/previous-versions/) to install torch 2.0.1 using pip. 

```
# ROCM 5.4.2 (Linux only)
pip install torch==2.0.1+rocm5.4.2 torchvision==0.15.2+rocm5.4.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/rocm5.4.2
# CUDA 11.7
pip install torch==2.0.1+cu117 torchvision==0.15.2+cu117 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu117
# CUDA 11.8
pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118
# CPU only
pip install torch==2.0.1+cpu torchvision==0.15.2+cpu torchaudio==2.0.2 --index-url https://download
```

But it would throw an error e.g. for installing 2.0.1+cu117 
```
ERROR: Could not find a version that satisfies the requirement torch==2.0.1+cu117 (from versions: 1.13.0+cu117, 1.13.1+cu117)
ERROR: No matching distribution found for torch==2.0.1+cu117
```
Commands for other versions above throw similar errors.

### Versions

Attempt to install 2.0.1

cc @svekars @carljparker"
111713,[dynamo] generic `is_` type shortcut is not appropriately guarded,"### üêõ Describe the bug


This hack
https://github.com/pytorch/pytorch/blob/5a2f97dee80ca27b732e12b61359d6e475a9c03b/torch/_dynamo/variables/builtin.py#L1310
in https://github.com/pytorch/pytorch/pull/104840

is too strong.

### Use-Cases
Support for tracing `is_` when there's type mismatch: https://github.com/pytorch/pytorch/issues/109504
Part of the way to: https://github.com/pytorch/pytorch/issues/111550

### Solution
Perhaps installing unalias check and guarding on as python constant might be good enough to solve generic is_ check without resorting to hacks like this.


### Repro
```python
import collections

def fn(x, y, z):
    z += 1
    return x is y, z

x = collections.OrderedDict({1: 2})
y = {1: 2}
z = torch.tensor([1])

opt_fn = torch.compile(fn, backend=""eager"", fullgraph=True)

assert opt_fn(x, y, z) == fn(x, y, z)  # Compile with x is y == False
assert opt_fn(x, x, z) == fn(x, x, z)  # Does not recompile as input types are not guarded
```

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng 

### Versions

main

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111711,[aotinductor] 14k models: CppCompileError: C++ compile error,"```
25 errors like: CppCompileError: C++ compile error (example ./generated/test_krrish94_nerf_pytorch.py:SinThetaByTheta # pytest ./generated/test_krrish94_nerf_pytorch.py -k test_001)
```

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111710,`fbgemm` update causes failures in `test_embedding.py`,"### üêõ Describe the bug

```
% python3 test/nn/test_embedding.py -k test_EmbeddingBag_per_sample_weights_and_new_offsets_cpu_int32_int32_bfloat16
...
AssertionError: Tensor-likes are not close!

Mismatched elements: 4 / 10 (40.0%)
Greatest absolute difference: 9.1875 at index (3, 1) (up to 0.1 allowed)
Greatest relative difference: 0.57421875 at index (3, 1) (up to 0 allowed)
```

Reverting https://github.com/pytorch/FBGEMM/pull/1851 fixes the problem

### Versions

Nightly

cc @ezyang @gchanan @zou3519 @kadeng"
111709,lintrunner job time keeps growing,"For example:
Sep 29 https://hud.pytorch.org/pytorch/pytorch/commit/bc047ec906d8e1730e2ccd8192cef3c3467d75d1 - 18 mins
Oct 06 https://hud.pytorch.org/pytorch/pytorch/commit/65d40a72c4ff3cf5218dffda8b5da60ea2163890 - 22 mins
Today, Oct 20 https://hud.pytorch.org/pytorch/pytorch/commit/303c54dbd9921d78ed01116547c063b450338c74 - 26 mins

If we want to reduce the time, we need to investigate what's taking so long.
Two possible candidates are ruff and clangtidy.

It would be nice to have time split by linter in the lintrunner job logs.


cc @ZainRizvi @huydhn @clee2000 @PaliC @malfet "
111706,DISABLED test_meta_outplace_fft_ifft_cpu_uint8 (__main__.TestMetaCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_outplace_fft_ifft_cpu_uint8&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17905842710).

Over the past 3 hours, it has been determined flaky in 12 workflow(s) with 36 failures and 12 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_outplace_fft_ifft_cpu_uint8`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

cc @ezyang @mruberry @Lezcano @peterbell10 @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111704,Add more flexibility on print / output console,"### üöÄ The feature, motivation and pitch

For a large number of debug usage, printing tensors on console is clearly usefull. The current C++ API output fix the `std::cout` precision on float to 4 decimals. 

> float can hold up to 7 decimal digits accurately while double can hold up to 15

The only 'user' parameter of `torch::print()` is the `int64_t linesize = 80`.

### Alternatives

I see at least two major options that can be usefull for common usage:

1. set the number of decimal (see `std::set_precision`) for floating-point numbers.
2. set the scientific output optional, not automatic (see `std::fixed` / `std::scientific`) 

### Additional context

I currently output (`std::cout << tensor`) two differents 8x8 Float Tensors for comparison:

```
 0.2360  0.0258  0.1689  0.1564 -0.2261  0.0567  0.1844  0.3033
 0.2940 -0.2500 -0.0653 -0.0805  0.2112 -0.1635  0.2915  0.3023
 0.2912  0.0944  0.1377 -0.1824 -0.1882 -0.2844  0.0189 -0.2718
-0.2812 -0.0292 -0.3035 -0.0724  0.1665 -0.2391  0.0724 -0.1974
-0.2716  0.1460 -0.3044  0.1312  0.2848  0.1549  0.2815  0.1874
 0.0980 -0.1967  0.1135  0.2974 -0.1395  0.2800 -0.2298  0.2627
 0.2153  0.1423  0.2779  0.0157 -0.3499  0.1718  0.2147  0.2121
 0.2856  0.2004  0.0951 -0.0757 -0.3016  0.0643 -0.2685 -0.1260
[ CUDAFloatType{8,8} ]

 0.2360  0.0258  0.1689  0.1564 -0.2261  0.0567  0.1844  0.3033
 0.2940 -0.2500 -0.0653 -0.0805  0.2112 -0.1635  0.2915  0.3023
 0.2912  0.0944  0.1377 -0.1824 -0.1882 -0.2844  0.0189 -0.2718
-0.2812 -0.0292 -0.3035 -0.0724  0.1665 -0.2391  0.0724 -0.1973
-0.2716  0.1460 -0.3044  0.1312  0.2848  0.1549  0.2815  0.1874
 0.0980 -0.1967  0.1135  0.2974 -0.1395  0.2800 -0.2298  0.2627
 0.2153  0.1423  0.2779  0.0157 -0.3499  0.1718  0.2147  0.2121
 0.2856  0.2004  0.0951 -0.0757 -0.3016  0.0643 -0.2685 -0.1260
[ CUDAFloatType{8,8} ]
```

But the `torch::allclose(actual, expected, rtol, atol);` (where `rtol = atol = 1e-5`) give me a `false`.

This `false` is probably `true`, but console output don't help for a quick check.

!!! Thank you for Torch !!!

cc @jbschlosser"
111695,Runnings SentenceTransformer encoding step causes Docker containers on Mac (Silicon) to crash with code 139,"### üêõ Describe the bug

Hi! Hopefully there isn't a similar issue already open. I couldn't find one after a search through the issues list. Feel free to mark as duplicate/close if it already exists.

I've created this repository with a minimal setup to reproduce the error: https://github.com/sabaimran/repro-torch-bug. You just have to clone it and run `docker-compose up` to see the error. Basically it runs the script below in a minimal Docker container:

```python
import torch
from langchain.embeddings import HuggingFaceEmbeddings

class EmbeddingsModel:
    def __init__(self):
        self.model_name = ""sentence-transformers/multi-qa-MiniLM-L6-cos-v1""
        encode_kwargs = {""normalize_embeddings"": True}

        if torch.cuda.is_available():
            # Use CUDA GPU
            device = torch.device(""cuda:0"")
        elif torch.backends.mps.is_available():
            # Use Apple M1 Metal Acceleration
            device = torch.device(""mps"")
        else:
            device = torch.device(""cpu"")

        self.device = device
        model_kwargs = {""device"": device}
        self.embeddings_model = HuggingFaceEmbeddings(
            model_name=self.model_name, encode_kwargs=encode_kwargs, model_kwargs=model_kwargs
        )

    def embed_documents(self, docs: List[str]):
        logger.info(f""Using device: {self.device} to embed {len(docs)} documents"")
        return self.embeddings_model.embed_documents(docs)

model = EmbeddingsModel()
embeddings = model.embed_documents([""this is a document"", ""so is this""])
print(f""Created embeddings of length {len(embeddings)}"")
```

If you run this code inside of a Docker container (with the appropriate dependencies), it will fail with exit code 139.

Pinning the `torch` package to `2.0.1` circumvents the error. See this other relevant issue: https://github.com/docker/for-mac/issues/7016

### Versions

Collecting environment information...
PyTorch version: 2.1.0
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: macOS 13.2.1 (arm64)
GCC version: Could not collect
Clang version: 14.0.3 (clang-1403.0.22.14.1)
CMake version: version 3.26.4
Libc version: N/A

Python version: 3.11.4 (main, Jul 10 2023, 18:52:37) [Clang 14.0.3 (clang-1403.0.22.14.1)] (64-bit runtime)
Python platform: macOS-13.2.1-arm64-arm-64bit
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Apple M2 Pro

Versions of relevant libraries:
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.26.1
[pip3] torch==2.1.0
[pip3] torchvision==0.16.0
[conda] Could not collect

cc @ezyang @gchanan @zou3519 @kadeng @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh @jgong5 @mingfeima @sanchitintel @ashokei @jingxu10 @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen @malfet @snadampal @albanD"
111693,"[export] 14k models: AssertionError: graph-captured input # 2, of type <class 'torch.nn.parameter.Parameter'>, is not among original inputs of types","167 errors like: AssertionError: graph-captured input # 2, of type <class 'torch.nn.parameter.Parameter'>, is not among original inputs of types: (<class 'torch.Tensor'>) (example ./generated/test_XPixelGroup_BasicSR.py:SPADEResnetBlock # pytest ./generated/test_XPixelGroup_BasicSR.py -k test_030)

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan"
111692,DISABLED test_sigmoid (__main__.TestQuantizedOps),"Platforms: mac, macos

This test was disabled because it is failing on main branch ([recent examples](http://torch-ci.com/failure/test_quantization.py%3A%3ATestQuantizedOps%3A%3Atest_sigmoid)).

This test is failing on MacOS x86 https://hud.pytorch.org/pytorch/pytorch/commit/ca7d084ff9b67675cfff0d175ea6b96fcedc4950

cc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @Xia-Weiwen @leslie-fang-intel @malfet @albanD"
111691,[aotinductor] 14k models: TypeError: make_boxed_func..g() missing 1 required positional argument: 'args' ,"347 errors like: TypeError: make_boxed_func..g() missing 1 required positional argument: 'args' (example ./generated/test_ludwig_ai_ludwig.py:SequenceReducer # pytest ./generated/test_ludwig_ai_ludwig.py -k test_015)

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111686,RecursionError for backend='inductor' with a loop,"### üêõ Describe the bug

Running the following code causes RecursionError.
It's not a very practical example, but it works totally fine in eager mode and with `torch.jit.script`.

``` python
import torch
class Net(torch.nn.Module):
    def forward(self, x):
        for i in range(1000):
            x = 1.0 * x
        return x
net = Net()
net = torch.compile(net)
x = torch.tensor([1.0])
print(net(x))
```

### Error logs

...

```
  File ""/home/sdym/.conda/envs/py39/lib/python3.9/site-packages/torch/_inductor/lowering.py"", line 397, in <listcomp>
    return fn(*[load(index) for load in loaders])
  File ""/home/sdym/.conda/envs/py39/lib/python3.9/site-packages/torch/_inductor/lowering.py"", line 397, in inner_fn
    return fn(*[load(index) for load in loaders])
  File ""/home/sdym/.conda/envs/py39/lib/python3.9/site-packages/torch/_inductor/lowering.py"", line 397, in <listcomp>
    return fn(*[load(index) for load in loaders])
  File ""/home/sdym/.conda/envs/py39/lib/python3.9/site-packages/torch/_inductor/ir.py"", line 2393, in loader
    return ops.load(self.name, indexer(index))
  File ""/home/sdym/.conda/envs/py39/lib/python3.9/site-packages/torch/_inductor/virtualized.py"", line 232, in inner
    return OpsWrapper._wrap(getattr(_ops, name)(*new_args, **new_kwargs))
  File ""/home/sdym/.conda/envs/py39/lib/python3.9/site-packages/torch/_inductor/virtualized.py"", line 132, in inner
    line = getattr(self.parent_handler, name)(*args, **kwargs)
  File ""/home/sdym/.conda/envs/py39/lib/python3.9/site-packages/torch/_inductor/virtualized.py"", line 69, in inner
    fargs = [_arg_str(a) for a in args]
  File ""/home/sdym/.conda/envs/py39/lib/python3.9/site-packages/torch/_inductor/virtualized.py"", line 69, in <listcomp>
    fargs = [_arg_str(a) for a in args]
  File ""/home/sdym/.conda/envs/py39/lib/python3.9/site-packages/torch/_inductor/virtualized.py"", line 59, in _arg_str
    return sympy_str(a)
  File ""/home/sdym/.conda/envs/py39/lib/python3.9/site-packages/torch/_inductor/utils.py"", line 395, in sympy_str
    return str(expr)
  File ""/home/sdym/.conda/envs/py39/lib/python3.9/site-packages/sympy/core/_print_helpers.py"", line 29, in __str__
    return sstr(self, order=None)
  File ""/home/sdym/.conda/envs/py39/lib/python3.9/site-packages/sympy/printing/printer.py"", line 372, in __call__
    return self.__wrapped__(*args, **kwargs)
  File ""/home/sdym/.conda/envs/py39/lib/python3.9/site-packages/sympy/printing/str.py"", line 999, in sstr
    p = StrPrinter(settings)
  File ""/home/sdym/.conda/envs/py39/lib/python3.9/site-packages/sympy/printing/printer.py"", line 261, in __init__
    self._settings = self._get_initial_settings()
  File ""/home/sdym/.conda/envs/py39/lib/python3.9/site-packages/sympy/printing/printer.py"", line 252, in _get_initial_settings
    settings = cls._default_settings.copy()
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
RecursionError: maximum recursion depth exceeded while calling a Python object
```


### Minified repro

_No response_

### Versions

Collecting environment information...
PyTorch version: 2.1.0+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Fedora release 36 (Thirty Six) (x86_64)
GCC version: (conda-forge gcc 10.3.0-16) 10.3.0
Clang version: 11.1.0 (https://github.com/conda-forge/clangdev-feedstock 2816c2cf231a2d3a6d621af9bbb2c590c9e63fe7)
CMake version: version 3.26.1
Libc version: glibc-2.35

Python version: 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21)  [GCC 10.3.0] (64-bit runtime)
Python platform: Linux-6.2.15-100.fc36.x86_64-x86_64-with-glibc2.35
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Address sizes:                   43 bits physical, 48 bits virtual
Byte Order:                      Little Endian
CPU(s):                          64
On-line CPU(s) list:             0-63
Vendor ID:                       AuthenticAMD
Model name:                      AMD Ryzen Threadripper PRO 3975WX 32-Cores
CPU family:                      23
Model:                           49
Thread(s) per core:              2
Core(s) per socket:              32
Socket(s):                       1
Stepping:                        0
Frequency boost:                 enabled
CPU(s) scaling MHz:              52%
CPU max MHz:                     4368.1641
CPU min MHz:                     2200.0000
BogoMIPS:                        6986.81
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sev sev_es
Virtualization:                  AMD-V
L1d cache:                       1 MiB (32 instances)
L1i cache:                       1 MiB (32 instances)
L2 cache:                        16 MiB (32 instances)
L3 cache:                        128 MiB (8 instances)
NUMA node(s):                    1
NUMA node0 CPU(s):               0-63
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Mmio stale data:   Not affected
Vulnerability Retbleed:          Mitigation; untrained return thunk; SMT enabled with STIBP protection
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected

Versions of relevant libraries:
[pip3] flake8==6.0.0
[pip3] flake8-bugbear==23.6.5
[pip3] flake8-comprehensions==3.3.0
[pip3] flake8-executable==2.0.4
[pip3] flake8-logging-format==0.9.0
[pip3] flake8-pyi==23.5.0
[pip3] flake8-simplify==0.19.3
[pip3] mypy==1.4.1
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.23.1
[pip3] numpydoc==1.5.0
[pip3] onnx==1.14.1
[pip3] pytorch-sphinx-theme==0.0.19
[pip3] torch==2.1.0
[pip3] torcheval-nightly==2022.12.27
[pip3] torchsnapshot-nightly==2022.11.28
[pip3] torchtnt==0.0.4
[pip3] triton==2.1.0
[conda] magma-cuda116             2.6.1                         0    pytorch
[conda] mkl                       2022.1.0           h84fe81f_915    conda-forge
[conda] mkl-include               2022.1.0           h84fe81f_915    conda-forge
[conda] numpy                     1.23.1                   pypi_0    pypi
[conda] numpydoc                  1.5.0                    pypi_0    pypi
[conda] pytorch-sphinx-theme      0.0.19                   pypi_0    pypi
[conda] torch                     2.1.0                    pypi_0    pypi
[conda] torcheval-nightly         2022.12.27               pypi_0    pypi
[conda] torchfix                  0.1.1                    pypi_0    pypi
[conda] torchsnapshot-nightly     2022.11.28               pypi_0    pypi
[conda] torchtnt                  0.0.4                    pypi_0    pypi
[conda] triton                    2.1.0                    pypi_0    pypi


cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111678,AOT Inductor Does not Work with minifier,"### üêõ Describe the bug

Because AOT Inductor attaches parameters to the GraphModule, it does not currently work with minifier. 

>   File ""/opt/dlami/nvme/eellison/work/pytorch/torch/_dynamo/repro/after_aot.py"", line 444, in repro_common
    assert not any(mod.named_parameters())

### Versions

master

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler"
111676,[export] self.buffer += 1 raises error,"```
import torch

class Mod(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.register_buffer(""foo"", torch.ones(2, 3))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        self.foo += x
        return self.foo

torch.export(Mod(), (torch.ones(2, 3),))
```
produces
```
Mutating module attribute foo during export.

from user code:
   File ""/tmp/ipykernel_578241/3307013751.py"", line 9, in forward
    self.foo += x

Set TORCH_LOGS=""+dynamo"" and TORCHDYNAMO_VERBOSE=1 for more information
```

Changing `self.foo += x` to the equivalent `self.foo.add_(x)` works as expected.

The motivation behind disallowing attribute mutation makes sense. However, buffers should be mutable, and dynamo should be smart enough to recognize that `+=` desugars to `add_` when done on a tensor.

cc @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan"
111674,Dynamo Compile samples should record file/line that raised exception,"### üêõ Describe the bug

@voznesenskym and I were looking at https://fburl.com/scuba/dynamo_compile/7pzz3bi1 and we noticed that while ""Fail reason"" doesn't include the file/line that raised the exception, which would be useful.

cc @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @yanboliang 

### Versions

main"
111671,Foreach optimizers don't work with torch.set_default_dtype(torch.float64),"```python
import torch

torch.set_default_dtype(torch.float64)
device = ""cuda""
model = torch.nn.Linear(200, 1, bias=True).to(device)
opt = torch.optim.Adam(model.parameters(),lr=0.001) 
x = torch.rand(1, 200).to(device)
model(x).sum().backward()
opt.step()
```
Error:
```
RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32 notwithstanding
```
Foreach is implicitly True because we are using cuda. Error goes away if we explicitly set foreach=False.

Versions: main

(originally reported [here](https://discuss.pytorch.org/t/tensors-of-the-same-index-must-be-on-the-same-device-and-the-same-dtype-except-step-tensors-that-can-be-cpu-and-float32-notwithstanding/190335/2))

cc @vincentqb @jbschlosser @albanD @janeyx99 @crcrpar"
111669,Buffer overflow not prevented on MPS devices,"### üêõ Describe the bug

When indexing using an indexing tensor (or list), it is possible to read or write outside the valid range of the tensor.

Minimal example:

```python
import torch

x = torch.arange(4, device=torch.device(""mps""))
y = x[:2]
y[torch.tensor([3])] = -1
x[3]
```

This code should raise an IndexError and leave x unchanged, but it instead gives -1.

In this example, the overflow reaches a known memory location, but perhaps in general it can reach arbitrary memory on the GPU.

### Versions

Collecting environment information...
PyTorch version: 2.1.0
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: macOS 13.5.2 (arm64)
GCC version: Could not collect
Clang version: 15.0.0 (clang-1500.0.40.1)
CMake version: version 3.27.7
Libc version: N/A

Python version: 3.10.8 (main, Nov 24 2022, 08:08:27) [Clang 14.0.6 ] (64-bit runtime)
Python platform: macOS-13.5.2-arm64-arm-64bit
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Apple M2 Pro

Versions of relevant libraries:
[pip3] flake8==6.1.0
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.26.1
[pip3] torch==2.1.0
[conda] numpy                     1.26.1                   pypi_0    pypi
[conda] torch                     2.1.0                    pypi_0    pypi

cc @kulinseth @albanD @malfet @DenisVieriu97 @razarmehr @abhudev"
111666,torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::binary_cross_entropy' to ONNX opset version 14 is not supported.,"### üöÄ The feature, motivation and pitch

Unable to export ONNX model from https://github.com/xue-pai/FuxiCTR/tree/main/model_zoo/AFM. While exporting onnx it throws torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::binary_cross_entropy' 

### Alternatives

_No response_

### Additional context

_No response_"
111663,[dynamo] Tracking: object identity,"### üöÄ The feature, motivation and pitch

This covers many things:
1. Tensor Identity
2. User objects identity - usual objects, enums, builtins??

Use cases
- [ ] https://github.com/pytorch/pytorch/issues/111550
- [x] https://github.com/pytorch/pytorch/issues/111556

Tensor Aliasing Methods and Obstacles
- [x] https://github.com/pytorch/pytorch/issues/111585
- [x] https://github.com/pytorch/pytorch/issues/111649
- [x] https://github.com/pytorch/pytorch/issues/111544

Overall Obstacles and Discussion
- [x] https://github.com/pytorch/pytorch/issues/111542
- [ ] https://github.com/pytorch/pytorch/issues/111562 - not sure if we want to implement non-aliasing for general objects


cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng"
111662,torch.dynamo (caching?) issues with `Optional[np.ndarray]` arguments,"### üêõ Describe the bug

```
$ cat nonz.py 
import torch
import numpy as np

def fn(x=None):
    if x is None:
        x = np.ones(3)
    return x**2

opt_fn = torch.compile(fn)

x = np.zeros((2, 2))
print(opt_fn(x))
print(opt_fn())
```

fails with

```
$ python nonz.py 
[[0. 0.]
 [0. 0.]]
ERROR RUNNING GUARDS fn nonz.py:9
lambda L, **___kwargs_ignored:
  ___guarded_code.valid and
  ___check_global_state() and
  hasattr(__as_tensor(L['x']), '_dynamo_dynamic_indices') == False and
  utils_device.CURRENT_DEVICE == None and
  ___skip_backend_check() or ___current_backend() == ___lookup_backend(139895339872512) and
  ___check_tensors(__as_tensor(L['x']), tensor_check_names=tensor_check_names)
Traceback (most recent call last):
  File ""nonz.py"", line 20, in <module>
    print(opt_fn())
  File ""/home/ev-br/repos/pytorch/torch/_dynamo/eval_frame.py"", line 410, in _fn
    return fn(*args, **kwargs)
  File ""<string>"", line 7, in guard
RuntimeError: Could not infer dtype of NoneType
```

Curiously, exchanging the order of calls, i.e. making it 

```
print(opt_fn())
print(opt_fn(x))
```

works fine and produces the correct result. Removing the numpy from the equation --- chaging all `np.` to `torch.` also works fine and produces the correct result.

### Versions

main

cc @mruberry @rgommers @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng"
111655,"can not use ""import torch.nn.LayerNorm as Norm""","### üêõ Describe the bug

Why can't I import when I want to invoke LayerNorm with the ""import torch.nn.LayerNorm as Norm"" statement.
![image](https://github.com/pytorch/pytorch/assets/108966996/542d592f-a0de-4b65-985a-b9f92afc950d)


### Versions

2.1.0"
111654,"Static Linking C++, Op not available at runtime","### üêõ Describe the bug

When linking with static libtorch and torchvision libraries, I am able to build, but at runtime, I get an error about an `Unknown builtin op: aten::mul`. 

I have found references indicating that including <torchvision/vision.h> should cause the operators to be registered so they are linked in, but that doesn't seem to do the trick.

I've also found references indicating that forcing the linker to link the ""whole archive"" for libtorch_cpu.a should force it to include all the operators in the linked executable. I have done this, and it does overcome the problem - however, this feels a bit like a workaround, and we aren't able to use that as a long-term solution. When I link in the whole archive, the executable jumps from 87MB to 339MB.

I've also found some references suggesting calling `c10::RegisterOps`, or `torch::RegisterOps`, neither of which seem to exist.  I found both `c10::RegisterOperators` and `torch::RegisterOperators`, but calling them doesn't seem to have any effect - admittedly, I might be using them incorrectly, all I did was add a call to `torch::RegisterOperators();` which didn't cause any build errors, but did not overcome the runtime ""Unknown builtin op: aten::mul"" error.

I tried to make a minimal example:
```c++
// According to: https://github.com/pytorch/vision/#c-api
// and https://github.com/pytorch/vision/issues/2915
//  In order to get the torchvision operators registered with
//  torch (eg. for the JIT), all you need to do is to ensure
//  that you #include <torchvision/vision.h> in your project.
#include <vision.h>
#include <ATen/core/ivalue.h>
#include <fstream>
#include <torch/script.h>
#include <torch/torch.h>
#include <vector>
using namespace std;

int main(int argc, char* argv[])
{
  torch::NoGradGuard noGradGuard;

  // Load a trained model that has been converted to torchscript
  ifstream modelFile(""torchscriptModel.pt"");
  torch::jit::script::Module model;
  model = torch::jit::load(modelFile);
  modelFile.close();

  // Set model to eval mode
  model.eval();

  // Generate a random inference image
  float* imgPix = new float[100*100];
  // Normally set image pixels here, left uninitialized for minimal example

  // Convert image pixels to format required by forward
  at::Tensor imgTensor = torch::from_blob(imgPix, {100, 100, 1});
  at::Tensor imgTensorPermuted = imgTensor.permute({2, 0, 1});
  imgTensorPermuted.unsqueeze_(0);

  vector< at::Tensor > imageTensorVec;
  imageTensorVec.push_back(imgTensorPermuted);

  vector< torch::jit::IValue > inputToModel;
  inputToModel.push_back(torch::cat(imageTensorVec));

  at::Tensor forwardResult = model.forward(inputToModel).toTensor();

  delete [] imgPix;

  return 0;
}
```

To build this, I use the following command:
```
g++ minimalExample.cpp \
    -D_GLIBCXX_USE_CXX11_ABI=1 \
    -I /usr/src/vision/torchvision/csrc/ \
    -I /usr/src/pytorch/build/lib.linux-x86_64-3.8/torch/include/torch/csrc/api/include/ \
    -I /usr/src/pytorch/build/lib.linux-x86_64-3.8/torch/include/ \
    -Wl,--start-group \
       /usr/src/vision/build/libtorchvision.a \
       /usr/src/pytorch/build/lib/libc10.a \
       /usr/src/pytorch/build/lib/libtorch_cpu.a \
    -Wl,--end-group \
    /usr/src/pytorch/build/lib/libprotobuf.a \
    /usr/src/pytorch/build/lib/libfbgemm.a \
    /usr/src/pytorch/build/sleef/lib/libsleef.a \
    /usr/src/pytorch/build/lib/libasmjit.a \
    /usr/src/pytorch/build/lib/libonnx.a \
    /usr/src/pytorch/build/lib/libonnx_proto.a \
    /usr/src/pytorch/build/lib/libcpuinfo.a \
    /usr/src/pytorch/build/lib/libclog.a \
    /usr/src/pytorch/build/lib/libkineto.a \
    /usr/src/pytorch/build/lib/libnnpack.a \
    /usr/src/pytorch/build/lib/libpytorch_qnnpack.a \
    /usr/src/pytorch/build/lib/libXNNPACK.a \
    /usr/src/pytorch/build/lib/libpthreadpool.a \
    -Wl,--start-group \
       /opt/intel/oneapi/mkl/2023.2.0/lib/intel64/libmkl_tbb_thread.a \
       /opt/intel/oneapi/mkl/2023.2.0/lib/intel64/libmkl_core.a \
       /opt/intel/oneapi/mkl/2023.2.0/lib/intel64/libmkl_blacs_openmpi_lp64.a \
       /opt/intel/oneapi/mkl/2023.2.0/lib/intel64/libmkl_intel_lp64.a \
       /usr/src/onetbb_installed/lib64/libtbb.a \
    -Wl,--end-group \
    /usr/local/lib/libompitrace.a \
    /usr/local/lib/libmpi.a \
    /usr/local/lib/libopen-rte.a \
    /usr/local/lib/libopen-pal.a \
    /usr/local/lib/libz.a \
    /usr/lib64/libc_nonshared.a \
    -lrt \
    -ldl \
    -fopenmp \
    -pthread \
    -o minimalExample.exe
```

As I said, this will build successfully, but it does give a warning when building:
```
/usr/src/vision/torchvision/csrc/vision.h:10:40: warning: ‚Äò_register_ops‚Äô initialized and declared ‚Äòextern‚Äô
 extern ""C"" VISION_INLINE_VARIABLE auto _register_ops = &cuda_version;
                                        ^~~~~~~~~~~~~
```

When I run the executable, though, I get the following error:
```
$ ./minimalExample.exe 
terminate called after throwing an instance of 'torch::jit::ErrorReport'
  what():  
Unknown builtin op: aten::mul.
Could not find any similar ops to aten::mul. This op may not exist or may not be currently supported in TorchScript.
:
  File ""<string>"", line 3

def mul(a : float, b : Tensor) -> Tensor:
  return b * a
         ~~~~~ <--- HERE
def add(a : float, b : Tensor) -> Tensor:
  return b + a
'mul' is being compiled since it was called from 'full_out_0_4'
  File ""<string>"", line 3

def full_out_0_4(size:List[int], fill_value:number, *, out:Tensor) -> Tensor:
  return torch.full(size, fill_value, out=out)
                                          ~~~ <--- HERE

Abort (core dumped)
```

The minimal example runs as expected, without error, if I link the `libtorch_cpu.a` whole archive, by changing the corresponding line in the build command to:
```
    -Wl,--start-group \
       /usr/src/vision/build/libtorchvision.a \
       /usr/src/pytorch/build/lib/libc10.a \
       -Wl,--whole-archive /usr/src/pytorch/build/lib/libtorch_cpu.a -Wl,--no-whole-arhive \
    -Wl,--end-group \
```

but as I said, the size of the executable jumps way higher, and seems like overkill.

I wasn't sure if this should be a forum post or an issue report, but given that I thought the include of <vision.h> was supposed to manage this, it felt more like an issue report to me.

### Versions

I'm not sure this is especially valuable in this situation.  The example is running on an old OS with CPU-only support.  The conversion to torchscript was done on a more modern machine with python and pytorch installed, but the machine I am running on is a severely stripped-down machine without python at all.

If I run the minimalExample.exe on the modern machine, it performs the same way though (i.e. errors at runtime without the whole-archive stuff, but runs successfully with the whole-archive stuff).  So, here's the env for that machine in case its helpful:

```
Collecting environment information...
PyTorch version: 1.13.0a0+git7c98e70
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.8.10 (default, May 26 2023, 14:05:08)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-1024-fips-x86_64-with-glibc2.29
Is CUDA available: True
CUDA runtime version: 11.8.89
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA RTX 6000 Ada Generation
GPU 1: NVIDIA RTX 6000 Ada Generation
GPU 2: NVIDIA RTX 6000 Ada Generation

Nvidia driver version: 535.98
cuDNN version: Probably one of the following:
/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   46 bits physical, 57 bits virtual
CPU(s):                          72
On-line CPU(s) list:             0-71
Thread(s) per core:              2
Core(s) per socket:              18
Socket(s):                       2
NUMA node(s):                    2
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           106
Model name:                      Intel(R) Xeon(R) Gold 6354 CPU @ 3.00GHz
Stepping:                        6
Frequency boost:                 enabled
CPU MHz:                         804.039
CPU max MHz:                     3600.0000
CPU min MHz:                     800.0000
BogoMIPS:                        6000.00
Virtualization:                  VT-x
L1d cache:                       1.7 MiB
L1i cache:                       1.1 MiB
L2 cache:                        45 MiB
L3 cache:                        78 MiB
NUMA node0 CPU(s):               0-17,36-53
NUMA node1 CPU(s):               18-35,54-71
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear pconfig flush_l1d arch_capabilities

Versions of relevant libraries:
[pip3] numpy==1.17.4
[pip3] torch==1.13.0a0+git7c98e70
[pip3] torchvision==0.14.0a0+5ce4506
[conda] Could not collect
```

cc @malfet @seemethere @jbschlosser @datumbox @vfdev-5 @pmeier"
111652,Tensor -> scalar dunders graph break under dynamo,"### üêõ Describe the bug

Compiling `int(torch.ones(1))` incurs a graph break, while `len(torch.ones(1)` does not.

Repro:

```
$ cat dunder.py 
import torch
import numpy as np

def fn(x):
    return int(x)

opt_fn = torch.compile(fn)
x = torch.empty(1)
r = opt_fn(x)
print(r, type(r))
```

results in 

```
$ TORCH_LOGS=""graph_breaks"" python dunder.py 
[2023-10-20 17:55:43,251] [0/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: call_function BuiltinVariable(int) [TensorVariable()] {} from user code at:
[2023-10-20 17:55:43,251] [0/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File ""dunder.py"", line 5, in fn
[2023-10-20 17:55:43,251] [0/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return int(x)
[2023-10-20 17:55:43,251] [0/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] 
0 <class 'int'>
```

Replacing `int` with `len` does work: it is dispatched to `TensorVariable.call_method`, which special-cases `__len__`. For `int(tensor)` though,  `call_method` is not called.


### Versions

main

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng"
111651,DISABLED test_meta_outplace_fft_ifft_cpu_int64 (__main__.TestMetaCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_outplace_fft_ifft_cpu_int64&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17893852781).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_outplace_fft_ifft_cpu_int64`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

cc @ezyang @mruberry @Lezcano @peterbell10 @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111650,FSDP CPU Offload + fp16 + sharded grad scaler crash / hang,"### üêõ Describe the bug

I get the following when running the above combination:

```
ERROR:aiplatform.error_reporting.error_reporting:Exception Found: Could not run 'ate     n::_amp_foreach_non_finite_check_and_unscale_' with arguments from the 'CPU' backend     . This could be because the operator doesn't exist for this backend, or was omitted      during the selective/custom build process (if using custom build). If you are a Face     book employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for p     ossible resolutions. 'aten::_amp_foreach_non_finite_check_and_unscale_' is only avai     lable for these backends: [CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerB     ackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, Aut     ogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, Autogr     adIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPri     vateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTens     or, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTo     rchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDy     namicLayerFrontMode, PreDispatch, PythonDispatcher].
```

in my case, the error seems to be reported but the job doesn't crash and just hangs, which is interesting and might be related to other collectives going on?

### Versions

main

cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin"
111649,[dynamo] higher-order ops do not preserve `FakeTensor` for in-place ops,"### üêõ Describe the bug

```python
def fn(z):
    x = z.clone()
    y = torch.vmap(torch.Tensor.acos_)(x)
    # y's fake tensor is not x's fake tensor in terms of pyobjects
    return y is x

fn_opt = torch.compile(backend=""eager"", fullgraph=True, dynamic=True)(fn)

z = torch.ones(4, 1)

self.assertEqual(fn(z), fn_opt(z))
```

### Solution
Not sure if this is a bug. But ideally, we should not expect x's fake tensor to be different from y's, as then it can be a method to preserve object identity throughout recursive calls to FX.

One possible way to do this might be to reuse the FakeTensor from the inputs when doing fx tracing for higher order ops.
 
However, if it is unavoidable (e.g. as a result of fx tracing requirements), another solution is simply to propagate the storage of the original fake tensor to the higher order op fake tensor.

### Versions

main

cc @eellison @zou3519 @Chillee @samdow @kshitij12345 @janeyx99 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng"
111648,FSDP: clean names for use_orig_params?,"### üöÄ The feature, motivation and pitch

When use_orig_params=True, I still get names that are prefixed with `_fsdp_wrapped_module` at each nested level. Wonder if we should just strip out the `_fsdp_wrapped_module` and return the exact original local names. 

### Alternatives

_No response_

### Additional context

_No response_

cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin"
111646,torchrun: elastic training not restarted on missing keep-alive heartbeat/scale-down event,"### üêõ Describe the bug

When running elastic training with C10d backend and multiple nodes, the workers need to be restarted in case of a down-scale event. If this does not happen, as right now, the remaining workers get stuck in NCCL operations and wait by default 30 minutes until they finish with a timeout error.

*Expected Behaviour*
If a worker misses its heartbeat/leaves the rendevous, a new rendevous should happen.

*Minimal Example*

There are two scripts for the master worker and a faulty worker

**master**
```python
import torch.distributed.run
import logging
logging.getLogger(""torch.distributed.elastic.rendezvous.dynamic_rendezvous"").level=10
torch.distributed.run.main([""--nnodes=1:4"",""--rdzv-backend=c10d"",
                            ""--rdzv-endpoint=localhost:9999"",
                            ""--rdzv-conf=last_call_timeout=10"",
                            ""--no-python"",
                            ""bash"",""-c"",f""echo start; sleep 600; echo done""])
```

**faulty**
```python
import torch.distributed.run
import logging
logging.getLogger(""torch.distributed.elastic.rendezvous.dynamic_rendezvous"").level=10
torch.distributed.run.main([""--nnodes=1:4"",""--rdzv-backend=c10d"",
                            ""--local-addr=faulty"",
                            ""--rdzv-endpoint=localhost:9999"",
                            ""--rdzv-conf=last_call_timeout=10"",
                            ""--no-python"",
                            ""bash"",""-c"",f""echo start; sleep 10; kill -9 $$PPID""])
```
The minimal example enables debug logging to highlight the problematic area.
It does the following:
**master** runs normally, while **faulty** simulates a forceful termination after 10 seconds.
Both ranks print `start` when a new generation is started.

Run first master and then faulty script.

The output 
```
[2023-10-20 11:55:30,978] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2023-10-20 11:55:30,983] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [INFO] The node 'georg-AERO-15-YC_945987_0' attempts to join the next round of the rendezvous 'none'.
[2023-10-20 11:55:31,071] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' added itself to the participants of round 0 of the rendezvous 'none'. Pending sync.
[2023-10-20 11:55:31,077] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' updated its keep-alive heartbeat time for the rendezvous 'none'. Pending sync.
[2023-10-20 11:55:31,080] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' has successfully synced its local changes with other nodes in the rendezvous 'none'.
[2023-10-20 11:55:36,088] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' updated its keep-alive heartbeat time for the rendezvous 'none'. Pending sync.
[2023-10-20 11:55:36,093] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' has successfully synced its local changes with other nodes in the rendezvous 'none'.
[2023-10-20 11:55:41,102] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' marked round 0 of the rendezvous 'none' as complete. Pending sync.
[2023-10-20 11:55:41,107] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' has successfully synced its local changes with other nodes in the rendezvous 'none'.
[2023-10-20 11:55:41,110] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [INFO] The node 'georg-AERO-15-YC_945987_0' has joined round 0 of the rendezvous 'none' as rank 0 in a world of size 2.
start
[2023-10-20 11:55:46,112] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' updated its keep-alive heartbeat time for the rendezvous 'none'. Pending sync.
[2023-10-20 11:55:46,115] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' has successfully synced its local changes with other nodes in the rendezvous 'none'.
[2023-10-20 11:55:46,117] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' has sent a keep-alive heartbeat to the rendezvous 'none'.
[2023-10-20 11:55:51,121] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' updated its keep-alive heartbeat time for the rendezvous 'none'. Pending sync.
[2023-10-20 11:55:51,124] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' has successfully synced its local changes with other nodes in the rendezvous 'none'.
[2023-10-20 11:55:51,126] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' has sent a keep-alive heartbeat to the rendezvous 'none'.
[2023-10-20 11:55:56,129] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' updated its keep-alive heartbeat time for the rendezvous 'none'. Pending sync.
[2023-10-20 11:55:56,132] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' has successfully synced its local changes with other nodes in the rendezvous 'none'.
[2023-10-20 11:55:56,134] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' has sent a keep-alive heartbeat to the rendezvous 'none'.
[2023-10-20 11:56:01,138] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' updated its keep-alive heartbeat time for the rendezvous 'none'. Pending sync.
[2023-10-20 11:56:01,140] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' has successfully synced its local changes with other nodes in the rendezvous 'none'.
[2023-10-20 11:56:01,141] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' has sent a keep-alive heartbeat to the rendezvous 'none'.
[2023-10-20 11:56:06,144] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' updated its keep-alive heartbeat time for the rendezvous 'none'. Pending sync.
[2023-10-20 11:56:06,147] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' has successfully synced its local changes with other nodes in the rendezvous 'none'.
[2023-10-20 11:56:06,150] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' has sent a keep-alive heartbeat to the rendezvous 'none'.
[2023-10-20 11:56:11,153] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' updated its keep-alive heartbeat time for the rendezvous 'none'. Pending sync.
[2023-10-20 11:56:11,156] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] As part of the sync operation the node(s) 'faulty_945990_0' have been removed from the rendezvous 'none' since they had no heartbeat.
[2023-10-20 11:56:11,156] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [DEBUG] The node 'georg-AERO-15-YC_945987_0' has successfully synced its local changes with other nodes in the rendezvous 'none'.
```

Expected would now be that after the missing heartbeat and removal of faulty the rendezvous process should be started again, as is also documented in [rdzv_doc](https://github.com/pytorch/pytorch/blob/619ae87a1d1ae086f59a64d3b71dbfe4af8b804a/docs/source/elastic/etcd_rdzv_diagram.png) also the [__init__.py](https://github.com/pytorch/pytorch/blob/619ae87a1d1ae086f59a64d3b71dbfe4af8b804a/torch/distributed/elastic/rendezvous/__init__.py#L71) mention a restart in such a case.


### Versions

```
Collecting environment information...
PyTorch version: 2.1.0
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.26.4
Libc version: glibc-2.35

Python version: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-86-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080 Laptop GPU
Nvidia driver version: 535.104.12
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      39 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             16
On-line CPU(s) list:                0-15
Vendor ID:                          GenuineIntel
Model name:                         Intel(R) Core(TM) i9-10980HK CPU @ 2.40GHz
CPU family:                         6
Model:                              165
Thread(s) per core:                 2
Core(s) per socket:                 8
Socket(s):                          1
Stepping:                           2
CPU max MHz:                        5300,0000
CPU min MHz:                        800,0000
BogoMIPS:                           6199.99
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp pku ospke md_clear flush_l1d arch_capabilities
Virtualization:                     VT-x
L1d cache:                          256 KiB (8 instances)
L1i cache:                          256 KiB (8 instances)
L2 cache:                           2 MiB (8 instances)
L3 cache:                           16 MiB (1 instance)
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-15
Vulnerability Gather data sampling: Mitigation; Microcode
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Not affected
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Retbleed:             Mitigation; Enhanced IBRS
Vulnerability Spec rstack overflow: Not affected
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
Vulnerability Srbds:                Mitigation; Microcode
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] mypy==1.6.0
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.26.0
[pip3] optree==0.9.1
[pip3] torch==2.1.0
[pip3] torchaudio==2.1.0
[pip3] torchvision==0.16.0
[pip3] triton==2.1.0
[conda] blas                      1.0                         mkl  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch
[conda] mkl                       2023.1.0         h213fc3f_46343  
[conda] mkl-include               2023.1.0         h06a4308_46343  
[conda] mkl-service               2.4.0           py311h5eee18b_1  
[conda] mkl_fft                   1.3.8           py311h5eee18b_0  
[conda] mkl_random                1.2.4           py311hdb19cb5_0  
[conda] numpy                     1.26.0          py311h08b1b3b_0  
[conda] numpy-base                1.26.0          py311hf175353_0  
[conda] optree                    0.9.1                    pypi_0    pypi
[conda] pytorch                   2.1.0           py3.11_cuda11.8_cudnn8.7.0_0    pytorch
[conda] pytorch-cuda              11.8                 h7e8668a_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                2.1.0               py311_cu118    pytorch
[conda] torchtriton               2.1.0                     py311    pytorch
[conda] torchvision               0.16.0              py311_cu118    pytorch
```


cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu"
111645,"ValueError: Using a target size (torch.Size([491])) that is different to the input size (torch.Size([1, 491])) is deprecated. Please ensure they have the same size.","### üêõ Describe the bug

Hi, I have a script that i am trying to run, but it gives the following error -

```
``
batch_size_16,learning_rate_0.001,epoch_times_45
selected_9606_protein_scores.csv
selected_9606_protein_scores.csv
Traceback (most recent call last):
  File ""/home/bvsbic/Downloads/MLtest/Validation1.py"", line 1084, in <module>
    Terms = ['BP', 'MF', 'CC']
  File ""/home/bvsbic/Downloads/MLtest/Validation1.py"", line 1065, in validation
    test_set = benchmark[test_index].tolist()
  File ""/home/bvsbic/Downloads/MLtest/Validation1.py"", line 903, in Main
    domain_train_out, domain_test_out, domain_t = Domain_train(0.001, 32, train_benchmark, test_benchmark, 45,
  File ""/home/bvsbic/Downloads/MLtest/Validation1.py"", line 639, in Domain_train
    loss = loss_function(out, GO_annotiations)
  File ""/home/bvsbic/Downloads/MLtest/okenv/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/bvsbic/Downloads/MLtest/okenv/lib/python3.9/site-packages/torch/nn/modules/loss.py"", line 530, in forward
    return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)
  File ""/home/bvsbic/Downloads/MLtest/okenv/lib/python3.9/site-packages/torch/nn/functional.py"", line 2518, in binary_cross_entropy
    raise ValueError(""Using a target size ({}) that is different to the input size ({}) is deprecated. ""
***ValueError: Using a target size (torch.Size([491])) that is different to the input size (torch.Size([1, 491])) is deprecated. Please ensure they have the same size.***
```

``` 
Actually, I am new here, and I don't know how to solve it; plz help me. Here is whole code  given below-

### Versions

import os
# Disable HIP and set CUDA device
os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""  # Use NVIDIA GPU 0
os.environ[""ROCBLAS_LAYER""] = ""0""  # Disable ROCm/HIP
os.environ[""ROCFFT_LAYER""] = ""0""  # Disable ROCm/HIP
import torch
import my_Utils
# Set GPU memory fraction (adjust the fraction as needed)
#torch.cuda.set_per_process_memory_fraction(0.5)
torch.backends.cuda.max_split_size_mb = 256  # Adjust this value as needed
from torch.utils.data import Dataset
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from sklearn import metrics
from sklearn.metrics import roc_auc_score, roc_curve, auc, precision_score, recall_score, f1_score, average_precision_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from torch.nn import BCEWithLogitsLoss
import os
import time
import random


# seqSet = 'seqSet.csv'
# domainSet = 'domainSet.csv'
# Benchmark_list = open('data.human.benchmark.list', 'r')
torch.manual_seed(100)
os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""

# You can adjust this value as needed
#os.environ[""PYTORCH_CUDA_ALLOC_CONF""] = ""0""
time_start=time.time()
# GO_IDs = []

CFG = {
    'cfg00': [16, 'M', 16, 'M'],
    'cfg01': [16, 'M', 32, 'M'],
    'cfg02': [32, 'M'],
    'cfg03': [64, 'M'],
    'cfg04': [16, 'M', 16, 'M',32, 'M'],
    'cfg05': [64, 'M', 32, 'M',16, 'M'],
    'cfg06': [64, 'M', 32, 'M',32, 'M'],
    'cfg07': [128, 'M', 64, 'M2'],
    'cfg08': [512, 'M', 128, 'M2',32, 'M2'],
}
OUT_nodes = {
    'BP': 491,
    'MF': 321,
    'CC': 240,
}


Thresholds = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1,
              0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2,
              0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3,
              0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4,
              0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5,
              0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6,
              0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7,
              0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8,
              0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9,
              0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99]

file_path = '/home/bvsbic/Downloads/MLtest/protVec_dict.npy'
ProtVec = np.load(file_path, allow_pickle=True).item()
#ProtVec = np.load('protVec_dict.npy', allow_pickle=True).item()
# ProtDict = np.load('data/prot_kmerWord_dict.npy').item()
Seqfile_name = 'seqSet.csv'
# Domainfile_name = 'data/domainSet.csv'
Domainfile_name = 'NewdomainSet.csv'
GOfile_name = 'humanProteinGO.csv'


   



class Dataload(Dataset):
    def __init__(self, benchmark_list, seqfile_name, domainfile_name, GOfile_name, func='MF', transform=None):
        self.benchmark_list = benchmark_list
        self.sequeces = {}
        self.max_seq_len = 1500  # Â∫èÂàóÈïøÂ∫¶Â∞è‰∫é5000ÁöÑÂ∫èÂàó‰∏≠ÁöÑÊúÄÂ§ßÂÄºwei 4981 Â§ß‰∫é1000ÁöÑÊúâ1827Êù°
        self.doamins = {}
        self.max_domains_len = 357  # ËõãÁôΩË¥®ÊâÄÂåÖÂê´ÁöÑdomainÊï∞ÈáèÁöÑÁ¨¨‰∫åÂ§ßÂÄºÔºàÊúÄÂ§ßÂÄº‰∏∫1242ÔºåËàçÂéªËØ•ËõãÁôΩË¥®Ôºâ
        self.ppiVecs = {}
        self.GO_annotiations = {}
        # self.max_GOnums_len = 0     #Âê´ÊúâGOÊ†áÊ≥®ÊúÄÂ§öÁöÑËõãÁôΩË¥®ÁöÑGOÊï∞Èáè

        with open(seqfile_name, 'r') as f:  #seqfile_name = 'seqSet.csv'
            for line in f:
                items = line.strip().split(',')
                prot, seq = items[0], items[1]
                self.sequeces[prot] = seq
        self.protDict = ProtVec

        with open(domainfile_name, 'r') as f:   #domainfile_name = 'domainSet.csv'
            for line in f:
                items = line.strip().split(',')
                prot, domains = items[0], items[1:]
                domains = [int(x) for x in domains]
                self.doamins[prot] = domains


        # ppi_file = 'PPI_data/selected_uniprot_protein_scores.csv'
        # ppi_file = 'PPI_data/selected_uniprot_protein_links.csv'
        ppi_file = 'selected_9606_protein_scores.csv'
        print(ppi_file)
        with open(ppi_file, 'r') as f:
            num = 1
            for line in f:
                if num == 1:
                    num = 2
                    continue
                items = line.strip().split(',')
                prot, vector =items[0], items[1:]
                self.ppiVecs[prot] = vector

        with open(GOfile_name, 'r') as f:       #GOfile_name = 'humanProteinGO.csv'
            num = 1
            for line in f:
                if num == 1:
                    num = 2
                    # items = line.strip().split(',')
                    # GO_IDs = items
                    continue
                items = line.strip().split(',')
                if func == 'BP':
                    prot, GO_annotiation = items[0], items[1:492]
                elif func == 'MF':
                    prot, GO_annotiation = items[0], items[492:813]
                elif func == 'CC':
                    prot, GO_annotiation = items[0], items[813:]
                # prot, GO_annotiation = items[0], items[1:]
                self.GO_annotiations[prot] = GO_annotiation

    
        

    def __getitem__(self, idx):
        iprID = self.benchmark_list[idx]

        # # Ëé∑ÂèñseqÁöÑËæìÂÖ•ÂêëÈáè
        # seq = self.sequeces[iprID]
        # if len(seq) >= self.max_seq_len:
        #     seq = seq[0:self.max_seq_len]
        # seqSentence = my_Utils.mer_k_Sentence(seq, self.protDict, 3)
        # seqSentence = np.array(seqSentence, dtype=int)
        # seqSentence = np.pad(seqSentence, (0, self.max_seq_len - len(seqSentence)), 'constant', constant_values=0)
        # seqSentence = torch.from_numpy(seqSentence).type(torch.LongTensor).cuda()

        # Ëé∑ÂèñseqÁöÑËæìÂÖ•Áü©Èòµ
        seq = self.sequeces[iprID]
        if len(seq) > self.max_seq_len:
            seq = seq[0:self.max_seq_len]
        seqMatrix = my_Utils.mer_k(seq, self.protDict, 3)
        seqMatrix = np.array(seqMatrix, dtype=float)
        if (seqMatrix.shape[0]) < self.max_seq_len:
            seqMatrix = np.pad(seqMatrix, ((0, self.max_seq_len - (seqMatrix.shape[0])), (0, 0)),
                               'constant', constant_values=0)
        seqMatrix = seqMatrix.T
        seqMatrix = torch.from_numpy(seqMatrix).type(torch.FloatTensor).cuda()

        #Ëé∑ÂèñdomainÁöÑËæìÂÖ•ÂêëÈáè
        domain_s = self.doamins[iprID]
        if len(domain_s) >= self.max_domains_len:
            domain_s = np.array(domain_s[0:self.max_domains_len], dtype=int)
        # if len(domain_s) < self.max_domains_len:
        else:
            domain_s = np.array(domain_s, dtype=int)
            domain_s = np.pad(domain_s, ((0, self.max_domains_len-len(domain_s))), 'constant', constant_values=0)
        domainSentence = torch.from_numpy(domain_s).type(torch.LongTensor).cuda()

        # Ëé∑ÂèñPPIÁöÑËæìÂÖ•ÂêëÈáè
        if iprID not in self.ppiVecs:
            ppiVect = np.zeros((18901), dtype=float).tolist()
        else:
            ppiVect = self.ppiVecs[iprID]
            ppiVect = [float(x) for x in ppiVect]
        ppiVect = torch.Tensor(ppiVect).cuda()
        ppiVect = ppiVect.type(torch.FloatTensor)

        #Ëé∑ÂèñËõãÁôΩË¥®ÁöÑGOÊ†áÊ≥®ÂêëÈáè
        GO_annotiations = self.GO_annotiations[iprID]
        GO_annotiations = [int(x) for x in GO_annotiations]
        GO_annotiations = torch.Tensor(GO_annotiations).cuda()
        # GO_annotiations = GO_annotiations.type(torch.LongTensor)
        GO_annotiations = GO_annotiations.type(torch.FloatTensor)
        #GO_annotiations = one_hot_encode_target(GO_annotiations)

        return seqMatrix, domainSentence, ppiVect, GO_annotiations

    def __len__(self):
        return len(self.benchmark_list)     #ËøîÂõûËõãÁôΩË¥®ÁöÑÊï∞Èáè


class weight_Dataload(Dataset):
    def __init__(self, benchmark_list, seqdict, domaindict, ppidict, GOfile_name, func = 'MF', transform=None):
        self.benchmark = benchmark_list
        self.weghtdict = {}
        self.GO_annotiations = {}

        for i in range(len(benchmark_list)):
            prot = benchmark_list[i]
            # temp = torch.cat((seqdict[prot], domaindict[prot]), 0)
            # temp = torch.cat((temp, ppidict[prot]), 0)
            temp = [seqdict[prot], domaindict[prot], ppidict[prot]]
            temp = np.array(temp)
            self.weghtdict[benchmark_list[i]] = temp.flatten().tolist()
            assert len(seqdict[prot]) == len(domaindict[prot]) == len(ppidict[prot]) == OUT_nodes[func]

        with open(GOfile_name, 'r') as f:       #GOfile_name = 'data/humanProteinGO.csv'
            num = 1
            for line in f:
                if num == 1:
                    num = 2
                    # items = line.strip().split(',')
                    # GO_IDs = items
                    continue
                items = line.strip().split(',')
                if func == 'BP':
                    prot, GO_annotiation = items[0], items[1:492]
                elif func == 'MF':
                    prot, GO_annotiation = items[0], items[492:813]
                elif func == 'CC':
                    prot, GO_annotiation = items[0], items[813:]
                # prot, GO_annotiation = items[0], items[1:]
                self.GO_annotiations[prot] = GO_annotiation



    def __getitem__(self, idx):
        prot = self.benchmark[idx]

        #Ëé∑Âèñweight_classifierÁöÑËæìÂÖ•ÂêëÈáè
        weight_features = self.weghtdict[prot]
        weight_features = [float(x) for x in weight_features]
        weight_features = torch.Tensor(weight_features).cuda()
        weight_features = weight_features.type(torch.FloatTensor)


        # Ëé∑ÂèñËõãÁôΩË¥®ÁöÑGOÊ†áÊ≥®ÂêëÈáè
        GO_annotiations = self.GO_annotiations[prot]
        GO_annotiations = [int(x) for x in GO_annotiations]
        GO_annotiations = torch.Tensor(GO_annotiations).cuda()
        # GO_annotiations = GO_annotiations.type(torch.LongTensor)
        GO_annotiations = GO_annotiations.type(torch.FloatTensor)

        return weight_features, GO_annotiations

    def __len__(self):
        return len(self.benchmark)


class Seq_Module(nn.Module):
    def __init__(self, func):
        super(Seq_Module, self).__init__()
        # self.seq_emblayer = nn.Embedding(8001, 128, padding_idx=0)
        self.seq_CNN = self.SeqConv1d(CFG['cfg05']).cuda()
        self.seq_FClayer = nn.Linear(3008, 1024).cuda()
        #self.seq_outlayer = nn.Linear(1024, 491).cuda()
        self.seq_outlayer = nn.Linear(1024, OUT_nodes[func]).cuda()
        #self.seq_outlayer = nn.Linear(1024, OUT_nodes[func]).cuda()
        # Adjust num_classes as needed
        #self.seq_outlayer = nn.Linear(1024, num_classes)

    def forward(self, seqMatrix):
        # seqMatrix = self.seq_emblayer(seqSentence)
        seq_out = self.seq_CNN(seqMatrix)
        seq_out = seq_out.view(seq_out.size(0), -1)  # Â±ïÂπ≥Â§öÁª¥ÁöÑÂç∑ÁßØÂõæ
        # print(seq_out)
        seq_out = F.dropout(self.seq_FClayer(seq_out), p=0.3, training=self.training)
        seq_out = F.relu(seq_out)
        #seq_out = self.seq_outlayer(seq_out)
        seq_out = F.sigmoid(self.seq_outlayer(seq_out))
        #seq_out = F.sigmoid(seq_out)
        return seq_out

    # sequenceÁöÑ1D_CNNÊ®°Âûã
    def SeqConv1d(self, cfg):
        layers = []
        in_channels = 100
        for x in cfg:
            if x == 'M':
                layers += [nn.MaxPool1d(kernel_size=2)]
            elif x == 'M2':
                layers += [nn.MaxPool1d(kernel_size=2, stride=1)]
            else:
                layers += [nn.Conv1d(in_channels, x, kernel_size=16, stride=1, padding=8),
                           nn.ReLU(inplace=True)]
                in_channels = x
        return nn.Sequential(*layers)


class Domain_Module(nn.Module):
    def __init__(self, func):
        super(Domain_Module, self).__init__()
        self.dom_emblayer = nn.Embedding(14243, 128, padding_idx=0).cuda()
        self.dom_CNN = self.DomainConv1d(CFG['cfg07']).cuda()
        self.dom_FClayer = nn.Linear(1088, 512).cuda()
        self.dom_outlayer = nn.Linear(512, OUT_nodes[func]).cuda()
        #self.dom_outlayer = nn.Linear(512, OUT_nodes[func]).cuda()


    def forward(self, domainSentence): #seq 4981*100  ,domain
        domain_matrix = self.dom_emblayer(domainSentence)
        domain_out = self.dom_CNN(domain_matrix)
        domain_out = domain_out.view(domain_out.size(0), -1)  # Â±ïÂπ≥Â§öÁª¥ÁöÑÂç∑ÁßØÂõæ
        # print(domain_out)
        domain_out = F.dropout(self.dom_FClayer(domain_out), p=0.3, training=self.training)
        domain_out = F.relu(domain_out)
        domain_out = self.dom_outlayer(domain_out)
        domain_out = F.sigmoid(domain_out)
        return domain_out

    # domainÁöÑ1D_CNNÊ®°Âûã
    def DomainConv1d(self, cfg):
        layers = []
        # in_channels = 128
        in_channels = 357
        for x in cfg:
            if x == 'M':
                layers += [nn.MaxPool1d(kernel_size=2, stride=2)]
            elif x == 'M2':
                layers += [nn.MaxPool1d(kernel_size=2, stride=1)]
            else:
                layers += [nn.Conv1d(in_channels, x, kernel_size=2, stride=2, padding=2),
                           nn.ReLU(inplace=True)]
                in_channels = x
        return nn.Sequential(*layers)


class PPI_Module(nn.Module):
    def __init__(self, func):
        super(PPI_Module, self).__init__()
        self.ppi_inputlayer = nn.Linear(18901, 4096).cuda()
        self.ppi_hiddenlayer = nn.Linear(4096, 1024).cuda()
        self.ppi_outlayer = nn.Linear(1024, OUT_nodes[func]).cuda()
        #self.ppi_outlayer = nn.Linear(1024, OUT_nodes[func]).cuda()

    def forward(self, ppiVec):
        ppi_out = F.dropout(self.ppi_inputlayer(ppiVec), p=0.00005, training=self.training)
        ppi_out = F.dropout(self.ppi_hiddenlayer(ppi_out), p=0.3, training=self.training)
        ppi_out = self.ppi_outlayer(ppi_out)
        ppi_out = F.sigmoid(ppi_out)
        return ppi_out


class Weight_classifier(nn.Module):
    def __init__(self, func):
        super(Weight_classifier, self).__init__()
        # self.weight_layer = nn.Linear(OUT_nodes[func]*3, OUT_nodes[func])
        self.weight_layer = MaskedLinear(OUT_nodes[func]*3, OUT_nodes[func], '{}_maskmatrix.csv'.format(func)).cuda()
        self.outlayer= nn.Linear(OUT_nodes[func], OUT_nodes[func])

    def forward(self, weight_features):
        weight_out = self.weight_layer(weight_features)
        # weight_out = F.sigmoid(weight_out)
        weight_out = F.relu(weight_out)
        weight_out = F.sigmoid(self.outlayer(weight_out))
        return weight_out


class MaskedLinear(nn.Linear):
    def __init__(self, in_features, out_features, relation_file, bias=True):
        super(MaskedLinear, self).__init__(in_features, out_features, bias)

        mask = self.readRelationFromFile(relation_file)
        self.register_buffer('mask', mask)
        self.iter = 0

    def forward(self, input):
        masked_weight = self.weight * self.mask
        return F.linear(input, masked_weight, self.bias)

    def readRelationFromFile(self, relation_file):
        mask = []
        with open(relation_file, 'r') as f:
            for line in f:
                l = [int(x) for x in line.strip().split(',')]
                for item in l:
                    assert item == 1 or item == 0  # relation Âè™ËÉΩ‰∏∫0ÊàñËÄÖ1
                mask.append(l)
        return Variable(torch.Tensor(mask))


def benchmark_set_split(term_arg='MF'):
    benchmark_file = '/home/bvsbic/Downloads/MLtest/{}_benchmarkSet_2.csv'.format(
        term_arg)
    print(benchmark_file)
    trainset, testset = [], []
    all_data = []
    with open(benchmark_file, 'r') as f:
        for line in f:
            item = line.strip()
            all_data.append(item)
    idx_list = np.arange(len(all_data)).tolist()
    # nums = {
    #     'BP': 10000,
    #     'MF': 10000,
    #     'CC': 10600,
    #     'test': 10
    # }
    nums = {
        'BP': 491, #10700,
        'MF': 321, #10500,
        'CC':  240, #10000,
        'test': 10
    }
    # random_index = random.sample(idx_list, nums[term_arg])   #11000ÔºåÂú®0--idx_listËåÉÂõ¥ÂÜÖÈöèÊú∫‰∫ßÁîünums[term_arg}‰∏™ÈöèÊú∫Êï∞
    #
    random_index = []
    with open('{}_random_index.csv'.format(term_arg), 'r') as f:
        for line in f:
            item = line.strip().split(',')
            random_index.append(int(item[0]))
    for i in range(len(all_data)):
        if i in random_index:
            trainset.append(all_data[i])
        else:
            testset.append(all_data[i])
    assert len(trainset) + len(testset) == len(all_data)
    return trainset, testset


def calculate_performance(actual, pred_prob, threshold=0.4, average='micro'):
    pred_lable = []
    for l in range(len(pred_prob)):
        eachline = (np.array(pred_prob[l]) > threshold).astype(int)
        eachline = eachline.tolist()
        pred_lable.append(eachline)
    f_score = f1_score(np.array(actual), np.array(pred_lable), average=average)
    recall = recall_score(np.array(actual), np.array(pred_lable), average=average)
    precision = precision_score(np.array(actual), np.array(pred_lable), average=average)
    return f_score, recall,  precision


def cacul_aupr(lables, pred):
    precision, recall, _thresholds = metrics.precision_recall_curve(lables, pred)
    aupr = metrics.auc(recall, precision)
    return aupr


def Seq_train(learningrate, batchsize, train_benchmark, test_benchmark, epochtime, func='MF'):
    print('{}  seqmodel start'.format(func))
    seq_model = Seq_Module(func).cuda()
    batch_size = 16  # You can reduce the batch size to a smaller value
    #batch_size = batchsize
    learning_rate = learningrate
    epoch_times = epochtime
    print(seq_model)
    print('batch_size_{},learning_rate_{},epoch_times_{}'.format(batch_size, learning_rate, epoch_times))
    #loss_function = nn.BCEWithLogitsLoss()
    loss_function = nn.BCEWithLogitsLoss(reduction='mean')  # Add reduction argument
    #loss_function = nn.BCELoss()
    optimizer = optim.Adam(seq_model.parameters(), lr=learning_rate, weight_decay = 0.00001)

    train_dataset = Dataload(train_benchmark, Seqfile_name, Domainfile_name, GOfile_name, func=func)
    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_dataset = Dataload(test_benchmark, Seqfile_name, Domainfile_name, GOfile_name, func=func)
    test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)

    seq_model.train()
    best_fscore = 0
    for epoch in range(epoch_times):
        _loss = 0
        batch_num = 0
        torch.cuda.empty_cache()  # Add this line to release GPU memory
        for batch_idx, (seqMatrix, domainStence, ppiVect, GO_annotiations) in enumerate(train_data_loader):
            seqMatrix = Variable(seqMatrix).cuda()
            GO_annotiations = torch.squeeze(GO_annotiations)
            GO_annotiations = Variable(GO_annotiations.unsqueeze(1)).cuda()
            #GO_annotiations = Variable(GO_annotiations).cuda()
            GO_annotiations = torch.squeeze(GO_annotiations, dim=0)  # Remove the first dimension
            #GO_annotiations = GO_annotiations.unsqueeze(0)  # Add a batch dimension
            #GO_annotiations = GO_annotiations.unsqueeze(-1)
            out = seq_model(seqMatrix)
            print(""Output Shape:"", out.shape)
            print(""Target Shape:"", GO_annotiations.shape)
            # Reshape the model's output to match the shape of GO_annotiations
            out = out.view(GO_annotiations.shape)
            optimizer.zero_grad()
            loss = loss_function(out, GO_annotiations)
            batch_num += 1
            loss.backward()
            optimizer.step()
            _loss += loss.item()

            # Before training loop
            torch.cuda.empty_cache()
        epoch_loss = ""{}"".format(_loss / batch_num)
        t_loss = 0
        test_batch_num = 0
        pred = []
        actual = []
        for idx, (seqMatrix, domainStence, ppiVect, GO_annotiations) in enumerate(test_data_loader):
            seqMatrix = Variable(seqMatrix).cuda()
            #GO_annotiations = Variable(GO_annotiations).cuda()
            GO_annotiations = Variable(GO_annotiations.unsqueeze(1)).cuda()
            GO_annotiations = torch.squeeze(GO_annotiations, dim=0)  # Remove the first dimension
            #GO_annotiations = GO_annotiations.unsqueeze(0)  # Add a batch dimension
            out = seq_model(seqMatrix)
            test_batch_num = test_batch_num + 1
            pred.append(out.data[0].cpu().tolist())
            actual.append(GO_annotiations.data[0].cpu().tolist())
            one_loss = loss_function(out, GO_annotiations)
            t_loss += one_loss.item()
           # t_loss += one_loss.data[0]
        test_loss = ""{}"".format(t_loss / test_batch_num)
        fpr, tpr, th = roc_curve(np.array(actual).flatten(), np.array(pred).flatten(), pos_label=1)
        auc_score = auc(fpr, tpr)
        score_dict = {}
        each_best_fcore = 0

        each_best_scores = []
        for i in range(len(Thresholds)):
            f_score, recall, precision = calculate_performance(
                actual, pred, threshold=Thresholds[i], average='micro')
            if f_score >= each_best_fcore:
                each_best_fcore = f_score
                each_best_scores = [Thresholds[i], f_score, recall, precision, auc_score]
            scores = [f_score, recall, precision, auc_score]
            score_dict[Thresholds[i]] = scores
        if each_best_fcore >= best_fscore:
            best_fscore = each_best_fcore
            best_scores = each_best_scores
            best_score_dict = score_dict
            torch.save(seq_model, 'savedpkl/Seq1DVal_{}_{}_{}_{}.pkl'.format(func, batch_size, learning_rate, epoch_times))
        t, f_score, recall = each_best_scores[0], each_best_scores[1], each_best_scores[2]
        precision, auc_score = each_best_scores[3], each_best_scores[4]
        print('epoch{},loss{},testloss:{},t{},f_score{}, auc{}, recall{}, precision{}'.format(
            epoch, epoch_loss, test_loss, t, f_score, auc_score, recall, precision))
    bestthreshold, f_max, recall_max = best_scores[0], best_scores[1], best_scores[2]
    prec_max, bestauc_score = best_scores[3], best_scores[4]
    print('lr:{},batch:{},epoch{},f_max:{}\nauc{},recall_max{},prec_max{},threshold:{}'.format(
        learning_rate, batch_size, epoch_times,
        f_max, bestauc_score, recall_max, prec_max, bestthreshold))
    test_Seqmodel = torch.load('savedpkl/Seq1DVal_{}_{}_{}_{}.pkl'.format(func, batch_size, learning_rate, epoch_times)).cuda()
    t_loss = 0
    seq_test_outs = {}
    # seq_test_outs = []
    pred = []
    actual = []
    score_dict = {}
    batch_num = 0
    for batch_idx, (seqMatrix, domainStence, ppiVect, GO_annotiations) in enumerate(test_data_loader):
        seqMatrix = Variable(seqMatrix).cuda()
        GO_annotiations = Variable(GO_annotiations.unsqueeze(1)).cuda()
        #GO_annotiations = Variable(GO_annotiations).cuda()
        GO_annotiations = torch.squeeze(GO_annotiations, dim=0)  # Remove the first dimension
        #GO_annotiations = GO_annotiations.unsqueeze(0)  # Add a batch dimension
        out = test_Seqmodel(seqMatrix)
        batch_num += 1
        seq_test_outs[test_benchmark[batch_idx]] = out.data[0].cpu().tolist()
        pred.append(out.data[0].cpu().tolist())
        actual.append(GO_annotiations.data[0].cpu().tolist())
        loss = loss_function(out, GO_annotiations)
        t_loss += one_loss.item()
        #t_loss += loss.data[0]
    test_loss = ""{}"".format(t_loss / batch_num)
    fpr, tpr, th = roc_curve(np.array(actual).flatten(), np.array(pred).flatten(), pos_label=1)
    auc_score = auc(fpr, tpr)
    each_best_fcore = 0
    for i in range(len(Thresholds)):
        f_score, recall, precision = calculate_performance(
            actual, pred, threshold=Thresholds[i], average='micro')
        if f_score > each_best_fcore:
            each_best_fcore = f_score
            each_best_scores = [Thresholds[i], f_score, recall, precision, auc_score]
        scores = [f_score, recall, precision, auc_score]
        score_dict[Thresholds[i]] = scores
    bestthreshold, f_max, recall_max = each_best_scores[0], each_best_scores[1], each_best_scores[2]
    prec_max, bestauc_score = each_best_scores[3], each_best_scores[4]

    print('test_loss:{},lr:{},batch:{},epoch{},f_max:{}\nauc_score{},recall_max{},prec_max{},threshold:{}'.format(
        test_loss, learning_rate, batch_size, epoch_times,
        f_max, auc_score ,recall_max, prec_max, bestthreshold))

    output_dir = 'out/weight_out/'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    with open('out/weight_out/Seqout{}_lr{}_bat{}_epo{}.csv'.format(
            func, learning_rate, batch_size, epoch_times), 'w') as f:
        f.write('lr:{},batchsize:{},epochtimes:{}\n'.format(learning_rate, batch_size, epoch_times))
        f.write('f_max:{},recall_max{},prec_max{},auc_score:{}\n'.format(
            f_max,recall_max, prec_max, auc_score))
        f.write('threshold,f_score,recall,precision, roc_auc,auc\n')
        for i in range(len(Thresholds)):
            f.write('{},'.format(str(Thresholds[i])))
            f.write('{}\n'.format(','.join(str(x) for x in score_dict[Thresholds[i]])))
        for key, var in seq_test_outs.items():
            f.write('{},'.format(str(key)))
            f.write('{}\n'.format(','.join(str(x) for x in var)))

    #Ëé∑ÂèñÂÜçÊúÄ‰ºòÊ®°Âûã‰∏ãÁöÑËÆ≠ÁªÉÈõÜÁöÑËæìÂá∫
    train_out_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)
    seq_train_outs = {}
    for batch_idx, (seqMatrix, domainsMatrix, ppiVect, GO_annotiations) in enumerate(train_out_loader):
        seqMatrix = Variable(seqMatrix).cuda()
        # GO_annotiations = Variable(GO_annotiations).cuda()
        out = test_Seqmodel(seqMatrix)
        seq_train_outs[train_benchmark[batch_idx]] = out.data[0].cpu().tolist()
    return seq_train_outs, seq_test_outs,bestthreshold        #ËøîÂõûÂÜçÊúÄ‰ºòÁöÑSeqÊ®°Âûã‰∏ãÁöÑËÆ≠ÁªÉÈõÜÁöÑËæìÂá∫ÂíåÊµãËØïÈõÜÁöÑËæìÂá∫ÔºåÁî®‰∫éËÆ≠ÁªÉweight_classifier


def Domain_train(learningrate, batchsize, train_benchmark, test_benchmark, epochtime, func='MF'):
    print('{}  domainmodel start'.format(func))
    domain_model = Domain_Module(func).cuda()
    batch_size = 16 # You can reduce the batch size to a smaller value
    #batch_size = batchsize
    learning_rate = learningrate
    epoch_times = epochtime
    print(domain_model)
    print('batch_size_{},learning_rate_{},epoch_times_{}'.format(batch_size, learning_rate, epoch_times))
    loss_function = nn.BCELoss()
    optimizer = optim.Adam(domain_model.parameters(), lr=learning_rate, weight_decay=0.00001)

    train_dataset = Dataload(train_benchmark, Seqfile_name, Domainfile_name, GOfile_name, func=func)
    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_dataset = Dataload(test_benchmark, Seqfile_name, Domainfile_name, GOfile_name, func=func)
    test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)

    domain_model.train()
    best_fscore = 0
    for epoch in range(epoch_times):
        _loss = 0
        batch_num = 0
        torch.cuda.empty_cache()  # Add this line to release GPU memory
        for batch_idx, (seqMatrix, domainStence, ppiVect, GO_annotiations) in enumerate(train_data_loader):
            domainStence = Variable(domainStence).cuda()
            GO_annotiations = torch.squeeze(GO_annotiations)
            GO_annotiations = Variable(GO_annotiations).cuda()
            out = domain_model(domainStence)
            optimizer.zero_grad()
            loss = loss_function(out, GO_annotiations)
            batch_num += 1
            loss.backward()
            optimizer.step()
            _loss += loss.item()
           # _loss += loss.data[0]
        epoch_loss = ""{}"".format(_loss / batch_num)
        t_loss = 0
        test_batch_num = 0
        pred = []
        actual = []
        for idx, (seqMatrix, domainStence, ppiVect, GO_annotiations) in enumerate(test_data_loader):
            domainStence = Variable(domainStence).cuda()
            GO_annotiations = Variable(GO_annotiations).cuda()
            out = domain_model(domainStence)
            test_batch_num = test_batch_num + 1
            pred.append(out.data[0].cpu().tolist())
            actual.append(GO_annotiations.data[0].cpu().tolist())
            one_loss = loss_function(out, GO_annotiations)
            t_loss += one_loss.item()  # Use .item() to get the scalar value
            #t_loss += one_loss.data[0]
        test_loss = ""{}"".format(t_loss / test_batch_num)
        fpr, tpr, th = roc_curve(np.array(actual).flatten(), np.array(pred).flatten(), pos_label=1)
        auc_score = auc(fpr, tpr)
        score_dict = {}
        each_best_fcore = 0
        each_best_scores = []
        for i in range(len(Thresholds)):
            f_score, recall, precision = calculate_performance(
                actual, pred, threshold=Thresholds[i], average='micro')
            if f_score >= each_best_fcore:
                each_best_fcore = f_score
                each_best_scores = [Thresholds[i], f_score, recall, precision, auc_score]
            scores = [f_score, recall, precision, auc_score]
            score_dict[Thresholds[i]] = scores
        if each_best_fcore >= best_fscore:
            best_fscore = each_best_fcore
            best_scores = each_best_scores
            best_score_dict = score_dict
            torch.save(domain_model,
                       'savedpkl/Doamin1DVal_{}_{}_{}_{}.pkl'.format(func, batch_size, learning_rate, epoch_times))
        t, f_score, recall = each_best_scores[0], each_best_scores[1], each_best_scores[2]
        precision, auc_score = each_best_scores[3], each_best_scores[4]
        print('epoch{},loss{},testloss:{},t{},f_score{}, auc{}, recall{}, precision{}'.format(
            epoch, epoch_loss, test_loss, t, f_score, auc_score, recall, precision))
    bestthreshold, f_max, recall_max = best_scores[0], best_scores[1], best_scores[2]
    prec_max, bestauc_score = best_scores[3], best_scores[4]
    print('lr:{},batch:{},epoch{},f_max:{}\nauc{},recall_max{},prec_max{},threshold:{}'.format(
        learning_rate, batch_size, epoch_times,
        f_max, bestauc_score, recall_max, prec_max, bestthreshold))
    test_Domainmodel = torch.load(
        'savedpkl/Doamin1DVal_{}_{}_{}_{}.pkl'.format(func, batch_size, learning_rate, epoch_times)).cuda()
    t_loss = 0
    doamin_test_outs = {}
    pred = []
    actual = []
    score_dict = {}
    batch_num = 0
    for batch_idx, (seqMatrix, domainStence, ppiVect, GO_annotiations) in enumerate(test_data_loader):
        domainStence = Variable(domainStence).cuda()
        GO_annotiations = Variable(GO_annotiations).cuda()
        out = test_Domainmodel(domainStence)
        batch_num += 1
        doamin_test_outs[test_benchmark[batch_idx]] = out.data[0].cpu().tolist()
        pred.append(out.data[0].cpu().tolist())
        actual.append(GO_annotiations.data[0].cpu().tolist())
        loss = loss_function(out, GO_annotiations)
        t_loss += loss.item()
        #t_loss += loss.data[0]
    test_loss = ""{}"".format(t_loss / batch_num)
    fpr, tpr, th = roc_curve(np.array(actual).flatten(), np.array(pred).flatten(), pos_label=1)
    auc_score = auc(fpr, tpr)
    each_best_fcore = 0
    for i in range(len(Thresholds)):
        f_score, recall, precision = calculate_performance(
            actual, pred, threshold=Thresholds[i], average='micro')
        if f_score > each_best_fcore:
            each_best_fcore = f_score
            each_best_scores = [Thresholds[i], f_score, recall, precision, auc_score]
        scores = [f_score, recall, precision, auc_score]
        score_dict[Thresholds[i]] = scores
    bestthreshold, f_max, recall_max = each_best_scores[0], each_best_scores[1], each_best_scores[2]
    prec_max, bestauc_score = each_best_scores[3], each_best_scores[4]

    print('test_loss:{},lr:{},batch:{},epoch{},f_max:{}\nauc_score{},recall_max{},prec_max{},threshold:{}'.format(
        test_loss, learning_rate, batch_size, epoch_times,
        f_max, auc_score, recall_max, prec_max, bestthreshold))

    with open('out/weight_out/Domainout{}_lr{}_bat{}_epo{}.csv'.format(
            func, learning_rate, batch_size, epoch_times), 'w') as f:
        f.write('lr:{},batchsize:{},epochtimes:{}\n'.format(learning_rate, batch_size, epoch_times))
        f.write('f_max:{},recall_max{},prec_max{},auc_score:{}\n'.format(
            f_max, recall_max, prec_max, auc_score))
        f.write('threshold,f_score,recall,precision, roc_auc,auc\n')
        for i in range(len(Thresholds)):
            f.write('{},'.format(str(Thresholds[i])))
            f.write('{}\n'.format(','.join(str(x) for x in score_dict[Thresholds[i]])))
        for key, var in doamin_test_outs.items():
            f.write('{},'.format(str(key)))
            f.write('{}\n'.format(','.join(str(x) for x in var)))

    # Ëé∑ÂèñÂÜçÊúÄ‰ºòÊ®°Âûã‰∏ãÁöÑËÆ≠ÁªÉÈõÜÁöÑËæìÂá∫
    train_out_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)
    domain_train_outs = {}
    for batch_idx, (seqMatrix, domainStence, ppiVect, GO_annotiations) in enumerate(train_out_loader):
        domainStence = Variable(domainStence).cuda()
        # GO_annotiations = Variable(GO_annotiations).cuda()
        out = test_Domainmodel(domainStence)
        domain_train_outs[train_benchmark[batch_idx]] = out.data[0].cpu().tolist()
    return domain_train_outs, doamin_test_outs, bestthreshold  # ËøîÂõûÂÜçÊúÄ‰ºòÁöÑDomainÊ®°Âûã‰∏ãÁöÑËÆ≠ÁªÉÈõÜÁöÑËæìÂá∫ÂíåÊµãËØïÈõÜÁöÑËæìÂá∫ÔºåÁî®‰∫éËÆ≠ÁªÉweight_classifier


def PPI_train(learningrate, batchsize, train_benchmark, test_benchmark, epochtime, func='MF'):
    print('{}  PPImodel start'.format(func))
    ppi_model = PPI_Module(func).cuda()
    batch_size = 16  # You can reduce the batch size to a smaller value
    #batch_size = batchsize
    learning_rate = learningrate
    epoch_times = epochtime
    print(ppi_model)
    print('batch_size_{},learning_rate_{},epoch_times_{}'.format(batch_size, learning_rate, epoch_times))
    loss_function = nn.BCELoss()
    optimizer = optim.Adam(ppi_model.parameters(), lr=learning_rate, weight_decay=0.00001)

    train_dataset = Dataload(train_benchmark, Seqfile_name, Domainfile_name, GOfile_name, func=func)
    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_dataset = Dataload(test_benchmark, Seqfile_name, Domainfile_name, GOfile_name, func=func)
    test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)

    ppi_model.train()
    best_fscore = 0
    for epoch in range(epoch_times):
        _loss = 0
        batch_num = 0
        torch.cuda.empty_cache()  # Add this line to release GPU memory
        for batch_idx, (seqMatrix, domainStence, ppiVect, GO_annotiations) in enumerate(train_data_loader):
            ppiVect = Variable(ppiVect).cuda()
            GO_annotiations = torch.squeeze(GO_annotiations)
            GO_annotiations = Variable(GO_annotiations.unsqueeze(1)).cuda()
            GO_annotiations = torch.squeeze(GO_annotiations, dim=0)
            #GO_annotiations = Variable(GO_annotiations).cuda()
            out = ppi_model(ppiVect)
            # Reshape the model's output to match the shape of GO_annotiations
            out = out.view(GO_annotiations.shape)
            optimizer.zero_grad()
            loss = loss_function(out, GO_annotiations)
            batch_num += 1
            loss.backward()
            optimizer.step()
            _loss += loss.item()
            #_loss += loss.data[0]
        epoch_loss = ""{}"".format(_loss / batch_num)
        t_loss = 0
        test_batch_num = 0
        pred = []
        actual = []
        for idx, (seqMatrix, domainStence, ppiVect, GO_annotiations) in enumerate(test_data_loader):
            ppiVect = Variable(ppiVect).cuda()
            GO_annotiations = Variable(GO_annotiations.unsqueeze(1)).cuda()
            GO_annotiations = torch.squeeze(GO_annotiations, dim=0)
            #GO_annotiations = Variable(GO_annotiations).cuda()
            out = ppi_model(ppiVect)
            test_batch_num = test_batch_num + 1
            pred.append(out.data[0].cpu().tolist())
            actual.append(GO_annotiations.data[0].cpu().tolist())
            one_loss = loss_function(out, GO_annotiations)
            t_loss += one_loss.item()  # Use .item() to get the scalar value
            #t_loss += one_loss.data[0]
        test_loss = ""{}"".format(t_loss / test_batch_num)
        fpr, tpr, th = roc_curve(np.array(actual).flatten(), np.array(pred).flatten(), pos_label=1)
        auc_score = auc(fpr, tpr)
        score_dict = {}
        each_best_fcore = 0
        each_best_scores = []
        for i in range(len(Thresholds)):
            f_score, recall, precision = calculate_performance(
                actual, pred, threshold=Thresholds[i], average='micro')
            if f_score >= each_best_fcore:
                each_best_fcore = f_score
                each_best_scores = [Thresholds[i], f_score, recall, precision, auc_score]
            scores = [f_score, recall, precision, auc_score]
            score_dict[Thresholds[i]] = scores
        if each_best_fcore >= best_fscore:
            best_fscore = each_best_fcore
            best_scores = each_best_scores
            best_score_dict = score_dict
            torch.save(ppi_model,
                       'savedpkl/PPIVal_{}_{}_{}_{}.pkl'.format(func, batch_size, learning_rate, epoch_times))
        t, f_score, recall = each_best_scores[0], each_best_scores[1], each_best_scores[2]
        precision, auc_score = each_best_scores[3], each_best_scores[4]
        print('epoch{},loss{},testloss:{},t{},f_score{}, auc{}, recall{}, precision{}'.format(
            epoch, epoch_loss, test_loss, t, f_score, auc_score, recall, precision))
    bestthreshold, f_max, recall_max = best_scores[0], best_scores[1], best_scores[2]
    prec_max, bestauc_score = best_scores[3], best_scores[4]
    print('lr:{},batch:{},epoch{},f_max:{}\nauc{},recall_max{},prec_max{},threshold:{}'.format(
        learning_rate, batch_size, epoch_times,
        f_max, bestauc_score, recall_max, prec_max, bestthreshold))
    test_PPImodel = torch.load(
        'savedpkl/PPIVal_{}_{}_{}_{}.pkl'.format(func, batch_size, learning_rate, epoch_times)).cuda()
    t_loss = 0
    ppi_test_outs = {}
    pred = []
    actual = []
    score_dict = {}
    batch_num = 0
    for batch_idx, (seqMatrix, domainStence, ppiVect, GO_annotiations) in enumerate(test_data_loader):
        ppiVect = Variable(ppiVect).cuda()
        GO_annotiations = Variable(GO_annotiations.unsqueeze(1)).cuda()
        GO_annotiations = torch.squeeze(GO_annotiations, dim=0)
        #GO_annotiations = Variable(GO_annotiations).cuda()
        out = test_PPImodel(ppiVect)
        batch_num += 1
        ppi_test_outs[test_benchmark[batch_idx]] = out.data[0].cpu().tolist()
        pred.append(out.data[0].cpu().tolist())
        actual.append(GO_annotiations.data[0].cpu().tolist())
        loss = loss_function(out, GO_annotiations)
        t_loss += loss.item()
        #t_loss += loss.data[0]
    test_loss = ""{}"".format(t_loss / batch_num)
    fpr, tpr, th = roc_curve(np.array(actual).flatten(), np.array(pred).flatten(), pos_label=1)
    auc_score = auc(fpr, tpr)
    each_best_fcore = 0
    for i in range(len(Thresholds)):
        f_score, recall, precision = calculate_performance(
            actual, pred, threshold=Thresholds[i], average='micro')
        if f_score > each_best_fcore:
            each_best_fcore = f_score
            each_best_scores = [Thresholds[i], f_score, recall, precision, auc_score]
        scores = [f_score, recall, precision, auc_score]
        score_dict[Thresholds[i]] = scores
    bestthreshold, f_max, recall_max = each_best_scores[0], each_best_scores[1], each_best_scores[2]
    prec_max, bestauc_score = each_best_scores[3], each_best_scores[4]

    print('test_loss:{},lr:{},batch:{},epoch{},f_max:{}\nauc_score{},recall_max{},prec_max{},threshold:{}'.format(
        test_loss, learning_rate, batch_size, epoch_times,
        f_max, auc_score, recall_max, prec_max, bestthreshold))

    with open('out/weight_out/PPIout{}_lr{}_bat{}_epo{}.csv'.format(
            func, learning_rate, batch_size, epoch_times), 'w') as f:
        f.write('lr:{},batchsize:{},epochtimes:{}\n'.format(learning_rate, batch_size, epoch_times))
        f.write('f_max:{},recall_max{},prec_max{},auc_score:{}\n'.format(
            f_max, recall_max, prec_max, auc_score))
        f.write('threshold,f_score,recall,precision, roc_auc,auc\n')
        for i in range(len(Thresholds)):
            f.write('{},'.format(str(Thresholds[i])))
            f.write('{}\n'.format(','.join(str(x) for x in score_dict[Thresholds[i]])))
        for key, var in ppi_test_outs.items():
            f.write('{},'.format(str(key)))
            f.write('{}\n'.format(','.join(str(x) for x in var)))

    # Ëé∑ÂèñÂÜçÊúÄ‰ºòÊ®°Âûã‰∏ãÁöÑËÆ≠ÁªÉÈõÜÁöÑËæìÂá∫
    train_out_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)
    ppi_train_outs = {}
    for batch_idx, (seqMatrix, domainStence, ppiVect, GO_annotiations) in enumerate(train_out_loader):
        ppiVect = Variable(ppiVect).cuda()
        # GO_annotiations = Variable(GO_annotiations).cuda()
        out = test_PPImodel(ppiVect)
        ppi_train_outs[train_benchmark[batch_idx]] = out.data[0].cpu().tolist()
    return ppi_train_outs, ppi_test_outs, bestthreshold  # ËøîÂõûÂÜçÊúÄ‰ºòÁöÑPPIÊ®°Âûã‰∏ãÁöÑËÆ≠ÁªÉÈõÜÁöÑËæìÂá∫ÂíåÊµãËØïÈõÜÁöÑËæìÂá∫ÔºåÁî®‰∫éËÆ≠ÁªÉweight_classifier


def Main(train_benchmark, test_benchmark, func='MF'):
    if func == 'BP':
        seq_train_out, seq_test_out, seq_t = Seq_train(0.0001, 16, train_benchmark, test_benchmark, 30, func)  # 15
        domain_train_out, domain_test_out, domain_t = Domain_train(0.001, 32, train_benchmark, test_benchmark, 45,
                                                                       func)  # 40
    else:
        seq_train_out, seq_test_out, seq_t = Seq_train(0.001, 8, train_benchmark, test_benchmark, 17, func)    #15
        domain_train_out, domain_test_out, domain_t = Domain_train(0.001, 16, train_benchmark, test_benchmark, 38, func)  #40
    ppi_train_out, ppi_test_out, ppi_t = PPI_train(0.0001, 8, train_benchmark, test_benchmark, 38, func)   #40

    print('{}  Weight_model start'.format(func))
    learning_rate = 0.001
    batch_size = 32
    epoch_times = 40
    weight_model = Weight_classifier(func).cuda()
    print(weight_model)
    print('batch_size_{},learning_rate_{},epoch_times_{}'.format(batch_size, learning_rate, epoch_times))
    loss_function = nn.BCELoss()
    optimizer = optim.Adam(weight_model.parameters(), lr=learning_rate, weight_decay=0.00001)

    train_dataset = weight_Dataload(train_benchmark, seq_train_out, domain_train_out, ppi_train_out, GOfile_name, func=func)
    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_dataset = weight_Dataload(test_benchmark, seq_test_out, domain_test_out, ppi_test_out, GOfile_name, func=func)
    test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)

    weight_model.train()
    best_fscore = 0
    for epoch in range(epoch_times):
        _loss = 0
        batch_num = 0
        for batch_idx, (weight_features, GO_annotiations) in enumerate(train_data_loader):
            weight_features = Variable(weight_features).cuda()
            # print(weight_features)
            #GO_annotiations = torch.squeeze(GO_annotiations)
            GO_annotiations = GO_annotiations.view(1, -1)  # Reshape target tensor
            GO_annotiations = Variable(GO_annotiations.view(1, -1)).cuda()  # Reshape and apply to the model
            #GO_annotiations = Variable(GO_annotiations).cuda()
            out = torch.sigmoid(weight_model(weight_features))
            #out = weight_model(weight_features)
            optimizer.zero_grad()
            loss = loss_function(out, GO_annotiations)
            batch_num += 1
            loss.backward()
            optimizer.step()
            _loss += loss.item()
            #_loss += loss.data[0]
        epoch_loss = ""{}"".format(_loss / batch_num)
        t_loss = 0
        test_batch_num = 0
        pred = []
        actual = []
        for idx, (weight_features, GO_annotiations) in enumerate(test_data_loader):
            weight_features = Variable(weight_features).cuda()
            GO_annotiations = Variable(GO_annotiations.view(1, -1)).cuda()  # Reshape and apply to the model
            #GO_annotiations = Variable(GO_annotiations).cuda()
            out = weight_model(weight_features)
            test_batch_num = test_batch_num + 1
            pred.append(out.data[0].cpu().tolist())
            actual.append(GO_annotiations.data[0].cpu().tolist())
            one_loss = loss_function(out, GO_annotiations)
            t_loss += one_loss.item()
            #t_loss += one_loss.data[0]
        test_loss = ""{}"".format(t_loss / test_batch_num)
        fpr, tpr, th = roc_curve(np.array(actual).flatten(), np.array(pred).flatten(), pos_label=1)
        auc_score = auc(fpr, tpr)
        score_dict = {}
        each_best_fcore = 0
        each_best_scores = []
        for i in range(len(Thresholds)):
            f_score, recall, precision = calculate_performance(
                actual, pred, threshold=Thresholds[i], average='micro')
            if f_score >= each_best_fcore:
                each_best_fcore = f_score
                each_best_scores = [Thresholds[i], f_score, recall, precision, auc_score]
            scores = [f_score, recall, precision, auc_score]
            score_dict[Thresholds[i]] = scores
        if each_best_fcore >= best_fscore:
            best_fscore = each_best_fcore
            best_scores = each_best_scores
            best_score_dict = score_dict
            torch.save(weight_model,
                       'savedpkl/WeightVal_{}_{}_{}_{}.pkl'.format(func, batch_size, learning_rate, epoch_times))
        t, f_score, recall = each_best_scores[0], each_best_scores[1], each_best_scores[2]
        precision, auc_score = each_best_scores[3], each_best_scores[4]
        print('epoch{},loss{},testloss:{},t{},f_score{}, auc{}, recall{}, precision{}'.format(
            epoch, epoch_loss, test_loss, t, f_score, auc_score, recall, precision))
    bestthreshold, f_max, recall_max = best_scores[0], best_scores[1], best_scores[2]
    prec_max, bestauc_score = best_scores[3], best_scores[4]
    print('lr:{},batch:{},epoch{},f_max:{}\nauc{},recall_max{},prec_max{},threshold:{}'.format(
        learning_rate, batch_size, epoch_times,
        f_max, bestauc_score, recall_max, prec_max, bestthreshold))
    # return best_scores

    test_weight_model = torch.load(
        'savedpkl/WeightVal_{}_{}_{}_{}.pkl'.format(func, batch_size, learning_rate, epoch_times)).cuda()
    t_loss = 0
    weight_test_outs = {}
    pred = []
    actual = []
    score_dict = {}
    batch_num = 0
    for batch_idx, (weight_features, GO_annotiations) in enumerate(test_data_loader):
        weight_features = Variable(weight_features).cuda()
        GO_annotiations = Variable(GO_annotiations.view(1, -1)).cuda()  # Reshape and apply to the model
        #GO_annotiations = Variable(GO_annotiations).cuda()
        out = test_weight_model(weight_features)
        batch_num += 1
        weight_test_outs[test_benchmark[batch_idx]] = out.data[0].cpu().tolist()
        pred.append(out.data[0].cpu().tolist())
        actual.append(GO_annotiations.data[0].cpu().tolist())
        loss = loss_function(out, GO_annotiations)
        t_loss += loss.item()
        #t_loss += loss.data[0]
    test_loss = ""{}"".format(t_loss / batch_num)
    fpr, tpr, th = roc_curve(np.array(actual).flatten(), np.array(pred).flatten(), pos_label=1)
    auc_score = auc(fpr, tpr)
    aupr = cacul_aupr(np.array(actual).flatten(), np.array(pred).flatten())
    each_best_fcore = 0
    for i in range(len(Thresholds)):
        f_score, recall, precision = calculate_performance(
            actual, pred, threshold=Thresholds[i], average='micro')
        if f_score > each_best_fcore:
            each_best_fcore = f_score
            each_best_scores = [Thresholds[i], f_score, recall, precision, auc_score, aupr]
        scores = [f_score, recall, precision, auc_score]
        score_dict[Thresholds[i]] = scores
    bestthreshold, f_max, recall_max = each_best_scores[0], each_best_scores[1], each_best_scores[2]
    prec_max, bestauc_score, aupr_score = each_best_scores[3], each_best_scores[4], each_best_scores[5]

    print('test_loss:{},lr:{},batch:{},epoch{},f_max:{}\nauc_score{},recall_max{},prec_max{},threshold:{}'.format(
        test_loss, learning_rate, batch_size, epoch_times,
        f_max, auc_score, recall_max, prec_max, bestthreshold))

    # with open('out/weight_out/Weight_out{}_lr{}_bat{}_epo{}.csv'.format(
    #         func, learning_rate, batch_size, epoch_times), 'w') as f:
    #     f.write('lr:{},batchsize:{},epochtimes:{}\n'.format(learning_rate, batch_size, epoch_times))
    #     f.write('f_max:{},recall_max{},prec_max{},auc_score:{}\n'.format(
    #         f_max, recall_max, prec_max, auc_score))
    #     f.write('threshold,f_score,recall,precision, roc_auc,auc\n')
    #     for i in range(len(Thresholds)):
    #         f.write('{},'.format(str(Thresholds[i])))
    #         f.write('{}\n'.format(','.join(str(x) for x in score_dict[Thresholds[i]])))
    #     for key, var in weight_test_outs.items():
    #         f.write('{},'.format(str(key)))
    #         f.write('{}\n'.format(','.join(str(x) for x in var)))
    return each_best_scores


def read_benchmark(term_arg='MF'):
    benchmark_file = '{}_benchmarkSet_2.csv'.format(term_arg)
    print(benchmark_file)
    all_data = []
    with open(benchmark_file, 'r') as f:
        for line in f:
            item = line.strip()
            all_data.append(item)
    return all_data


def validation(func='MF', k_fold=5):
    kf = KFold(n_splits=k_fold)
    benchmark = np.array(read_benchmark(func))
    scores = []
    for train_index, test_index in kf.split(benchmark):
        train_set = benchmark[train_index].tolist()
        test_set = benchmark[test_index].tolist()
        each_fold_scores = Main(train_set, test_set, func=func)
        scores.append(each_fold_scores)
    f_maxs, pre_maxs, rec_maxs, auc_s, aupr_s = [], [], [], [], []
    for i in range(len(scores)):
        f_maxs.append(scores[i][1])
        rec_maxs.append(scores[i][2])
        pre_maxs.append(scores[i][3])
        auc_s.append(scores[i][4])
        aupr_s.append(scores[i][5])
    f_mean = np.mean(np.array(f_maxs))
    rec_mean = np.mean(np.array(rec_maxs))
    pre_mean = np.mean(np.array(pre_maxs))
    auc_mean = np.mean(np.array(auc_s))
    aupr_mean = np.mean(np.array(aupr_s))
    print('{}:f_mean{},rec_mean{},pre_mean{},auc_mean{}, aupr_mean{}'.format(
        func, f_mean, rec_mean, pre_mean, auc_mean, aupr_mean))

if __name__ == '__main__':
    Terms = ['BP', 'MF', 'CC']
    validation(Terms[0], 5)
    # run(func=Terms[1])
    # learning_rates = [0.001]
    # # learning_rates = [0.001, 0.0001, 0.01, 0.00001]
    # # batchsizes = [8, 16, 32, 64]
    # batchsizes = [32]
    # is_train = True
    # for i in range(len(learning_rates)):
    #     for j in range(len(batchsizes)):
    #         if is_train:
    #             Main(learning_rates[i], batchsizes[j], 40, func=Terms[0], is_train=True)
    #             is_first = False
    #         else:
    #             Main(learning_rates[i], batchsizes[j], 40, func=Terms[0], is_train=False)
    time_end = time.time()

    print('time cost', time_end - time_start,'s')




Thank you!"
111644,DISABLED test_nested_tensor_chunk_cpu_float16 (__main__.TestNestedTensorDeviceTypeCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_nested_tensor_chunk_cpu_float16&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17886253401).

Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 15 failures and 5 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_nested_tensor_chunk_cpu_float16`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111643,There is a repeated expression and the wrong word ,"### üìö The doc issue


> this page: <https://pytorch.org/tutorials/prototype/prototype_index.html>



<img width=""844"" alt=""image"" src=""https://github.com/pytorch/pytorch/assets/9199175/2e0fb141-32e8-41a0-995f-b128617f99ae"">


### Suggest a potential alternative/fix

_No response_"
111641,Can't export a pth model to onnx (RuntimeError: Couldn't lower all tuples),"### üêõ Describe the bug

Hello everyone, I am trying to convert a pytorch model to onnx using the library onnx in torch. The model architecture and weights are in the repository https://github.com/Sense-X/Co-DETR , I am usign the model CO-Dino with a Swin-L backbone, however when I launch the model exportation I encounter this error ```RuntimeError: Couldn't lower all tuples```.

![image](https://github.com/pytorch/pytorch/assets/59453891/62666dfd-e2ff-4ef1-a674-54e8f9ef70ea)

PS: Sorry, I can't copy poste a simple python code to reproduce the problem. 


### Versions

[pip3] numpy==1.24.3
[pip3] onnx==1.14.1
[pip3] onnxruntime==1.8.1
[pip3] onnxruntime-gpu==1.8.1
[pip3] torch==1.11.0
[pip3] torchaudio==0.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                0.11.0               py38_cu113    pytorch
[conda] torchvision               0.12.0               py38_cu113    pytorch
"
111640,[RFC] Enable Int8-Mixed-BF16 PT2E PTQ Quantization with Inductor,"### üöÄ The feature, motivation and pitch

[Intel-Extension-for-PyTorch](https://github.com/intel/intel-extension-for-pytorch) (IPEX) offers an advanced int8-mixed-bf16 quantization path, which transforms the output of quantized Conv/GEMM operations into the BF16 data type if there is no subsequent quantized operator. This enhancement significantly improves the inference performance of models such as Bert/DistilBert, as the pointwise operators following GEMM will operate with BF16 data instead of FP32.

* Here is the [example code](https://gist.github.com/leslie-fang-intel/acb568f1416c7395b18cd64774aa5a64) for how use this feature with IPEX.
* Please note that this feature may result in accuracy loss for certain models. With IPEX, we have verified its accuracy in models such as Bert, DistilBert, stable diffusion, and some other LLM models. However, we have also observed accuracy issues in models like vision transformers.
* Similarly, we recently recive a feature request in https://github.com/pytorch/pytorch/issues/111487.


### Alternatives

We typically have two options to enable this feature.

### Option 1: Use Autocast
Autocast is naturally employed for BF16 optimization in Inductor. Similarly, we can harness it for PT2E int8-mixed-bf16 features to generate a pattern like `q -> dq -> float32_to_bfloat16 -> conv -> bfloat16_to_fp32 -> q -> dq`. 
* `to_bfloat16` node before conv should be inserted when used Autocast + `torch.compile` together, since conv is in whitelist of Autocast.
* As for inserting `bfloat16_to_fp32` node after conv node, we need to extend the implementation of https://github.com/pytorch/pytorch/blob/93a9b1314b4bc88ccddc0aa438d4d332955027a8/torch/ao/quantization/fx/_decomposed.py#L36-L64 by add these lines at beginning of this function
```
if input.dtype == torch.bfloat16:
  input = input.to(torch.float32)
``` 

Here's an example code snippet:
```
exported_model = capture_pre_autograd_graph(
    model,
    example_inputs
)

# Create X86InductorQuantizer
quantizer = X86InductorQuantizer()
quantizer.set_global(xiq.get_default_x86_inductor_quantization_config())
# PT2E Quantization flow
prepared_model = prepare_pt2e(exported_model, quantizer)
# Calibration
converted_model = convert_pt2e(prepared_model)
torch.ao.quantization.move_exported_model_to_eval(converted_model)

with torch.autocast(device_type=""cpu"", dtype=torch.bfloat16, enabled=enable_int8_mixed_bf16), torch.no_grad():
	optimized_model = torch.compile(converted_model)

	# Int8-Mixed-BF16 Inference
	quant_output = optimized_model(images)
```

* Pros:
	* Utilize the existing int8-mixed-fp32 quantizer and PT2E flow implementation.
	* Make use of the existing Autocast operator list and mechanism.
* Cons:
	* The Autocast mechanism will convert each input, including the convolution's bias, to BF16. However, for X86InductorQuantizer and the associated Inductor optimization, we anticipate that using float32 for the bias input may yield better accuracy.

### Option 2: Add BFloat16 as a quantization type in PT2E Flow (in QuantizationSpec)
Alternatively, we can introduce BFloat16 as a quantization type in PT2E Flow (within QuantizationSpec). 
* We may need to extend the Observer implementation to annotate its use for int8-mixed-bf16, depending on the quantization recipe.
* During the convert phase, we will examine the observer information to determine if it has been annotated with int8-mixed-bf16. 
	* If the input of a quantization node is in BFloat16 data type, an additional `to_float` node will be inserted before the quantization node.
	* Following the dequantization node, an additional `to_bf16` node will be inserted.

```
exported_model = capture_pre_autograd_graph(
    model,
    example_inputs
)

# Create X86InductorQuantizer
quantizer = X86InductorQuantizer()
quantizer.set_global(xiq.get_default_x86_inductor_quantization_config(dtype=BFloat16))
# PT2E Quantization flow
prepared_model = prepare_pt2e(exported_model, quantizer)
# Calibration
converted_model = convert_pt2e(prepared_model)
torch.ao.quantization.move_exported_model_to_eval(converted_model)

with torch.no_grad():
	optimized_model = torch.compile(converted_model)

	# Int8-Mixed-BF16 Inference
	quant_output = optimized_model(images)
```

* Pros:
	* We can achieve more flexibility with a customized implementation for int8-mixed-bf16 quantization, allowing us to overcome certain limitations in Autocast, such as bias conversion.. 
* Cons:
	* Non-trivial changes may need in QuantizationSpec, Observer, Quantizer and PT2 Flow convert implementation.

We prefer option 1 as it requires fewer changes in the PT2E quantization flow and is clear and straightforward.

### Additional context

### Optimization Inside Inductor
* Conv/GEMM
	Here is the pattern after quantization flow we expect to see in Inductor.
	```
	q -> dq -> float32_to_bfloat16 -> conv -> bfloat16_to_fp32 -> q -> dq
	```
	* Step 1: In the weight prepack phase, `dq -> float32_to_bfloat16 -> conv` will be matched at first to generate a `qconv_bf16_output` node with `int8` input dtype and `bfloat16` output dtype.
	* Step 2: Further more, we will check if `bfloat16_to_fp32 -> q` pattern exists after this `qconv_bf16_output` node. If so, we will further merge `qconv_bf16_output -> bfloat16_to_fp32 -> q` into a `qconv` node with with `int8` input dtype and `int8` output dtype.

* Non-Conv/GEMM.
	* Non-Conv/GEMM pattern will lowering in Inductor CPP Backend for Code Generation.

### Enabling Plans
Here is some plans to follow up the option 1:
* Makes all `onednn.qconv1d_pointwise/linear_pointwise` operators support BF16 output.
* Remove the annotation of output at conv/linear in `X86InductorQuantizer`.
* Extend the decomposed quant to support bf16 input.
* Extend Weight prepack pattern matcher of `dequant -> to_bf16 -> conv/linear`.
* Extend QConv/Linear int8-mixed-bf16 output patterns matcher of `dequant -> to_bf16 -> conv/linear -> to_fp32 -> quant`.
* Extend Postop-passes pattern match of Conv/Linear ReLU/Add/Add_ReLU fusion with FP32/BF16 output.

cc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @Xia-Weiwen"
111638,DISABLED test_meta_outplace_fft_ifft_cpu_float64 (__main__.TestMetaCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_outplace_fft_ifft_cpu_float64&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17883819575).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_outplace_fft_ifft_cpu_float64`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

cc @ezyang @mruberry @Lezcano @peterbell10 @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111636,torch2.1.0 DDP+compile+dynamic_shape cause error,"### üêõ Describe the bug

Excuse me! When I use **torch2.1.0** DDP+compile to wrapper huggingface transformers gpt2 model, it will cause error only for the last batch data (shape is smaller than batch_size). Detailed logs as follows:
```bash
[default0]:10/20/2023 13:45:50 - INFO - __main__ - local_rank=0, rank=0, world_size=2
[default1]:10/20/2023 13:45:50 - INFO - __main__ - local_rank=1, rank=1, world_size=2
[default0]:10/20/2023 13:45:53 - INFO - __main__ - GPT2LMHeadModel(
[default0]:  (transformer): GPT2Model(
[default0]:    (wte): Embedding(50257, 768)
[default0]:    (wpe): Embedding(1024, 768)
[default0]:    (drop): Dropout(p=0.1, inplace=False)
[default0]:    (h): ModuleList(
[default0]:      (0-11): 12 x GPT2Block(
[default0]:        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
[default0]:        (attn): GPT2Attention(
[default0]:          (c_attn): Conv1D()
[default0]:          (c_proj): Conv1D()
[default0]:          (attn_dropout): Dropout(p=0.1, inplace=False)
[default0]:          (resid_dropout): Dropout(p=0.1, inplace=False)
[default0]:        )
[default0]:        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
[default0]:        (mlp): GPT2MLP(
[default0]:          (c_fc): Conv1D()
[default0]:          (c_proj): Conv1D()
[default0]:          (act): NewGELUActivation()
[default0]:          (dropout): Dropout(p=0.1, inplace=False)
[default0]:        )
[default0]:      )
[default0]:    )
[default0]:    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
[default0]:  )
[default0]:  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
[default0]:)

[default0]:10/20/2023 13:35:36 - INFO - __main__ - [rank 0][0/9]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:[rank0]:[2023-10-20 13:35:36,584] [0/0] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[default0]:10/20/2023 13:35:41 - INFO - __main__ - [rank 0][0/9]: workers well
[default0]:10/20/2023 13:35:41 - INFO - __main__ - [rank 0][1/9]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:35:42 - INFO - __main__ - [rank 0][1/9]: workers well
[default0]:10/20/2023 13:35:42 - INFO - __main__ - [rank 0][2/9]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:35:42 - INFO - __main__ - [rank 0][2/9]: workers well
[default0]:10/20/2023 13:35:42 - INFO - __main__ - [rank 0][3/9]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:35:42 - INFO - __main__ - [rank 0][3/9]: workers well
[default1]:10/20/2023 13:35:42 - INFO - __main__ - [rank 1][0/9]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default1]:[rank1]:[2023-10-20 13:35:43,025] [0/0] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[default0]:10/20/2023 13:35:42 - INFO - __main__ - [rank 0][4/9]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:35:43 - INFO - __main__ - [rank 0][4/9]: workers well
[default0]:10/20/2023 13:35:43 - INFO - __main__ - [rank 0][5/9]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:35:43 - INFO - __main__ - [rank 0][5/9]: workers well
[default0]:10/20/2023 13:35:43 - INFO - __main__ - [rank 0][6/9]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:35:44 - INFO - __main__ - [rank 0][6/9]: workers well
[default0]:10/20/2023 13:35:44 - INFO - __main__ - [rank 0][7/9]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:35:44 - INFO - __main__ - [rank 0][7/9]: workers well
[default0]:10/20/2023 13:35:44 - INFO - __main__ - [rank 0][8/9]: batch_data['input_ids'].shape=torch.Size([7, 1024])
[default0]:[rank0]:[2023-10-20 13:35:44,536] [0/1] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[default1]:10/20/2023 13:35:48 - INFO - __main__ - [rank 1][0/9]: workers well
[default1]:10/20/2023 13:35:48 - INFO - __main__ - [rank 1][1/9]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default1]:10/20/2023 13:35:48 - INFO - __main__ - [rank 1][1/9]: workers well
[default1]:10/20/2023 13:35:48 - INFO - __main__ - [rank 1][2/9]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default1]:10/20/2023 13:35:48 - INFO - __main__ - [rank 1][2/9]: workers well
[default1]:10/20/2023 13:35:49 - INFO - __main__ - [rank 1][3/9]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default1]:10/20/2023 13:35:49 - INFO - __main__ - [rank 1][3/9]: workers well
[default1]:10/20/2023 13:35:49 - INFO - __main__ - [rank 1][4/9]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default1]:10/20/2023 13:35:49 - INFO - __main__ - [rank 1][4/9]: workers well
[default1]:10/20/2023 13:35:49 - INFO - __main__ - [rank 1][5/9]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default1]:10/20/2023 13:35:50 - INFO - __main__ - [rank 1][5/9]: workers well
[default1]:10/20/2023 13:35:50 - INFO - __main__ - [rank 1][6/9]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default1]:10/20/2023 13:35:50 - INFO - __main__ - [rank 1][6/9]: workers well
[default1]:10/20/2023 13:35:50 - INFO - __main__ - [rank 1][7/9]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default1]:10/20/2023 13:35:50 - INFO - __main__ - [rank 1][7/9]: workers well
[default1]:10/20/2023 13:35:50 - INFO - __main__ - [rank 1][8/9]: batch_data['input_ids'].shape=torch.Size([7, 1024])

[default0]:Traceback (most recent call last):
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/fx/graph_module.py"", line 274, in __call__
[default0]:    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
[default0]:    return self._call_impl(*args, **kwargs)
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
[default0]:    return forward_call(*args, **kwargs)
[default0]:  File ""<eval_with_key>.63"", line 24, in forward
[default0]:    matmul = torch.matmul(permute, transpose);  permute = transpose = None
[default0]:RuntimeError: Cannot call sizes() on tensor with symbolic sizes/strides
[default0]:
[default0]:Call using an FX-traced Module, line 24 of the traced Module's generated forward function:
[default0]:    transpose = permute_1.transpose(-1, -2)
[default0]:    matmul = torch.matmul(permute, transpose);  permute = transpose = None
[default0]:
[default0]:~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
[default0]:    full = torch.full([], 8.0, dtype = torch.float32, device = device(type='cuda', index=0))
[default0]:
[default0]:    truediv = matmul / full;  matmul = full = None

[default0]:Traceback (most recent call last):
[default0]:  File ""/home/yuzhe.wu/grace-compute-dl-benchmark/nlp/gpt/gpt.py"", line 294, in <module>
[default0]:    main()
[default0]:  File ""/home/yuzhe.wu/grace-compute-dl-benchmark/nlp/gpt/gpt.py"", line 276, in main
[default0]:    validate(args, 0, eval_dataloader, model, world_size, device, summary_writer)
[default0]:  File ""/home/yuzhe.wu/grace-compute-dl-benchmark/nlp/gpt/gpt.py"", line 114, in validate
[default0]:    outputs = model(**batch_data)
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
[default0]:    return self._call_impl(*args, **kwargs)
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
[default0]:    return forward_call(*args, **kwargs)
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py"", line 328, in _fn
[default0]:    return fn(*args, **kwargs)
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/external_utils.py"", line 17, in inner
[default0]:    return fn(*args, **kwargs)
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
[default0]:    return self._call_impl(*args, **kwargs)
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
[default0]:    return forward_call(*args, **kwargs)
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/distributed.py"", line 1519, in forward
[default0]:    else self._run_ddp_forward(*inputs, **kwargs)
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/distributed.py"", line 1355, in _run_ddp_forward
[default0]:    return self.module(*inputs, **kwargs)  # type: ignore[index]
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
[default0]:    return self._call_impl(*args, **kwargs)
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
[default0]:    return forward_call(*args, **kwargs)
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py"", line 487, in catch_errors
[default0]:    return hijacked_callback(frame, cache_entry, hooks, frame_state)
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py"", line 641, in _convert_frame
[default0]:    result = inner_convert(frame, cache_size, hooks, frame_state)
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py"", line 133, in _fn
[default0]:    return fn(*args, **kwargs)
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py"", line 389, in _convert_frame_assert
[default0]:    return _compile(
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py"", line 569, in _compile
[default0]:    guarded_code = compile_inner(code, one_graph, hooks, transform)
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/utils.py"", line 189, in time_wrapper
[default0]:    r = func(*args, **kwargs)
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py"", line 491, in compile_inner
[default0]:    out_code = transform_code_object(code, transform)
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/bytecode_transformation.py"", line 1028, in transform_code_object
[default0]:    transformations(instructions, code_options)
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py"", line 458, in transform
[default0]:    tracer.run()
[default0]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py"", line 2074, in run
[default1]:Traceback (most recent call last):
[default1]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/fx/graph_module.py"", line 274, in __call__
[default1]:    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[default1]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
[default1]:    return self._call_impl(*args, **kwargs)
[default1]:  File ""/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
[default1]:    return forward_call(*args, **kwargs)
[default1]:  File ""<eval_with_key>.63"", line 24, in forward
[default1]:    matmul = torch.matmul(permute, transpose);  permute = transpose = None
[default1]:RuntimeError: Cannot call sizes() on tensor with symbolic sizes/strides

```

However, the single gpu (without DDP wrapper) workers well:
```bash
[default0]:10/20/2023 12:00:20 - INFO - __main__ - local_rank=0, rank=0, world_size=1
[default0]:10/20/2023 12:00:24 - INFO - __main__ - GPT2LMHeadModel(
[default0]:  (transformer): GPT2Model(
[default0]:    (wte): Embedding(50257, 768)
[default0]:    (wpe): Embedding(1024, 768)
[default0]:    (drop): Dropout(p=0.1, inplace=False)
[default0]:    (h): ModuleList(
[default0]:      (0-11): 12 x GPT2Block(
[default0]:        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
[default0]:        (attn): GPT2Attention(
[default0]:          (c_attn): Conv1D()
[default0]:          (c_proj): Conv1D()
[default0]:          (attn_dropout): Dropout(p=0.1, inplace=False)
[default0]:          (resid_dropout): Dropout(p=0.1, inplace=False)
[default0]:        )
[default0]:        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
[default0]:        (mlp): GPT2MLP(
[default0]:          (c_fc): Conv1D()
[default0]:          (c_proj): Conv1D()
[default0]:          (act): NewGELUActivation()
[default0]:          (dropout): Dropout(p=0.1, inplace=False)
[default0]:        )
[default0]:      )
[default0]:    )
[default0]:    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
[default0]:  )
[default0]:  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
[default0]:)

[default0]:10/20/2023 13:27:16 - INFO - __main__ - [rank 0][0/17]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:27:21 - INFO - __main__ - [rank 0][0/17]: workers well
[default0]:10/20/2023 13:27:21 - INFO - __main__ - [rank 0][1/17]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:27:21 - INFO - __main__ - [rank 0][1/17]: workers well
[default0]:10/20/2023 13:27:21 - INFO - __main__ - [rank 0][2/17]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:27:22 - INFO - __main__ - [rank 0][2/17]: workers well
[default0]:10/20/2023 13:27:22 - INFO - __main__ - [rank 0][3/17]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:27:22 - INFO - __main__ - [rank 0][3/17]: workers well
[default0]:10/20/2023 13:27:22 - INFO - __main__ - [rank 0][4/17]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:27:23 - INFO - __main__ - [rank 0][4/17]: workers well
[default0]:10/20/2023 13:27:23 - INFO - __main__ - [rank 0][5/17]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:27:23 - INFO - __main__ - [rank 0][5/17]: workers well
[default0]:10/20/2023 13:27:23 - INFO - __main__ - [rank 0][6/17]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:27:23 - INFO - __main__ - [rank 0][6/17]: workers well
[default0]:10/20/2023 13:27:23 - INFO - __main__ - [rank 0][7/17]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:27:24 - INFO - __main__ - [rank 0][7/17]: workers well
[default0]:10/20/2023 13:27:24 - INFO - __main__ - [rank 0][8/17]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:27:24 - INFO - __main__ - [rank 0][8/17]: workers well
[default0]:10/20/2023 13:27:24 - INFO - __main__ - [rank 0][9/17]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:27:24 - INFO - __main__ - [rank 0][9/17]: workers well
[default0]:10/20/2023 13:27:24 - INFO - __main__ - [rank 0][10/17]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:27:25 - INFO - __main__ - [rank 0][10/17]: workers well
[default0]:10/20/2023 13:27:25 - INFO - __main__ - [rank 0][11/17]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:27:25 - INFO - __main__ - [rank 0][11/17]: workers well
[default0]:10/20/2023 13:27:25 - INFO - __main__ - [rank 0][12/17]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:27:26 - INFO - __main__ - [rank 0][12/17]: workers well
[default0]:10/20/2023 13:27:26 - INFO - __main__ - [rank 0][13/17]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:27:26 - INFO - __main__ - [rank 0][13/17]: workers well
[default0]:10/20/2023 13:27:26 - INFO - __main__ - [rank 0][14/17]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:27:26 - INFO - __main__ - [rank 0][14/17]: workers well
[default0]:10/20/2023 13:27:26 - INFO - __main__ - [rank 0][15/17]: batch_data['input_ids'].shape=torch.Size([16, 1024])
[default0]:10/20/2023 13:27:27 - INFO - __main__ - [rank 0][15/17]: workers well
[default0]:10/20/2023 13:27:27 - INFO - __main__ - [rank 0][16/17]: batch_data['input_ids'].shape=torch.Size([13, 1024])
[default0]:10/20/2023 13:27:32 - INFO - __main__ - [rank 0][16/17]: workers well

```

The following is complete code in `demo.py`:
```python
import os
import sys
import math
import logging
import torch
import torch.distributed as dist
from typing import List
from itertools import chain
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from datasets import load_dataset
from transformers import (
    default_data_collator,
    CONFIG_MAPPING,
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
)

logging.basicConfig(
      format=""%(asctime)s - %(levelname)s - %(name)s - %(message)s"",
      datefmt=""%m/%d/%Y %H:%M:%S"",
      level=logging.INFO,
)
logger = logging.getLogger(__name__)

def print_rank_0(message):
    """"""If distributed is initialized, print only on rank 0.""""""
    if dist.is_initialized():
        if dist.get_rank() == 0:
            logger.info(message)
    else:
        logger.info(message)

def build_datasets(tokenizer, phase):
    dataset_name = ""wikitext""
    dataset_config_name=""wikitext-103-raw-v1""
    cache_dir=""./.cache/wikitext/wikitext103""
    validation_split_percentage = 5
    raw_datasets = load_dataset(
        dataset_name, dataset_config_name, cache_dir=cache_dir
    )
    if ""validation"" not in raw_datasets.keys():
        raw_datasets[""validation""] = load_dataset(
            dataset_name,
            dataset_config_name,
            split=f""train[:{validation_split_percentage}%]"",
        )
        raw_datasets[""train""] = load_dataset(
            dataset_name,
            dataset_config_name,
            split=f""train[{validation_split_percentage}%:]"",
        )

    # Preprocessing the datasets.
    # First we tokenize all the texts.
    column_names = raw_datasets[""train""].column_names
    text_column_name = ""text"" if ""text"" in column_names else column_names[0]

    def tokenize_function(examples):
        return tokenizer(examples[text_column_name])

    tokenized_datasets = raw_datasets.map(
        tokenize_function,
        batched=True,
        num_proc=16,
        remove_columns=column_names,
        load_from_cache_file=True,
        desc=""Running tokenizer on dataset"",
    )

    block_size = tokenizer.model_max_length
    if block_size > 1024:
        logger.warning(
            ""The chosen tokenizer supports a `model_max_length` that is longer than the default""
            "" `block_size` value of 1024. If you would like to use a longer `block_size` up to""
            "" `tokenizer.model_max_length` you can override this with `--block_size xxx`.""
        )
        block_size = 1024

    # Main data processing function that will concatenate all texts from our dataset and generate
    # chunks of block_size.
    def group_texts(examples):
        # Concatenate all texts.
        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
        total_length = len(concatenated_examples[list(examples.keys())[0]])
        # We drop the small remainder, we could add padding if the model supported it
        # instead of this drop, you can
        # customize this part to your needs.
        if total_length >= block_size:
            total_length = (total_length // block_size) * block_size
        # Split by chunks of max_len.
        result = {
            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
            for k, t in concatenated_examples.items()
        }
        result[""labels""] = result[""input_ids""].copy()
        return result

    # Note that with `batched=True`, this map processes 1,000 texts together,
    # so group_texts throws away a remainde for each of those groups of 1,000 texts.
    # You can adjust that batch_siz here but a higher value might be slowe to preprocess.
    #
    # To speed up this part, we use multiprocessing.
    # See the documentation of the map method for more information:
    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map

    lm_datasets = tokenized_datasets.map(
        group_texts,
        batched=True,
        num_proc=16,
        load_from_cache_file=True,
        desc=f""Grouping texts in chunks of {block_size}"",
    )

    return lm_datasets[phase]

if __name__ == ""__main__"":
    local_rank = int(os.environ[""LOCAL_RANK""])
    dist.init_process_group(backend=""nccl"")
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    logger.info(f""local_rank={local_rank}, rank={rank}, world_size={world_size}"")
    
    device = torch.device(""cuda"", local_rank)
    seed = 1234
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.set_device(local_rank)

    ### build model and tokenizer ###
    model_name_or_path = ""gpt2""
    config = AutoConfig.from_pretrained(model_name_or_path)
    tokenizer = AutoTokenizer.from_pretrained(
        model_name_or_path, use_fast=True
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_name_or_path,
        from_tf=bool("".ckpt"" in model_name_or_path),
        config=config,
        low_cpu_mem_usage=False
    )
    embedding_size = model.get_input_embeddings().weight.shape[0]
    if len(tokenizer) > embedding_size:
        model.resize_token_embeddings(len(tokenizer))
    model.tie_weights()

    print_rank_0(model)
    model = model.to(device)

    # wrapper DDP
    if world_size > 1:
        model = DDP(model, device_ids=[device], output_device=device)

    # wrapper torch compile
    # torch._logging.set_logs(dynamo=logging.INFO)
    # torch._dynamo.config.verbose = True

    def comstom_backend(
        gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]
    ):
        # gm.graph.print_tabular()
        return gm.forward

    torch._dynamo.reset()
    model = torch.compile(model, backend=comstom_backend)

    ### build data loader ###
    # Downloading and loading a dataset from the hub.
    train_dataset = build_datasets(tokenizer, ""train"")
    eval_dataset = build_datasets(tokenizer, ""test"")
    train_dataloader = DataLoader(
        train_dataset,
        collate_fn=default_data_collator,
        shuffle=False,
        batch_size=16,
        sampler=DistributedSampler(train_dataset),
    )
    eval_dataloader = DataLoader(
        eval_dataset,
        collate_fn=default_data_collator,
        shuffle=False,
        batch_size=16,
        sampler=DistributedSampler(eval_dataset),
    )

    ### start eval ###
    model.eval()
    with torch.no_grad():
        for idx, batch_data in enumerate(eval_dataloader):
            # H2D
            batch_data[""input_ids""] = batch_data[""input_ids""].to(device)
            batch_data[""attention_mask""] = batch_data[""attention_mask""].to(device)
            batch_data[""labels""] = batch_data[""labels""].to(device)
            logger.info(f""[rank {rank}][{idx}/{len(eval_dataloader)}]: batch_data['input_ids'].shape={batch_data['input_ids'].shape}"")
           
            # forward
            outputs = model(**batch_data)
            logger.info(f""[rank {rank}][{idx}/{len(eval_dataloader)}]: workers well"")
```

The launch script `run.sh` is:
```bash
#!/bin/bash
set -e

NUM_GPUS=$1
python -u -m torch.distributed.run --nproc_per_node $NUM_GPUS --nnodes 1 --node_rank 0 --master_port 6777 --master_addr localhost --max_restarts 0 --tee 3 demo.py
``` 
The execute command is `bash run.sh 1 (or 2)`. 

### Versions

Environments:
```bash
Collecting environment information...
PyTorch version: 2.1.0+cu118
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 10.5.0-1ubuntu1~20.04) 10.5.0
Clang version: 10.0.0-4ubuntu1
CMake version: version 3.25.2
Libc version: glibc-2.31

Python version: 3.9.13 (main, Aug 25 2022, 23:26:10)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.4.0-42-generic-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: 11.8.89
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration:
GPU 0: NVIDIA A100 80GB PCIe
GPU 1: NVIDIA A100 80GB PCIe
GPU 2: NVIDIA A100 80GB PCIe
GPU 3: NVIDIA A100 80GB PCIe
GPU 4: NVIDIA A100 80GB PCIe
GPU 5: NVIDIA A100 80GB PCIe
GPU 6: NVIDIA A100 80GB PCIe
GPU 7: NVIDIA A100 80GB PCIe

Nvidia driver version: 525.85.12
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   46 bits physical, 57 bits virtual
CPU(s):                          64
On-line CPU(s) list:             0-63
Thread(s) per core:              2
Core(s) per socket:              16
Socket(s):                       2
NUMA node(s):                    2
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           106
Model name:                      Intel(R) Xeon(R) Gold 6346 CPU @ 3.10GHz
Stepping:                        6
Frequency boost:                 enabled
CPU MHz:                         799.979
CPU max MHz:                     3600.0000
CPU min MHz:                     800.0000
BogoMIPS:                        6200.00
Virtualization:                  VT-x
L1d cache:                       1.5 MiB
L1i cache:                       1 MiB
L2 cache:                        40 MiB
L3 cache:                        72 MiB
NUMA node0 CPU(s):               0-15,32-47
NUMA node1 CPU(s):               16-31,48-63
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear pconfig flush_l1d arch_capabilities

Versions of relevant libraries:
[pip3] flake8==4.0.1
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.21.5
[pip3] numpydoc==1.4.0
[pip3] torch==2.1.0+cu118
[pip3] torchvision==0.16.0
[pip3] triton==2.1.0
[conda] blas                      1.0                         mkl
[conda] mkl                       2021.4.0           h06a4308_640
[conda] mkl-service               2.4.0            py39h7f8727e_0
[conda] mkl_fft                   1.3.1            py39hd3c417c_0
[conda] mkl_random                1.2.2            py39h51133e4_0
[conda] numpy                     1.21.5           py39h6c91a56_3
[conda] numpy-base                1.21.5           py39ha15fc14_3
[conda] numpydoc                  1.4.0            py39h06a4308_0
[conda] torch                     2.1.0+cu118              pypi_0    pypi
[conda] torchvision               0.16.0                   pypi_0    pypi
[conda] triton                    2.1.0                    pypi_0    pypi
```

Looking forward to your reply, thanks a lot!

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111634,Batched matmul gives incorrect result on MPS devices,"### üêõ Describe the bug

When the dimensions are large enough, batched matmul gives the wrong answer on MPS devices.

Minimal example:

```
import torch

zeros = torch.zeros(911, 9, 1, device=torch.device(""mps""))
ones = torch.ones(1, 32769, device=torch.device(""mps""))
zeros @ ones
```

This should give a tensor of 0s, but it instead gives a tensor in which 50,505,735 of the entries are 1. If the operation is performed a second time with the same tensors, the number of 1s changes to 182,632,455.

The dimensions in this example are minimal, i.e. the code runs correctly if any of them is made any smaller.

### Versions

PyTorch version: 2.1.0
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: macOS 13.5.2 (arm64)
GCC version: Could not collect
Clang version: 15.0.0 (clang-1500.0.40.1)
CMake version: version 3.27.7
Libc version: N/A

Python version: 3.10.8 (main, Nov 24 2022, 08:08:27) [Clang 14.0.6 ] (64-bit runtime)
Python platform: macOS-13.5.2-arm64-arm-64bit
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Apple M2 Pro

Versions of relevant libraries:
[pip3] flake8==6.1.0
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.26.1
[pip3] torch==2.1.0
[conda] numpy                     1.26.1                   pypi_0    pypi
[conda] torch                     2.1.0                    pypi_0    pypi

cc @ezyang @gchanan @zou3519 @kadeng @kulinseth @albanD @malfet @DenisVieriu97 @razarmehr @abhudev"
111633,Status Tracker And Summary of Support Needed: Make Dynamo Generated Artifacts Debuggable,"Currently, dynamically translated bytecode from Dynamo can only be run as a black box. With the help of recent tools like https://github.com/thuml/depyf , it is possible to decompile bytecode into source code, and to enable debugging tools to debug those generated bytecodes.

This issue (status tracker) ducoments the progress and support needed from PyTorch side to fulfill the goal.

# General idea:

PyTorch generate bytecode (and many other artifacts) dynamically. We can decompile and dump the source code into files, let users set breakpoints, and re-run the program to hit those breakpoints.

# Main difficulty:

In essence, artifacts (transformed bytecode, compiled functions, etc.) are generated **dynamically**. We need to have a stable naming for each artifact so that users can recognize them and reuse them across runs.

# Some minor concerns:

Some function names in Dynamo are not valid (notoriously, the resume functions have `<resume in xxx>` name). We need to assign valid names for them.

# Example usage:

The main usage come from the `depyf` side. The rough idea is to set up a src directory, and dump src there for debugging.

```diff
+ import depyf
+ # set up a hook to dump src of dynamically generated artifacts
+ depyf.enable_torch_compile_debugging(dump_dir=""/path/to/dumped/src"")
def toy_example(a, b):
    x = a / (torch.abs(a) + 1)
    if b.sum() < 0:
        b = b * -1
    return x * b

for _ in range(100):
    toy_example(torch.randn(10), torch.randn(10))
```

The ideal result looks like this:

![image](https://github.com/pytorch/pytorch/assets/23236638/8f586170-7745-4cf6-9950-2d16fe831a46)

```[tasklist]
### Tasks
- [x] make code name of resume functions valid, finished in https://github.com/pytorch/pytorch/pull/111635 .
```


cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111632,"[dynamo][profiler] console spew of ...""torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored"" for pages...","### üêõ Describe the bug

when compiling either NanoGPT or T5 using FSDP, get pages of console spew of this warning:
`[rank1]:[2023-10-20 04:13:04,946] [6/21] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored`

Request that this be modified to a warn once type of warning - it is warning the same thing over and over, and then on every rank, spamming up pages of console view.  

for example:
<img width=""1434"" alt=""console_prof_function_warning"" src=""https://github.com/pytorch/pytorch/assets/46302957/bc3451ed-5770-4814-ae5d-b8d47f0d0fa6"">

you can repro using latest nightly and run this for T5 example (nanogpt one is worse in terms of total spew):
https://github.com/lessw2020/transformer_framework/blob/main/run_training.sh (adjust to 8 gpus if needed)
*you may need to edit config/t5_config and ensure use_torch_compile  = True

### Versions

PyTorch version: 2.2.0.dev20231019+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.25.0
Libc version: glibc-2.31

Python version: 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03)  [GCC 11.3.0] (64-bit runtime)
Python platform: Linux-5.15.0-1036-aws-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: 11.7.99
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A10G
GPU 1: NVIDIA A10G
GPU 2: NVIDIA A10G
GPU 3: NVIDIA A10G

Nvidia driver version: 525.85.12
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   48 bits physical, 48 bits virtual
CPU(s):                          48
On-line CPU(s) list:             0-47
Thread(s) per core:              2
Core(s) per socket:              24
Socket(s):                       1
NUMA node(s):                    1
Vendor ID:                       AuthenticAMD
CPU family:                      23
Model:                           49
Model name:                      AMD EPYC 7R32
Stepping:                        0
CPU MHz:                         2799.998
BogoMIPS:                        5599.99
Hypervisor vendor:               KVM
Virtualization type:             full
L1d cache:                       768 KiB
L1i cache:                       768 KiB
L2 cache:                        12 MiB
L3 cache:                        96 MiB
NUMA node0 CPU(s):               0-47
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Mmio stale data:   Not affected
Vulnerability Retbleed:          Mitigation; untrained return thunk; SMT enabled with STIBP protection
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save rdpid

Versions of relevant libraries:
[pip3] flake8==4.0.1
[pip3] flake8-bugbear==22.4.25
[pip3] flake8-polyfill==1.0.2
[pip3] memory-efficient-attention-pytorch==0.1.6
[pip3] mypy==1.0.1
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.24.3
[pip3] pytorch-triton==2.1.0+6e4932cda8
[pip3] torch==2.2.0.dev20231019+cu121
[pip3] torch-model-archiver==0.5.3b20220226
[pip3] torch-workflow-archiver==0.2.8b20230512
[pip3] torchaudio==2.2.0.dev20231019+cu121
[pip3] torchmultimodal==0.1.0b0
[pip3] torchserve==0.6.0b20220513
[pip3] torchtext==0.14.1
[pip3] torchvision==0.17.0.dev20231019+cu121
[pip3] triton==2.0.0.dev20221202
[pip3] triton-nightly==2.1.0.dev20231012235740
[pip3] vit-pytorch==1.2.2
[conda] blas                      1.0                         mkl    conda-forge
[conda] captum                    0.5.0                         0    pytorch
[conda] cudatoolkit               11.7.0              hd8887f6_11    conda-forge
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] libblas                   3.9.0            16_linux64_mkl    conda-forge
[conda] libcblas                  3.9.0            16_linux64_mkl    conda-forge
[conda] liblapack                 3.9.0            16_linux64_mkl    conda-forge
[conda] magma-cuda117             2.6.1                         1    pytorch
[conda] mkl                       2022.2.1         h84fe81f_16997    conda-forge
[conda] numpy                     1.24.3           py39h6183b62_0    conda-forge
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] pytorch-triton            2.1.0+6e4932cda8          pypi_0    pypi
[conda] torch                     2.2.0.dev20231019+cu121          pypi_0    pypi
[conda] torch-model-archiver      0.5.3                    py39_0    pytorch
[conda] torch-workflow-archiver   0.2.8                    py39_0    pytorch
[conda] torchaudio                2.2.0.dev20231019+cu121          pypi_0    pypi
[conda] torchmultimodal           0.1.0b0                  pypi_0    pypi
[conda] torchserve                0.6.0                    py39_0    pytorch
[conda] torchtext                 0.14.1                     py39    pytorch
[conda] torchvision               0.17.0.dev20231019+cu121          pypi_0    pypi
[conda] triton                    2.0.0.dev20221202          pypi_0    pypi
[conda] triton-nightly            2.1.0.dev20231012235740          pypi_0    pypi
[conda] vit-pytorch               1.2.2                    pypi_0    pypi

cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @wconstab @bdhirsh @anijain2305"
111629,DISABLED test_narrow_cpu_float64 (__main__.TestNestedTensorDeviceTypeCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_narrow_cpu_float64&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17880663159).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_narrow_cpu_float64`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111626,DISABLED test_meta_outplace_fft_hfft_cpu_uint8 (__main__.TestMetaCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_outplace_fft_hfft_cpu_uint8&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17879045178).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_outplace_fft_hfft_cpu_uint8`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

cc @ezyang @mruberry @Lezcano @peterbell10 @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111623,Missing `ignored_param` when calling wrapper_cls (FSDP) recursively,"https://github.com/pytorch/pytorch/blob/935f6977542affc0d16c66333a13d60dae6aa5fa/torch/distributed/fsdp/wrap.py#L561

When calling FSDP class recursively, `ignored_param` (which is supposed to be passed by `ignored_states`) seems to be missing. When I pass parameters to ignore to FSDP, this information is lost at this interface when calling FSDP class recursively. 

I think `ignored_states` should be re-defined here and should be part of `kwargs`.

cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu"
111621,maximum Python version supported is not indicated,"### üìö The doc issue

https://pytorch.org/get-started/locally/

Python 3.12 has been released, but it's not supported by the PyTorch wheels. This is not indicated on the page. Only the minimum Python version is indicated 

### Suggest a potential alternative/fix

Indicate both min and max Python versions supported when installing via pip.

cc @svekars @carljparker"
111619,DISABLED test_cat_nhwc (__main__.TestQuantizedOps),"Platforms: macos

This test was disabled because it is failing on main branch ([recent examples](http://torch-ci.com/failure/test_quantization.py%3A%3ATestQuantizedOps%3A%3Atest_cat_nhwc)).

This periodic test looks flaky over the past few weeks.

cc @malfet @albanD"
111607,DISABLED test_meta_outplace_fft_hfft_cpu_float64 (__main__.TestMetaCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_outplace_fft_hfft_cpu_float64&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17872251167).

Over the past 3 hours, it has been determined flaky in 10 workflow(s) with 30 failures and 10 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_outplace_fft_hfft_cpu_float64`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

cc @ezyang @mruberry @Lezcano @peterbell10 @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111606,DISABLED test_narrow_cpu_float32 (__main__.TestNestedTensorDeviceTypeCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_narrow_cpu_float32&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17867918159).

Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 15 failures and 5 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_narrow_cpu_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111603,`Enum` used as a key of the input raises guards error,"### üêõ Describe the bug

Using `Enum` as a key of the input raises a guards-related error:

```python
import torch
from enum import Enum

class MyEnum(Enum):
    A = ""a""

class SomeModel(torch.nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.linear = torch.nn.Linear(1, 1)

    def forward(self, x) -> torch.Tensor:
        return self.linear(x[MyEnum.A])

x = {MyEnum.A: torch.rand(100, 1)}
model = torch.compile(SomeModel())
model(x)
model(x)
```

Possibly related to #99605 and its fix PR #99680

### Error logs

```
ERROR RUNNING GUARDS forward /home/akihiro/work/github.com/pyg-team/pytorch_geometric/test_compile.py:12
lambda L, **___kwargs_ignored:
  ___guarded_code.valid and
  ___check_type_id(L['x'], 94746595577440) and
  set(L['x'].keys()) == {L[""MyEnum""].A} and
  ___check_obj_id(L['self'], 139872498725504) and
  L['self'].training == True and
  hasattr(L['x'][L[""MyEnum""].A], '_dynamo_dynamic_indices') == False and
  ___is_grad_enabled() and
  not ___are_deterministic_algorithms_enabled() and
  ___is_torch_function_enabled() and
  utils_device.CURRENT_DEVICE == None and
  ___check_obj_id(G['MyEnum'].A, 139869662261408) and
  ___check_tensors(L['x'][L[""MyEnum""].A], tensor_check_names=tensor_check_names)
Traceback (most recent call last):
  File ""/home/akihiro/work/github.com/pyg-team/pytorch_geometric/test_compile.py"", line 18, in <module>
    model(x)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py"", line 328, in _fn
    return fn(*args, **kwargs)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""<string>"", line 8, in guard
KeyError: 'MyEnum'
```

### Minified repro

_No response_

### Versions

Collecting environment information...
PyTorch version: 2.1.0+cu118
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.22.5
Libc version: glibc-2.31

Python version: 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) [GCC 12.3.0] (64-bit runtime)
Python platform: Linux-5.13.0-1031-aws-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: 11.5.119
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: Tesla T4
Nvidia driver version: 510.47.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   46 bits physical, 48 bits virtual
CPU(s):                          8
On-line CPU(s) list:             0-7
Thread(s) per core:              2
Core(s) per socket:              4
Socket(s):                       1
NUMA node(s):                    1
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           85
Model name:                      Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz
Stepping:                        7
CPU MHz:                         2499.998
BogoMIPS:                        4999.99
Hypervisor vendor:               KVM
Virtualization type:             full
L1d cache:                       128 KiB
L1i cache:                       128 KiB
L2 cache:                        4 MiB
L3 cache:                        35.8 MiB
NUMA node0 CPU(s):               0-7
Vulnerability Itlb multihit:     KVM: Mitigation: VMX unsupported
Vulnerability L1tf:              Mitigation; PTE Inversion
Vulnerability Mds:               Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown
Vulnerability Meltdown:          Mitigation; PTI
Vulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown
Vulnerability Spec store bypass: Vulnerable
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Retpolines, STIBP disabled, RSB filling
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke avx512_vnni

Versions of relevant libraries:
[pip3] numpy==1.24.1
[pip3] onnx==1.14.1
[pip3] onnxruntime==1.16.0
[pip3] pytorch-lightning==2.0.9.post0
[pip3] pytorch-memlab==0.3.0
[pip3] torch==2.1.0+cu118
[pip3] torch_frame==0.1.0
[pip3] torch_geometric==2.4.0
[pip3] torchmetrics==1.2.0
[pip3] torchvision==0.16.0
[pip3] triton==2.1.0
[conda] numpy                     1.24.1                   pypi_0    pypi
[conda] pytorch-lightning         2.0.9.post0              pypi_0    pypi
[conda] pytorch-memlab            0.3.0                    pypi_0    pypi
[conda] torch                     2.1.0+cu118              pypi_0    pypi
[conda] torch-frame               0.1.0                    pypi_0    pypi
[conda] torch-geometric           2.4.0                    pypi_0    pypi
[conda] torchmetrics              1.2.0                    pypi_0    pypi
[conda] torchvision               0.16.0                   pypi_0    pypi
[conda] triton                    2.1.0                    pypi_0    pypi

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng"
111601,shared object initialization failed trying to run stable diffusion on RX 7700 XT ,"### üêõ Describe the bug

Hi PyTorch Team,

I'm trying to run stable diffusion via Automatic1111/stable-diffusion-webui as well as ComfyUI. No matter what I tried, it resulted in a `RuntimeError: HIP error: shared object initialization failed` and in case of stable-diffusion-webui additionally in the segfault below when calling different torch methods.

Setup:
* GPU: Radeon RX 7700 XT
* Ubuntu 22.04.3 + ROCm 5.7.1 + torch rocm 5.7 nightly

Using the latest [rocm/pytorch docker](https://hub.docker.com/r/rocm/pytorch/tags) let to the same results.
I attached the stack traces both for stable-diffusion-webui as well as ComfyUI.

Thanks a lot in advance for looking into this issue

**Error when starting stable-diffusion-webui**
```
loading stable diffusion model: RuntimeError
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.9/lib/python3.9/threading.py"", line 937, in _bootstrap
    self._bootstrap_inner()
  File ""/opt/conda/envs/py_3.9/lib/python3.9/threading.py"", line 980, in _bootstrap_inner
    self.run()
  File ""/opt/conda/envs/py_3.9/lib/python3.9/threading.py"", line 917, in run
    self._target(*self._args, **self._kwargs)
  File ""/sd/stable-diffusion-webui/modules/initialize.py"", line 147, in load_model
    shared.sd_model  # noqa: B018
  File ""/sd/stable-diffusion-webui/modules/shared_items.py"", line 110, in sd_model
    return modules.sd_models.model_data.get_sd_model()
  File ""/sd/stable-diffusion-webui/modules/sd_models.py"", line 519, in get_sd_model
    load_model()
  File ""/sd/stable-diffusion-webui/modules/sd_models.py"", line 646, in load_model
    load_model_weights(sd_model, checkpoint_info, state_dict, timer)
  File ""/sd/stable-diffusion-webui/modules/sd_models.py"", line 363, in load_model_weights
    model.load_state_dict(state_dict, strict=False)
  File ""/sd/stable-diffusion-webui/modules/sd_disable_initialization.py"", line 223, in <lambda>
    module_load_state_dict = self.replace(torch.nn.Module, 'load_state_dict', lambda *args, **kwargs: load_state_dict(module_load_state_dict, *args, **kwargs))
  File ""/sd/stable-diffusion-webui/modules/sd_disable_initialization.py"", line 221, in load_state_dict
    original(module, state_dict, strict=strict)
  File ""/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 2042, in load_state_dict
    load(self, state_dict)
  File ""/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 2030, in load
    load(child, child_state_dict, child_prefix)
  File ""/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 2030, in load
    load(child, child_state_dict, child_prefix)
  File ""/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 2030, in load
    load(child, child_state_dict, child_prefix)
  [Previous line repeated 1 more time]
  File ""/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 2024, in load
    module._load_from_state_dict(
  File ""/sd/stable-diffusion-webui/modules/sd_disable_initialization.py"", line 225, in <lambda>
    linear_load_from_state_dict = self.replace(torch.nn.Linear, '_load_from_state_dict', lambda *args, **kwargs: load_from_state_dict(linear_load_from_state_dict, *args, **kwargs))
  File ""/sd/stable-diffusion-webui/modules/sd_disable_initialization.py"", line 191, in load_from_state_dict
    module._parameters[name] = torch.nn.parameter.Parameter(torch.zeros_like(param, device=device, dtype=dtype), requires_grad=param.requires_grad)
  File ""/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/_meta_registrations.py"", line 2731, in zeros_like
    res.fill_(0)
RuntimeError: HIP error: shared object initialization failed
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing HIP_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.

Stable diffusion model failed to load
Applying attention optimization: Doggettx... done.
**Segmentation fault (core dumped)**
```

**Error when starting ComfyUI**
``` 
ERROR:root:!!! Exception during processing !!!
ERROR:root:Traceback (most recent call last):
  File ""/home/hannes/projects/ComfyUI/execution.py"", line 153, in recursive_execute
    output_data, output_ui = get_output_data(obj, input_data_all)
  File ""/home/hannes/projects/ComfyUI/execution.py"", line 83, in get_output_data
    return_values = map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True)
  File ""/home/hannes/projects/ComfyUI/execution.py"", line 76, in map_node_over_list
    results.append(getattr(obj, func)(**slice_dict(input_data_all, i)))
  File ""/home/hannes/projects/ComfyUI/nodes.py"", line 476, in load_checkpoint
    out = comfy.sd.load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=True, embedding_directory=folder_paths.get_folder_paths(""embeddings""))
  File ""/home/hannes/projects/ComfyUI/comfy/sd.py"", line 427, in load_checkpoint_guess_config
    model = model_config.get_model(sd, ""model.diffusion_model."", device=inital_load_device)
  File ""/home/hannes/projects/ComfyUI/comfy/supported_models_base.py"", line 48, in get_model
    out = model_base.BaseModel(self, model_type=self.model_type(state_dict, prefix), device=device)
  File ""/home/hannes/projects/ComfyUI/comfy/model_base.py"", line 24, in __init__
    self.diffusion_model = UNetModel(**unet_config, device=device)
  File ""/home/hannes/projects/ComfyUI/comfy/ldm/modules/diffusionmodules/openaimodel.py"", line 417, in __init__
    ResBlock(
  File ""/home/hannes/projects/ComfyUI/comfy/ldm/modules/diffusionmodules/openaimodel.py"", line 171, in __init__
    nn.GroupNorm(32, channels, dtype=dtype, device=device),
  File ""/home/hannes/projects/ComfyUI/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py"", line 271, in __init__
    self.reset_parameters()
  File ""/home/hannes/projects/ComfyUI/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py"", line 275, in reset_parameters
    init.ones_(self.weight)
  File ""/home/hannes/projects/ComfyUI/venv/lib/python3.10/site-packages/torch/nn/init.py"", line 212, in ones_
    return _no_grad_fill_(tensor, 1.)
  File ""/home/hannes/projects/ComfyUI/venv/lib/python3.10/site-packages/torch/nn/init.py"", line 59, in _no_grad_fill_
    return tensor.fill_(val)
RuntimeError: HIP error: shared object initialization failed
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing HIP_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.
```

### Versions

Collecting environment information...
PyTorch version: 2.1.0a0+git413b4cd
Is debug build: False
CUDA used to build PyTorch: N/A
ROCM used to build PyTorch: 5.7.31921-d1770ee1b

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: 17.0.0 (https://github.com/RadeonOpenCompute/llvm-project roc-5.7.0 23352 d1e13c532a947d0cbfc94759c00dcf152294aa13)
CMake version: version 3.26.4
Libc version: glibc-2.31

Python version: 3.9.18 (main, Sep 11 2023, 13:41:44)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.2.0-34-generic-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: AMD Radeon RX 7700 XT
Nvidia driver version: Could not collect
cuDNN version: Could not collect
HIP runtime version: 5.7.31921
MIOpen runtime version: 2.20.0
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Byte Order:                         Little Endian
Address sizes:                      43 bits physical, 48 bits virtual
CPU(s):                             8
On-line CPU(s) list:                0-7
Thread(s) per core:                 2
Core(s) per socket:                 4
Socket(s):                          1
NUMA node(s):                       1
Vendor ID:                          AuthenticAMD
CPU family:                         23
Model:                              17
Model name:                         AMD Ryzen 5 2400G with Radeon Vega Graphics
Stepping:                           0
Frequency boost:                    enabled
CPU MHz:                            1560.001
CPU max MHz:                        3600.0000
CPU min MHz:                        1600.0000
BogoMIPS:                           7200.10
Virtualization:                     AMD-V
L1d cache:                          128 KiB
L1i cache:                          256 KiB
L2 cache:                           2 MiB
L3 cache:                           4 MiB
NUMA node0 CPU(s):                  0-7
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        Not affected
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Not affected
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Not affected
Vulnerability Retbleed:             Mitigation; untrained return thunk; SMT vulnerable
Vulnerability Spec rstack overflow: Mitigation; safe RET
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Not affected
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sev sev_es

Versions of relevant libraries:
[pip3] mypy==0.960
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.23.5
[pip3] open-clip-torch==2.20.0
[pip3] pytorch-lightning==1.9.4
[pip3] torch==2.1.0a0+git413b4cd
[pip3] torchdiffeq==0.2.3
[pip3] torchmetrics==1.2.0
[pip3] torchsde==0.2.6
[pip3] torchvision==0.16.0a0+8835737
[pip3] triton==2.1.0
[conda] No relevant packages


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang"
111592,[dynamo] in-place ops misattribute source to `None`,"### üêõ Describe the bug

When assigning from an in-place op, we ought to attribute the source to the original tensor. Not doing so is I believe erroneous. 

Source is one method by which we might deal with aliasing in order to support object-equivalence within Dynamo, including `is` and `set.__contains__`.


### Repro
```python
def fn(x):
    y = x.add_(1)
    return y is x  # y's source in Dynamo is `None`

fn_opt = torch.compile(backend=""eager"", fullgraph=True, dynamic=True)(fn)

z = torch.ones(4, 1)
fn_opt(z)
```

Blocker to: https://github.com/pytorch/pytorch/issues/111585

### Versions

main

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111585,[fx] Detecting aliasing of `fx.Node` representing `Tensor`,"### üöÄ The feature, motivation and pitch

One possible method is to use the fake tensor to check for aliasing

```python
TensorVariable.proxy.node.meta[‚Äòexample_value‚Äô]
```

However, for some reason, this doesn‚Äôt seem to work with `torch.vmap`:

```python
import torch

op = torch.Tensor.acos_

def fn(z):
  x = z.clone()
  result = torch.vmap(op)(x)
  assert(x is result)  # Fails in nopython=True when `is_` is implemented as testing equality of fake tensor
  
torch.compile(fn)(torch.zeros(10))

```

### Alternatives

NIL

### Additional context
Originally discovered: https://github.com/pytorch/pytorch/pull/111557#discussion_r1365886117

cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv"
111583,DISABLED test_vmapjvpall_linalg_det_singular_cpu_float32 (__main__.TestOperatorsCPU),"Platforms: macos

This test was disabled because it is failing on main branch ([recent examples](http://torch-ci.com/failure/functorch%2Ftest_ops.py%3A%3ATestOperatorsCPU%3A%3Atest_vmapjvpall_linalg_det_singular_cpu_float32)).

cc @malfet @albanD"
111580,Dynamic shapes doesn't work for torch.diff / resize__symint in some cases,"```python
a = torch.tensor([0, 2])
b = torch.tensor([1])

def fn(a, b):
    a = a.clone()
    b = b.clone()
    torch.diff(a, n=0, out=b)
    return b

compiled_f = torch.compile(fn, fullgraph=True, backend=""eager"", dynamic=True)
out = compiled_f(a, b)

# Works
a = torch.tensor([0, 3, 4]) # size changed
b = torch.tensor([1]) # size is the same
out = compiled_f(a, b)

# Doesn't work
a = torch.tensor([0, 3, 4]) # size changed
b = torch.tensor([1, 2]) # size changed
out = compiled_f(a, b)
```

Versions: main after https://github.com/pytorch/pytorch/pull/111530

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111577,Prolonged network hiccup preventing retrieval of workflow job id,"Please see https://github.com/pytorch/pytorch/pull/111483 for context

First known bad is https://hud.pytorch.org/pytorch/pytorch/commit/e0b035c220a4db2a15b53848cd16ac6416fcf323
Got fixed in https://hud.pytorch.org/pytorch/pytorch/commit/543dc757463aa0ad559c49337c98eece6a25150f?

~How is it that the second I notice it happening it gets fixed... Some quantum observation thing going on here...~

cc @ZainRizvi @kit1980 @huydhn"
111574,"`illegal memory access` for `torch.sparse.mm(src, other) / deg.view(-1, 1).clamp_(min=1)`","### üêõ Describe the bug

Original Issue from PyG: https://github.com/pyg-team/pytorch_geometric/issues/8213
Failing example: https://github.com/pyg-team/pytorch_geometric/blob/master/examples/rev_gnn.py
```
CUDA_LAUNCH_BLOCKING=1 python3 /workspace/examples/rev_gnn.py
Traceback (most recent call last):
  File ""/workspace/examples/rev_gnn.py"", line 187, in <module>
    loss = train(epoch)
  File ""/workspace/examples/rev_gnn.py"", line 125, in train
    out = model(data.x, data.adj_t)[data.train_mask]
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/workspace/examples/rev_gnn.py"", line 76, in forward
    x = conv(x, edge_index, mask)
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/models/rev_gnn.py"", line 166, in forward
    return self._fn_apply(args, self._forward, self._inverse)
  File ""/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/models/rev_gnn.py"", line 181, in _fn_apply
    out = InvertibleFunction.apply(
  File ""/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py"", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File ""/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/models/rev_gnn.py"", line 52, in forward
    outputs = ctx.fn(*x)
  File ""/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/models/rev_gnn.py"", line 283, in _forward
    y_in = xs[i] + self.convs[i](y_in, edge_index, *args[i])
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/workspace/examples/rev_gnn.py"", line 35, in forward
    return self.conv(x, edge_index)
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/conv/sage_conv.py"", line 130, in forward
    out = self.propagate(edge_index, x=x, size=size)
  File ""/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/conv/message_passing.py"", line 431, in propagate
    out = self.message_and_aggregate(edge_index, **msg_aggr_kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/conv/sage_conv.py"", line 149, in message_and_aggregate
    return spmm(adj_t, x[0], reduce=self.aggr)
  File ""/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/spmm.py"", line 99, in spmm
    return torch.sparse.mm(src, other) / deg.view(-1, 1).clamp_(min=1)
RuntimeError: CUDA error: an illegal memory access was encountered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
```

### Versions

```
python collect_env.py
Collecting environment information...
PyTorch version: 2.1.0a0+32f93b1
Is debug build: False
CUDA used to build PyTorch: 12.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.27.6
Libc version: glibc-2.35

Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-150-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.2.140
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA RTX A5000
GPU 1: NVIDIA RTX A5000

Nvidia driver version: 530.41.03
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.5
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Address sizes:                   46 bits physical, 48 bits virtual
Byte Order:                      Little Endian
CPU(s):                          16
On-line CPU(s) list:             0-15
Vendor ID:                       GenuineIntel
Model name:                      Intel(R) Core(TM) i7-9800X CPU @ 3.80GHz
CPU family:                      6
Model:                           85
Thread(s) per core:              2
Core(s) per socket:              8
Socket(s):                       1
Stepping:                        4
CPU max MHz:                     4500.0000
CPU min MHz:                     1200.0000
BogoMIPS:                        7599.80
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req md_clear flush_l1d arch_capabilities
Virtualization:                  VT-x
L1d cache:                       256 KiB (8 instances)
L1i cache:                       256 KiB (8 instances)
L2 cache:                        8 MiB (8 instances)
L3 cache:                        16.5 MiB (1 instance)
NUMA node(s):                    1
NUMA node0 CPU(s):               0-15
Vulnerability Itlb multihit:     KVM: Mitigation: Split huge pages
Vulnerability L1tf:              Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable
Vulnerability Mds:               Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Meltdown:          Mitigation; PTI
Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Retbleed:          Mitigation; IBRS
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT vulnerable

Versions of relevant libraries:
[pip3] flake8==6.1.0
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.22.2
[pip3] onnx==1.14.0
[pip3] pytorch-quantization==2.1.2
[pip3] torch==2.1.0a0+32f93b1
[pip3] torch_geometric==2.4.0
[pip3] torch-tensorrt==0.0.0
[pip3] torchdata==0.6.0+5bbcd77
[pip3] torchmetrics==1.2.0
[pip3] torchtext==0.16.0a0
[pip3] torchvision==0.16.0a0
[pip3] triton==2.1.0+e621604
[pip3] tritonclient==2.38.0.69485441
[conda] Could not collect
```

cc @ezyang @gchanan @zou3519 @kadeng @alexsamardzic @nikitaved @pearu @cpuhrsch @amjames @bhosmer @ptrblck"
111573,Tensors in different devices,"https://github.com/pytorch/pytorch/blob/4e310fd87521061ddc2f8e5ea93cc623563929cb/torch/optim/adam.py#L108-L108

torch+cu118-2.1.0, Ubuntu 22.04.2 LTS

When the device of params and grads is `cuda:0`, the device of `state_steps` is still `cpu`. This leads to an error:

```
Tensors of the same index must be on the same device and the same dtype except step tensors that can be CPU and float32 notwithstanding.
```

Unfortunately, I cannot provide a minimum replication, because this problem only occurs when I use [gcastle](https://github.com/huawei-noah/trustworthyAI/blob/master/gcastle/README.md) package. After debugging, I'm sure this problem is caused by the pytorch Adam optimizer.

```
from castle.algorithms import *
model = DAG_GNN(device_type='gpu', batch_size=3500, device_ids=0)
model.learn(x)
```

where `x` is any random numpy matrix, for example shape [2000, 13].

My suggestion is using `torch.tensor(0., device=p.device)` in `adam.py` Ln.108. Because in Ln.106 there is the same usage, I think it will not cause problem.

cc @vincentqb @jbschlosser @albanD @janeyx99 @crcrpar"
111571,DISABLED test_meta_outplace_fft_hfft_cpu_complex64 (__main__.TestMetaCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_outplace_fft_hfft_cpu_complex64&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17857582924).

Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 18 failures and 6 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_outplace_fft_hfft_cpu_complex64`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

cc @ezyang @mruberry @Lezcano @peterbell10 @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111570,Tensor `.cuda()` very slow with specific array sizes ,"### üêõ Describe the bug

I've been profiling some streaming code and found a strange case where copying a tensor to the GPU is much slower for specific array sizes.

This is specific to arrays that are fortran-ordered to begin with and the issue is mitigated by calling `.clone()` on the tensor before `.cuda()`.

```
rows = 327680   # 2 ** 16 * 5
np_arr = np.asfortranarray(np.random.randn(rows, 2000).astype(np.float32))
arr = torch.from_numpy(np_arr)
x = arr[10_000: 50_000]   # grab a subset of 40k rows

%%time
_ = x.cuda()
```

This takes around 800ms on my machine.

With `rows = 327680 - 1`, it takes 280ms on my machine.
With `rows = 327680 + 1`, it takes 170ms on my machine.
With `rows = 100000`, it takes 80ms on my machine.

In all cases, adding a `.clone()` prior to `.cuda()` seems to reduce the time to around 80ms. In general it seems that multiples of 2^16 perhaps perform slower (I've tried with 2^16 and 2^16 - 1 and there seems to be a significant difference).

### Versions

Collecting environment information...
PyTorch version: 2.1.0+cu118
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.35

Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-5.15.86-flatcar-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 11.7.99
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA A100-SXM4-80GB
Nvidia driver version: 470.103.01
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Address sizes:                   48 bits physical, 48 bits virtual
Byte Order:                      Little Endian
CPU(s):                          128
On-line CPU(s) list:             0-127
Vendor ID:                       AuthenticAMD
Model name:                      AMD EPYC 7542 32-Core Processor
CPU family:                      23
Model:                           49
Thread(s) per core:              2
Core(s) per socket:              32
Socket(s):                       2
Stepping:                        0

Versions of relevant libraries:
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.26.1
[pip3] pytorch-lightning==2.1.0
[pip3] torch==2.1.0+cu118
[pip3] torchmetrics==1.2.0
[pip3] triton==2.1.0
[conda] Could not collect

cc @ptrblck"
111569,"[dynamo] so-called global state guard is installed on global, when in fact values are thread-local","### üêõ Describe the bug

https://github.com/pytorch/pytorch/blob/971f67c9880b4037064b4bd24b858c80e6c69174/torch/_dynamo/convert_frame.py#L113

https://github.com/pytorch/pytorch/blob/971f67c9880b4037064b4bd24b858c80e6c69174/torch/_dynamo/convert_frame.py#L377

This means that we are not thread-safe with respect to compiling multiple functions at once across different threads.

### Specific Scenarios

For instance, one call to compile may set a new global state based on its thread-local values, and then another thread reads off those values when it is instantiating or checking a guard.

### Details

See that many of the values it captures:
https://github.com/pytorch/pytorch/blob/971f67c9880b4037064b4bd24b858c80e6c69174/torch/csrc/dynamo/guards.cpp#L438

Are in fact thread-local values:

https://github.com/pytorch/pytorch/blob/fa995626a8e181e3666b27fdb4edbe6116b22ee3/c10/core/AutogradState.h#L13

### Versions

main

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym "
111567,DISABLED test_narrow_cpu_float16 (__main__.TestNestedTensorDeviceTypeCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_narrow_cpu_float16&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17849375503).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_narrow_cpu_float16`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111566,build: failure when building pytorch with TBB,"
## Issue description

I want to build pytorch from source using TBB but not OMP, and I try v1.10.2 and v1.13.1, both failed. plz help.


## Code example

Error messages:
```
[ 98%] Linking CXX executable ../../../../bin/torch_shm_manager
/data/wangjie/dependtool/pytorch/build/lib/libtorch_cpu.soÔºöÂØπ‚Äòstd::allocator<std::pair<long, std::tuple<torch::jit::SourceRange, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, c10::intrusive_ptr<torch::jit::InlinedCallStack, c10::detail::intrusive_target_default_null_type<torch::jit::InlinedCallStack> > > > >::allocator()‚ÄôÊú™ÂÆö‰πâÁöÑÂºïÁî®
collect2: error: ld returned 1 exit status
```

- PyTorch or Caffe2:
- How you installed PyTorch (conda, pip, source): source
- Build command you used (if compiling from source): USE_CUDA=0 BUILD_TEST=0 USE_TBB=1 USE_OPENMP=0 MKLDNN_CPU_RUNTIME=TBB MKL_THREADING=TBB ATEN_THREADING=TBB  python setup.py install
- OS: centos7
- PyTorch version: v1.13.1 & v1.10.2
- Python version: 3.10.13
- CUDA/cuDNN version:
- GPU models and configuration:
- GCC version (if compiling from source):3.7.0
- CMake version:3.21.0
- Libc version: glibc-2.17
- Versions of any other relevant libraries:


cc @malfet @seemethere"
111564,misusing percision value in test_cuda function in torch/testing/_internal/common_nn.py.,"### üêõ Describe the bug

I found tf32_precision of NewModuleTest class. is not using  tf32 test. Like Conv2d_groups testcase.
In test_nn.py file, using add_test to collect testcases and enable tf32 testcases, which is using with tf32_on(self, test.tf32_precision) to set tf32 config and also modify self.precision value. But self.precision is not using in test_cuda.
we can find self.precision in test_cuda function is not tf32_percision in add_test function. From add_test to test_cuda, self variable is to testcase, and test in add_test is self in test_cuda.
def add_test(test, decorator=None) path:  https://github.com/pytorch/pytorch/blob/main/test/test_nn.py#L7484
def tf32_on(self, tf32_precision=1e-5): https://github.com/pytorch/pytorch/blob/main/torch/testing/_internal/common_cuda.py#L94
def test_cuda(self, test_case):  https://github.com/pytorch/pytorch/blob/main/torch/testing/_internal/common_nn.py#L4450

```
# before enter to test_cuda function.
-> test.test_cuda(self, **kwargs)
(Pdb) p self.percision
(Pdb) p test.__dict__[""precision""]
0.0002
(Pdb) p self.__dict__[""_precision""]
0.005
(Pdb) p test.tf32_precision
0.005
# enter test_cuda function
(Pdb) n
> /projs/platform/shangang/anaconda3/envs/shangang_conda_torch19/lib/python3.7/site-packages/torch/testing/_internal/common_nn.py(6029)test_cuda()
-> if not TEST_CUDA or not self.should_test_cuda:
(Pdb) p test_case.precision
0.005
(Pdb) p self.precision
0.0002 // still using 0.0002 to compare cuda tf32 result to cpu result, without using tf32_percision.
````

Is this a bug of testcases?

### Versions

Collecting environment information...
PyTorch version: 1.13.0a0+gitd922c29
Is debug build: True
CUDA used to build PyTorch: 11.6
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.6 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.27.4
Libc version: glibc-2.17

Python version: 3.7.16 (default, Jan 17 2023, 22:20:44)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-4.15.0-161-generic-x86_64-with-debian-buster-sid
Is CUDA available: True
CUDA runtime version: 11.6.55
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: Tesla V100-PCIE-32GB
GPU 1: Tesla V100-PCIE-32GB
GPU 2: Tesla V100-PCIE-32GB
GPU 3: Tesla V100-PCIE-32GB
GPU 4: Tesla V100-PCIE-32GB
GPU 5: Tesla V100-PCIE-32GB
GPU 6: Tesla V100-PCIE-32GB
GPU 7: Tesla V100-PCIE-32GB

Nvidia driver version: 510.85.02
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.6
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.20.3
[pip3] pytorch-lightning==1.6.5
[pip3] torch==1.13.0a0+gitd922c29
[pip3] torchaudio==0.9.0
[pip3] torchmetrics==0.11.0
[pip3] torchvision==0.10.0+cu111
[conda] cudatoolkit               11.3.1               h2bc3f7f_2    defaults
[conda] cudatoolkit-dev           11.6.0               h72bdee0_5    conda-forge
[conda] numpy                     1.20.3                   pypi_0    pypi
[conda] pytorch-lightning         1.6.5                    pypi_0    pypi
[conda] torch                     1.13.0a0+gitd922c29          pypi_0    pypi
[conda] torchaudio                0.9.0                    pypi_0    pypi
[conda] torchmetrics              0.11.0                   pypi_0    pypi
[conda] torchvision               0.10.0+cu111             pypi_0    pypi
"
111563,"Higher-order derivatives extremely slow, increasing exponentially","### üêõ Describe the bug

In my application, I need to take the nth order mixed derivative of a function. However, I found that the torch.autograd.grad computation time increases exponentially as n increases. Is this expected, and is there any way around it?

This is my code for differentiating a function self.F (from R^n -> R^1):

```
def differentiate(self, x):
    x.requires_grad_(True)
    xi = [x[...,i] for i in range(x.shape[-1])]
    dyi = self.F(torch.stack(xi, dim=-1))
    for i in range(self.dim):
        start_time = time.time()
        dyi = torch.autograd.grad(dyi.sum(), xi[i], retain_graph=True, create_graph=True)[0]
        grad_time = time.time() - start_time
        print(grad_time)
    return dyi
```

And these are the times printed for each iteration of the above loop:
```
0.0037012100219726562
0.005133152008056641
0.008165121078491211
0.019922733306884766
0.059255123138427734
0.1910409927368164
0.6340939998626709
2.1612229347229004
11.042078971862793
```

I assume this is because the size of the computation graph is increasing? Is there any way around this? I thought I might be able to circumvent this issue by taking a functional approach (presumably obviating the need for a computation graph), using torch.func.grad. However, this actually increased the runtime of the same code! Am I not understanding torch.func.grad properly?

### Versions

torch 2.1.0

cc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved @soulitzer @Lezcano @Varal7"
111562,[dynamo] `not aliased -> aliased` Guard only implemented for Tensors,"### üêõ Describe the bug

For tensors, both `aliased -> not aliased` and `not aliased -> aliased` can lead to guard failure.
- `aliased -> not aliased`: L['x'] is L['y']""
- `not aliased -> aliased`: Duplicate tensor found where not expected! L['y']should not alias to anything, but is aliased""

For other objects, only `aliased -> not aliased`
- `aliased -> not aliased`: L['x'] is L['y']""

### Diagnosis

This is due to the guards deduping tensors:
https://github.com/pytorch/pytorch/blob/aa3243bceb8c84f56f326b9e8c60ecc9794bbce4/torch/csrc/dynamo/guards.cpp#L404
But not objects

### Question:
1. If there is 1 alias, which then increases to 2, will it still trigger recompile? Yes:
```python
def fn(z, x, y):
    if x is y:
        return z + x * 2
    else:
        return z + x + y

fn_opt = torch.compile(backend='eager', fullgraph=True, dynamic=True)(fn)

x = torch.zeros(2)
y = torch.ones(2)

self.assertEqual(fn(x, x, y), fn_opt(x, x, y))
self.assertEqual(fn(x, x, x), fn_opt(x, x, x))
```

### Versions

main

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111561,DISABLED test_meta_outplace_addmm_decomposed_cpu_complex64 (__main__.TestMetaCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_outplace_addmm_decomposed_cpu_complex64&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17846849108).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_outplace_addmm_decomposed_cpu_complex64`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

cc @ezyang @mruberry @Lezcano @peterbell10 @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111559,[RFC] Add GradScaler on CPU,"### üöÄ The feature, motivation and pitch

To enable FP16 support on CPU https://github.com/pytorch/pytorch/issues/97068, GradScaler is necessary for FP16 training to prevent grad overflows.


### Alternatives

* Step1: Add the corresponding kernels of `_amp_foreach_non_finite_check_and_unscale_` and `_amp_update_scale_kernels` on CPU
Follow the implements of CUDA.
* Step2: Frontend API for GradScaler on CPU:
 1: Move the common logic codes of GradScaler to a file `torch/amp/grad_scaler.py`. Most parts of GradScaler can be abstracted as a base class since the algorithm of GradScaler is same on CPU and CUDA.  
 2: Add 2 derived class for CPU and CUDA to set device related config.
The design of Frontend API for GradScaler  is same as autocast.

* API usage:

```Python
# Creates a GradScaler once at the beginning of training.

# Before: only available on CUDA
# scaler = torch.cuda.amp.GradScaler()

# After: available on CUDA and CPU
# scaler = torch.cuda.amp.GradScaler()
scaler = torch.cpu.amp.GradScaler()
for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)

        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.
        scaler.scale(loss).backward()

        # scaler.step() first unscales gradients of the optimizer's params.
        # If gradients don't contain infs/NaNs, optimizer.step() is then called,
        # otherwise, optimizer.step() is skipped.
        scaler.step(optimizer)

        # Updates the scale for next iteration.
        scaler.update()
```




### Additional context

This RFC depends on FP16 supports for operators https://github.com/pytorch/pytorch/issues/97068 and FP16 support of autocast on CPU https://github.com/pytorch/pytorch/issues/96093"
111556,[dynamo] Implement `set.__contains__` for tensors based on object identity,"### üöÄ The feature, motivation and pitch

Workaround to https://github.com/pytorch/pytorch/issues/111544 for `set.__contains__` case

### Alternatives

_No response_

### Additional context

Related https://github.com/pytorch/pytorch/issues/111550

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111552,[Bug]: some parameters' grad is None when using FSDP with torch2.1.0,"### üêõ Describe the bug
To reproduce the problem: Training [InternLM](https://github.com/InternLM/InternLM/tree/develop) with [config fsdp=True](https://github.com/InternLM/InternLM/blob/develop/configs/7B_sft.py#L157):
```shell
srun -p llm -n8 --ntasks-per-node=8 --cpus-per-task=4 --gpus-per-task=1 python train.py --config ./configs/7B_sft.py
```

```python
zero1 parallel (dict):
    1. size: int
        * if size <= 0, the size of the zero process group is equal to the size of the dp process group,
            so parameters will be divided within the range of dp.
        * if size == 1, zero is not used, and all dp groups retain the full amount of model parameters.
        * if size > 1 and size <= dp world size, the world size of zero is a subset of dp world size.
        For smaller models, it is usually a better choice to split the parameters within nodes with a setting <= 8.
    2. fsdp: bool, enable/disable torch's fully sharded data parallel, defaults to False.
""""""
parallel = dict(
    zero1=dict(size=-1, fsdp=True),
    tensor=1,
    pipeline=dict(size=1, interleaved_overlap=True),
    sequence_parallel=True,
)
```

The error message is shown as follows:
```shell
2023-10-19 14:58:44,720 ERROR train.py:318 in <module> -- Raise exception from SH-IDC1-10-140-1-139 with rank id: 0
Traceback (most recent call last):
  File ""/mnt/petrelfs/huangting.p/workspace/InternLM/train.py"", line 316, in <module>
    main(args)
  File ""/mnt/petrelfs/huangting.p/workspace/InternLM/train.py"", line 240, in main
    trainer_result = trainer.step()
  File ""/mnt/petrelfs/huangting.p/workspace/InternLM/internlm/core/trainer.py"", line 195, in step
    return self._engine.step()
  File ""/mnt/petrelfs/huangting.p/workspace/InternLM/internlm/core/engine.py"", line 118, in step
    success, grad_norm = self.optimizer.step()
  File ""/mnt/petrelfs/share_data/llm_env/miniconda3-py39_4/envs/llm-torch2.1-flash2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py"", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File ""/mnt/petrelfs/huangting.p/workspace/InternLM/internlm/solver/optimizer/fsdp_optimizer.py"", line 118, in step
    norm_group = self._compute_norm_with_fsdp_flatten(group_idx)
  File ""/mnt/petrelfs/huangting.p/workspace/InternLM/internlm/solver/optimizer/fsdp_optimizer.py"", line 92, in _compute_norm_with_fsdp_flatten
    norm_group = compute_norm(gradients=gradients, parameters=params, last_stage=True)
  File ""/mnt/petrelfs/huangting.p/workspace/InternLM/internlm/solver/optimizer/utils.py"", line 270, in compute_norm
    tensor_parallel_grads.append(g.data.float())
AttributeError: 'NoneType' object has no attribute 'data'
```

### Versions
```shell
# torch2.1.0
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```
![‰ºÅ‰∏öÂæÆ‰ø°Êà™Âõæ_c513d224-c444-4f69-ba6f-732f4e3b5574](https://github.com/pytorch/pytorch/assets/20810277/15d9d339-e1f4-4cde-803d-38c6d4f29905)


**Note that when using torch2.0.1, the error will not happen.**

cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin"
111551,Custom `ModuleDict.__getitem__(key: tuple)` produces a graph break,"### üêõ Describe the bug

#97932 enabled TorchDynamo to support modules with custom `__getitem__`. However, if the key is a tuple, it produces a graph break due to this assertion https://github.com/pytorch/pytorch/blob/v2.1.0/torch/_dynamo/variables/nn_module.py#L545.

```python
import torch
from torch_geometric.nn.module_dict import ModuleDict

class SomeModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.module_dict = ModuleDict({
            (""author"", ""writes"", ""paper""): torch.nn.Linear(1, 1),
        })

    def forward(self, x):
        x = self.module_dict[(""author"", ""writes"", ""paper"")](x)
        return x

model = torch.compile(SomeModel())
model(torch.randn(100, 1))
```

`torch_geometric.nn.module_dict.ModuleDict` is defined at https://github.com/pyg-team/pytorch_geometric/blob/2.4.0/torch_geometric/nn/module_dict.py

### Error logs

```
Traceback (most recent call last):
  File ""/home/akihiro/work/github.com/pyg-team/pytorch_geometric/test/nn/test_compile_hetero.py"", line 16, in <module>
    model(torch.randn(100, 1))
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py"", line 328, in _fn
    return fn(*args, **kwargs)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py"", line 490, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py"", line 641, in _convert_frame
    result = inner_convert(frame, cache_size, hooks, frame_state)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py"", line 133, in _fn
    return fn(*args, **kwargs)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py"", line 389, in _convert_frame_assert
    return _compile(
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py"", line 569, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/_dynamo/utils.py"", line 189, in time_wrapper
    r = func(*args, **kwargs)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py"", line 491, in compile_inner
    out_code = transform_code_object(code, transform)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py"", line 1028, in transform_code_object
    transformations(instructions, code_options)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py"", line 458, in transform
    tracer.run()
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py"", line 2074, in run
    super().run()
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py"", line 724, in run
    and self.step()
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py"", line 688, in step
    getattr(self, inst.opname)(inst)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py"", line 392, in wrapper
    return inner_fn(self, inst)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py"", line 168, in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py"", line 618, in call_function
    result = handler(tx, *args, **kwargs)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py"", line 950, in call_getitem
    return args[0].call_method(tx, ""__getitem__"", args[1:], kwargs)
  File ""/home/akihiro/.conda/envs/pyg310/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py"", line 545, in call_method
    assert isinstance(key, (str, int))
AssertionError: 

from user code:
   File ""/home/akihiro/work/github.com/pyg-team/pytorch_geometric/test/nn/test_compile_hetero.py"", line 12, in forward
    x = self.module_dict[(""author"", ""writes"", ""paper"")](x)

Set TORCH_LOGS=""+dynamo"" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True
```

### Minified repro

_No response_

### Versions

```
Collecting environment information...
PyTorch version: 2.1.0+cu118
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.22.5
Libc version: glibc-2.31

Python version: 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) [GCC 12.3.0] (64-bit runtime)
Python platform: Linux-5.13.0-1031-aws-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: 11.5.119
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: Tesla T4
Nvidia driver version: 510.47.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   46 bits physical, 48 bits virtual
CPU(s):                          8
On-line CPU(s) list:             0-7
Thread(s) per core:              2
Core(s) per socket:              4
Socket(s):                       1
NUMA node(s):                    1
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           85
Model name:                      Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz
Stepping:                        7
CPU MHz:                         2499.998
BogoMIPS:                        4999.99
Hypervisor vendor:               KVM
Virtualization type:             full
L1d cache:                       128 KiB
L1i cache:                       128 KiB
L2 cache:                        4 MiB
L3 cache:                        35.8 MiB
NUMA node0 CPU(s):               0-7
Vulnerability Itlb multihit:     KVM: Mitigation: VMX unsupported
Vulnerability L1tf:              Mitigation; PTE Inversion
Vulnerability Mds:               Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown
Vulnerability Meltdown:          Mitigation; PTI
Vulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown
Vulnerability Spec store bypass: Vulnerable
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Retpolines, STIBP disabled, RSB filling
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke avx512_vnni

Versions of relevant libraries:
[pip3] numpy==1.24.1
[pip3] onnx==1.14.1
[pip3] onnxruntime==1.16.0
[pip3] pytorch-lightning==2.0.9.post0
[pip3] pytorch-memlab==0.3.0
[pip3] torch==2.1.0+cu118
[pip3] torch_frame==0.1.0
[pip3] torch_geometric==2.4.0
[pip3] torchmetrics==1.2.0
[pip3] torchvision==0.16.0
[pip3] triton==2.1.0
[conda] numpy                     1.24.1                   pypi_0    pypi
[conda] pytorch-lightning         2.0.9.post0              pypi_0    pypi
[conda] pytorch-memlab            0.3.0                    pypi_0    pypi
[conda] torch                     2.1.0+cu118              pypi_0    pypi
[conda] torch-frame               0.1.0                    pypi_0    pypi
[conda] torch-geometric           2.4.0                    pypi_0    pypi
[conda] torchmetrics              1.2.0                    pypi_0    pypi
[conda] torchvision               0.16.0                   pypi_0    pypi
[conda] triton                    2.1.0                    pypi_0    pypi
```

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng"
111550,[dynamo] Implement full `is_` checking,"### üöÄ The feature, motivation and pitch

This requires checking equality of objects.

For consts, this just requires checking the backing const.

~~For `VariableTracker`s with sources, this requires checking the sources. I believe we do not need to actually reference the original object. Hope this is right? @voznesenskym~~ 

source is not always available. `example_value` `FakeTensor` is better.

### Alternatives

Users cannot do things like deduplicate tensors or other objects in traced code

### Additional context

Required for properly tracing `nn.modules`: https://github.com/pytorch/pytorch/pull/111548
Related: https://github.com/pytorch/pytorch/issues/109504

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng"
111549,DISABLED test_detach_cpu_float64 (__main__.TestNestedTensorDeviceTypeCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_detach_cpu_float64&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17842173875).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_detach_cpu_float64`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111547,Bug with as_strided_tensorimpl for MPS devices,"### üêõ Describe the bug

I am experiencing a problem on my M1 Pro MacBook with training a Neural ODE model.

My use-case is quite extensive and depends on multiple python files and some custom integration routines, so I am unable to provide a trimmed-down version of my code. I have, however, provided a traceback of the error.

```python 

Traceback (most recent call last):
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/damage_neural/neural.py"", line 370, in <module>
    r = ode.odeint_adjoint(model, y[0],t,block_size=time_chunk_size)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/pyoptmat/ode.py"", line 656, in odeint_adjoint
    return wrapper.apply(solver, times, *adjoint_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/torch/autograd/function.py"", line 551, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/pyoptmat/ode.py"", line 583, in forward
    y = solver.integrate(times, cache_adjoint=True)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/pyoptmat/ode.py"", line 370, in integrate
    result[k : k + self.n] = self.block_update(
                             ^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/pyoptmat/ode.py"", line 497, in block_update
    dy = chunktime.newton_raphson_chunk(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/pyoptmat/chunktime.py"", line 41, in newton_raphson_chunk
    R, J = fn(x)
           ^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/pyoptmat/ode.py"", line 493, in RJ
    yd, yJ = func(times, y)
             ^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1519, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1528, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/damage_neural/neural.py"", line 231, in forward
    dy_dot_dy = vmap(vmap(jacfwd(self.rate, argnums = 1)))(t, y, erate, T)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/torch/_functorch/apis.py"", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/torch/_functorch/vmap.py"", line 266, in vmap_impl
    return _flat_vmap(
           ^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/torch/_functorch/vmap.py"", line 38, in fn
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/torch/_functorch/vmap.py"", line 379, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/torch/_functorch/apis.py"", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/torch/_functorch/vmap.py"", line 266, in vmap_impl
    return _flat_vmap(
           ^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/torch/_functorch/vmap.py"", line 38, in fn
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/torch/_functorch/vmap.py"", line 379, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/torch/_functorch/eager_transforms.py"", line 1132, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/torch/_functorch/apis.py"", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/torch/_functorch/vmap.py"", line 266, in vmap_impl
    return _flat_vmap(
           ^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/torch/_functorch/vmap.py"", line 38, in fn
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/torch/_functorch/vmap.py"", line 379, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/torch/_functorch/eager_transforms.py"", line 1123, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/torch/_functorch/vmap.py"", line 38, in fn
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/lib/python3.11/site-packages/torch/_functorch/eager_transforms.py"", line 969, in _jvp_with_argnums
    result_duals = func(*duals)
                   ^^^^^^^^^^^^
  File ""/Users/ganesh/ArgonneWork/BlackBox_NN/env/damage_neural/neural.py"", line 193, in rate
    x = torch.cat([y, erate.unsqueeze(-1), T.unsqueeze(-1)], dim = -1)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: !self.is_mps() INTERNAL ASSERT FAILED at ""/Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp"":1166, please report a bug to PyTorch. as_strided_tensorimpl does not work with MPS; call self.as_strided(...) instead
```

Please let me know if there are any further details you might require.

Thanks.

### Versions

ct_env
Collecting environment information...
PyTorch version: 2.2.0.dev20231018
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: macOS 13.5.1 (arm64)
GCC version: Could not collect
Clang version: 15.0.0 (clang-1500.0.40.1)
CMake version: Could not collect
Libc version: N/A

Python version: 3.11.4 (main, Jul  5 2023, 08:40:20) [Clang 14.0.6 ] (64-bit runtime)
Python platform: macOS-13.5.1-arm64-arm-64bit
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Apple M1 Pro

Versions of relevant libraries:
[pip3] numpy==1.26.1
[pip3] torch==2.2.0.dev20231018
[conda] functorch                 2.0.0                    pypi_0    pypi
[conda] numpy                     1.25.2                   pypi_0    pypi
[conda] torch                     2.0.1                    pypi_0    pypi
(env) (base) ganesh@Ganeshs-MacBook-Pro-3 / % 



cc @kulinseth @albanD @malfet @DenisVieriu97 @razarmehr @abhudev"
111546,`nn.modules.Module` relies on non-deterministic hashing to avoid overloading eq on Tensors,"### üêõ Describe the bug

https://github.com/pytorch/pytorch/blob/b2b5f1377b9bd79c5cf8404ae0d7567bbd343323/torch/nn/modules/module.py#L2164

Uses `__contains__` to tell if tensor has been seen. 

However, due to the nefarious nature of `__contains__`, which checks `__eq__`, if there is a hash collision of non ID-equal tensors, this would result in checking tensor equality, which is incompatible with the semantics required here (ID match, i.e. `operator.is_`).

This can result in
1. error due to broadcasting
2. error due to attempting to cast non-single-elem tensor to scalar bool. 

See: https://github.com/pytorch/pytorch/issues/111542

### Possible Solutions:
1. use ID match instead. To be implemented: https://github.com/pytorch/pytorch/issues/109504

### Versions

main"
111544,"[dynamo] `set.__contains__` is not properly implemented for tensors,  by virtue of `eq(Tensor, Tensor)` being inconsistently implemented","### üêõ Describe the bug

```python
param = torch.zeros(5)
param2 = torch.zeros(5)

tensor_list = set()
tensor_list.add(param2)
print(param2 in tensor_list)  # False

def fn(param, param2):
    tensor_list = set([param2])
    return param in tensor_list 

ret = torch.compile(fn, onegraph=True)(param, param2)
```

```
torch._dynamo.exc.Unsupported: comparison TensorVariable() <built-in function eq> TensorVariable()
```

Inconsistent behaviour
```python
param = torch.zeros(5)
param2 = torch.zeros(5)

tensor_list = set()
tensor_list.add(param2)
print(param2 in tensor_list)  # False

def fn(param, param2):
    tensor_list = set([param2])
    return param in tensor_list

ret = torch.compile(fn, fullgraph=True)(param, param2)
assert ret == fn(param, param2)   # RuntimeError: Boolean value of Tensor with more than one value is ambiguous

```

Root cause: `__contains__` based on equality of tensors has inconsistent behaviour due to overloading eq https://github.com/pytorch/pytorch/issues/111542

### Versions

main

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111542,[torch.Tensor] containers `__contains__` behaviour is inconsistent with `Tensor` due to overloading `eq`,"### üêõ Describe the bug

Relevance: usage of tensor set `__contains__`
https://github.com/pytorch/pytorch/blob/b2b5f1377b9bd79c5cf8404ae0d7567bbd343323/torch/nn/modules/module.py#L2164

Trips up dynamo because set `__contains__` in dynamo is not shortcutted on hashes but on equality.

### Failure case: list

See:
```python
param = torch.zeros(5)
param2 = torch.zeros(5, 10)

tensor_list = [param2]
print(param in tensor_list)  # error - tried to broadcast due to equality
```
```python
param = torch.zeros(5)
param2 = torch.zeros(5)

tensor_list = [param2]
print(param in tensor_list)  # Fails
```

This returns True
```python
param = torch.zeros(5)

tensor_list = [param]
print(param in tensor_list)  # True, due to hash equality
```

This means that for the purpose of `contains` behaviour, tensor equality of values is unnecessary. Only of `id` match.

CPython implementation details:
https://github.com/python/cpython/blob/642eb8df951f2f1d4bf4d93ee568707c5bf40a96/Objects/listobject.c#L452

### Failure case: set
It gets worse: inconsistent behaviour in set.

```python
param = torch.zeros(5)
param2 = torch.zeros(5, 10)

tensor_list = set({param2})
print(param in tensor_list)   # False, as we never had to go to equality check, due to first hashing on other items
```

Note that whether we run into the broadcasting error behaviour is entirely dependent on the hashing function chosen by set. This is entirely non-deterministic and unpleasant.

CPython implementation details:
Ultimately checks equality of object just like list
https://github.com/python/cpython/blob/642eb8df951f2f1d4bf4d93ee568707c5bf40a96/Objects/setobject.c#L88

### Solution
Tensor `__contains__` behaviour ought to depend on ID match, and have nothing to do with values.

Ideally, python equality (RichEqualsBool) is not overloaded to the `torch.eq` operation. We can hopefully overload `operator.eq` without overloading `RichEqualsBool`.

The reason this is impossible is that CPython only shortcuts equality, but not inequality.

https://github.com/python/cpython/blob/642eb8df951f2f1d4bf4d93ee568707c5bf40a96/Objects/object.c#L847

Note that this issue is not unique to Pytorch, numpy has the same behaviour.

### Versions

main

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111538,Propose to add constant padding mode to the `torch.nn.functional.grid_sample` function,"### üöÄ The feature, motivation and pitch

I'm working on a registration project. When I am using `torch.nn.functional.grid_sample` to apply the displacement field to the moving image, I find I am unable to use constant values like 255 to pad it (at least it is not that intuitive) just like the way in `np.pad`. So I propose to add one. If possible I would really want to implement it myself, though I am a completely new contributor to the pytorch project.

### Alternatives

As a workaround, I pre-pad the image with 255 pixels on the border and use the `border` mode.

### Additional context

Here are two samples,
![`zeros` mode](https://github.com/pytorch/pytorch/assets/54883050/aa30dc3e-ab8d-4b24-9b11-8b2357e09c8d)
![pre-pad `border` mode](https://github.com/pytorch/pytorch/assets/54883050/9ddca4f5-f7bc-48ca-bb33-5474254b1e23)


cc @albanD @mruberry @jbschlosser @walterddr @mikaylagawarecki"
111536,DISABLED test_Conv2d_naive_groups_cuda_float16 (__main__.TestConvolutionNNDeviceTypeCUDA),"Platforms: rocm

Failure observed in ROCm5.7 CI upgrade PR, so skipping until resolved: https://github.com/pytorch/pytorch/pull/110465#issuecomment-1758427256

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang"
111535,DISABLED test_meta_outplace_addmm_decomposed_cpu_complex128 (__main__.TestMetaCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_outplace_addmm_decomposed_cpu_complex128&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17841787282).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_outplace_addmm_decomposed_cpu_complex128`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

ConnectionTimeoutError: Connect timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/main/test/test_meta.py -2 (connected: false, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 0)
headers: {}

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111533,DISABLED test_Conv2d_groups_nobias_v2 (__main__.TestConvolutionNN),"Platforms: rocm

Failure observed in ROCm5.7 CI upgrade PR, so skipping until resolved: https://github.com/pytorch/pytorch/pull/110465#issuecomment-1758427256

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang"
111532,DISABLED test_Conv2d_groups_nobias (__main__.TestConvolutionNN),"Platforms: rocm

Failure observed in ROCm5.7 CI upgrade PR, so skipping until resolved: https://github.com/pytorch/pytorch/pull/110465#issuecomment-1758427256

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang"
111528,"[dynamo] `no_grad`, `enable_grad` - `_NoParamDecoratorContextManager` are not handled correctly","### üêõ Describe the bug

Root cause of https://github.com/pytorch/pytorch/issues/109138

Reproducer:
```python
import torch

def cool_name(x):
    return x.sin()

def fn(x):
    return torch.no_grad(cool_name)(x)

x = torch.zeros(10)
result = fn(x)
print(result)
result = torch.compile(fn, backend=""eager"", fullgraph=True)(x)
print(result)
```

Also fails:
```python
def fn(x):
    @torch.no_grad
    def cool_name(x):
        return x.sin()

    return cool_name(x)

x = torch.zeros(10)
result = fn(x)
print(result)
result = torch.compile(fn, backend=""eager"", fullgraph=True)(x)
print(result)
```

Does not fail: no_grad is instantiated outside of compile region
```python
@torch.no_grad
def cool_name(x):
    return x.sin()

def fn(x):
    return cool_name(x)

x = torch.zeros(10)
result = fn(x)
print(result)
result = torch.compile(fn, backend=""eager"", fullgraph=True)(x)
print(result)
```

### Solution
Handle it properly when it is called as a function. It might need to instantiate the gradmodevariable whenever the function is called. 

To do so, one can put a `context_var_hook` which sets up the context var when the function is called.

### Versions

main

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"
111527,UNSTABLE rocm / linux-focal-rocm5.6-py3.8 / test (default),"ROCm job is broken after https://github.com/pytorch/pytorch/pull/111381.  AMD is looking into it (I assume).  Ping me if you want to revert.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @seemethere @malfet @pytorch/pytorch-dev-infra"
111526,Build consumes 32GB RAM triggers OOM crash when compiling CUDA object,"### üêõ Describe the bug

I've carefully followed the build from source instructions, and the related CUDA and cuDNN instructions, but no matter how many times I start over or try different combinations (12.2, 12.1, 11.8, and corresponding cudnn), I get the same result:

The build progresses fine compiling until it gets to this:

Building CUDA object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim256_bf16_sm80.cu.o
FAILED: caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim256_bf16_sm80.cu.o

At that point my browser tabs all crash, the system locks up for 20s or so, then the terminal crashes and the system becomes responsive again.

Ubuntu 22.04, 6.2.0-34-generic (I also tried .26)

nvidia-smi
525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0

nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Cuda compilation tools, release 11.8, V11.8.89
Build cuda_11.8.r11.8/compiler.31833905_0

cuDNN: 8.9.5.29-1+cuda11.8

I've run the CUDA toolkit and cuDNN samples, they both work fine.
I also followed the instructions [here](https://medium.com/@zhanwenchen/build-pytorch-from-source-with-cuda-11-8-565ab737bfc8), same result.

### Error logs

[1/609] Building CUDA object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim256_bf16_sm80.cu.o
FAILED: caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim256_bf16_sm80.cu.o 
/usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DAT_PER_OPERATOR_HEADERS -DHAVE_MALLOC_USABLE_SIZE=1 -DHAVE_MMAP=1 -DHAVE_SHM_OPEN=1 -DHAVE_SHM_UNLINK=1 -DIDEEP_USE_MKL -DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -DONNXIFI_ENABLE_EXT=1 -DONNX_ML=1 -DONNX_NAMESPACE=onnx_torch -DTORCH_CUDA_BUILD_MAIN_LIB -DUSE_C10D_GLOO -DUSE_C10D_NCCL -DUSE_CUDA -DUSE_DISTRIBUTED -DUSE_EXPERIMENTAL_CUDNN_V8_API -DUSE_EXTERNAL_MZCRC -DUSE_FLASH_ATTENTION -DUSE_MEM_EFF_ATTENTION -DUSE_NCCL -DUSE_RPC -DUSE_TENSORPIPE -D_FILE_OFFSET_BITS=64 -Dtorch_cuda_EXPORTS -DTORCH_ASSERT_NO_OPERATORS -I/home/bobby/dev/pytorch/build/aten/src -I/home/bobby/dev/pytorch/aten/src -I/home/bobby/dev/pytorch/build -I/home/bobby/dev/pytorch -I/home/bobby/dev/pytorch/cmake/../third_party/benchmark/include -I/home/bobby/dev/pytorch/third_party/onnx -I/home/bobby/dev/pytorch/build/third_party/onnx -I/home/bobby/dev/pytorch/third_party/foxi -I/home/bobby/dev/pytorch/build/third_party/foxi -I/home/bobby/dev/pytorch/aten/src/THC -I/home/bobby/dev/pytorch/aten/src/ATen/cuda -I/home/bobby/dev/pytorch/aten/src/ATen/../../../third_party/cutlass/include -I/home/bobby/dev/pytorch/build/caffe2/aten/src -I/home/bobby/dev/pytorch/aten/src/ATen/.. -I/home/bobby/dev/pytorch/build/nccl/include -I/home/bobby/dev/pytorch/c10/cuda/../.. -I/home/bobby/dev/pytorch/c10/.. -I/home/bobby/dev/pytorch/third_party/tensorpipe -I/home/bobby/dev/pytorch/build/third_party/tensorpipe -I/home/bobby/dev/pytorch/third_party/tensorpipe/third_party/libnop/include -I/home/bobby/dev/pytorch/torch/csrc/api -I/home/bobby/dev/pytorch/torch/csrc/api/include -isystem /home/bobby/dev/pytorch/build/third_party/gloo -isystem /home/bobby/dev/pytorch/cmake/../third_party/gloo -isystem /home/bobby/dev/pytorch/cmake/../third_party/tensorpipe/third_party/libuv/include -isystem /home/bobby/dev/pytorch/cmake/../third_party/googletest/googlemock/include -isystem /home/bobby/dev/pytorch/cmake/../third_party/googletest/googletest/include -isystem /home/bobby/dev/pytorch/third_party/protobuf/src -isystem /home/bobby/miniconda3/envs/rcnn/include -isystem /home/bobby/dev/pytorch/third_party/gemmlowp -isystem /home/bobby/dev/pytorch/third_party/neon2sse -isystem /home/bobby/dev/pytorch/third_party/XNNPACK/include -isystem /home/bobby/dev/pytorch/third_party/ittapi/include -isystem /home/bobby/dev/pytorch/cmake/../third_party/eigen -isystem /usr/local/cuda/include -isystem /home/bobby/dev/pytorch/third_party/ideep/mkl-dnn/include/oneapi/dnnl -isystem /home/bobby/dev/pytorch/third_party/ideep/include -isystem /home/bobby/dev/pytorch/cmake/../third_party/cudnn_frontend/include -D_GLIBCXX_USE_CXX11_ABI=1 -Xfatbin -compress-all -DONNX_NAMESPACE=onnx_torch -gencode arch=compute_86,code=sm_86 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda  -Wno-deprecated-gpu-targets --expt-extended-lambda -DCUB_WRAPPED_NAMESPACE=at_cuda_detail -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -O3 -DNDEBUG -std=c++17 -Xcompiler=-fPIC -DMKL_HAS_SBGEMM -DTORCH_USE_LIBUV -DCAFFE2_USE_GLOO -Xcompiler=-Wall,-Wextra,-Wdeprecated,-Wno-unused-parameter,-Wno-unused-function,-Wno-missing-field-initializers,-Wno-unknown-pragmas,-Wno-type-limits,-Wno-array-bounds,-Wno-unknown-pragmas,-Wno-strict-overflow,-Wno-strict-aliasing,-Wno-maybe-uninitialized -MD -MT caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim256_bf16_sm80.cu.o -MF caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim256_bf16_sm80.cu.o.d -x cu -c /home/bobby/dev/pytorch/aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim256_bf16_sm80.cu -o caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim256_bf16_sm80.cu.o
Killed
[2/609] Building CUDA object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim64_bf16_sm80.cu.o
FAILED: caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim64_bf16_sm80.cu.o 
/usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DAT_PER_OPERATOR_HEADERS -DHAVE_MALLOC_USABLE_SIZE=1 -DHAVE_MMAP=1 -DHAVE_SHM_OPEN=1 -DHAVE_SHM_UNLINK=1 -DIDEEP_USE_MKL -DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -DONNXIFI_ENABLE_EXT=1 -DONNX_ML=1 -DONNX_NAMESPACE=onnx_torch -DTORCH_CUDA_BUILD_MAIN_LIB -DUSE_C10D_GLOO -DUSE_C10D_NCCL -DUSE_CUDA -DUSE_DISTRIBUTED -DUSE_EXPERIMENTAL_CUDNN_V8_API -DUSE_EXTERNAL_MZCRC -DUSE_FLASH_ATTENTION -DUSE_MEM_EFF_ATTENTION -DUSE_NCCL -DUSE_RPC -DUSE_TENSORPIPE -D_FILE_OFFSET_BITS=64 -Dtorch_cuda_EXPORTS -DTORCH_ASSERT_NO_OPERATORS -I/home/bobby/dev/pytorch/build/aten/src -I/home/bobby/dev/pytorch/aten/src -I/home/bobby/dev/pytorch/build -I/home/bobby/dev/pytorch -I/home/bobby/dev/pytorch/cmake/../third_party/benchmark/include -I/home/bobby/dev/pytorch/third_party/onnx -I/home/bobby/dev/pytorch/build/third_party/onnx -I/home/bobby/dev/pytorch/third_party/foxi -I/home/bobby/dev/pytorch/build/third_party/foxi -I/home/bobby/dev/pytorch/aten/src/THC -I/home/bobby/dev/pytorch/aten/src/ATen/cuda -I/home/bobby/dev/pytorch/aten/src/ATen/../../../third_party/cutlass/include -I/home/bobby/dev/pytorch/build/caffe2/aten/src -I/home/bobby/dev/pytorch/aten/src/ATen/.. -I/home/bobby/dev/pytorch/build/nccl/include -I/home/bobby/dev/pytorch/c10/cuda/../.. -I/home/bobby/dev/pytorch/c10/.. -I/home/bobby/dev/pytorch/third_party/tensorpipe -I/home/bobby/dev/pytorch/build/third_party/tensorpipe -I/home/bobby/dev/pytorch/third_party/tensorpipe/third_party/libnop/include -I/home/bobby/dev/pytorch/torch/csrc/api -I/home/bobby/dev/pytorch/torch/csrc/api/include -isystem /home/bobby/dev/pytorch/build/third_party/gloo -isystem /home/bobby/dev/pytorch/cmake/../third_party/gloo -isystem /home/bobby/dev/pytorch/cmake/../third_party/tensorpipe/third_party/libuv/include -isystem /home/bobby/dev/pytorch/cmake/../third_party/googletest/googlemock/include -isystem /home/bobby/dev/pytorch/cmake/../third_party/googletest/googletest/include -isystem /home/bobby/dev/pytorch/third_party/protobuf/src -isystem /home/bobby/miniconda3/envs/rcnn/include -isystem /home/bobby/dev/pytorch/third_party/gemmlowp -isystem /home/bobby/dev/pytorch/third_party/neon2sse -isystem /home/bobby/dev/pytorch/third_party/XNNPACK/include -isystem /home/bobby/dev/pytorch/third_party/ittapi/include -isystem /home/bobby/dev/pytorch/cmake/../third_party/eigen -isystem /usr/local/cuda/include -isystem /home/bobby/dev/pytorch/third_party/ideep/mkl-dnn/include/oneapi/dnnl -isystem /home/bobby/dev/pytorch/third_party/ideep/include -isystem /home/bobby/dev/pytorch/cmake/../third_party/cudnn_frontend/include -D_GLIBCXX_USE_CXX11_ABI=1 -Xfatbin -compress-all -DONNX_NAMESPACE=onnx_torch -gencode arch=compute_86,code=sm_86 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda  -Wno-deprecated-gpu-targets --expt-extended-lambda -DCUB_WRAPPED_NAMESPACE=at_cuda_detail -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -O3 -DNDEBUG -std=c++17 -Xcompiler=-fPIC -DMKL_HAS_SBGEMM -DTORCH_USE_LIBUV -DCAFFE2_USE_GLOO -Xcompiler=-Wall,-Wextra,-Wdeprecated,-Wno-unused-parameter,-Wno-unused-function,-Wno-missing-field-initializers,-Wno-unknown-pragmas,-Wno-type-limits,-Wno-array-bounds,-Wno-unknown-pragmas,-Wno-strict-overflow,-Wno-strict-aliasing,-Wno-maybe-uninitialized -MD -MT caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim64_bf16_sm80.cu.o -MF caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim64_bf16_sm80.cu.o.d -x cu -c /home/bobby/dev/pytorch/aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim64_bf16_sm80.cu -o caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim64_bf16_sm80.cu.o
Killed


### Minified repro

_No response_

### Versions

PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.26.4
Libc version: glibc-2.35

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.2.0-34-generic-x86_64-with-glibc2.17
Is CUDA available: N/A
CUDA runtime version: 11.8.89
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080
Nvidia driver version: 525.125.06
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.5
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: N/A

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      46 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             24
On-line CPU(s) list:                0-23
Vendor ID:                          GenuineIntel
Model name:                         13th Gen Intel(R) Core(TM) i7-13700K
CPU family:                         6
Model:                              183
Thread(s) per core:                 2
Core(s) per socket:                 16
Socket(s):                          1
Stepping:                           1
CPU max MHz:                        5400.0000
CPU min MHz:                        800.0000
BogoMIPS:                           6835.20
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities
Virtualization:                     VT-x
L1d cache:                          640 KiB (16 instances)
L1i cache:                          768 KiB (16 instances)
L2 cache:                           24 MiB (10 instances)
L3 cache:                           30 MiB (1 instance)
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-23
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        Not affected
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Not affected
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Not affected
Vulnerability Retbleed:             Not affected
Vulnerability Spec rstack overflow: Not affected
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] optree==0.9.2
[conda] blas                      1.0                         mkl  
[conda] magma-cuda118             2.6.1                         1    pytorch
[conda] mkl                       2023.1.0         h213fc3f_46343  
[conda] mkl-include               2023.1.0         h06a4308_46343  
[conda] mkl-service               2.4.0            py38h5eee18b_1  
[conda] mkl_fft                   1.3.8            py38h5eee18b_0  
[conda] mkl_random                1.2.4            py38hdb19cb5_0  
[conda] numpy                     1.24.4                   pypi_0    pypi
[conda] numpy-base                1.24.3           py38h060ed82_1  
[conda] optree                    0.9.2                    pypi_0    pypi


cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305"
111525,Functorch FCD breaks with tensor subclasses,"### üêõ Describe the bug

Classes that inherits from torch.Tensor do not work with firct class dimensions when `__getitem__` is overwritten:

```python
import torch
from torch import Tensor
from functorch import dim

class MyTensor(Tensor):
    def __getitem__(self, item):
        return super().__getitem__(item)

t = Tensor([[1, 2],[3, 4]])
d0 = dim.dims(1)
t[d0] # works

t = MyTensor([[1, 2],[3, 4]])
d0 = dim.dims(1)
t[d0] # breaks

```

which gives
```
ValueError                                Traceback (most recent call last)
Cell In[3], line 1
----> 1 t[d0]

Cell In[2], line 7, in MyTensor.__getitem__(self, item)
      6 def __getitem__(self, item):
----> 7     return super().__getitem__(item)

ValueError: dimension d0 is unbound
```

I guess that since `t[d0]` in the regular `Tensor` case is not a `Tensor` anymore but a `functorch.dim.Tensor`, the expected behaviour is ill-defined. Maybe a better error message in this case would help to let users know what is going on?

cc @ezyang @msaroufim @albanD @zou3519 @Chillee @samdow @kshitij12345 @janeyx99 @zdevito



### Versions

Recent nightly built ('2.2.0.dev20231011')"
111522,Insufficient hasattr guards on user defined objects,"### üêõ Describe the bug

```
import torch
import torch._dynamo

@torch.compile(backend=""eager"", fullgraph=True)
def f(x, y):
    return x + y.a

class A:
    a = 2

print(f(torch.zeros(2), A()))

del A.a

print(f(torch.zeros(2), A()))
```

produces

```
ERROR RUNNING GUARDS f /data/users/ezyang/a/pytorch/a.py:4
lambda L, **___kwargs_ignored:
  ___guarded_code.valid and
  ___check_global_state() and
  hasattr(L['x'], '_dynamo_dynamic_indices') == False and
  ___check_type_id(L['y'], 102721216) and
  ___check_type_id(L['y'].a, 7640416) and
  L['y'].a == 2 and
  utils_device.CURRENT_DEVICE == None and
  ___skip_backend_check() or ___current_backend() == ___lookup_backend(140059152699776) and
  ___check_tensors(L['x'], tensor_check_names=tensor_check_names)
Traceback (most recent call last):
  File ""/data/users/ezyang/a/pytorch/a.py"", line 15, in <module>
    print(f(torch.zeros(2), A()))
  File ""/data/users/ezyang/a/pytorch/torch/_dynamo/eval_frame.py"", line 401, in _fn
    return fn(*args, **kwargs)
  File ""<string>"", line 13, in guard
AttributeError: 'A' object has no attribute 'a'
```

### Versions

main

cc @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng"
111519,[pt2+profiler] attach aot_id to CompiledFunction,"### üöÄ The feature, motivation and pitch

torch-compiled models will have profiles that contain `CompiledFunction` and `CompiledFunctionBackward` regions.

Meanwhile PT2 logs (e.g. TORCH_COMPILE_DEBUG=1) will also dump the graphs. But in graphs that have multiple CompiledFunctions, it's kind of hard to figure out which graph (in the logs) maps to a given CompiledFunction (in the profile).

We should figure out how to attach the aot_id to the CompiledFunction profiler event.

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 who had this great idea :) 

### Alternatives

_No response_

### Additional context

Idea: autograd.Functions have handling in C++ to add the record_function event. That's where the CompiledFunction event comes from.

It might be possible to attach a static parameter or method on the CompiledFunction (like `_compiled_autograd_key()`) that is used for additional information attached to the C++ RecordFunction.

We should make sure this doesn't slow down CompiledFunction, especially in the no-profiling case. It would be great if we can collect the aot_id during CompiledFunction construction so that we don't have to do the python/C++ conversion on each iteration."
111517,MPS Performance regressions on Sonoma 14.0 ,"### üêõ Describe the bug

This issue is related to #77799. tldr: speed 50% slower, big memory leaks. 

Basically, since upgrading to Sonoma, performance of the MPS device on sentence transformers models has taken a big nosedive. I don't have an apples-to-apples (ü•Å) comparison exactly, but an M1 ultra on Sonoma is 50% slower than an M1 Pro on Ventura. [Here](https://github.com/pytorch/pytorch/issues/77799#issuecomment-1304882735) are some numbers I collected on Ventura with an M1 ultra, not sure that data can be exactly compared, but it looks like the ratio between inference time on M1 Ultra/Ventura and M1 Ultra / Sonoma is about 1:2. 

On Sonoma (M1 Ultra):
```
In [1]: from sentence_transformers import SentenceTransformer

In [2]: import torch

In [5]: %timeit model.encode([""hi""], device=torch.device(""cpu""))
85.3 ms ¬± 12 ms per loop (mean ¬± std. dev. of 7 runs, 10 loops each)

In [6]: %timeit model.encode([""hi""], device=torch.device(""mps""))
23 ms ¬± 616 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1 loop each)
```


The MPS device is clearly functioning and present, but 23 ms is about 50% slower than an M1 Pro on Ventura. Here is the Ventura data:

```
In [2]: import torch

In [3]: model = SentenceTransformer(""all-mpnet-base-v2"")

In [4]: %timeit model.encode([""hi""], device=torch.device(""mps""))
14.7 ms ¬± 854 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1 loop each)

In [5]: %timeit model.encode([""hi""], device=torch.device(""cpu""))
40.1 ms ¬± 408 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)
```

Both cases with with today's nightly PyTorch, and sentence-transformers==2.2.2, transformers==4.34.1. 


Additionally, let me know if I should file another ticket for this, but I observe huge memory leaks when using the MPS device. I often conduct a lot of bulk embedding using this model, and after ~32k calls to `model.forward`, I see memory usage exceeding 100GB and increasing. It's compressed, so it looks leaked. I observe this problem when running torch in an Flask wrapper.

I am wondering if something about Apple's Metal Performance Shaders implementation changed recently. 

### Versions

Collecting environment information...
PyTorch version: 2.2.0.dev20231018
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: macOS 14.0 (arm64)
GCC version: Could not collect
Clang version: 15.0.0 (clang-1500.0.40.1)
CMake version: version 3.27.6
Libc version: N/A

Python version: 3.11.5 (main, Aug 24 2023, 15:09:45) [Clang 14.0.3 (clang-1403.0.22.14.1)] (64-bit runtime)
Python platform: macOS-14.0-arm64-arm-64bit
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Apple M1 Ultra

Versions of relevant libraries:
[pip3] mypy==1.4.1
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.25.2
[pip3] torch==2.2.0.dev20231018
[pip3] torchsort==0.1.9
[pip3] torchvision==0.17.0.dev20231018
[conda] Could not collect

cc @ezyang @gchanan @zou3519 @kadeng @kulinseth @albanD @malfet @DenisVieriu97 @razarmehr @abhudev"
111514,DISABLED test_detach_cpu_float32 (__main__.TestNestedTensorDeviceTypeCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_detach_cpu_float32&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17831419866).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_detach_cpu_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305"
111513,DISABLED test_meta_outplace_addmm_cpu_complex128 (__main__.TestMetaCPU),"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_outplace_addmm_cpu_complex128&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17830649623).

Over the past 3 hours, it has been determined flaky in 8 workflow(s) with 24 failures and 8 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_outplace_addmm_cpu_complex128`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

cc @ezyang @mruberry @Lezcano @peterbell10 @msaroufim @wconstab @bdhirsh @anijain2305"
111509,Sparse Tensor Sum Still Does Not Work for PyTorch Geometric,"### üêõ Describe the bug

original issue: https://github.com/pytorch/pytorch/issues/98796
There was a PR to fix it: https://github.com/pytorch/pytorch/commit/a54043516fad1a37134f280f3e75f85a5b2daa13

However the issue still persists for this example: https://github.com/pyg-team/pytorch_geometric/blob/master/examples/correct_and_smooth.py
(new error message but still broken at the same step)
```
python3 example/correct_and_smooth.py
...
Epoch: 300, Loss: 0.9754, Train: 0.7694, Val: 0.7390, Test: 0.5940
Traceback (most recent call last):
  File ""/workspace/examples/correct_and_smooth.py"", line 72, in <module>
    deg = adj_t.sum(dim=1).to(torch.float)
RuntimeError: reduction operations on CSR tensors with keepdim=False is unsupported
```

### Versions

```
python collect_env.py
Collecting environment information...
PyTorch version: 2.1.0a0+32f93b1
Is debug build: False
CUDA used to build PyTorch: 12.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.27.6
Libc version: glibc-2.35

Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-150-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.2.140
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA RTX A5000
GPU 1: NVIDIA RTX A5000

Nvidia driver version: 530.41.03
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.5
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.5
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Address sizes:                   46 bits physical, 48 bits virtual
Byte Order:                      Little Endian
CPU(s):                          16
On-line CPU(s) list:             0-15
Vendor ID:                       GenuineIntel
Model name:                      Intel(R) Core(TM) i7-9800X CPU @ 3.80GHz
CPU family:                      6
Model:                           85
Thread(s) per core:              2
Core(s) per socket:              8
Socket(s):                       1
Stepping:                        4
CPU max MHz:                     4500.0000
CPU min MHz:                     1200.0000
BogoMIPS:                        7599.80
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req md_clear flush_l1d arch_capabilities
Virtualization:                  VT-x
L1d cache:                       256 KiB (8 instances)
L1i cache:                       256 KiB (8 instances)
L2 cache:                        8 MiB (8 instances)
L3 cache:                        16.5 MiB (1 instance)
NUMA node(s):                    1
NUMA node0 CPU(s):               0-15
Vulnerability Itlb multihit:     KVM: Mitigation: Split huge pages
Vulnerability L1tf:              Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable
Vulnerability Mds:               Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Meltdown:          Mitigation; PTI
Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Retbleed:          Mitigation; IBRS
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT vulnerable

Versions of relevant libraries:
[pip3] flake8==6.1.0
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.22.2
[pip3] onnx==1.14.0
[pip3] pytorch-quantization==2.1.2
[pip3] torch==2.1.0a0+32f93b1
[pip3] torch_geometric==2.4.0
[pip3] torch-tensorrt==0.0.0
[pip3] torchdata==0.6.0+5bbcd77
[pip3] torchmetrics==1.2.0
[pip3] torchtext==0.16.0a0
[pip3] torchvision==0.16.0a0
[pip3] triton==2.1.0+e621604
[pip3] tritonclient==2.38.0.69485441
[conda] Could not collect
```

cc @alexsamardzic @nikitaved @pearu @cpuhrsch @amjames @bhosmer"
